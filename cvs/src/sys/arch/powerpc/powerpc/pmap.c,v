head	1.167;
access;
symbols
	OPENBSD_6_2:1.167.0.2
	OPENBSD_6_2_BASE:1.167
	OPENBSD_6_1:1.166.0.4
	OPENBSD_6_1_BASE:1.166
	OPENBSD_6_0:1.164.0.2
	OPENBSD_6_0_BASE:1.164
	OPENBSD_5_9:1.163.0.2
	OPENBSD_5_9_BASE:1.163
	OPENBSD_5_8:1.160.0.4
	OPENBSD_5_8_BASE:1.160
	OPENBSD_5_7:1.142.0.2
	OPENBSD_5_7_BASE:1.142
	OPENBSD_5_6:1.129.0.4
	OPENBSD_5_6_BASE:1.129
	OPENBSD_5_5:1.125.0.4
	OPENBSD_5_5_BASE:1.125
	OPENBSD_5_4:1.119.0.4
	OPENBSD_5_4_BASE:1.119
	OPENBSD_5_3:1.119.0.2
	OPENBSD_5_3_BASE:1.119
	OPENBSD_5_2:1.118.0.6
	OPENBSD_5_2_BASE:1.118
	OPENBSD_5_1_BASE:1.118
	OPENBSD_5_1:1.118.0.4
	OPENBSD_5_0:1.118.0.2
	OPENBSD_5_0_BASE:1.118
	OPENBSD_4_9:1.117.0.4
	OPENBSD_4_9_BASE:1.117
	OPENBSD_4_8:1.117.0.2
	OPENBSD_4_8_BASE:1.117
	OPENBSD_4_7:1.108.0.2
	OPENBSD_4_7_BASE:1.108
	OPENBSD_4_6:1.107.0.6
	OPENBSD_4_6_BASE:1.107
	OPENBSD_4_5:1.107.0.2
	OPENBSD_4_5_BASE:1.107
	OPENBSD_4_4:1.105.0.2
	OPENBSD_4_4_BASE:1.105
	OPENBSD_4_3:1.103.0.2
	OPENBSD_4_3_BASE:1.103
	OPENBSD_4_2:1.101.0.2
	OPENBSD_4_2_BASE:1.101
	OPENBSD_4_1:1.97.0.2
	OPENBSD_4_1_BASE:1.97
	OPENBSD_4_0:1.96.0.4
	OPENBSD_4_0_BASE:1.96
	OPENBSD_3_9:1.96.0.2
	OPENBSD_3_9_BASE:1.96
	OPENBSD_3_8:1.89.0.2
	OPENBSD_3_8_BASE:1.89
	OPENBSD_3_7:1.88.0.4
	OPENBSD_3_7_BASE:1.88
	OPENBSD_3_6:1.88.0.2
	OPENBSD_3_6_BASE:1.88
	SMP_SYNC_A:1.87
	SMP_SYNC_B:1.87
	OPENBSD_3_5:1.86.0.2
	OPENBSD_3_5_BASE:1.86
	OPENBSD_3_4:1.82.0.2
	OPENBSD_3_4_BASE:1.82
	UBC_SYNC_A:1.80
	OPENBSD_3_3:1.80.0.2
	OPENBSD_3_3_BASE:1.80
	OPENBSD_3_2:1.76.0.2
	OPENBSD_3_2_BASE:1.76
	OPENBSD_3_1:1.65.0.2
	OPENBSD_3_1_BASE:1.65
	UBC_SYNC_B:1.77
	UBC:1.52.0.2
	UBC_BASE:1.52
	OPENBSD_3_0:1.45.0.2
	OPENBSD_3_0_BASE:1.45
	OPENBSD_2_9_BASE:1.27
	OPENBSD_2_9:1.27.0.2
	OPENBSD_2_8:1.20.0.2
	OPENBSD_2_8_BASE:1.20
	OPENBSD_2_7:1.16.0.2
	OPENBSD_2_7_BASE:1.16
	SMP:1.14.0.2
	SMP_BASE:1.14
	kame_19991208:1.13
	OPENBSD_2_6:1.11.0.2
	OPENBSD_2_6_BASE:1.11
	OPENBSD_2_5:1.9.0.2
	OPENBSD_2_5_BASE:1.9
	OPENBSD_2_4:1.7.0.2
	OPENBSD_2_4_BASE:1.7
	OPENBSD_2_3:1.6.0.2
	OPENBSD_2_3_BASE:1.6
	OPENBSD_2_2:1.5.0.4
	OPENBSD_2_2_BASE:1.5
	OPENBSD_2_1:1.5.0.2
	OPENBSD_2_1_BASE:1.5
	powerpc_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.167
date	2017.05.16.20.52.54;	author kettenis;	state Exp;
branches;
next	1.166;
commitid	TAfYVWo1VHXXhwkl;

1.166
date	2016.10.19.08.28.20;	author guenther;	state Exp;
branches;
next	1.165;
commitid	VoR9X3uHTxRSYC5r;

1.165
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.164;
commitid	RlO92XR575sygHqm;

1.164
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.163;
commitid	N0upL0onl7Raz5yi;

1.163
date	2015.10.08.10.20.14;	author kettenis;	state Exp;
branches;
next	1.162;
commitid	XyklQUq12w27Mxrq;

1.162
date	2015.09.11.22.02.18;	author kettenis;	state Exp;
branches;
next	1.161;
commitid	BLslNV9mBJowqn8K;

1.161
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.160;
commitid	WSD3bUAMn8qMj0PM;

1.160
date	2015.07.20.00.16.16;	author kettenis;	state Exp;
branches;
next	1.159;
commitid	9NWFjmLiRuWXuw8P;

1.159
date	2015.06.05.10.15.54;	author mpi;	state Exp;
branches;
next	1.158;
commitid	uwNtpIVcgFh7le1n;

1.158
date	2015.06.05.10.06.35;	author mpi;	state Exp;
branches;
next	1.157;
commitid	3YilPUzW0Wm2K3zJ;

1.157
date	2015.06.05.10.04.34;	author mpi;	state Exp;
branches;
next	1.156;
commitid	hXgVSJVXckQBKCvM;

1.156
date	2015.06.05.09.53.40;	author mpi;	state Exp;
branches;
next	1.155;
commitid	hsyAHSdvOD9hiO8I;

1.155
date	2015.06.05.09.48.01;	author mpi;	state Exp;
branches;
next	1.154;
commitid	qltyoRHtc7tNsbvc;

1.154
date	2015.06.05.09.42.10;	author mpi;	state Exp;
branches;
next	1.153;
commitid	cQLAjjLub2xdUy4X;

1.153
date	2015.06.05.09.38.52;	author mpi;	state Exp;
branches;
next	1.152;
commitid	gxKIZU5o4l8daoVb;

1.152
date	2015.06.05.09.32.22;	author mpi;	state Exp;
branches;
next	1.151;
commitid	BukPRswPxOZZm7ey;

1.151
date	2015.06.05.09.31.19;	author mpi;	state Exp;
branches;
next	1.150;
commitid	Xff6BYO0myuEtiXY;

1.150
date	2015.06.05.09.30.03;	author mpi;	state Exp;
branches;
next	1.149;
commitid	I91nweUhIETJ9se0;

1.149
date	2015.06.05.09.25.21;	author mpi;	state Exp;
branches;
next	1.148;
commitid	UX2wEk7MVGcwVnVu;

1.148
date	2015.06.05.09.18.50;	author mpi;	state Exp;
branches;
next	1.147;
commitid	bleEYNi2unUAyUKO;

1.147
date	2015.06.05.09.09.58;	author mpi;	state Exp;
branches;
next	1.146;
commitid	2q9dz6CraF3WGMTn;

1.146
date	2015.06.05.09.05.35;	author mpi;	state Exp;
branches;
next	1.145;
commitid	7yL1Ub6vKoZUXb9F;

1.145
date	2015.04.23.14.42.02;	author mpi;	state Exp;
branches;
next	1.144;
commitid	u5iblz9DSuIC7ki8;

1.144
date	2015.03.31.16.00.38;	author mpi;	state Exp;
branches;
next	1.143;
commitid	MMWnK4Mn4W58afQM;

1.143
date	2015.03.31.15.51.05;	author mpi;	state Exp;
branches;
next	1.142;
commitid	V6LJj7jikqpWX3RD;

1.142
date	2015.02.09.13.35.44;	author deraadt;	state Exp;
branches;
next	1.141;
commitid	6dyy3ukeU7UdcPiK;

1.141
date	2015.02.09.13.34.49;	author deraadt;	state Exp;
branches;
next	1.140;
commitid	5yMidS9tJP3CVY3y;

1.140
date	2015.01.22.19.47.00;	author deraadt;	state Exp;
branches;
next	1.139;
commitid	g0NiGW6gmXZxTw0W;

1.139
date	2015.01.22.17.55.46;	author mpi;	state Exp;
branches;
next	1.138;
commitid	GiDlPECBAognbItE;

1.138
date	2015.01.21.19.10.26;	author mpi;	state Exp;
branches;
next	1.137;
commitid	W9PpI0syxV48fQkK;

1.137
date	2015.01.20.17.04.20;	author mpi;	state Exp;
branches;
next	1.136;
commitid	fChEQy2ai2DbX4qD;

1.136
date	2014.12.23.01.12.33;	author dlg;	state Exp;
branches;
next	1.135;
commitid	WtzpaSWEyxrQ2afQ;

1.135
date	2014.12.17.14.40.03;	author deraadt;	state Exp;
branches;
next	1.134;
commitid	K0kmKIIOCpnBGvf2;

1.134
date	2014.11.25.10.45.07;	author mpi;	state Exp;
branches;
next	1.133;
commitid	LQFutZsp6YlxR2dl;

1.133
date	2014.11.18.15.20.15;	author deraadt;	state Exp;
branches;
next	1.132;
commitid	QMdiFt2wnhzK9sgf;

1.132
date	2014.11.16.12.30.58;	author deraadt;	state Exp;
branches;
next	1.131;
commitid	yv0ECmCdICvq576h;

1.131
date	2014.11.02.00.11.32;	author kettenis;	state Exp;
branches;
next	1.130;
commitid	cs61c6jtX9iQ8Nf6;

1.130
date	2014.10.27.19.16.38;	author kettenis;	state Exp;
branches;
next	1.129;
commitid	SiQYUxK6e8AUP4Pf;

1.129
date	2014.05.09.18.16.15;	author miod;	state Exp;
branches;
next	1.128;

1.128
date	2014.04.26.14.19.04;	author mpi;	state Exp;
branches;
next	1.127;

1.127
date	2014.04.01.20.42.39;	author mpi;	state Exp;
branches;
next	1.126;

1.126
date	2014.03.31.18.58.41;	author mpi;	state Exp;
branches;
next	1.125;

1.125
date	2014.02.09.11.25.58;	author mpi;	state Exp;
branches;
next	1.124;

1.124
date	2014.02.08.23.49.20;	author miod;	state Exp;
branches;
next	1.123;

1.123
date	2014.02.08.13.17.40;	author miod;	state Exp;
branches;
next	1.122;

1.122
date	2013.12.29.19.09.21;	author brad;	state Exp;
branches;
next	1.121;

1.121
date	2013.08.19.08.39.30;	author mpi;	state Exp;
branches;
next	1.120;

1.120
date	2013.08.07.08.19.05;	author kettenis;	state Exp;
branches;
next	1.119;

1.119
date	2012.08.30.18.14.26;	author mpi;	state Exp;
branches;
next	1.118;

1.118
date	2011.05.30.22.25.22;	author oga;	state Exp;
branches;
next	1.117;

1.117
date	2010.08.07.03.50.01;	author krw;	state Exp;
branches;
next	1.116;

1.116
date	2010.07.16.06.22.31;	author kettenis;	state Exp;
branches;
next	1.115;

1.115
date	2010.06.26.23.24.44;	author guenther;	state Exp;
branches;
next	1.114;

1.114
date	2010.04.24.17.56.06;	author kettenis;	state Exp;
branches;
next	1.113;

1.113
date	2010.04.15.21.30.29;	author deraadt;	state Exp;
branches;
next	1.112;

1.112
date	2010.04.09.17.36.08;	author drahn;	state Exp;
branches;
next	1.111;

1.111
date	2010.04.02.18.04.42;	author deraadt;	state Exp;
branches;
next	1.110;

1.110
date	2010.04.02.16.35.31;	author drahn;	state Exp;
branches;
next	1.109;

1.109
date	2010.03.31.21.02.42;	author drahn;	state Exp;
branches;
next	1.108;

1.108
date	2009.07.21.22.34.02;	author kettenis;	state Exp;
branches;
next	1.107;

1.107
date	2008.10.17.14.04.07;	author drahn;	state Exp;
branches;
next	1.106;

1.106
date	2008.09.13.18.18.25;	author drahn;	state Exp;
branches;
next	1.105;

1.105
date	2008.06.14.10.55.20;	author mk;	state Exp;
branches;
next	1.104;

1.104
date	2008.04.26.22.37.41;	author drahn;	state Exp;
branches;
next	1.103;

1.103
date	2007.11.04.13.43.39;	author martin;	state Exp;
branches;
next	1.102;

1.102
date	2007.09.15.14.28.17;	author krw;	state Exp;
branches;
next	1.101;

1.101
date	2007.05.27.15.46.02;	author drahn;	state Exp;
branches;
next	1.100;

1.100
date	2007.05.13.03.45.59;	author drahn;	state Exp;
branches;
next	1.99;

1.99
date	2007.05.03.18.40.21;	author miod;	state Exp;
branches;
next	1.98;

1.98
date	2007.04.13.18.12.16;	author miod;	state Exp;
branches;
next	1.97;

1.97
date	2007.02.22.20.34.46;	author thib;	state Exp;
branches;
next	1.96;

1.96
date	2005.12.29.23.54.49;	author kettenis;	state Exp;
branches;
next	1.95;

1.95
date	2005.12.17.07.31.27;	author miod;	state Exp;
branches;
next	1.94;

1.94
date	2005.11.13.03.56.26;	author brad;	state Exp;
branches;
next	1.93;

1.93
date	2005.10.09.14.01.11;	author drahn;	state Exp;
branches;
next	1.92;

1.92
date	2005.10.08.06.03.00;	author drahn;	state Exp;
branches;
next	1.91;

1.91
date	2005.10.03.04.47.30;	author drahn;	state Exp;
branches;
next	1.90;

1.90
date	2005.10.03.02.18.50;	author drahn;	state Exp;
branches;
next	1.89;

1.89
date	2005.05.02.19.03.35;	author kettenis;	state Exp;
branches;
next	1.88;

1.88
date	2004.06.24.22.35.56;	author drahn;	state Exp;
branches;
next	1.87;

1.87
date	2004.05.20.09.20.42;	author kettenis;	state Exp;
branches;
next	1.86;

1.86
date	2004.01.25.13.22.10;	author miod;	state Exp;
branches;
next	1.85;

1.85
date	2004.01.03.00.57.06;	author pvalchev;	state Exp;
branches;
next	1.84;

1.84
date	2003.12.20.22.40.28;	author miod;	state Exp;
branches;
next	1.83;

1.83
date	2003.10.31.03.06.16;	author drahn;	state Exp;
branches;
next	1.82;

1.82
date	2003.07.02.21.30.12;	author drahn;	state Exp;
branches;
next	1.81;

1.81
date	2003.06.03.01.35.30;	author drahn;	state Exp;
branches;
next	1.80;

1.80
date	2003.02.26.21.54.44;	author drahn;	state Exp;
branches;
next	1.79;

1.79
date	2003.01.30.15.38.09;	author drahn;	state Exp;
branches;
next	1.78;

1.78
date	2002.11.06.00.17.27;	author art;	state Exp;
branches;
next	1.77;

1.77
date	2002.10.13.18.26.12;	author krw;	state Exp;
branches;
next	1.76;

1.76
date	2002.09.15.09.01.59;	author deraadt;	state Exp;
branches;
next	1.75;

1.75
date	2002.09.15.02.02.44;	author deraadt;	state Exp;
branches;
next	1.74;

1.74
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.73;

1.73
date	2002.07.24.02.19.28;	author drahn;	state Exp;
branches;
next	1.72;

1.72
date	2002.07.15.17.01.25;	author drahn;	state Exp;
branches;
next	1.71;

1.71
date	2002.07.12.20.28.55;	author drahn;	state Exp;
branches;
next	1.70;

1.70
date	2002.06.10.00.12.15;	author drahn;	state Exp;
branches;
next	1.69;

1.69
date	2002.06.10.00.06.47;	author drahn;	state Exp;
branches;
next	1.68;

1.68
date	2002.06.07.21.49.35;	author drahn;	state Exp;
branches;
next	1.67;

1.67
date	2002.06.04.00.09.08;	author deraadt;	state Exp;
branches;
next	1.66;

1.66
date	2002.05.18.20.11.06;	author drahn;	state Exp;
branches;
next	1.65;

1.65
date	2002.03.22.21.01.07;	author drahn;	state Exp;
branches;
next	1.64;

1.64
date	2002.03.21.05.42.43;	author drahn;	state Exp;
branches;
next	1.63;

1.63
date	2002.03.21.05.39.22;	author drahn;	state Exp;
branches;
next	1.62;

1.62
date	2002.03.14.18.43.51;	author drahn;	state Exp;
branches;
next	1.61;

1.61
date	2002.03.14.03.15.59;	author millert;	state Exp;
branches;
next	1.60;

1.60
date	2002.03.13.18.27.36;	author drahn;	state Exp;
branches;
next	1.59;

1.59
date	2002.03.08.02.52.36;	author drahn;	state Exp;
branches;
next	1.58;

1.58
date	2002.01.25.04.04.55;	author drahn;	state Exp;
branches;
next	1.57;

1.57
date	2002.01.25.03.55.23;	author drahn;	state Exp;
branches;
next	1.56;

1.56
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2002.01.13.05.27.40;	author drahn;	state Exp;
branches;
next	1.54;

1.54
date	2002.01.06.06.02.59;	author drahn;	state Exp;
branches;
next	1.53;

1.53
date	2002.01.06.04.04.48;	author drahn;	state Exp;
branches;
next	1.52;

1.52
date	2001.12.13.19.14.41;	author drahn;	state Exp;
branches
	1.52.2.1;
next	1.51;

1.51
date	2001.11.29.16.43.42;	author drahn;	state Exp;
branches;
next	1.50;

1.50
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.49;

1.49
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.47;

1.47
date	2001.11.06.19.53.16;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2001.11.06.02.53.57;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2001.09.19.20.50.57;	author mickey;	state Exp;
branches;
next	1.44;

1.44
date	2001.09.18.13.59.23;	author drahn;	state Exp;
branches;
next	1.43;

1.43
date	2001.09.15.13.21.06;	author drahn;	state Exp;
branches;
next	1.42;

1.42
date	2001.09.03.21.45.08;	author drahn;	state Exp;
branches;
next	1.41;

1.41
date	2001.08.25.05.06.29;	author drahn;	state Exp;
branches;
next	1.40;

1.40
date	2001.08.18.06.18.49;	author drahn;	state Exp;
branches;
next	1.39;

1.39
date	2001.08.10.15.05.48;	author drahn;	state Exp;
branches;
next	1.38;

1.38
date	2001.08.06.19.03.33;	author drahn;	state Exp;
branches;
next	1.37;

1.37
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.07.09.02.14.05;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2001.06.27.06.19.54;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.06.27.04.37.21;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.06.24.23.29.36;	author drahn;	state Exp;
branches;
next	1.31;

1.31
date	2001.06.10.15.20.16;	author drahn;	state Exp;
branches;
next	1.30;

1.30
date	2001.06.08.08.09.22;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.05.09.15.31.26;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.05.05.21.26.40;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.04.18.06.26.12;	author drahn;	state Exp;
branches;
next	1.26;

1.26
date	2001.03.29.19.03.34;	author drahn;	state Exp;
branches;
next	1.25;

1.25
date	2001.03.03.05.29.33;	author drahn;	state Exp;
branches;
next	1.24;

1.24
date	2001.02.22.03.26.24;	author drahn;	state Exp;
branches;
next	1.23;

1.23
date	2001.02.20.04.29.08;	author drahn;	state Exp;
branches;
next	1.22;

1.22
date	2001.02.16.05.18.06;	author drahn;	state Exp;
branches;
next	1.21;

1.21
date	2001.01.24.21.23.05;	author drahn;	state Exp;
branches;
next	1.20;

1.20
date	2000.10.24.02.05.36;	author drahn;	state Exp;
branches;
next	1.19;

1.19
date	2000.07.28.13.02.12;	author rahnds;	state Exp;
branches;
next	1.18;

1.18
date	2000.07.12.13.49.54;	author rahnds;	state Exp;
branches;
next	1.17;

1.17
date	2000.06.15.03.20.53;	author rahnds;	state Exp;
branches;
next	1.16;

1.16
date	2000.03.23.03.52.55;	author rahnds;	state Exp;
branches;
next	1.15;

1.15
date	2000.03.20.07.05.53;	author rahnds;	state Exp;
branches;
next	1.14;

1.14
date	2000.01.14.05.42.17;	author rahnds;	state Exp;
branches
	1.14.2.1;
next	1.13;

1.13
date	99.11.09.00.20.42;	author rahnds;	state Exp;
branches;
next	1.12;

1.12
date	99.10.28.04.28.03;	author rahnds;	state Exp;
branches;
next	1.11;

1.11
date	99.09.03.18.01.50;	author art;	state Exp;
branches;
next	1.10;

1.10
date	99.07.05.20.56.26;	author rahnds;	state Exp;
branches;
next	1.9;

1.9
date	99.03.22.02.41.21;	author rahnds;	state Exp;
branches;
next	1.8;

1.8
date	99.01.11.05.11.54;	author millert;	state Exp;
branches;
next	1.7;

1.7
date	98.08.22.18.32.00;	author rahnds;	state Exp;
branches;
next	1.6;

1.6
date	98.03.04.10.58.16;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	97.01.21.17.00.10;	author rahnds;	state Exp;
branches;
next	1.4;

1.4
date	97.01.09.21.19.02;	author rahnds;	state Exp;
branches;
next	1.3;

1.3
date	97.01.09.03.07.16;	author rahnds;	state Exp;
branches;
next	1.2;

1.2
date	96.12.28.06.22.14;	author rahnds;	state Exp;
branches;
next	1.1;

1.1
date	96.12.21.20.35.58;	author rahnds;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	96.12.21.20.35.58;	author rahnds;	state Exp;
branches;
next	;

1.14.2.1
date	2000.03.24.09.08.43;	author niklas;	state Exp;
branches;
next	1.14.2.2;

1.14.2.2
date	2001.05.14.21.36.57;	author niklas;	state Exp;
branches;
next	1.14.2.3;

1.14.2.3
date	2001.07.04.10.23.01;	author niklas;	state Exp;
branches;
next	1.14.2.4;

1.14.2.4
date	2001.10.31.03.07.56;	author nate;	state Exp;
branches;
next	1.14.2.5;

1.14.2.5
date	2001.11.13.21.04.16;	author niklas;	state Exp;
branches;
next	1.14.2.6;

1.14.2.6
date	2001.11.13.22.14.34;	author niklas;	state dead;
branches;
next	1.14.2.7;

1.14.2.7
date	2002.03.29.19.46.01;	author niklas;	state Exp;
branches;
next	1.14.2.8;

1.14.2.8
date	2002.03.30.08.32.27;	author niklas;	state Exp;
branches;
next	1.14.2.9;

1.14.2.9
date	2003.03.27.23.42.35;	author niklas;	state Exp;
branches;
next	1.14.2.10;

1.14.2.10
date	2003.06.07.11.13.18;	author ho;	state Exp;
branches;
next	1.14.2.11;

1.14.2.11
date	2004.02.19.10.49.57;	author niklas;	state Exp;
branches;
next	1.14.2.12;

1.14.2.12
date	2004.06.05.23.10.57;	author niklas;	state Exp;
branches;
next	;

1.52.2.1
date	2002.01.31.22.55.21;	author niklas;	state Exp;
branches;
next	1.52.2.2;

1.52.2.2
date	2002.06.11.03.37.28;	author art;	state Exp;
branches;
next	1.52.2.3;

1.52.2.3
date	2002.10.29.00.28.08;	author art;	state Exp;
branches;
next	1.52.2.4;

1.52.2.4
date	2002.10.29.14.34.06;	author drahn;	state Exp;
branches;
next	1.52.2.5;

1.52.2.5
date	2003.05.19.21.49.44;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.167
log
@Implement copyin32(9).

ok mpi@@, visa@@
@
text
@/*	$OpenBSD: pmap.c,v 1.166 2016/10/19 08:28:20 guenther Exp $ */

/*
 * Copyright (c) 2015 Martin Pieuchot
 * Copyright (c) 2001, 2002, 2007 Dale Rahn.
 * All rights reserved.
 *
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 */

/*
 * powerpc lazy icache managment.
 * The icache does not snoop dcache accesses. The icache also will not load
 * modified data from the dcache, but the unmodified data in ram.
 * Before the icache is loaded, the dcache must be synced to ram to prevent
 * the icache from loading stale data.
 * pg->pg_flags PG_PMAP_EXE bit is used to track if the dcache is clean
 * and the icache may have valid data in it.
 * if the PG_PMAP_EXE bit is set (and the page is not currently RWX)
 * the icache will only have valid code in it. If the bit is clear
 * memory may not match the dcache contents or the icache may contain
 * data from a previous page.
 *
 * pmap enter
 * !E  NONE 	-> R	no action
 * !E  NONE|R 	-> RW	no action
 * !E  NONE|R 	-> RX	flush dcache, inval icache (that page only), set E
 * !E  NONE|R 	-> RWX	flush dcache, inval icache (that page only), set E
 * !E  NONE|RW 	-> RWX	flush dcache, inval icache (that page only), set E
 *  E  NONE 	-> R	no action
 *  E  NONE|R 	-> RW	clear PG_PMAP_EXE bit
 *  E  NONE|R 	-> RX	no action
 *  E  NONE|R 	-> RWX	no action
 *  E  NONE|RW 	-> RWX	-invalid source state
 *
 * pamp_protect
 *  E RW -> R	- invalid source state
 * !E RW -> R	- no action
 *  * RX -> R	- no action
 *  * RWX -> R	- sync dcache, inval icache
 *  * RWX -> RW	- clear PG_PMAP_EXE
 *  * RWX -> RX	- sync dcache, inval icache
 *  * * -> NONE	- no action
 * 
 * pmap_page_protect (called with arg PROT_NONE if page is to be reused)
 *  * RW -> R	- as pmap_protect
 *  * RX -> R	- as pmap_protect
 *  * RWX -> R	- as pmap_protect
 *  * RWX -> RW	- as pmap_protect
 *  * RWX -> RX	- as pmap_protect
 *  * * -> NONE - clear PG_PMAP_EXE
 * 
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/queue.h>
#include <sys/pool.h>
#include <sys/atomic.h>

#include <uvm/uvm_extern.h>

#include <machine/pcb.h>
#include <powerpc/powerpc.h>
#include <powerpc/bat.h>
#include <machine/pmap.h>

struct bat battable[16];

struct dumpmem dumpmem[VM_PHYSSEG_MAX];
u_int ndumpmem;

struct pmap kernel_pmap_;
static struct mem_region *pmap_mem, *pmap_avail;
struct mem_region pmap_allocated[10];
int pmap_cnt_avail;
int pmap_cnt_allocated;

struct pte_64  *pmap_ptable64;
struct pte_32  *pmap_ptable32;
int	pmap_ptab_cnt;
u_int	pmap_ptab_mask;

#define HTABSIZE_32	(pmap_ptab_cnt * 64)
#define HTABMEMSZ_64	(pmap_ptab_cnt * 8 * sizeof(struct pte_64))
#define HTABSIZE_64	(ffs(pmap_ptab_cnt) - 12)

static u_int usedsr[NPMAPS / sizeof(u_int) / 8];

struct pte_desc {
	/* Linked list of phys -> virt entries */
	LIST_ENTRY(pte_desc) pted_pv_list;
	union {
		struct pte_32 pted_pte32;
		struct pte_64 pted_pte64;
	} p;
	pmap_t pted_pmap;
	vaddr_t pted_va;
};

void pmap_attr_save(paddr_t pa, u_int32_t bits);
void pmap_pted_ro(struct pte_desc *, vm_prot_t);
void pmap_pted_ro64(struct pte_desc *, vm_prot_t);
void pmap_pted_ro32(struct pte_desc *, vm_prot_t);

/*
 * Some functions are called in real mode and cannot be profiled.
 */
#define __noprof __attribute__((__no_instrument_function__))

/* VP routines */
int pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted, int flags);
struct pte_desc *pmap_vp_remove(pmap_t pm, vaddr_t va);
void pmap_vp_destroy(pmap_t pm);
struct pte_desc *pmap_vp_lookup(pmap_t pm, vaddr_t va) __noprof;

/* PV routines */
void pmap_enter_pv(struct pte_desc *pted, struct vm_page *);
void pmap_remove_pv(struct pte_desc *pted);


/* pte hash table routines */
static inline void *pmap_ptedinhash(struct pte_desc *);
void pte_insert32(struct pte_desc *) __noprof;
void pte_insert64(struct pte_desc *) __noprof;
void pmap_fill_pte64(pmap_t, vaddr_t, paddr_t, struct pte_desc *, vm_prot_t,
    int) __noprof;
void pmap_fill_pte32(pmap_t, vaddr_t, paddr_t, struct pte_desc *, vm_prot_t,
    int) __noprof;

void pmap_syncicache_user_virt(pmap_t pm, vaddr_t va);

void _pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags,
    int cache);
void pmap_remove_pted(pmap_t, struct pte_desc *);

/* setup/initialization functions */
void pmap_avail_setup(void);
void pmap_avail_fixup(void);
void pmap_remove_avail(paddr_t base, paddr_t end);
void *pmap_steal_avail(size_t size, int align);

/* asm interface */
int pte_spill_r(u_int32_t, u_int32_t, u_int32_t, int) __noprof;
int pte_spill_v(pmap_t, u_int32_t, u_int32_t, int) __noprof;

u_int32_t pmap_setusr(pmap_t pm, vaddr_t va);
void pmap_popusr(u_int32_t oldsr);

/* pte invalidation */
void pte_del(void *, vaddr_t);
void pte_zap(void *, struct pte_desc *);

/* XXX - panic on pool get failures? */
struct pool pmap_pmap_pool;
struct pool pmap_vp_pool;
struct pool pmap_pted_pool;

int pmap_initialized = 0;
int physmem;
int physmaxaddr;

#ifdef MULTIPROCESSOR
struct __mp_lock pmap_hash_lock;

#define	PMAP_HASH_LOCK_INIT()		__mp_lock_init(&pmap_hash_lock)

#define	PMAP_HASH_LOCK(s)						\
do {									\
	s = ppc_intr_disable();						\
	__mp_lock(&pmap_hash_lock);					\
} while (0)

#define	PMAP_HASH_UNLOCK(s)						\
do {									\
	__mp_unlock(&pmap_hash_lock);					\
	ppc_intr_enable(s);						\
} while (0)

#define	PMAP_VP_LOCK_INIT(pm)		mtx_init(&pm->pm_mtx, IPL_VM)

#define	PMAP_VP_LOCK(pm)						\
do {									\
	if (pm != pmap_kernel())					\
		mtx_enter(&pm->pm_mtx);					\
} while (0)

#define	PMAP_VP_UNLOCK(pm)						\
do {									\
	if (pm != pmap_kernel())					\
		mtx_leave(&pm->pm_mtx);					\
} while (0)

#define PMAP_VP_ASSERT_LOCKED(pm)					\
do {									\
	if (pm != pmap_kernel())					\
		MUTEX_ASSERT_LOCKED(&pm->pm_mtx);			\
} while (0)

#else /* ! MULTIPROCESSOR */

#define	PMAP_HASH_LOCK_INIT()		/* nothing */
#define	PMAP_HASH_LOCK(s)		(void)s
#define	PMAP_HASH_UNLOCK(s)		/* nothing */

#define	PMAP_VP_LOCK_INIT(pm)		/* nothing */
#define	PMAP_VP_LOCK(pm)		/* nothing */
#define	PMAP_VP_UNLOCK(pm)		/* nothing */
#define	PMAP_VP_ASSERT_LOCKED(pm)	/* nothing */
#endif /* MULTIPROCESSOR */

/* virtual to physical helpers */
static inline int
VP_SR(vaddr_t va)
{
	return (va >>VP_SR_POS) & VP_SR_MASK;
}

static inline int
VP_IDX1(vaddr_t va)
{
	return (va >> VP_IDX1_POS) & VP_IDX1_MASK;
}

static inline int
VP_IDX2(vaddr_t va)
{
	return (va >> VP_IDX2_POS) & VP_IDX2_MASK;
}

#if VP_IDX1_SIZE != VP_IDX2_SIZE 
#error pmap allocation code expects IDX1 and IDX2 size to be same
#endif
struct pmapvp {
	void *vp[VP_IDX1_SIZE];
};


/*
 * VP routines, virtual to physical translation information.
 * These data structures are based off of the pmap, per process.
 */

/*
 * This is used for pmap_kernel() mappings, they are not to be removed
 * from the vp table because they were statically initialized at the
 * initial pmap initialization. This is so that memory allocation 
 * is not necessary in the pmap_kernel() mappings.
 * Otherwise bad race conditions can appear.
 */
struct pte_desc *
pmap_vp_lookup(pmap_t pm, vaddr_t va)
{
	struct pmapvp *vp1;
	struct pmapvp *vp2;
	struct pte_desc *pted;

	PMAP_VP_ASSERT_LOCKED(pm);

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
	}

	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
	}

	pted = vp2->vp[VP_IDX2(va)];

	return pted;
}

/*
 * Remove, and return, pted at specified address, NULL if not present
 */
struct pte_desc *
pmap_vp_remove(pmap_t pm, vaddr_t va)
{
	struct pmapvp *vp1;
	struct pmapvp *vp2;
	struct pte_desc *pted;

	PMAP_VP_ASSERT_LOCKED(pm);

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
	}

	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
	}

	pted = vp2->vp[VP_IDX2(va)];
	vp2->vp[VP_IDX2(va)] = NULL;

	return pted;
}

/*
 * Create a V -> P mapping for the given pmap and virtual address
 * with reference to the pte descriptor that is used to map the page.
 * This code should track allocations of vp table allocations
 * so they can be freed efficiently.
 */
int
pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted, int flags)
{
	struct pmapvp *vp1;
	struct pmapvp *vp2;

	PMAP_VP_ASSERT_LOCKED(pm);

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		vp1 = pool_get(&pmap_vp_pool, PR_NOWAIT | PR_ZERO);
		if (vp1 == NULL) {
			if ((flags & PMAP_CANFAIL) == 0)
				panic("pmap_vp_enter: failed to allocate vp1");
			return ENOMEM;
		}
		pm->pm_vp[VP_SR(va)] = vp1;
	}

	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		vp2 = pool_get(&pmap_vp_pool, PR_NOWAIT | PR_ZERO);
		if (vp2 == NULL) {
			if ((flags & PMAP_CANFAIL) == 0)
				panic("pmap_vp_enter: failed to allocate vp2");
			return ENOMEM;
		}
		vp1->vp[VP_IDX1(va)] = vp2;
	}

	vp2->vp[VP_IDX2(va)] = pted;

	return 0;
}

static inline void
tlbie(vaddr_t va)
{
	asm volatile ("tlbie %0" :: "r"(va & ~PAGE_MASK));
}

static inline void
tlbsync(void)
{
	asm volatile ("tlbsync");
}
static inline void
eieio(void)
{
	asm volatile ("eieio");
}

static inline void
sync(void)
{
	asm volatile ("sync");
}

static inline void
tlbia(void)
{
	vaddr_t va;

	sync();
	for (va = 0; va < 0x00040000; va += 0x00001000)
		tlbie(va);
	eieio();
	tlbsync();
	sync();
}

static inline int
ptesr(sr_t *sr, vaddr_t va)
{
	return sr[(u_int)va >> ADDR_SR_SHIFT];
}

static inline int 
pteidx(sr_t sr, vaddr_t va)
{
	int hash;
	hash = (sr & SR_VSID) ^ (((u_int)va & ADDR_PIDX) >> ADDR_PIDX_SHIFT);
	return hash & pmap_ptab_mask;
}

#define PTED_VA_PTEGIDX_M	0x07
#define PTED_VA_HID_M		0x08
#define PTED_VA_MANAGED_M	0x10
#define PTED_VA_WIRED_M		0x20
#define PTED_VA_EXEC_M		0x40

static inline u_int32_t
PTED_HID(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_HID_M); 
}

static inline u_int32_t
PTED_PTEGIDX(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_PTEGIDX_M); 
}

static inline u_int32_t
PTED_MANAGED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_MANAGED_M); 
}

static inline u_int32_t
PTED_WIRED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_WIRED_M); 
}

static inline u_int32_t
PTED_VALID(struct pte_desc *pted)
{
	if (ppc_proc_is_64b)
		return (pted->p.pted_pte64.pte_hi & PTE_VALID_64);
	else 
		return (pted->p.pted_pte32.pte_hi & PTE_VALID_32);
}

/*
 * PV entries -
 * manipulate the physical to virtual translations for the entire system.
 * 
 * QUESTION: should all mapped memory be stored in PV tables? Or
 * is it alright to only store "ram" memory. Currently device mappings
 * are not stored.
 * It makes sense to pre-allocate mappings for all of "ram" memory, since
 * it is likely that it will be mapped at some point, but would it also
 * make sense to use a tree/table like is use for pmap to store device
 * mappings?
 * Further notes: It seems that the PV table is only used for pmap_protect
 * and other paging related operations. Given this, it is not necessary
 * to store any pmap_kernel() entries in PV tables and does not make
 * sense to store device mappings in PV either.
 *
 * Note: unlike other powerpc pmap designs, the array is only an array
 * of pointers. Since the same structure is used for holding information
 * in the VP table, the PV table, and for kernel mappings, the wired entries.
 * Allocate one data structure to hold all of the info, instead of replicating
 * it multiple times.
 *
 * One issue of making this a single data structure is that two pointers are
 * wasted for every page which does not map ram (device mappings), this 
 * should be a low percentage of mapped pages in the system, so should not
 * have too noticable unnecessary ram consumption.
 */

void
pmap_enter_pv(struct pte_desc *pted, struct vm_page *pg)
{
	if (__predict_false(!pmap_initialized)) {
		return;
	}

	mtx_enter(&pg->mdpage.pv_mtx);
	LIST_INSERT_HEAD(&(pg->mdpage.pv_list), pted, pted_pv_list);
	pted->pted_va |= PTED_VA_MANAGED_M;
	mtx_leave(&pg->mdpage.pv_mtx);
}

void
pmap_remove_pv(struct pte_desc *pted)
{
	struct vm_page *pg;

	if (ppc_proc_is_64b)
		pg = PHYS_TO_VM_PAGE(pted->p.pted_pte64.pte_lo & PTE_RPGN_64);
	else
		pg = PHYS_TO_VM_PAGE(pted->p.pted_pte32.pte_lo & PTE_RPGN_32);

	mtx_enter(&pg->mdpage.pv_mtx);
	pted->pted_va &= ~PTED_VA_MANAGED_M;
	LIST_REMOVE(pted, pted_pv_list);
	mtx_leave(&pg->mdpage.pv_mtx);
}


/* PTE_CHG_32 == PTE_CHG_64 */
/* PTE_REF_32 == PTE_REF_64 */
static __inline u_int
pmap_pte2flags(u_int32_t pte)
{
	return (((pte & PTE_REF_32) ? PG_PMAP_REF : 0) |
	    ((pte & PTE_CHG_32) ? PG_PMAP_MOD : 0));
}

static __inline u_int
pmap_flags2pte(u_int32_t flags)
{
	return (((flags & PG_PMAP_REF) ? PTE_REF_32 : 0) |
	    ((flags & PG_PMAP_MOD) ? PTE_CHG_32 : 0));
}

void
pmap_attr_save(paddr_t pa, u_int32_t bits)
{
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		return;

	atomic_setbits_int(&pg->pg_flags,  pmap_pte2flags(bits));
}

int
pmap_enter(pmap_t pm, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	struct pte_desc *pted;
	struct vm_page *pg;
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wt = (pa & PMAP_WT) != 0;
	int need_sync = 0;
	int cache, error = 0;

	KASSERT(!(wt && nocache));
	pa &= PMAP_PA_MASK;

	PMAP_VP_LOCK(pm);
	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted)) {
		pmap_remove_pted(pm, pted);
		/* we lost our pted if it was user */
		if (pm != pmap_kernel())
			pted = pmap_vp_lookup(pm, va);
	}

	pm->pm_stats.resident_count++;

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT | PR_ZERO);
		if (pted == NULL) {
			if ((flags & PMAP_CANFAIL) == 0) {
				error = ENOMEM;
				goto out;
			}
			panic("pmap_enter: failed to allocate pted");
		}
		error = pmap_vp_enter(pm, va, pted, flags);
		if (error) {
			pool_put(&pmap_pted_pool, pted);
			goto out;
		}
	}

	pg = PHYS_TO_VM_PAGE(pa);
	if (pg->pg_flags & PG_PMAP_UC)
		nocache = TRUE;
	if (wt)
		cache = PMAP_CACHE_WT;
	else if (pg != NULL && !(pg->pg_flags & PG_DEV) && !nocache)
		cache = PMAP_CACHE_WB;
	else
		cache = PMAP_CACHE_CI;

	/* Calculate PTE */
	if (ppc_proc_is_64b)
		pmap_fill_pte64(pm, va, pa, pted, prot, cache);
	else
		pmap_fill_pte32(pm, va, pa, pted, prot, cache);

	if (pg != NULL) {
		pmap_enter_pv(pted, pg); /* only managed mem */
	}

	/*
	 * Insert into HTAB
	 * We were told to map the page, probably called from vm_fault,
	 * so map the page!
	 */
	if (ppc_proc_is_64b)
		pte_insert64(pted);
	else
		pte_insert32(pted);

        if (prot & PROT_EXEC) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC)
			pm->pm_sr[sn] &= ~SR_NOEXEC;

		if (pg != NULL) {
			need_sync = ((pg->pg_flags & PG_PMAP_EXE) == 0);
			if (prot & PROT_WRITE)
				atomic_clearbits_int(&pg->pg_flags,
				    PG_PMAP_EXE);
			else
				atomic_setbits_int(&pg->pg_flags,
				    PG_PMAP_EXE);
		} else
			need_sync = 1;
	} else {
		/*
		 * Should we be paranoid about writeable non-exec 
		 * mappings ? if so, clear the exec tag
		 */
		if ((prot & PROT_WRITE) && (pg != NULL))
			atomic_clearbits_int(&pg->pg_flags, PG_PMAP_EXE);
	}

	/* only instruction sync executable pages */
	if (need_sync)
		pmap_syncicache_user_virt(pm, va);

out:
	PMAP_VP_UNLOCK(pm);
	return (error);
}

/*
 * Remove the given range of mapping entries.
 */
void
pmap_remove(pmap_t pm, vaddr_t sva, vaddr_t eva)
{
	struct pte_desc *pted;
	vaddr_t va;

	PMAP_VP_LOCK(pm);
	for (va = sva; va < eva; va += PAGE_SIZE) {
		pted = pmap_vp_lookup(pm, va);
		if (pted && PTED_VALID(pted))
			pmap_remove_pted(pm, pted);
	}
	PMAP_VP_UNLOCK(pm);
}

/*
 * remove a single mapping, notice that this code is O(1)
 */
void
pmap_remove_pted(pmap_t pm, struct pte_desc *pted)
{
	void *pte;
	int s;

	KASSERT(pm == pted->pted_pmap);
	PMAP_VP_ASSERT_LOCKED(pm);

	pm->pm_stats.resident_count--;

	PMAP_HASH_LOCK(s);
	if ((pte = pmap_ptedinhash(pted)) != NULL)
		pte_zap(pte, pted);
	PMAP_HASH_UNLOCK(s);

	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(pted->pted_va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0)
			pm->pm_sr[sn] |= SR_NOEXEC;
	}

	if (ppc_proc_is_64b)
		pted->p.pted_pte64.pte_hi &= ~PTE_VALID_64;
	else
		pted->p.pted_pte32.pte_hi &= ~PTE_VALID_32;

	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	if (pm != pmap_kernel()) {
		(void)pmap_vp_remove(pm, pted->pted_va);
		pool_put(&pmap_pted_pool, pted);
	}
}

/*
 * Enter a kernel mapping for the given page.
 * kernel mappings have a larger set of prerequisites than normal mappings.
 * 
 * 1. no memory should be allocated to create a kernel mapping.
 * 2. a vp mapping should already exist, even if invalid. (see 1)
 * 3. all vp tree mappings should already exist (see 1)
 * 
 */
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	struct pte_desc *pted;
	struct vm_page *pg;
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wt = (pa & PMAP_WT) != 0;
	pmap_t pm;
	int cache;

	KASSERT(!(wt && nocache));
	pa &= PMAP_PA_MASK;

	pm = pmap_kernel();

	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted))
		pmap_remove_pted(pm, pted); /* pted is reused */

	pm->pm_stats.resident_count++;

	if (prot & PROT_WRITE) {
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg != NULL)
			atomic_clearbits_int(&pg->pg_flags, PG_PMAP_EXE);
	}

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		panic("pted not preallocated in pmap_kernel() va %lx pa %lx",
		    va, pa);
	}

	pg = PHYS_TO_VM_PAGE(pa);
	if (wt)
		cache = PMAP_CACHE_WT;
	else if (pg != NULL && !(pg->pg_flags & PG_DEV) && !nocache)
		cache = PMAP_CACHE_WB;
	else
		cache = PMAP_CACHE_CI;

	/* Calculate PTE */
	if (ppc_proc_is_64b)
		pmap_fill_pte64(pm, va, pa, pted, prot, cache);
	else
		pmap_fill_pte32(pm, va, pa, pted, prot, cache);

	/*
	 * Insert into HTAB
	 * We were told to map the page, probably called from vm_fault,
	 * so map the page!
	 */
	if (ppc_proc_is_64b)
		pte_insert64(pted);
	else
		pte_insert32(pted);

	pted->pted_va |= PTED_VA_WIRED_M;

        if (prot & PROT_EXEC) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC)
			pm->pm_sr[sn] &= ~SR_NOEXEC;
	}
}

/*
 * remove kernel (pmap_kernel()) mappings
 */
void
pmap_kremove(vaddr_t va, vsize_t len)
{
	struct pte_desc *pted;

	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pted = pmap_vp_lookup(pmap_kernel(), va);
		if (pted && PTED_VALID(pted))
			pmap_remove_pted(pmap_kernel(), pted);
	}
}

static inline void *
pmap_ptedinhash(struct pte_desc *pted)
{
	vaddr_t va = pted->pted_va & ~PAGE_MASK;
	pmap_t pm = pted->pted_pmap;
	int sr, idx;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);

	if (ppc_proc_is_64b) {
		struct pte_64 *pte = pmap_ptable64;

		pte += (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		pte += PTED_PTEGIDX(pted);

		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->p.pted_pte64.pte_hi |
		    (PTED_HID(pted) ? PTE_HID_64 : 0)) == pte->pte_hi)
			return (pte);
	} else {
		struct pte_32 *pte = pmap_ptable32;

		pte += (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		pte += PTED_PTEGIDX(pted);

		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->p.pted_pte32.pte_hi |
		    (PTED_HID(pted) ? PTE_HID_32 : 0)) == pte->pte_hi)
			return (pte);
	}

	return (NULL);
}

/*
 * Delete a Page Table Entry, section 7.6.3.3.
 *
 * Note: pte must be locked.
 */
void
pte_del(void *pte, vaddr_t va)
{
	if (ppc_proc_is_64b)
		((struct pte_64 *)pte)->pte_hi &= ~PTE_VALID_64;
	else
		((struct pte_32 *)pte)->pte_hi &= ~PTE_VALID_32;

	sync();		/* Ensure update completed. */
	tlbie(va);	/* Invalidate old translation. */
	eieio();	/* Order tlbie before tlbsync. */
	tlbsync();	/* Ensure tlbie completed on all processors. */
	sync();		/* Ensure tlbsync and update completed. */
}

void
pte_zap(void *pte, struct pte_desc *pted)
{
	pte_del(pte, pted->pted_va);

	if (!PTED_MANAGED(pted))
		return;

	if (ppc_proc_is_64b) {
		pmap_attr_save(pted->p.pted_pte64.pte_lo & PTE_RPGN_64,
		    ((struct pte_64 *)pte)->pte_lo & (PTE_REF_64|PTE_CHG_64));
	} else {
		pmap_attr_save(pted->p.pted_pte32.pte_lo & PTE_RPGN_32,
		    ((struct pte_32 *)pte)->pte_lo & (PTE_REF_32|PTE_CHG_32));
	}
}

/*
 * What about execution control? Even at only a segment granularity.
 */
void
pmap_fill_pte64(pmap_t pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
	vm_prot_t prot, int cache)
{
	sr_t sr;
	struct pte_64 *pte64;

	sr = ptesr(pm->pm_sr, va);
	pte64 = &pted->p.pted_pte64;

	pte64->pte_hi = (((u_int64_t)sr & SR_VSID) <<
	   PTE_VSID_SHIFT_64) |
	    ((va >> ADDR_API_SHIFT_64) & PTE_API_64) | PTE_VALID_64;
	pte64->pte_lo = (pa & PTE_RPGN_64);


	if (cache == PMAP_CACHE_WB)
		pte64->pte_lo |= PTE_M_64;
	else if (cache == PMAP_CACHE_WT)
		pte64->pte_lo |= (PTE_W_64 | PTE_M_64);
	else
		pte64->pte_lo |= (PTE_M_64 | PTE_I_64 | PTE_G_64);

	if (prot & PROT_WRITE)
		pte64->pte_lo |= PTE_RW_64;
	else
		pte64->pte_lo |= PTE_RO_64;

	pted->pted_va = va & ~PAGE_MASK;

	if (prot & PROT_EXEC)
		pted->pted_va  |= PTED_VA_EXEC_M;
	else
		pte64->pte_lo |= PTE_N_64;

	pted->pted_pmap = pm;
}

/*
 * What about execution control? Even at only a segment granularity.
 */
void
pmap_fill_pte32(pmap_t pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
	vm_prot_t prot, int cache)
{
	sr_t sr;
	struct pte_32 *pte32;

	sr = ptesr(pm->pm_sr, va);
	pte32 = &pted->p.pted_pte32;

	pte32->pte_hi = ((sr & SR_VSID) << PTE_VSID_SHIFT_32) |
	    ((va >> ADDR_API_SHIFT_32) & PTE_API_32) | PTE_VALID_32;
	pte32->pte_lo = (pa & PTE_RPGN_32);

	if (cache == PMAP_CACHE_WB)
		pte32->pte_lo |= PTE_M_32;
	else if (cache == PMAP_CACHE_WT)
		pte32->pte_lo |= (PTE_W_32 | PTE_M_32);
	else
		pte32->pte_lo |= (PTE_M_32 | PTE_I_32 | PTE_G_32);

	if (prot & PROT_WRITE)
		pte32->pte_lo |= PTE_RW_32;
	else
		pte32->pte_lo |= PTE_RO_32;

	pted->pted_va = va & ~PAGE_MASK;

	/* XXX Per-page execution control. */
	if (prot & PROT_EXEC)
		pted->pted_va  |= PTED_VA_EXEC_M;

	pted->pted_pmap = pm;
}

int
pmap_test_attrs(struct vm_page *pg, u_int flagbit)
{
	u_int bits;
	struct pte_desc *pted;
	u_int ptebit = pmap_flags2pte(flagbit);
	int s;

	/* PTE_CHG_32 == PTE_CHG_64 */
	/* PTE_REF_32 == PTE_REF_64 */

	bits = pg->pg_flags & flagbit;
	if ((bits == flagbit))
		return bits;

	mtx_enter(&pg->mdpage.pv_mtx);
	LIST_FOREACH(pted, &(pg->mdpage.pv_list), pted_pv_list) {
		void *pte;

		PMAP_HASH_LOCK(s);
		if ((pte = pmap_ptedinhash(pted)) != NULL) {
			if (ppc_proc_is_64b) {
				struct pte_64 *ptp64 = pte;
				bits |=	pmap_pte2flags(ptp64->pte_lo & ptebit);
			} else {
				struct pte_32 *ptp32 = pte;
				bits |=	pmap_pte2flags(ptp32->pte_lo & ptebit);
			}
		}
		PMAP_HASH_UNLOCK(s);

		if (bits == flagbit)
			break;
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	atomic_setbits_int(&pg->pg_flags,  bits);

	return bits;
}

int
pmap_clear_attrs(struct vm_page *pg, u_int flagbit)
{
	u_int bits;
	struct pte_desc *pted;
	u_int ptebit = pmap_flags2pte(flagbit);
	int s;

	/* PTE_CHG_32 == PTE_CHG_64 */
	/* PTE_REF_32 == PTE_REF_64 */

	bits = pg->pg_flags & flagbit;

	mtx_enter(&pg->mdpage.pv_mtx);
	LIST_FOREACH(pted, &(pg->mdpage.pv_list), pted_pv_list) {
		void *pte;

		PMAP_HASH_LOCK(s);
		if ((pte = pmap_ptedinhash(pted)) != NULL) {
			if (ppc_proc_is_64b) {
				struct pte_64 *ptp64 = pte;

				bits |=	pmap_pte2flags(ptp64->pte_lo & ptebit);

				pte_del(ptp64, pted->pted_va);

				ptp64->pte_lo &= ~ptebit;
				eieio();
				ptp64->pte_hi |= PTE_VALID_64;
				sync();
			} else {
				struct pte_32 *ptp32 = pte;

				bits |=	pmap_pte2flags(ptp32->pte_lo & ptebit);

				pte_del(ptp32, pted->pted_va);

				ptp32->pte_lo &= ~ptebit;
				eieio();
				ptp32->pte_hi |= PTE_VALID_32;
				sync();
			}
		}
		PMAP_HASH_UNLOCK(s);
	}
	mtx_leave(&pg->mdpage.pv_mtx);

	/*
	 * this is done a second time, because while walking the list
	 * a bit could have been promoted via pmap_attr_save()
	 */
	bits |= pg->pg_flags & flagbit;
	atomic_clearbits_int(&pg->pg_flags,  flagbit);

	return bits;
}

/*
 * Garbage collects the physical map system for pages which are 
 * no longer used. Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but others may be collected
 * Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap_t pm)
{
	/* This could return unused v->p table layers which 
	 * are empty.
	 * could malicious programs allocate memory and eat
	 * these wired pages? These are allocated via pool.
	 * Are there pool functions which could be called
	 * to lower the pool usage here?
	 */
}

/*
 * Fill the given physical page with zeros.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	vaddr_t va = pmap_map_direct(pg);
	int i;

	/*
	 * Loop over & zero cache lines.  This code assumes that 64-bit
	 * CPUs have 128-byte cache lines.  We explicitely use ``dcbzl''
	 * here because we do not clear the DCBZ_SIZE bit of the HID5
	 * register in order to be compatible with code using ``dcbz''
	 * and assuming that cache line size is 32.
	 */
	if (ppc_proc_is_64b) {
		for (i = 0; i < PAGE_SIZE; i += 128)
			asm volatile ("dcbzl 0,%0" :: "r"(va + i));
		return;
	}

	for (i = 0; i < PAGE_SIZE; i += CACHELINESIZE)
		asm volatile ("dcbz 0,%0" :: "r"(va + i));
}

/*
 * Copy a page.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	vaddr_t srcva = pmap_map_direct(srcpg);
	vaddr_t dstva = pmap_map_direct(dstpg);

	memcpy((void *)dstva, (void *)srcva, PAGE_SIZE);
}

int pmap_id_avail = 0;

pmap_t
pmap_create(void)
{
	int i, k, try, tblidx, tbloff;
	int s, seg;
	pmap_t pm;

	pm = pool_get(&pmap_pmap_pool, PR_WAITOK|PR_ZERO);

	pmap_reference(pm);
	PMAP_VP_LOCK_INIT(pm);

	/*
	 * Allocate segment registers for this pmap.
	 * Try not to reuse pmap ids, to spread the hash table usage.
	 */
again:
	for (i = 0; i < NPMAPS; i++) {
		try = pmap_id_avail + i;
		try = try % NPMAPS; /* truncate back into bounds */
		tblidx = try / (8 * sizeof usedsr[0]);
		tbloff = try % (8 * sizeof usedsr[0]);
		if ((usedsr[tblidx] & (1 << tbloff)) == 0) {
			/* pmap create lock? */
			s = splvm();
			if ((usedsr[tblidx] & (1 << tbloff)) == 1) {
				/* entry was stolen out from under us, retry */
				splx(s); /* pmap create unlock */
				goto again;
			}
			usedsr[tblidx] |= (1 << tbloff); 
			pmap_id_avail = try + 1;
			splx(s); /* pmap create unlock */

			seg = try << 4;
			for (k = 0; k < 16; k++)
				pm->pm_sr[k] = (seg + k) | SR_NOEXEC;
			return (pm);
		}
	}
	panic("out of pmap slots");
}

/*
 * Add a reference to a given pmap.
 */
void
pmap_reference(pmap_t pm)
{
	atomic_inc_int(&pm->pm_refs);
}

/*
 * Retire the given pmap from service.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_destroy(pmap_t pm)
{
	int refs;

	refs = atomic_dec_int_nv(&pm->pm_refs);
	if (refs == -1)
		panic("re-entering pmap_destroy");
	if (refs > 0)
		return;

	/*
	 * reference count is zero, free pmap resources and free pmap.
	 */
	pmap_release(pm);
	pool_put(&pmap_pmap_pool, pm);
}

/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 */
void
pmap_release(pmap_t pm)
{
	int i, tblidx, tbloff;
	int s;

	pmap_vp_destroy(pm);
	i = (pm->pm_sr[0] & SR_VSID) >> 4;
	tblidx = i / (8  * sizeof usedsr[0]);
	tbloff = i % (8  * sizeof usedsr[0]);

	/* LOCK? */
	s = splvm();
	usedsr[tblidx] &= ~(1 << tbloff);
	splx(s);
}

void
pmap_vp_destroy(pmap_t pm)
{
	int i, j;
	struct pmapvp *vp1;
	struct pmapvp *vp2;

	for (i = 0; i < VP_SR_SIZE; i++) {
		vp1 = pm->pm_vp[i];
		if (vp1 == NULL)
			continue;

		for (j = 0; j < VP_IDX1_SIZE; j++) {
			vp2 = vp1->vp[j];
			if (vp2 == NULL)
				continue;
			
			pool_put(&pmap_vp_pool, vp2);
		}
		pm->pm_vp[i] = NULL;
		pool_put(&pmap_vp_pool, vp1);
	}
}

void
pmap_avail_setup(void)
{
	struct mem_region *mp;
	int pmap_physmem;

	ppc_mem_regions(&pmap_mem, &pmap_avail);
	pmap_cnt_avail = 0;
	pmap_physmem = 0;

	ndumpmem = 0;
	for (mp = pmap_mem; mp->size !=0; mp++, ndumpmem++) {
		pmap_physmem += atop(mp->size);
		dumpmem[ndumpmem].start = atop(mp->start);
		dumpmem[ndumpmem].end = atop(mp->start + mp->size);
	}

	if (physmem == 0)
		physmem = pmap_physmem;

	for (mp = pmap_avail; mp->size !=0 ; mp++) {
		if (physmaxaddr <  mp->start + mp->size)
			physmaxaddr = mp->start + mp->size;
	}

	for (mp = pmap_avail; mp->size !=0; mp++)
		pmap_cnt_avail += 1;
}

void
pmap_avail_fixup(void)
{
	struct mem_region *mp;
	u_int32_t align;
	u_int32_t end;

	mp = pmap_avail;
	while(mp->size !=0) {
		align = round_page(mp->start);
		if (mp->start != align) {
			pmap_remove_avail(mp->start, align);
			mp = pmap_avail;
			continue;
		}
		end = mp->start+mp->size;
		align = trunc_page(end);
		if (end != align) {
			pmap_remove_avail(align, end);
			mp = pmap_avail;
			continue;
		}
		mp++;
	}
}

/* remove a given region from avail memory */
void
pmap_remove_avail(paddr_t base, paddr_t end)
{
	struct mem_region *mp;
	int i;
	int mpend;

	/* remove given region from available */
	for (mp = pmap_avail; mp->size; mp++) {
		/*
		 * Check if this region holds all of the region
		 */
		mpend = mp->start + mp->size;
		if (base > mpend) {
			continue;
		}
		if (base <= mp->start) {
			if (end <= mp->start)
				break; /* region not present -??? */

			if (end >= mpend) {
				/* covers whole region */
				/* shorten */
				for (i = mp - pmap_avail;
				    i < pmap_cnt_avail;
				    i++) {
					pmap_avail[i] = pmap_avail[i+1];
				}
				pmap_cnt_avail--;
				pmap_avail[pmap_cnt_avail].size = 0;
			} else {
				mp->start = end;
				mp->size = mpend - end;
			}
		} else {
			/* start after the beginning */
			if (end >= mpend) {
				/* just truncate */
				mp->size = base - mp->start;
			} else {
				/* split */
				for (i = pmap_cnt_avail;
				    i > (mp - pmap_avail);
				    i--) {
					pmap_avail[i] = pmap_avail[i - 1];
				}
				pmap_cnt_avail++;
				mp->size = base - mp->start;
				mp++;
				mp->start = end;
				mp->size = mpend - end;
			}
		}
	}
	for (mp = pmap_allocated; mp->size != 0; mp++) {
		if (base < mp->start) {
			if (end == mp->start) {
				mp->start = base;
				mp->size += end - base;
				break;
			}
			/* lengthen */
			for (i = pmap_cnt_allocated; i > (mp - pmap_allocated);
			    i--) {
				pmap_allocated[i] = pmap_allocated[i - 1];
			}
			pmap_cnt_allocated++;
			mp->start = base;
			mp->size = end - base;
			return;
		}
		if (base == (mp->start + mp->size)) {
			mp->size += end - base;
			return;
		}
	}
	if (mp->size == 0) {
		mp->start = base;
		mp->size  = end - base;
		pmap_cnt_allocated++;
	}
}

void *
pmap_steal_avail(size_t size, int align)
{
	struct mem_region *mp;
	int start;
	int remsize;

	for (mp = pmap_avail; mp->size; mp++) {
		if (mp->size > size) {
			start = (mp->start + (align -1)) & ~(align -1);
			remsize = mp->size - (start - mp->start); 
			if (remsize >= 0) {
				pmap_remove_avail(start, start+size);
				return (void *)start;
			}
		}
	}
	panic ("unable to allocate region with size %zx align %x",
	    size, align);
}

/*
 * Similar to pmap_steal_avail, but operating on vm_physmem since
 * uvm_page_physload() has been called.
 */
vaddr_t
pmap_steal_memory(vsize_t size, vaddr_t *start, vaddr_t *end)
{
	int segno;
	u_int npg;
	vaddr_t va;
	paddr_t pa;
	struct vm_physseg *seg;

	size = round_page(size);
	npg = atop(size);

	for (segno = 0, seg = vm_physmem; segno < vm_nphysseg; segno++, seg++) {
		if (seg->avail_end - seg->avail_start < npg)
			continue;
		/*
		 * We can only steal at an ``unused'' segment boundary,
		 * i.e. either at the start or at the end.
		 */
		if (seg->avail_start == seg->start ||
		    seg->avail_end == seg->end)
			break;
	}
	if (segno == vm_nphysseg)
		va = 0;
	else {
		if (seg->avail_start == seg->start) {
			pa = ptoa(seg->avail_start);
			seg->avail_start += npg;
			seg->start += npg;
		} else {
			pa = ptoa(seg->avail_end) - size;
			seg->avail_end -= npg;
			seg->end -= npg;
		}
		/*
		 * If all the segment has been consumed now, remove it.
		 * Note that the crash dump code still knows about it
		 * and will dump it correctly.
		 */
		if (seg->start == seg->end) {
			if (vm_nphysseg-- == 1)
				panic("pmap_steal_memory: out of memory");
			while (segno < vm_nphysseg) {
				seg[0] = seg[1]; /* struct copy */
				seg++;
				segno++;
			}
		}

		va = (vaddr_t)pa;	/* 1:1 mapping */
		bzero((void *)va, size);
	}

	if (start != NULL)
		*start = VM_MIN_KERNEL_ADDRESS;
	if (end != NULL)
		*end = VM_MAX_KERNEL_ADDRESS;

	return (va);
}

void *msgbuf_addr;

/*
 * Initialize pmap setup.
 * ALL of the code which deals with avail needs rewritten as an actual
 * memory allocation.
 */ 
void
pmap_bootstrap(u_int kernelstart, u_int kernelend)
{
	struct mem_region *mp;
	int i, k;
	struct pmapvp *vp1;
	struct pmapvp *vp2;

	/*
	 * set the page size (default value is 4K which is ok)
	 */
	uvm_setpagesize();

	/*
	 * Get memory.
	 */
	pmap_avail_setup();

	/*
	 * Page align all regions.
	 * Non-page memory isn't very interesting to us.
	 * Also, sort the entries for ascending addresses.
	 */
	kernelstart = trunc_page(kernelstart);
	kernelend = round_page(kernelend);
	pmap_remove_avail(kernelstart, kernelend);

	msgbuf_addr = pmap_steal_avail(MSGBUFSIZE,4);

#ifdef DEBUG
	for (mp = pmap_avail; mp->size; mp++) {
		bzero((void *)mp->start, mp->size);
	}
#endif

#define HTABENTS_32 1024
#define HTABENTS_64 2048

	if (ppc_proc_is_64b) { 
		pmap_ptab_cnt = HTABENTS_64;
		while (pmap_ptab_cnt * 2 < physmem)
			pmap_ptab_cnt <<= 1;
	} else {
		pmap_ptab_cnt = HTABENTS_32;
		while (HTABSIZE_32 < (ptoa(physmem) >> 7))
			pmap_ptab_cnt <<= 1;
	}
	/*
	 * allocate suitably aligned memory for HTAB
	 */
	if (ppc_proc_is_64b) {
		pmap_ptable64 = pmap_steal_avail(HTABMEMSZ_64, HTABMEMSZ_64);
		bzero((void *)pmap_ptable64, HTABMEMSZ_64);
		pmap_ptab_mask = pmap_ptab_cnt - 1;
	} else {
		pmap_ptable32 = pmap_steal_avail(HTABSIZE_32, HTABSIZE_32);
		bzero((void *)pmap_ptable32, HTABSIZE_32);
		pmap_ptab_mask = pmap_ptab_cnt - 1;
	}

	/* allocate v->p mappings for pmap_kernel() */
	for (i = 0; i < VP_SR_SIZE; i++) {
		pmap_kernel()->pm_vp[i] = NULL;
	}
	vp1 = pmap_steal_avail(sizeof (struct pmapvp), 4);
	bzero (vp1, sizeof(struct pmapvp));
	pmap_kernel()->pm_vp[PPC_KERNEL_SR] = vp1;
	for (i = 0; i < VP_IDX1_SIZE; i++) {
		vp2 = vp1->vp[i] = pmap_steal_avail(sizeof (struct pmapvp), 4);
		bzero (vp2, sizeof(struct pmapvp));
		for (k = 0; k < VP_IDX2_SIZE; k++) {
			struct pte_desc *pted;
			pted = pmap_steal_avail(sizeof (struct pte_desc), 4);
			bzero (pted, sizeof (struct pte_desc));
			vp2->vp[k] = pted;
		}
	}

	/*
	 * Initialize kernel pmap and hardware.
	 */
#if NPMAPS >= PPC_KERNEL_SEGMENT / 16
	usedsr[PPC_KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((PPC_KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
#endif
	for (i = 0; i < 16; i++)
		pmap_kernel()->pm_sr[i] = (PPC_KERNEL_SEG0 + i) | SR_NOEXEC;

	if (ppc_nobat) {
		vp1 = pmap_steal_avail(sizeof (struct pmapvp), 4);
		bzero (vp1, sizeof(struct pmapvp));
		pmap_kernel()->pm_vp[0] = vp1;
		for (i = 0; i < VP_IDX1_SIZE; i++) {
			vp2 = vp1->vp[i] =
			    pmap_steal_avail(sizeof (struct pmapvp), 4);
			bzero (vp2, sizeof(struct pmapvp));
			for (k = 0; k < VP_IDX2_SIZE; k++) {
				struct pte_desc *pted;
				pted = pmap_steal_avail(sizeof (struct pte_desc), 4);
				bzero (pted, sizeof (struct pte_desc));
				vp2->vp[k] = pted;
			}
		}

		/* first segment contains executable pages */
		pmap_kernel()->pm_exec[0]++;
		pmap_kernel()->pm_sr[0] &= ~SR_NOEXEC;
	} else {
		/*
		 * Setup fixed BAT registers.
		 *
		 * Note that we still run in real mode, and the BAT
		 * registers were cleared in cpu_bootstrap().
		 */
		battable[0].batl = BATL(0x00000000, BAT_M);
		if (physmem > atop(0x08000000))
			battable[0].batu = BATU(0x00000000, BAT_BL_256M);
		else
			battable[0].batu = BATU(0x00000000, BAT_BL_128M);

		/* Map physical memory with BATs. */
		if (physmem > atop(0x10000000)) {
			battable[0x1].batl = BATL(0x10000000, BAT_M);
			battable[0x1].batu = BATU(0x10000000, BAT_BL_256M);
		}
		if (physmem > atop(0x20000000)) {
			battable[0x2].batl = BATL(0x20000000, BAT_M);
			battable[0x2].batu = BATU(0x20000000, BAT_BL_256M);
		}
		if (physmem > atop(0x30000000)) {
			battable[0x3].batl = BATL(0x30000000, BAT_M);
			battable[0x3].batu = BATU(0x30000000, BAT_BL_256M);
		}
		if (physmem > atop(0x40000000)) {
			battable[0x4].batl = BATL(0x40000000, BAT_M);
			battable[0x4].batu = BATU(0x40000000, BAT_BL_256M);
		}
		if (physmem > atop(0x50000000)) {
			battable[0x5].batl = BATL(0x50000000, BAT_M);
			battable[0x5].batu = BATU(0x50000000, BAT_BL_256M);
		}
		if (physmem > atop(0x60000000)) {
			battable[0x6].batl = BATL(0x60000000, BAT_M);
			battable[0x6].batu = BATU(0x60000000, BAT_BL_256M);
		}
		if (physmem > atop(0x70000000)) {
			battable[0x7].batl = BATL(0x70000000, BAT_M);
			battable[0x7].batu = BATU(0x70000000, BAT_BL_256M);
		}
	}

	ppc_kvm_stolen += reserve_dumppages( (caddr_t)(VM_MIN_KERNEL_ADDRESS +
	    ppc_kvm_stolen));

	pmap_avail_fixup();
	for (mp = pmap_avail; mp->size; mp++) {
		if (mp->start > 0x80000000)
			continue;
		if (mp->start + mp->size > 0x80000000)
			mp->size = 0x80000000 - mp->start;
		uvm_page_physload(atop(mp->start), atop(mp->start+mp->size),
		    atop(mp->start), atop(mp->start+mp->size), 0);
	}
}

void
pmap_enable_mmu(void)
{
	uint32_t scratch, sdr1;
	int i;

	if (!ppc_nobat) {
		/* DBAT0 used for initial segment */
		ppc_mtdbat0l(battable[0].batl);
		ppc_mtdbat0u(battable[0].batu);

		/* IBAT0 only covering the kernel .text */
		ppc_mtibat0l(battable[0].batl);
		ppc_mtibat0u(BATU(0x00000000, BAT_BL_8M));
	}

	for (i = 0; i < 16; i++)
		ppc_mtsrin(PPC_KERNEL_SEG0 + i, i << ADDR_SR_SHIFT);

	if (ppc_proc_is_64b)
		sdr1 = (uint32_t)pmap_ptable64 | HTABSIZE_64;
	else
		sdr1 = (uint32_t)pmap_ptable32 | (pmap_ptab_mask >> 10);

	asm volatile ("sync; mtsdr1 %0; isync" :: "r"(sdr1));
	tlbia();

	asm volatile ("eieio; mfmsr %0; ori %0,%0,%1; mtmsr %0; sync; isync"
	    : "=r"(scratch) : "K"(PSL_IR|PSL_DR|PSL_ME|PSL_RI));
}

/*
 * activate a pmap entry
 * NOOP on powerpc, all PTE entries exist in the same hash table.
 * Segment registers are filled on exit to user mode.
 */
void
pmap_activate(struct proc *p)
{
}

/*
 * deactivate a pmap entry
 * NOOP on powerpc
 */
void
pmap_deactivate(struct proc *p)
{
}

/*
 * pmap_extract: extract a PA for the given VA
 */

boolean_t
pmap_extract(pmap_t pm, vaddr_t va, paddr_t *pa)
{
	struct pte_desc *pted;

	if (pm == pmap_kernel() && va < physmaxaddr) {
		*pa = va;
		return TRUE;
	}

	PMAP_VP_LOCK(pm);
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted)) {
		PMAP_VP_UNLOCK(pm);
		return FALSE;
	}

	if (ppc_proc_is_64b)
		*pa = (pted->p.pted_pte64.pte_lo & PTE_RPGN_64) |
		    (va & ~PTE_RPGN_64);
	else
		*pa = (pted->p.pted_pte32.pte_lo & PTE_RPGN_32) |
		    (va & ~PTE_RPGN_32);

	PMAP_VP_UNLOCK(pm);
	return TRUE;
}

u_int32_t
pmap_setusr(pmap_t pm, vaddr_t va)
{
	u_int32_t sr;
	u_int32_t oldsr;

	sr = ptesr(pm->pm_sr, va);

	/* user address range lock?? */
	asm volatile ("mfsr %0,%1" : "=r" (oldsr): "n"(PPC_USER_SR));
	asm volatile ("isync; mtsr %0,%1; isync" :: "n"(PPC_USER_SR), "r"(sr));
	return oldsr;
}

void
pmap_popusr(u_int32_t sr)
{
	asm volatile ("isync; mtsr %0,%1; isync"
	    :: "n"(PPC_USER_SR), "r"(sr));
}

int
copyin(const void *udaddr, void *kaddr, size_t len)
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = PPC_USER_ADDR + ((u_int)udaddr & ~PPC_SEGMENT_MASK);
		l = (PPC_USER_ADDR + PPC_SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(&env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}
		bcopy(p, kaddr, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
}

int
copyout(const void *kaddr, void *udaddr, size_t len)
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = PPC_USER_ADDR + ((u_int)udaddr & ~PPC_SEGMENT_MASK);
		l = (PPC_USER_ADDR + PPC_SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(&env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}

		bcopy(kaddr, p, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
}

int
copyin32(const uint32_t *udaddr, uint32_t *kaddr)
{
	volatile uint32_t *p;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	if ((u_int)udaddr & 0x3)
		return EFAULT;

	p = PPC_USER_ADDR + ((u_int)udaddr & ~PPC_SEGMENT_MASK);
	oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
	if (setfault(&env)) {
		pmap_popusr(oldsr);
		curpcb->pcb_onfault = oldh;
		return EFAULT;
	}
	*kaddr = *p;
	pmap_popusr(oldsr);
	curpcb->pcb_onfault = oldh;
	return 0;
}

int
copyinstr(const void *udaddr, void *kaddr, size_t len, size_t *done)
{
	const u_char *uaddr = udaddr;
	u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = PPC_USER_ADDR + ((u_int)uaddr & ~PPC_SEGMENT_MASK);
		l = (PPC_USER_ADDR + PPC_SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(&env)) {
			if (done != NULL)
				*done =  cnt;

			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *up;
			*kp = c;
			if (c == 0) {
				if (done != NULL)
					*done = cnt + 1;

				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
		}
		pmap_popusr(oldsr);
	}
	curpcb->pcb_onfault = oldh;
	if (done != NULL)
		*done = cnt;

	return ENAMETOOLONG;
}

int
copyoutstr(const void *kaddr, void *udaddr, size_t len, size_t *done)
{
	u_char *uaddr = (void *)udaddr;
	const u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = PPC_USER_ADDR + ((u_int)uaddr & ~PPC_SEGMENT_MASK);
		l = (PPC_USER_ADDR + PPC_SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(&env)) {
			if (done != NULL)
				*done =  cnt;

			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *kp;
			*up = c;
			if (c == 0) {
				if (done != NULL)
					*done = cnt + 1;

				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
		}
		pmap_popusr(oldsr);
	}
	curpcb->pcb_onfault = oldh;
	if (done != NULL)
		*done = cnt;

	return ENAMETOOLONG;
}

/*
 * sync instruction cache for user virtual address.
 * The address WAS JUST MAPPED, so we have a VALID USERSPACE mapping
 */
void
pmap_syncicache_user_virt(pmap_t pm, vaddr_t va)
{
	vaddr_t start;
	int oldsr;

	if (pm != pmap_kernel()) {
		start = ((u_int)PPC_USER_ADDR + ((u_int)va &
		    ~PPC_SEGMENT_MASK));
		/* will only ever be page size, will not cross segments */

		/* USER SEGMENT LOCK - MPXXX */
		oldsr = pmap_setusr(pm, va);
	} else {
		start = va; /* flush mapped page */
	}

	syncicache((void *)start, PAGE_SIZE);

	if (pm != pmap_kernel()) {
		pmap_popusr(oldsr);
		/* USER SEGMENT UNLOCK -MPXXX */
	}
}

void
pmap_pted_ro(struct pte_desc *pted, vm_prot_t prot)
{
	if (ppc_proc_is_64b)
		pmap_pted_ro64(pted, prot);
	else
		pmap_pted_ro32(pted, prot);
}

void
pmap_pted_ro64(struct pte_desc *pted, vm_prot_t prot)
{
	pmap_t pm = pted->pted_pmap;
	vaddr_t va = pted->pted_va & ~PAGE_MASK;
	struct vm_page *pg;
	void *pte;
	int s;

	pg = PHYS_TO_VM_PAGE(pted->p.pted_pte64.pte_lo & PTE_RPGN_64);
	if (pg->pg_flags & PG_PMAP_EXE) {
		if ((prot & (PROT_WRITE | PROT_EXEC)) == PROT_WRITE) {
			atomic_clearbits_int(&pg->pg_flags, PG_PMAP_EXE);
		} else {
			pmap_syncicache_user_virt(pm, va);
		}
	}

	pted->p.pted_pte64.pte_lo &= ~PTE_PP_64;
	pted->p.pted_pte64.pte_lo |= PTE_RO_64;

	if ((prot & PROT_EXEC) == 0)
		pted->p.pted_pte64.pte_lo |= PTE_N_64;

	PMAP_HASH_LOCK(s);
	if ((pte = pmap_ptedinhash(pted)) != NULL) {
		struct pte_64 *ptp64 = pte;

		pte_del(ptp64, va);

		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(ptp64->pte_lo & PTE_RPGN_64,
			    ptp64->pte_lo & (PTE_REF_64|PTE_CHG_64));
		}

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp64->pte_lo &= ~(PTE_CHG_64|PTE_PP_64);
		ptp64->pte_lo |= PTE_RO_64;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp64->pte_hi |= PTE_VALID_64;
		sync();		/* Ensure updates completed. */
	}
	PMAP_HASH_UNLOCK(s);
}

void
pmap_pted_ro32(struct pte_desc *pted, vm_prot_t prot)
{
	pmap_t pm = pted->pted_pmap;
	vaddr_t va = pted->pted_va & ~PAGE_MASK;
	struct vm_page *pg;
	void *pte;
	int s;

	pg = PHYS_TO_VM_PAGE(pted->p.pted_pte32.pte_lo & PTE_RPGN_32);
	if (pg->pg_flags & PG_PMAP_EXE) {
		if ((prot & (PROT_WRITE | PROT_EXEC)) == PROT_WRITE) {
			atomic_clearbits_int(&pg->pg_flags, PG_PMAP_EXE);
		} else {
			pmap_syncicache_user_virt(pm, va);
		}
	}

	pted->p.pted_pte32.pte_lo &= ~PTE_PP_32;
	pted->p.pted_pte32.pte_lo |= PTE_RO_32;

	PMAP_HASH_LOCK(s);
	if ((pte = pmap_ptedinhash(pted)) != NULL) {
		struct pte_32 *ptp32 = pte;

		pte_del(ptp32, va);

		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(ptp32->pte_lo & PTE_RPGN_32,
			    ptp32->pte_lo & (PTE_REF_32|PTE_CHG_32));
		}

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp32->pte_lo &= ~(PTE_CHG_32|PTE_PP_32);
		ptp32->pte_lo |= PTE_RO_32;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp32->pte_hi |= PTE_VALID_32;
		sync();		/* Ensure updates completed. */
	}
	PMAP_HASH_UNLOCK(s);
}

/*
 * Lower the protection on the specified physical page.
 *
 * There are only two cases, either the protection is going to 0,
 * or it is going to read-only.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	struct pte_desc *pted;
	pmap_t pm;

	if (prot == PROT_NONE) {
		mtx_enter(&pg->mdpage.pv_mtx);
		while ((pted = LIST_FIRST(&(pg->mdpage.pv_list))) != NULL) {
			pmap_reference(pted->pted_pmap);
			pm = pted->pted_pmap;
			mtx_leave(&pg->mdpage.pv_mtx);

			PMAP_VP_LOCK(pm);

			/*
			 * We dropped the pvlist lock before grabbing
			 * the pmap lock to avoid lock ordering
			 * problems.  This means we have to check the
			 * pvlist again since somebody else might have
			 * modified it.  All we care about is that the
			 * pvlist entry matches the pmap we just
			 * locked.  If it doesn't, unlock the pmap and
			 * try again.
			 */
			mtx_enter(&pg->mdpage.pv_mtx);
			if ((pted = LIST_FIRST(&(pg->mdpage.pv_list))) == NULL ||
			    pted->pted_pmap != pm) {
				mtx_leave(&pg->mdpage.pv_mtx);
				PMAP_VP_UNLOCK(pm);
				pmap_destroy(pm);
				mtx_enter(&pg->mdpage.pv_mtx);
				continue;
			}

			pted->pted_va &= ~PTED_VA_MANAGED_M;
			LIST_REMOVE(pted, pted_pv_list);
			mtx_leave(&pg->mdpage.pv_mtx);

			pmap_remove_pted(pm, pted);

			PMAP_VP_UNLOCK(pm);
			pmap_destroy(pm);
			mtx_enter(&pg->mdpage.pv_mtx);
		}
		mtx_leave(&pg->mdpage.pv_mtx);
		/* page is being reclaimed, sync icache next use */
		atomic_clearbits_int(&pg->pg_flags, PG_PMAP_EXE);
		return;
	}

	mtx_enter(&pg->mdpage.pv_mtx);
	LIST_FOREACH(pted, &(pg->mdpage.pv_list), pted_pv_list)
		pmap_pted_ro(pted, prot);
	mtx_leave(&pg->mdpage.pv_mtx);
}

void
pmap_protect(pmap_t pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	if (prot & (PROT_READ | PROT_EXEC)) {
		struct pte_desc *pted;

		PMAP_VP_LOCK(pm);
		while (sva < eva) {
			pted = pmap_vp_lookup(pm, sva);
			if (pted && PTED_VALID(pted))
				pmap_pted_ro(pted, prot);
			sva += PAGE_SIZE;
		}
		PMAP_VP_UNLOCK(pm);
		return;
	}
	pmap_remove(pm, sva, eva);
}

/*
 * Restrict given range to physical memory
 */
void
pmap_real_memory(paddr_t *start, vsize_t *size)
{
	struct mem_region *mp;

	for (mp = pmap_mem; mp->size; mp++) {
		if (((*start + *size) > mp->start)
			&& (*start < (mp->start + mp->size)))
		{
			if (*start < mp->start) {
				*size -= mp->start - *start;
				*start = mp->start;
			}
			if ((*start + *size) > (mp->start + mp->size))
				*size = mp->start + mp->size - *start;
			return;
		}
	}
	*size = 0;
}

void
pmap_init()
{
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmap", NULL);
	pool_setlowat(&pmap_pmap_pool, 2);
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, IPL_VM, 0,
	    "vp", &pool_allocator_single);
	pool_setlowat(&pmap_vp_pool, 10);
	pool_init(&pmap_pted_pool, sizeof(struct pte_desc), 0, IPL_VM, 0,
	    "pted", NULL);
	pool_setlowat(&pmap_pted_pool, 20);

	PMAP_HASH_LOCK_INIT();

	pmap_initialized = 1;
}

void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
	paddr_t pa;
	vsize_t clen;

	while (len > 0) {
		/* add one to always round up to the next page */
		clen = round_page(va + 1) - va;
		if (clen > len)
			clen = len;

		if (pmap_extract(pr->ps_vmspace->vm_map.pmap, va, &pa)) {
			syncicache((void *)pa, clen);
		}

		len -= clen;
		va += clen;
	}
}

/* 
 * There are two routines, pte_spill_r and pte_spill_v
 * the _r version only handles kernel faults which are not user
 * accesses. The _v version handles all user faults and kernel copyin/copyout
 * "user" accesses.
 */
int
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr, int exec_fault)
{
	pmap_t pm;
	struct pte_desc *pted;
	struct pte_desc pted_store;

	/* lookup is done physical to prevent faults */

	/* 
	 * This function only handles kernel faults, not supervisor copyins.
	 */
	if (msr & PSL_PR)
		return 0;

	/* if copyin, throw to full excption handler */
	if (VP_SR(va) == PPC_USER_SR)
		return 0;

	pm = pmap_kernel();

	/* 0 - physmaxaddr mapped 1-1 */
	if (va < physmaxaddr) {
		u_int32_t aligned_va;
		vm_prot_t prot = PROT_READ | PROT_WRITE;
		extern caddr_t kernel_text;
		extern caddr_t etext;

		pted = &pted_store;

		if (va >= trunc_page((vaddr_t)&kernel_text) &&
		    va < round_page((vaddr_t)&etext)) {
			prot |= PROT_EXEC;
		}

		aligned_va = trunc_page(va);
		if (ppc_proc_is_64b) {
			pmap_fill_pte64(pm, aligned_va, aligned_va,
			    pted, prot, PMAP_CACHE_WB);
			pte_insert64(pted);
		} else {
			pmap_fill_pte32(pm, aligned_va, aligned_va,
			    pted, prot, PMAP_CACHE_WB);
			pte_insert32(pted);
		}
		return 1;
	}

	return pte_spill_v(pm, va, dsisr, exec_fault);
}

int
pte_spill_v(pmap_t pm, u_int32_t va, u_int32_t dsisr, int exec_fault)
{
	struct pte_desc *pted;
	int inserted = 0;

	/*
	 * If the current mapping is RO and the access was a write
	 * we return 0
	 */
	PMAP_VP_LOCK(pm);
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted))
		goto out;

	/* Attempted to write a read-only page. */
	if (dsisr & DSISR_STORE) {
		if (ppc_proc_is_64b) {
			if (pted->p.pted_pte64.pte_lo & PTE_RO_64)
				goto out;
		} else {
			if (pted->p.pted_pte32.pte_lo & PTE_RO_32)
				goto out;
		}
	}

	/* Attempted to execute non-executable page. */
	if ((exec_fault != 0) && ((pted->pted_va & PTED_VA_EXEC_M) == 0))
		goto out;

	inserted = 1;
	if (ppc_proc_is_64b)
		pte_insert64(pted);
	else
		pte_insert32(pted);

out:
	PMAP_VP_UNLOCK(pm);
	return (inserted);
}


/*
 * should pte_insert code avoid wired mappings?
 * is the stack safe?
 * is the pted safe? (physical)
 * -ugh
 */
void
pte_insert64(struct pte_desc *pted)
{
	struct pte_64 *ptp64;
	int off, secondary;
	int sr, idx, i;
	void *pte;
	int s;

	PMAP_HASH_LOCK(s);
	if ((pte = pmap_ptedinhash(pted)) != NULL)
		pte_zap(pte, pted);

	pted->pted_va &= ~(PTED_VA_HID_M|PTED_VA_PTEGIDX_M);

	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

	/*
	 * instead of starting at the beginning of each pteg,
	 * the code should pick a random location with in the primary
	 * then search all of the entries, then if not yet found,
	 * do the same for the secondary.
	 * this would reduce the frontloading of the pteg.
	 */

	/* first just try fill of primary hash */
	ptp64 = pmap_ptable64 + (idx) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp64[i].pte_hi & PTE_VALID_64)
			continue;

		pted->pted_va |= i;

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp64[i].pte_hi = pted->p.pted_pte64.pte_hi & ~PTE_VALID_64;
		ptp64[i].pte_lo = pted->p.pted_pte64.pte_lo;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp64[i].pte_hi |= PTE_VALID_64;
		sync();		/* Ensure updates completed. */

		goto out;
	}

	/* try fill of secondary hash */
	ptp64 = pmap_ptable64 + (idx ^ pmap_ptab_mask) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp64[i].pte_hi & PTE_VALID_64)
			continue;

		pted->pted_va |= (i | PTED_VA_HID_M);

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp64[i].pte_hi = pted->p.pted_pte64.pte_hi & ~PTE_VALID_64;
		ptp64[i].pte_lo = pted->p.pted_pte64.pte_lo;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp64[i].pte_hi |= (PTE_HID_64|PTE_VALID_64);
		sync();		/* Ensure updates completed. */

		goto out;
	}

	/* need decent replacement algorithm */
	off = ppc_mftb();
	secondary = off & 8;


	pted->pted_va |= off & (PTED_VA_PTEGIDX_M|PTED_VA_HID_M);

	idx = (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));

	ptp64 = pmap_ptable64 + (idx * 8);
	ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */

	if (ptp64->pte_hi & PTE_VALID_64) {
		vaddr_t va;

		/* Bits 9-19 */
		idx = (idx ^ ((ptp64->pte_hi & PTE_HID_64) ?
		    pmap_ptab_mask : 0));
		va = (ptp64->pte_hi >> PTE_VSID_SHIFT_64) ^ idx;
		va <<= ADDR_PIDX_SHIFT;
		/* Bits 4-8 */
		va |= (ptp64->pte_hi & PTE_API_64) << ADDR_API_SHIFT_32;
		/* Bits 0-3 */
		va |= (ptp64->pte_hi >> PTE_VSID_SHIFT_64)
		    << ADDR_SR_SHIFT;

		pte_del(ptp64, va);

		pmap_attr_save(ptp64->pte_lo & PTE_RPGN_64,
		    ptp64->pte_lo & (PTE_REF_64|PTE_CHG_64));
	}

	/* Add a Page Table Entry, section 7.6.3.1. */
	ptp64->pte_hi = pted->p.pted_pte64.pte_hi & ~PTE_VALID_64;
	if (secondary)
		ptp64->pte_hi |= PTE_HID_64;
	ptp64->pte_lo = pted->p.pted_pte64.pte_lo;
	eieio();	/* Order 1st PTE update before 2nd. */
	ptp64->pte_hi |= PTE_VALID_64;
	sync();		/* Ensure updates completed. */

out:
	PMAP_HASH_UNLOCK(s);
}

void
pte_insert32(struct pte_desc *pted)
{
	struct pte_32 *ptp32;
	int off, secondary;
	int sr, idx, i;
	void *pte;
	int s;

	PMAP_HASH_LOCK(s);
	if ((pte = pmap_ptedinhash(pted)) != NULL)
		pte_zap(pte, pted);

	pted->pted_va &= ~(PTED_VA_HID_M|PTED_VA_PTEGIDX_M);

	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

	/*
	 * instead of starting at the beginning of each pteg,
	 * the code should pick a random location with in the primary
	 * then search all of the entries, then if not yet found,
	 * do the same for the secondary.
	 * this would reduce the frontloading of the pteg.
	 */

	/* first just try fill of primary hash */
	ptp32 = pmap_ptable32 + (idx) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp32[i].pte_hi & PTE_VALID_32)
			continue;

		pted->pted_va |= i;

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp32[i].pte_hi = pted->p.pted_pte32.pte_hi & ~PTE_VALID_32;
		ptp32[i].pte_lo = pted->p.pted_pte32.pte_lo;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp32[i].pte_hi |= PTE_VALID_32;
		sync();		/* Ensure updates completed. */

		goto out;
	}

	/* try fill of secondary hash */
	ptp32 = pmap_ptable32 + (idx ^ pmap_ptab_mask) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp32[i].pte_hi & PTE_VALID_32)
			continue;

		pted->pted_va |= (i | PTED_VA_HID_M);

		/* Add a Page Table Entry, section 7.6.3.1. */
		ptp32[i].pte_hi = pted->p.pted_pte32.pte_hi & ~PTE_VALID_32;
		ptp32[i].pte_lo = pted->p.pted_pte32.pte_lo;
		eieio();	/* Order 1st PTE update before 2nd. */
		ptp32[i].pte_hi |= (PTE_HID_32|PTE_VALID_32);
		sync();		/* Ensure updates completed. */

		goto out;
	}

	/* need decent replacement algorithm */
	off = ppc_mftb();
	secondary = off & 8;

	pted->pted_va |= off & (PTED_VA_PTEGIDX_M|PTED_VA_HID_M);

	idx = (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));

	ptp32 = pmap_ptable32 + (idx * 8);
	ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */

	if (ptp32->pte_hi & PTE_VALID_32) {
		vaddr_t va;

		va = ((ptp32->pte_hi & PTE_API_32) << ADDR_API_SHIFT_32) |
		     ((((ptp32->pte_hi >> PTE_VSID_SHIFT_32) & SR_VSID)
			^(idx ^ ((ptp32->pte_hi & PTE_HID_32) ? 0x3ff : 0)))
			    & 0x3ff) << PAGE_SHIFT;

		pte_del(ptp32, va);

		pmap_attr_save(ptp32->pte_lo & PTE_RPGN_32,
		    ptp32->pte_lo & (PTE_REF_32|PTE_CHG_32));
	}

	/* Add a Page Table Entry, section 7.6.3.1. */
	ptp32->pte_hi = pted->p.pted_pte32.pte_hi & ~PTE_VALID_32;
	if (secondary)
		ptp32->pte_hi |= PTE_HID_32;
	ptp32->pte_lo = pted->p.pted_pte32.pte_lo;
	eieio();	/* Order 1st PTE update before 2nd. */
	ptp32->pte_hi |= PTE_VALID_32;
	sync();		/* Ensure updates completed. */

out:
	PMAP_HASH_UNLOCK(s);
}
@


1.166
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.165 2016/09/15 02:00:17 dlg Exp $ */
d1795 24
@


1.165
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.164 2016/06/07 06:23:19 dlg Exp $ */
d2161 1
a2161 1
pmap_proc_iflush(struct proc *p, vaddr_t addr, vsize_t len)
d2168 1
a2168 1
		clen = round_page(addr + 1) - addr;
d2172 1
a2172 1
		if (pmap_extract(p->p_vmspace->vm_map.pmap, addr, &pa)) {
d2177 1
a2177 1
		addr += clen;
@


1.164
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.163 2015/10/08 10:20:14 kettenis Exp $ */
d2145 1
a2145 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
a2146 1
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
d2148 2
a2149 3
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, 0, 0, "vp",
	    &pool_allocator_single);
	pool_setipl(&pmap_vp_pool, IPL_VM);
d2151 2
a2152 3
	pool_init(&pmap_pted_pool, sizeof(struct pte_desc), 0, 0, 0, "pted",
	    NULL);
	pool_setipl(&pmap_pted_pool, IPL_VM);
@


1.163
log
@Add a per-page flag to indicate that all mappings of that page should be
uncached.  To be used in the drm code.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.162 2015/09/11 22:02:18 kettenis Exp $ */
d2145 3
a2147 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmap", NULL);
d2151 1
d2155 1
@


1.162
log
@Make the powerpc pmap (more) mpsafe by protecting both the pmap itself and the
pv lists with a mutex.  This should make pmap_enter(9), pmap_remove(9) and
pmap_page_protect(9) safe to use without holding the kernel lock.

ok visa@@, mpi@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.161 2015/09/08 21:28:36 kettenis Exp $ */
d585 2
@


1.161
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.160 2015/07/20 00:16:16 kettenis Exp $ */
d492 1
d495 1
d501 8
d511 1
a655 1
	KERNEL_LOCK();
a662 1
	KERNEL_UNLOCK();
d975 1
d994 1
d1000 1
d1014 1
d1046 1
d1165 1
a1165 1
	pm->pm_refs++;
d1177 1
a1177 1
	refs = --pm->pm_refs;
d2047 1
d2049 1
d2051 2
d2054 25
d2080 1
d2082 2
d2085 1
d2091 1
d2094 1
@


1.160
log
@Make pmap_remove() grab the kernel lock.  This is a big hammer but makes MP
machines work again with the unlocked reaper.

ok mpi@@, deraadt@@
no objection from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.159 2015/06/05 10:15:54 mpi Exp $ */
d2097 1
a2097 1
	    &pool_allocator_nointr);
@


1.159
log
@Finally protect VP lookups to guarantee that a pted won't be freed or
reused by a CPU while another CPU is manipulating it.

This races occurs because the virtual spill handlers are run without
taking the KERNEL_LOCK for obvious reasons.  So use a per-pmap mutex
that CPUs must hold when modifying a pted in order to guarantee the
atomicity of operations *and* the coherence between pmap VPs tree and
what's in the HASH.

Thanks to dlg@@ for assisting me debugging this.  This change ends your
PowerPC pmap SMP show of the week.  GENERIC.MP on macppc should now be
stable enough to build ports without corrupting its own memory.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.158 2015/06/05 10:06:35 mpi Exp $ */
d645 1
d653 1
@


1.158
log
@Don't try to be clever when unrolling the loop in pmap_remove().

Needed for upcoming locking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.157 2015/06/05 10:04:34 mpi Exp $ */
d204 20
a225 3
#define	PMAP_HASH_ASSERT_LOCKED()	/* nothing */
#define	PMAP_HASH_ASSERT_UNLOCKED()	/* nothing */

a226 1

d230 4
d282 2
d309 2
d339 2
d540 1
a540 2
	int cache;
	int error;
d545 1
d561 2
a562 1
				return ENOMEM;
d569 1
a569 1
			return error;
d631 3
a633 1
	return 0;
d645 1
d651 1
d664 1
d1100 2
a1101 2
void
pmap_pinit(pmap_t pm)
d1105 1
d1107 1
a1107 1
	bzero(pm, sizeof (struct pmap));
d1110 1
d1137 1
a1137 1
			return;
a1142 13
/* 
 * Create and return a physical map.
 */
pmap_t 
pmap_create()
{
	pmap_t pmap;

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
	pmap_pinit(pmap);
	return (pmap);
}

d1681 1
d1683 2
a1684 1
	if (pted == NULL || !PTED_VALID(pted))
d1686 1
d1694 2
d2028 1
d2031 5
a2035 3
		while (!LIST_EMPTY(&(pg->mdpage.pv_list))) {
			pted = LIST_FIRST(&(pg->mdpage.pv_list));
			pmap_remove_pted(pted->pted_pmap, pted);
d2052 1
d2059 1
d2188 1
d2194 1
d2197 1
a2197 1
		return 0;
d2203 1
a2203 1
				return 0;
d2206 1
a2206 1
				return 0;
d2212 1
a2212 1
		return 0;
d2214 1
d2220 3
a2222 1
	return 1;
@


1.157
log
@Replace the per-entry locks by a global HASH lock.

Since this lock is recursive we can now guarantee the atomicity of
pte_inser{32,64}() when a pted has to be removed first.  This fixes
one of the races.

Using a __mp_lock here also allowed dlg@@ to provide me useful traces
to fix the next race.  Thanks for your help!

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.156 2015/06/05 09:53:40 mpi Exp $ */
d607 1
a607 1
/* 
d611 1
a611 1
pmap_remove(pmap_t pm, vaddr_t va, vaddr_t endva)
a612 5
	int i_sr, s_sr, e_sr;
	int i_vp1, s_vp1, e_vp1;
	int i_vp2, s_vp2, e_vp2;
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d614 1
d616 4
a619 43
	/* I suspect that if this loop were unrolled better 
	 * it would have better performance, testing i_sr and i_vp1
	 * in the middle loop seems excessive
	 */

	s_sr = VP_SR(va);
	e_sr = VP_SR(endva);
	for (i_sr = s_sr; i_sr <= e_sr; i_sr++) {
		vp1 = pm->pm_vp[i_sr];
		if (vp1 == NULL)
			continue;

		if (i_sr == s_sr)
			s_vp1 = VP_IDX1(va);
		else
			s_vp1 = 0;

		if (i_sr == e_sr)
			e_vp1 = VP_IDX1(endva);
		else
			e_vp1 = VP_IDX1_SIZE-1; 

		for (i_vp1 = s_vp1; i_vp1 <= e_vp1; i_vp1++) {
			vp2 = vp1->vp[i_vp1];
			if (vp2 == NULL)
				continue;

			if ((i_sr == s_sr) && (i_vp1 == s_vp1))
				s_vp2 = VP_IDX2(va);
			else
				s_vp2 = 0;

			if ((i_sr == e_sr) && (i_vp1 == e_vp1))
				e_vp2 = VP_IDX2(endva);
			else
				e_vp2 = VP_IDX2_SIZE;

			for (i_vp2 = s_vp2; i_vp2 < e_vp2; i_vp2++) {
				pted = vp2->vp[i_vp2];
				if (pted && PTED_VALID(pted))
					pmap_remove_pted(pm, pted);
			}
		}
d622 1
@


1.156
log
@Call pte_spill_v() from the real mode fault handler instead of rerolling
it.  This will reduce the number of places to audit for locking.

Note that for profiling purposes pte_spill_v() is now marked a __noprof
since per-CPU profiling buffers are not guaranteed to be 1:1 mapped and
cannot be accessed from the real mode fault handler.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.155 2015/06/05 09:48:01 mpi Exp $ */
d147 1
a147 1
void pmap_hash_remove(struct pte_desc *);
d188 1
a188 4
void pmap_hash_lock_init(void);
void pmap_hash_lock(int entry);
void pmap_hash_unlock(int entry)  __noprof;
int pmap_hash_lock_try(int entry) __noprof;
d190 1
a190 1
volatile unsigned int pmap_hash_lock_word = 0;
d192 13
a204 5
void
pmap_hash_lock_init()
{
	pmap_hash_lock_word = 0;
}
d206 2
a207 20
int
pmap_hash_lock_try(int entry)
{
	int val = 1 << entry;
	int success, tmp;
	__asm volatile (
	    "1: lwarx	%0, 0, %3	\n"
	    "	and.	%1, %2, %0	\n"
	    "	li	%1, 0		\n"
	    "	bne 2f			\n"
	    "	or	%0, %2, %0	\n"
	    "	stwcx.  %0, 0, %3	\n"
	    "	li	%1, 1		\n"
	    "	bne-	1b		\n"
	    "2:				\n"
	    : "=&r" (tmp), "=&r" (success)
	    : "r" (val), "r" (&pmap_hash_lock_word)
	    : "memory");
	return success;
}
d209 1
d211 2
a212 16
void
pmap_hash_lock(int entry)
{
	int attempt = 0;
	int locked = 0;
	do {
		if (pmap_hash_lock_word & (1 << entry)) {
			attempt++;
			if(attempt >0x20000000)
				panic("unable to obtain lock on entry %d",
				    entry);
			continue;
		}
		locked = pmap_hash_lock_try(entry);
	} while (locked == 0);
}
a213 5
void
pmap_hash_unlock(int entry)
{
	atomic_clearbits_int(&pmap_hash_lock_word,  1 << entry);
}
d671 3
d678 4
a681 1
	pmap_hash_remove(pted);
a880 26
 * remove specified entry from hash table.
 * all information is present in pted to look up entry
 */
void
pmap_hash_remove(struct pte_desc *pted)
{
	void *pte;
#ifdef MULTIPROCESSOR
	int s, i = PTED_PTEGIDX(pted);
#endif

#ifdef MULTIPROCESSOR
	s = ppc_intr_disable();
	pmap_hash_lock(i);
#endif

	if ((pte = pmap_ptedinhash(pted)) != NULL)
		pte_zap(pte, pted);

#ifdef MULTIPROCESSOR
	pmap_hash_unlock(i);
	ppc_intr_enable(s);
#endif
}

/*
a964 1
#ifdef MULTIPROCESSOR
a965 1
#endif
d977 1
a977 4
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		pmap_hash_lock(PTED_PTEGIDX(pted));
#endif
d987 2
a988 4
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(PTED_PTEGIDX(pted));
		ppc_intr_enable(s);
#endif
a1002 1
#ifdef MULTIPROCESSOR
a1003 1
#endif
d1013 1
a1013 4
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		pmap_hash_lock(PTED_PTEGIDX(pted));
#endif
d1039 1
a1039 4
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(PTED_PTEGIDX(pted));
		ppc_intr_enable(s);
#endif
a1953 1
#ifdef MULTIPROCESSOR
a1954 1
#endif
d1971 1
a1971 4
#ifdef MULTIPROCESSOR
	s = ppc_intr_disable();
	pmap_hash_lock(PTED_PTEGIDX(pted));
#endif
d1989 1
a1989 4
#ifdef MULTIPROCESSOR
	pmap_hash_unlock(PTED_PTEGIDX(pted));
	ppc_intr_enable(s);
#endif
a1998 1
#ifdef MULTIPROCESSOR
a1999 1
#endif
d2013 1
a2013 4
#ifdef MULTIPROCESSOR
	s = ppc_intr_disable();
	pmap_hash_lock(PTED_PTEGIDX(pted));
#endif
d2031 1
a2031 4
#ifdef MULTIPROCESSOR
	pmap_hash_unlock(PTED_PTEGIDX(pted));
	ppc_intr_enable(s);
#endif
d2112 2
a2244 1
#ifdef MULTIPROCESSOR
a2245 1
#endif
d2247 1
a2247 4
	/*
	 * Note that we do not hold the HASH lock for pted here to
	 * handle multiple faults.
	 */
d2249 1
a2249 1
		pmap_hash_remove(pted);
d2263 1
d2269 1
a2269 7
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		if (pmap_hash_lock_try(i) == 0) {
			ppc_intr_enable(s);
			continue;
		}
#endif
d2279 1
a2279 5
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(i);
		ppc_intr_enable(s);
#endif
		return;
d2281 1
a2286 7
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		if (pmap_hash_lock_try(i) == 0) {
			ppc_intr_enable(s);
			continue;
		}
#endif
d2297 1
a2297 5
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(i);
		ppc_intr_enable(s);
#endif
		return;
a2299 3
#ifdef MULTIPROCESSOR
busy:
#endif
a2303 7
#ifdef MULTIPROCESSOR
	s = ppc_intr_disable();
	if (pmap_hash_lock_try(off & 7) == 0) {
		ppc_intr_enable(s);
		goto busy;
	}
#endif
d2341 2
a2342 4
#ifdef MULTIPROCESSOR
	pmap_hash_unlock(off & 7);
	ppc_intr_enable(s);
#endif
a2351 1
#ifdef MULTIPROCESSOR
a2352 1
#endif
d2354 1
a2354 4
	/*
	 * Note that we do not hold the HASH lock for pted here to
	 * handle multiple faults.
	 */
d2356 1
a2356 1
		pmap_hash_remove(pted);
d2370 1
a2375 7
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		if (pmap_hash_lock_try(i) == 0) {
			ppc_intr_enable(s);
			continue;
		}
#endif
d2386 1
a2386 5
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(i);
		ppc_intr_enable(s);
#endif
		return;
d2388 1
a2393 7
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		if (pmap_hash_lock_try(i) == 0) {
			ppc_intr_enable(s);
			continue;
		}
#endif
d2404 1
a2404 5
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(i);
		ppc_intr_enable(s);
#endif
		return;
a2406 3
#ifdef MULTIPROCESSOR
busy:
#endif
a2409 7
#ifdef MULTIPROCESSOR
	s = ppc_intr_disable();
	if (pmap_hash_lock_try(off & 7) == 0) {
		ppc_intr_enable(s);
		goto busy;
	}
#endif
d2441 2
a2442 4
#ifdef MULTIPROCESSOR
	pmap_hash_unlock(off & 7);
	ppc_intr_enable(s);
#endif
@


1.155
log
@Rewrite PTE manipulation routines to better match the PEM.

Document every operation, make sure to call "sync" when appropriate so
that other CPUs see the bit changes and finally grab a lock where it was
missing to grantee atomicity.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.154 2015/06/05 09:42:10 mpi Exp $ */
d169 1
a2263 1
			return 1;
a2267 1
			return 1;
d2269 1
a2269 1
		/* NOTREACHED */
d2272 1
a2272 33
	/*
	 * If the current mapping is RO and the access was a write
	 * we return 0
	 */
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted))
		return 0;

	if (ppc_proc_is_64b) {
		/* check write fault and we have a readonly mapping */
		if ((dsisr & (1 << (31-6))) &&
		    (pted->p.pted_pte64.pte_lo & 0x1))
			return 0;
		if ((exec_fault != 0)
		    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
			/* attempted to execute non-executable page */
			return 0;
		}
		pte_insert64(pted);
	} else {
		/* check write fault and we have a readonly mapping */
		if ((dsisr & (1 << (31-6))) &&
		    (pted->p.pted_pte32.pte_lo & 0x1))
			return 0;
		if ((exec_fault != 0)
		    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
			/* attempted to execute non-executable page */
			return 0;
		}
		pte_insert32(pted);
	}

	return 1;
d2288 13
a2300 14
	if (ppc_proc_is_64b) {
		/* check write fault and we have a readonly mapping */
		if ((dsisr & (1 << (31-6))) &&
		    (pted->p.pted_pte64.pte_lo & 0x1))
			return 0;
	} else {
		/* check write fault and we have a readonly mapping */
		if ((dsisr & (1 << (31-6))) &&
		    (pted->p.pted_pte32.pte_lo & 0x1))
			return 0;
	}
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executable page */
d2302 1
a2302 1
	}
d2307 1
@


1.154
log
@Split pteclrbits() into pmap_{test,clear}_attrs().

This should not introduce any behavior change but makes the code easier
to read and later easier to protect.  This also brings this pmap closer
to what others do.

Thanks to kettenis@@ for spotting a bad typo!

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.153 2015/06/05 09:38:52 mpi Exp $ */
d4 1
d8 1
a8 1
 *   
d32 1
a32 1
 */  
a124 4
static inline void tlbsync(void);
static inline void tlbie(vaddr_t ea);
void tlbia(void);

d174 2
a175 1
void pte_zap(void *ptp, struct pte_desc *pted);
a371 1
/* PTE manipulation/calculations */
d375 1
a375 1
	__asm volatile ("tlbie %0" :: "r"(va));
d381 12
a392 1
	__asm volatile ("sync; tlbsync; sync");
d395 2
a396 2
void
tlbia()
d400 1
a400 1
	__asm volatile ("sync");
d403 1
d405 1
d867 5
d873 1
a873 1
pte_zap(void *ptp, struct pte_desc *pted)
d875 4
d880 11
a890 2
	struct pte_64 *ptp64 = (void*) ptp;
	struct pte_32 *ptp32 = (void*) ptp;
d892 2
a893 4
	if (ppc_proc_is_64b)
		ptp64->pte_hi &= ~PTE_VALID_64;
	else 
		ptp32->pte_hi &= ~PTE_VALID_32;
a894 5
	__asm volatile ("sync");
	tlbie(pted->pted_va);
	__asm volatile ("sync");
	tlbsync();
	__asm volatile ("sync");
d896 2
a897 3
		if (PTED_MANAGED(pted))
			pmap_attr_save(pted->p.pted_pte64.pte_lo & PTE_RPGN_64,
			    ptp64->pte_lo & (PTE_REF_64|PTE_CHG_64));
d899 2
a900 3
		if (PTED_MANAGED(pted))
			pmap_attr_save(pted->p.pted_pte32.pte_lo & PTE_RPGN_32,
			    ptp32->pte_lo & (PTE_REF_32|PTE_CHG_32));
d970 1
d1015 3
d1029 4
d1042 4
a1045 1

d1060 3
d1072 4
d1081 3
a1083 4
				ptp64->pte_hi &= ~PTE_VALID_64;
				__asm__ volatile ("sync");
				tlbie(pted->pted_va & ~PAGE_MASK);
				tlbsync();
d1085 1
a1085 1
				__asm__ volatile ("sync");
d1087 1
d1092 3
a1094 4
				ptp32->pte_hi &= ~PTE_VALID_32;
				__asm__ volatile ("sync");
				tlbie(pted->pted_va & ~PAGE_MASK);
				tlbsync();
d1096 1
a1096 1
				__asm__ volatile ("sync");
d1098 1
d1101 4
d2019 3
d2038 4
d2045 2
a2046 4
		ptp64->pte_hi &= ~PTE_VALID_64;
		__asm__ volatile ("sync");
		tlbie(va);
		tlbsync();
d2051 3
a2053 2
		ptp64->pte_lo &= ~PTE_CHG_64;
		ptp64->pte_lo &= ~PTE_PP_64;
d2055 1
a2055 1
		__asm__ volatile ("sync");
d2057 1
d2059 4
d2072 3
d2088 4
d2095 2
a2096 4
		ptp32->pte_hi &= ~PTE_VALID_32;
		__asm__ volatile ("sync");
		tlbie(va);
		tlbsync();
d2101 3
a2103 2
		ptp32->pte_lo &= ~PTE_CHG_32;
		ptp32->pte_lo &= ~PTE_PP_32;
d2105 1
a2105 1
		__asm__ volatile ("sync");
d2107 1
d2109 4
d2362 4
a2365 3
	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

d2367 1
a2367 1
		pte_zap(pte, pted);
d2371 3
a2392 1
		/* not valid, just load */
d2394 3
a2396 2
		ptp64[i].pte_hi =
		    pted->p.pted_pte64.pte_hi & ~PTE_VALID_64;
d2398 1
a2398 1
		__asm__ volatile ("sync");
d2400 1
a2400 1
		__asm volatile ("sync");
d2422 3
a2424 2
		ptp64[i].pte_hi =
		    (pted->p.pted_pte64.pte_hi | PTE_HID_64) & ~PTE_VALID_64;
d2426 3
a2428 3
		__asm__ volatile ("sync");
		ptp64[i].pte_hi |= PTE_VALID_64;
		__asm volatile ("sync");
d2441 1
a2441 1
	__asm__ volatile ("mftb %0" : "=r"(off));
d2458 1
d2461 1
a2461 3
		ptp64->pte_hi &= ~PTE_VALID_64;
		__asm volatile ("sync");
		
a2471 1
		tlbie(va);
d2473 2
a2474 1
		tlbsync();
d2479 2
d2482 1
a2482 7
		ptp64->pte_hi =
		    (pted->p.pted_pte64.pte_hi | PTE_HID_64) &
		    ~PTE_VALID_64;
	 else 
		ptp64->pte_hi = pted->p.pted_pte64.pte_hi & 
		    ~PTE_VALID_64;

d2484 1
a2484 1
	__asm__ volatile ("sync");
d2486 1
d2505 4
a2508 3
	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

d2510 1
a2510 1
		pte_zap(pte, pted);
d2514 3
a2536 1
		/* not valid, just load */
d2538 2
d2542 1
a2542 1
		__asm__ volatile ("sync");
d2544 1
a2544 1
		__asm volatile ("sync");
d2566 3
a2568 2
		ptp32[i].pte_hi =
		    (pted->p.pted_pte32.pte_hi | PTE_HID_32) & ~PTE_VALID_32;
d2570 3
a2572 3
		__asm__ volatile ("sync");
		ptp32[i].pte_hi |= PTE_VALID_32;
		__asm volatile ("sync");
d2585 1
a2585 1
	__asm__ volatile ("mftb %0" : "=r"(off));
d2601 1
a2603 2
		ptp32->pte_hi &= ~PTE_VALID_32;
		__asm volatile ("sync");
a2608 1
		tlbie(va);
d2610 2
a2611 1
		tlbsync();
d2615 3
d2619 1
a2619 4
		ptp32->pte_hi =
		    (pted->p.pted_pte32.pte_hi | PTE_HID_32) & ~PTE_VALID_32;
	else
		ptp32->pte_hi = pted->p.pted_pte32.pte_hi & ~PTE_VALID_32;
d2621 1
a2621 1
	__asm__ volatile ("sync");
d2623 1
@


1.153
log
@More usages of pmap_ptedinhash().

If you wonder why pte_insert{32,64}() is not using pmap_hash_remove() if
it finds a conflicting PTE in the HASH, it's because in the current state
trying to grab the same lock a second time would lead to a deadlock.

This is much easier to reproduce on G5 (or G4 with BAT disabled).

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.152 2015/06/05 09:32:22 mpi Exp $ */
a988 4
/*
 * read/clear bits from pte/attr cache, for reference/change
 * ack, copied code in the pte flush code....
 */
d990 1
a990 1
pteclrbits(struct vm_page *pg, u_int flagbit, u_int clear)
a998 3
	/*
	 *  First try the attribute cache
	 */
d1000 1
a1000 1
	if ((bits == flagbit) && (clear == 0))
d1003 33
a1035 5
	/* cache did not contain all necessary bits,
	 * need to walk thru pv table to collect all mappings for this
	 * page, copying bits to the attribute cache 
	 * then reread the attribute cache.
	 */
d1044 7
a1050 10
				if (clear) {
					ptp64->pte_hi &= ~PTE_VALID_64;
					__asm__ volatile ("sync");
					tlbie(pted->pted_va & ~PAGE_MASK);
					tlbsync();
					ptp64->pte_lo &= ~ptebit;
					__asm__ volatile ("sync");
					ptp64->pte_hi |= PTE_VALID_64;
				} else if (bits == flagbit)
					break;
d1055 7
a1061 10
				if (clear) {
					ptp32->pte_hi &= ~PTE_VALID_32;
					__asm__ volatile ("sync");
					tlbie(pted->pted_va & ~PAGE_MASK);
					tlbsync();
					ptp32->pte_lo &= ~ptebit;
					__asm__ volatile ("sync");
					ptp32->pte_hi |= PTE_VALID_32;
				} else if (bits == flagbit)
					break;
d1066 6
a1071 9
	if (clear) {
		/*
		 * this is done a second time, because while walking the list
		 * a bit could have been promoted via pmap_attr_save()
		 */
		bits |= pg->pg_flags & flagbit;
		atomic_clearbits_int(&pg->pg_flags,  flagbit); 
	} else
		atomic_setbits_int(&pg->pg_flags,  bits);
@


1.152
log
@Remove DEBUG stuff.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.151 2015/06/05 09:31:19 mpi Exp $ */
d892 1
a892 5
	vaddr_t va = pted->pted_va;
	pmap_t pm = pted->pted_pmap;
	struct pte_64 *ptp64;
	struct pte_32 *ptp32;
	int sr, idx;
d894 1
a894 1
	int s;
d897 4
a900 2
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);
d902 2
a903 2
	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));
	/* determine which pteg mapping is present in */
a904 4
	if (ppc_proc_is_64b) {
		int entry = PTED_PTEGIDX(pted); 
		ptp64 = pmap_ptable64 + (idx * 8);
		ptp64 += entry; /* increment by entry into pteg */
d906 2
a907 2
		s = ppc_intr_disable();
		pmap_hash_lock(entry);
a908 37
		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is not
		 * currently in the HASH.
		 */
		if ((pted->p.pted_pte64.pte_hi | 
		    (PTED_HID(pted) ? PTE_HID_64 : 0)) == ptp64->pte_hi) {
			pte_zap((void*)ptp64, pted);
		}
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(entry);
		ppc_intr_enable(s);
#endif
	} else {
		int entry = PTED_PTEGIDX(pted); 
		ptp32 = pmap_ptable32 + (idx * 8);
		ptp32 += entry; /* increment by entry into pteg */
#ifdef MULTIPROCESSOR
		s = ppc_intr_disable();
		pmap_hash_lock(entry);
#endif
		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is not
		 * currently in the HASH.
		 */
		if ((pted->p.pted_pte32.pte_hi |
		    (PTED_HID(pted) ? PTE_HID_32 : 0)) == ptp32->pte_hi) {
			pte_zap((void*)ptp32, pted);
		}
#ifdef MULTIPROCESSOR
		pmap_hash_unlock(entry);
		ppc_intr_enable(s);
#endif
	}
a2278 2
	int off;
	int secondary;
d2280 1
d2282 1
a2286 1

d2290 2
a2291 6
	ptp64 = pmap_ptable64 +
	    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if ((pted->p.pted_pte64.pte_hi |
	    (PTED_HID(pted) ? PTE_HID_64 : 0)) == ptp64->pte_hi)
		pte_zap(ptp64,pted);
a2420 2
	int off;
	int secondary;
d2422 1
d2424 1
d2432 2
a2433 7
	/* determine if ptp is already mapped */
	ptp32 = pmap_ptable32 +
	    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if ((pted->p.pted_pte32.pte_hi |
	    (PTED_HID(pted) ? PTE_HID_32 : 0)) == ptp32->pte_hi)
		pte_zap(ptp32,pted);
a2443 1

@


1.151
log
@Make use of ptesr() instead of rerolling it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.150 2015/06/05 09:30:03 mpi Exp $ */
a90 4
#include <machine/db_machdep.h>
#include <ddb/db_extern.h>
#include <ddb/db_output.h>

a123 2
void print_pteg(pmap_t pm, vaddr_t va);

a178 3
/* debugging */
void pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...));

a2601 187

#ifdef DEBUG_PMAP
void
print_pteg(pmap_t pm, vaddr_t va)
{
	int sr, idx;
	struct pte_32 *ptp;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr,  va);

	ptp = pmap_ptable32 + idx  * 8;
	db_printf("va %x, sr %x, idx %x\n", va, sr, idx);

	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
	ptp = pmap_ptable32 + (idx ^ pmap_ptab_mask) * 8;
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
}


/* debugger assist function */
int pmap_prtrans(u_int pid, vaddr_t va);

void
pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...))
{
	vaddr_t va;
	va = pted->pted_va & ~PAGE_MASK;
	print("\n pted %x", pted);
	if (PTED_VALID(pted)) {
		print(" va %x:", pted->pted_va & ~PAGE_MASK);
		print(" HID %d", PTED_HID(pted) ? 1: 0);
		print(" PTEGIDX %x", PTED_PTEGIDX(pted));
		print(" MANAGED %d", PTED_MANAGED(pted) ? 1: 0);
		print(" WIRED %d\n", PTED_WIRED(pted) ? 1: 0);
		if (ppc_proc_is_64b) {
			print("ptehi %x ptelo %x ptp %x Aptp %x\n",
			    pted->p.pted_pte64.pte_hi,
			    pted->p.pted_pte64.pte_lo,
			    pmap_ptable64 +
				8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
			    pmap_ptable64 +
				8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
				    ^ pmap_ptab_mask)
			    );
		} else {
			print("ptehi %x ptelo %x ptp %x Aptp %x\n",
			    pted->p.pted_pte32.pte_hi,
			    pted->p.pted_pte32.pte_lo,
			    pmap_ptable32 +
				8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
			    pmap_ptable32 +
				8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
				    ^ pmap_ptab_mask)
			    );
		}
	}
}

int pmap_user_read(int size, vaddr_t va);
int
pmap_user_read(int size, vaddr_t va)
{
	unsigned char  read1;
	unsigned short read2;
	unsigned int   read4;
	int err;

	if (size == 1) {
		err = copyin((void *)va, &read1, 1);
		if (err == 0) {
			db_printf("byte read %x\n", read1);
		}
	} else if (size == 2) {
		err = copyin((void *)va, &read2, 2);
		if (err == 0) {
			db_printf("short read %x\n", read2);
		}
	} else if (size == 4) {
		err = copyin((void *)va, &read4, 4);
		if (err == 0) {
			db_printf("int read %x\n", read4);
		}
	} else {
		return 1;
	}


	return 0;
}

int pmap_dump_pmap(u_int pid);
int
pmap_dump_pmap(u_int pid)
{
	pmap_t pm;
	struct proc *p;
	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);

		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
		}
		pm = p->p_vmspace->vm_map.pmap;
	}
	printf("pmap %x:\n", pm);
	printf("segid %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x",
	    pm->pm_sr[0], pm->pm_sr[1], pm->pm_sr[2], pm->pm_sr[3],
	    pm->pm_sr[4], pm->pm_sr[5], pm->pm_sr[6], pm->pm_sr[7],
	    pm->pm_sr[8], pm->pm_sr[9], pm->pm_sr[10], pm->pm_sr[11],
	    pm->pm_sr[12], pm->pm_sr[13], pm->pm_sr[14], pm->pm_sr[15]);

	return 0;
}

int
pmap_prtrans(u_int pid, vaddr_t va)
{
	struct proc *p;
	pmap_t pm;
	struct pmapvp *vp1;
	struct pmapvp *vp2;
	struct pte_desc *pted;

	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);

		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
		}
		pm = p->p_vmspace->vm_map.pmap;
	}

	db_printf(" pid %d, va 0x%x pmap %x\n", pid, va, pm);
	vp1 = pm->pm_vp[VP_SR(va)];
	db_printf("sr %x id %x vp1 %x", VP_SR(va), pm->pm_sr[VP_SR(va)],
	    vp1);

	if (vp1) {
		vp2 = vp1->vp[VP_IDX1(va)];
		db_printf(" vp2 %x", vp2);

		if (vp2) {
			pted = vp2->vp[VP_IDX2(va)];
			pmap_print_pted(pted, db_printf);

		}
	}
	print_pteg(pm, va);

	return 0;
}
int pmap_show_mappings(paddr_t pa);

int
pmap_show_mappings(paddr_t pa) 
{
	struct pte_desc *pted;
	struct vm_page *pg;

	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL) {
		db_printf("pa %x: unmanaged\n");
	} else {
		LIST_FOREACH(pted, &(pg->mdpage.pv_list), pted_pv_list) {
			pmap_print_pted(pted, db_printf);
		}
	}
	return 0;
}
#endif
@


1.150
log
@Merge various copies of the same code into a new function to determine
if a PTE is present in the HASH.

Note that atomicity is currently not guaranteed between this check and
the following operations.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.149 2015/06/05 09:25:21 mpi Exp $ */
d1785 1
a1785 1
	sr = pm->pm_sr[(u_int)va >> ADDR_SR_SHIFT];
d1788 2
a1789 4
	asm volatile ("mfsr %0,%1"
	    : "=r" (oldsr): "n"(PPC_USER_SR));
	asm volatile ("isync; mtsr %0,%1; isync"
	    :: "n"(PPC_USER_SR), "r"(sr));
@


1.149
log
@Introduce pmap_pted_ro() a simple wrapper for the 32/64 bits versions
that does not call pmap_vp_lookup().

Carreful readers would have notice the removal of the bits on the virtual
address with a page mask, this change allows me to find the 13 years old
bug fixed in r1.145.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.148 2015/06/05 09:18:50 mpi Exp $ */
d821 45
d1068 6
a1073 26
		vaddr_t va = pted->pted_va & ~PAGE_MASK;
		pmap_t pm = pted->pted_pmap;
		struct pte_64 *ptp64;
		struct pte_32 *ptp32;
		int sr, idx;

		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);

		/* determine which pteg mapping is present in */
		if (ppc_proc_is_64b) {
			ptp64 = pmap_ptable64 +
				(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
			ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */

			/*
			 * We now have the pointer to where it will be, if it is
			 * currently mapped. If the mapping was thrown away in
			 * exchange for another page mapping, then this page is
			 * not currently in the HASH.
			 *
			 * if we are not clearing bits, and have found all of the
			 * bits we want, we can stop
			 */
			if ((pted->p.pted_pte64.pte_hi |
			    (PTED_HID(pted) ? PTE_HID_64 : 0)) == ptp64->pte_hi) {
d1078 1
a1078 1
					tlbie(va);
d1085 3
a1087 17
			}
		} else {
			ptp32 = pmap_ptable32 +
				(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
			ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */

			/*
			 * We now have the pointer to where it will be, if it is
			 * currently mapped. If the mapping was thrown away in
			 * exchange for another page mapping, then this page is
			 * not currently in the HASH.
			 *
			 * if we are not clearing bits, and have found all of the
			 * bits we want, we can stop
			 */
			if ((pted->p.pted_pte32.pte_hi |
			    (PTED_HID(pted) ? PTE_HID_32 : 0)) == ptp32->pte_hi) {
d1092 1
a1092 1
					tlbie(va);
a2017 1
	struct pte_64 *ptp64;
d2019 1
a2019 1
	int sr, idx;
d2036 2
a2037 2
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);
a2038 13
	/* determine which pteg mapping is present in */
	ptp64 = pmap_ptable64 +
	    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */

	/*
	 * We now have the pointer to where it will be, if it is
	 * currently mapped. If the mapping was thrown away in
	 * exchange for another page mapping, then this page is
	 * not currently in the HASH.
	 */
	if ((pted->p.pted_pte64.pte_hi | (PTED_HID(pted) ? PTE_HID_64 : 0))
	    == ptp64->pte_hi) {
d2060 2
a2061 3
	struct pte_32 *ptp32;
	struct vm_page *pg = NULL;
	int sr, idx;
d2075 2
a2076 7
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);

	/* determine which pteg mapping is present in */
	ptp32 = pmap_ptable32 +
	    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */
a2077 8
	/*
	 * We now have the pointer to where it will be, if it is
	 * currently mapped. If the mapping was thrown away in
	 * exchange for another page mapping, then this page is
	 * not currently in the HASH.
	 */
	if ((pted->p.pted_pte32.pte_hi | (PTED_HID(pted) ? PTE_HID_32 : 0))
	    == ptp32->pte_hi) {
@


1.148
log
@Do only one VP lookup when removing a page.

This simplify pmap_remove() & friends by re-using an already fetched PTE
descriptor.

There's currently a race on MP system where one CPU can reuse a pted
while another one is still trying to insert it in the HASH.  This commit
starts reducing the number of pmap_vp_lookup() calls to help fix this
race.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.147 2015/06/05 09:09:58 mpi Exp $ */
d135 3
a137 2
void pmap_page_ro64(pmap_t pm, vaddr_t va, vm_prot_t prot);
void pmap_page_ro32(pmap_t pm, vaddr_t va, vm_prot_t prot);
d1994 1
a1994 1
pmap_page_ro64(pmap_t pm, vaddr_t va, vm_prot_t prot)
d1996 11
a2007 1
	struct pte_desc *pted;
a2010 4
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted))
		return;

d2059 1
a2059 1
pmap_page_ro32(pmap_t pm, vaddr_t va, vm_prot_t prot)
d2061 2
a2063 1
	struct pte_desc *pted;
a2066 4
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted))
		return;

d2132 2
a2133 6
	LIST_FOREACH(pted, &(pg->mdpage.pv_list), pted_pv_list) {
		if (ppc_proc_is_64b)
			pmap_page_ro64(pted->pted_pmap, pted->pted_va, prot);
		else
			pmap_page_ro32(pted->pted_pmap, pted->pted_va, prot);
	}
d2140 7
a2146 10
		if (ppc_proc_is_64b) {
			while (sva < eva) {
				pmap_page_ro64(pm, sva, prot);
				sva += PAGE_SIZE;
			}
		} else {
			while (sva < eva) {
				pmap_page_ro32(pm, sva, prot);
				sva += PAGE_SIZE;
			}
@


1.147
log
@Remove the MANAGED flag when removing a PV entry.

Even if this change is not strickly needed, because the memory will be
returned to the pool it helped me track the use-after-free.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.146 2015/06/05 09:05:35 mpi Exp $ */
d167 1
a167 1
void pmap_remove_pg(pmap_t pm, vaddr_t va);
d550 1
a550 1
		pmap_remove_pg(pm, va);
d646 1
d683 1
a683 1
				e_vp2 = VP_IDX2_SIZE; 
d686 3
a688 6
				if (vp2->vp[i_vp2] != NULL) {
					pmap_remove_pg(pm,
					    (i_sr << VP_SR_POS) |
					    (i_vp1 << VP_IDX1_POS) |
					    (i_vp2 << VP_IDX2_POS));
				}
d697 1
a697 1
pmap_remove_pg(pmap_t pm, vaddr_t va)
d699 1
a699 1
	struct pte_desc *pted;
a700 11
	if (pm == pmap_kernel()) {
		pted = pmap_vp_lookup(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			return;
		}
	} else {
		pted = pmap_vp_remove(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			return;
		}
	}
d706 1
a706 1
		u_int sn = VP_SR(va);
d722 2
a723 1
	if (pm != pmap_kernel())
d725 1
d754 1
a754 1
		pmap_remove_pg(pm, va); /* pted is reused */
d811 7
a817 2
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE)
		pmap_remove_pg(pmap_kernel(), va);
d2121 1
a2121 1
			pmap_remove_pg(pted->pted_pmap, pted->pted_va);
@


1.146
log
@Remove unneeded splvm() calls and the pool_setipl(9) hack of r1.140.

By instructing spl(9) calls on MP machines I figured out that their high
cost was hiding a race condition involving PTE reuse in our pmap.  Thanks
to deraadt@@ for finding a way to trigger such panic by adding a couple of
splvm().

This should make the races easier to trigger but will be addressed
shortly.

This commit starts your PowerPC pmap SMP show of the week.

ok kettenis@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.145 2015/04/23 14:42:02 mpi Exp $ */
d501 1
@


1.145
log
@Fix 13 years old typo that should be responsible for the unhappiness
of UVM on PowerPC architectures by breaking pmap_is_referenced() and
friends.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.144 2015/03/31 16:00:38 mpi Exp $ */
a347 2
 * 
 * Should this be called under splvm?
a539 1
	int s;
a546 3
	/* MP - Acquire lock for this pmap */

	s = splvm();
d559 1
a559 1
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT | PR_ZERO);	
a561 1
				splx(s);
a568 1
			splx(s);
a626 2
	splx(s);

a630 1
	/* MP - free pmap lock */
a700 1
	int s;
a701 6
	/*
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
	 */
	s = splvm();
a704 1
			splx(s);
d706 1
a706 1
		} 
a709 1
			splx(s);
a735 2

	splx(s);
d755 1
a755 1
	int cache, s;
a761 3
	/* MP - lock pmap. */
	s = splvm();

a812 2

	splx(s);
a855 1
 * LOCKS... should the caller lock?
a1007 1
	int s;
a1025 3
	/* need lock for this pv */
	s = splvm();

a1105 1
	splx(s);
a2120 1
	int s;
a2122 3
	/* need to lock for this pv */
	s = splvm();

a2129 1
		splx(s);
d2132 1
a2132 1
	
a2138 1
	splx(s);
a2143 1
	int s;
a2144 1
		s = splvm();
a2155 1
		splx(s);
a2192 1
	pool_setipl(&pmap_vp_pool, IPL_VM);
a2194 1
	pool_setipl(&pmap_pted_pool, IPL_VM);
@


1.144
log
@Make it possisble to disable block address translation mechanism on
processors that support it.

Due to the way trap code is patched it is currently not possible to
enabled/disable BAT at runtime.

ok miod@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.143 2015/03/31 15:51:05 mpi Exp $ */
d1059 1
a1059 1
		vaddr_t va = pted->pted_va & PAGE_MASK;
@


1.143
log
@Merge two versions of ppc_check_procid().

ok miod@@, kettenis@@ as part of a larger diff
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.142 2015/02/09 13:35:44 deraadt Exp $ */
d88 1
d95 2
d1572 5
d1641 11
a1651 1
	if (ppc_proc_is_64b) {
a1665 17
	}

	ppc_kvm_stolen += reserve_dumppages( (caddr_t)(VM_MIN_KERNEL_ADDRESS +
	    ppc_kvm_stolen));


	/*
	 * Initialize kernel pmap and hardware.
	 */
#if NPMAPS >= PPC_KERNEL_SEGMENT / 16
	usedsr[PPC_KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((PPC_KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
#endif
	for (i = 0; i < 16; i++) {
		pmap_kernel()->pm_sr[i] = (PPC_KERNEL_SEG0 + i) | SR_NOEXEC;
		ppc_mtsrin(PPC_KERNEL_SEG0 + i, i << ADDR_SR_SHIFT);
	}
a1666 1
	if (ppc_proc_is_64b) {
d1670 12
d1683 30
a1712 5
		asm volatile ("sync; mtsdr1 %0; isync"
		    :: "r"((u_int)pmap_ptable64 | HTABSIZE_64));
	} else 
		asm volatile ("sync; mtsdr1 %0; isync"
		    :: "r"((u_int)pmap_ptable32 | (pmap_ptab_mask >> 10)));
d1714 2
a1715 4
	pmap_avail_fixup();


	tlbia();
d1726 31
@


1.142
log
@oops, accidental commit
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.140 2015/01/22 19:47:00 deraadt Exp $ */
a1566 2

	ppc_check_procid();
@


1.141
log
@sync
@
text
@d156 1
a156 1
    int, int) __noprof;
d158 1
a158 1
    int, int) __noprof;
d538 1
d544 3
a577 3
	pa &= PMAP_PA_MASK;

	/* Calculate PTE */
d579 4
a582 2
	if (pg != NULL && !nocache)
		cache = PMAP_CACHE_WB; /* managed memory is cacheable */
d586 1
d588 1
a588 1
		pmap_fill_pte64(pm, va, pa, pted, prot, flags, cache);
d590 1
a590 1
		pmap_fill_pte32(pm, va, pa, pted, prot, flags, cache);
d767 1
a767 1
_pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags, int cache)
d771 2
a772 1
	int s;
d774 4
d789 1
d802 7
a808 7
	if (cache == PMAP_CACHE_DEFAULT) {
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg != NULL && (pg->pg_flags & PG_DEV) == 0)
			cache = PMAP_CACHE_WB;
		else
			cache = PMAP_CACHE_CI;
	}
d812 1
a812 1
		pmap_fill_pte64(pm, va, pa, pted, prot, flags, cache);
d814 1
a814 1
		pmap_fill_pte32(pm, va, pa, pted, prot, flags, cache);
a838 12
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	_pmap_kenter_pa(va, pa, prot, 0, PMAP_CACHE_DEFAULT);
}

void
pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable)
{
	_pmap_kenter_pa(va, pa, prot, 0, cacheable);
}

d952 1
a952 1
	vm_prot_t prot, int flags, int cache)
d992 1
a992 1
	vm_prot_t prot, int flags, int cache)
d1714 4
a1717 3
/* 
 * Get the physical page address for the given pmap/virtual address.
 */ 
d1723 5
d1729 1
a1729 7
	if (pted == NULL || !PTED_VALID(pted)) {
		if (pm == pmap_kernel() && va < 0x80000000) {
			/* XXX - this is not true without BATs */
			/* if in kernel, va==pa for 0-0x80000000 */
			*pa = va;
			return TRUE;
		}
d1731 1
a1731 1
	}
d2245 1
a2245 1
			    pted, prot, 0, PMAP_CACHE_WB);
d2250 1
a2250 1
			    pted, prot, 0, PMAP_CACHE_WB);
@


1.140
log
@pool_setipl() on both pmap pools as a workaround for some sort of MP
race.  This will certainly be revisited, but too much time has been
spent on it for now.
ok mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.139 2015/01/22 17:55:46 mpi Exp $ */
d156 1
a156 1
    int) __noprof;
d158 1
a158 1
    int) __noprof;
a537 1
	boolean_t wt = (pa & PMAP_WT) != 0;
a542 3
	KASSERT(!(wt && nocache));
	pa &= PMAP_PA_MASK;

d574 3
d578 2
a579 4
	if (wt)
		cache = PMAP_CACHE_WT;
	else if (pg != NULL && !(pg->pg_flags & PG_DEV) && !nocache)
		cache = PMAP_CACHE_WB;
a582 1
	/* Calculate PTE */
d584 1
a584 1
		pmap_fill_pte64(pm, va, pa, pted, prot, cache);
d586 1
a586 1
		pmap_fill_pte32(pm, va, pa, pted, prot, cache);
d763 1
a763 1
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
d767 1
a767 2
	boolean_t nocache = (pa & PMAP_NOCACHE) != 0;
	boolean_t wt = (pa & PMAP_WT) != 0;
a768 4
	int cache, s;

	KASSERT(!(wt && nocache));
	pa &= PMAP_PA_MASK;
a779 1

d792 7
a798 7
	pg = PHYS_TO_VM_PAGE(pa);
	if (wt)
		cache = PMAP_CACHE_WT;
	else if (pg != NULL && !(pg->pg_flags & PG_DEV) && !nocache)
		cache = PMAP_CACHE_WB;
	else
		cache = PMAP_CACHE_CI;
d802 1
a802 1
		pmap_fill_pte64(pm, va, pa, pted, prot, cache);
d804 1
a804 1
		pmap_fill_pte32(pm, va, pa, pted, prot, cache);
d829 12
d954 1
a954 1
	vm_prot_t prot, int cache)
d994 1
a994 1
	vm_prot_t prot, int cache)
d1716 3
a1718 4
/*
 * pmap_extract: extract a PA for the given VA
 */

a1723 5
	if (pm == pmap_kernel() && va < physmaxaddr) {
		*pa = va;
		return TRUE;
	}

d1725 7
a1731 1
	if (pted == NULL || !PTED_VALID(pted))
d1733 1
a1733 1

d2247 1
a2247 1
			    pted, prot, PMAP_CACHE_WB);
d2252 1
a2252 1
			    pted, prot, PMAP_CACHE_WB);
@


1.139
log
@Let powerpc's bus_space(9) use the same pmap and uvm interfaces than the
other archs.

Specify the caching policy by passing PMAP_* flags to pmap_kenter_pa()
like the majority of our archs do and kill pmap_kenter_cache().

Spread some pmap_update() along the way.

While here remove the unused flag argument from pmap_fill_pte().

Finally convert the bus map/unmap functions to km_alloc/free() instead
of uvm_km_valloc/free().

Inputs from kettenis@@ and miod@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.138 2015/01/21 19:10:26 mpi Exp $ */
d2171 1
d2174 1
@


1.138
log
@Even without BATs memory under ``physmaxaddr'' is mapped 1:1 in the
kernel, so update pmap_extract() accordingly and save a VP lookup.

While here unify pted checks after the VP lookups.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.137 2015/01/20 17:04:20 mpi Exp $ */
d156 1
a156 1
    int, int) __noprof;
d158 1
a158 1
    int, int) __noprof;
d538 1
d544 3
a577 3
	pa &= PMAP_PA_MASK;

	/* Calculate PTE */
d579 4
a582 2
	if (pg != NULL && !nocache)
		cache = PMAP_CACHE_WB; /* managed memory is cacheable */
d586 1
d588 1
a588 1
		pmap_fill_pte64(pm, va, pa, pted, prot, flags, cache);
d590 1
a590 1
		pmap_fill_pte32(pm, va, pa, pted, prot, flags, cache);
d767 1
a767 1
_pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags, int cache)
d771 2
a772 1
	int s;
d774 4
d789 1
d802 7
a808 7
	if (cache == PMAP_CACHE_DEFAULT) {
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg != NULL && (pg->pg_flags & PG_DEV) == 0)
			cache = PMAP_CACHE_WB;
		else
			cache = PMAP_CACHE_CI;
	}
d812 1
a812 1
		pmap_fill_pte64(pm, va, pa, pted, prot, flags, cache);
d814 1
a814 1
		pmap_fill_pte32(pm, va, pa, pted, prot, flags, cache);
a838 12
void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	_pmap_kenter_pa(va, pa, prot, 0, PMAP_CACHE_DEFAULT);
}

void
pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable)
{
	_pmap_kenter_pa(va, pa, prot, 0, cacheable);
}

d952 1
a952 1
	vm_prot_t prot, int flags, int cache)
d992 1
a992 1
	vm_prot_t prot, int flags, int cache)
d2243 1
a2243 1
			    pted, prot, 0, PMAP_CACHE_WB);
d2248 1
a2248 1
			    pted, prot, 0, PMAP_CACHE_WB);
@


1.137
log
@Various cleanups. Explicitly include <sys/atomic.h>, Use pmap_remove_pg()
for the kernel pmap and kill pmap_kremove_pg().  Finally guard the hash
lock code under "MULTIPROCESSOR" to explicit which part of the code
received some MP love.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.136 2014/12/23 01:12:33 dlg Exp $ */
d1716 4
a1719 3
/* 
 * Get the physical page address for the given pmap/virtual address.
 */ 
d1725 5
d1731 1
a1731 7
	if (pted == NULL || !PTED_VALID(pted)) {
		if (pm == pmap_kernel() && va < 0x80000000) {
			/* XXX - this is not true without BATs */
			/* if in kernel, va==pa for 0-0x80000000 */
			*pa = va;
			return TRUE;
		}
d1733 1
a1733 1
	}
d2257 2
a2258 6
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
		return 0;
	}

	/* if the current mapping is RO and the access was a write
d2261 2
a2262 1
	if (!PTED_VALID(pted)) {
a2263 1
	} 
a2296 5
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
		return 0;
	}

d2298 1
a2298 1
	 * if the current mapping is RO and the access was a write
d2301 2
a2302 1
	if (!PTED_VALID(pted)) {
d2304 1
a2304 1
	}
@


1.136
log
@force the pool of pmapvp onto PAGE_SIZE allocations by specifying a
pool allocator. pmapvp is 1024 bytes, and the size * 8 change in pools
without an allocator being specified tries to place it on large pages.
you need pmap to use large pages, and pmap isnt set up yet.

fixed a very early fault on macppc.
debugged with and tested by krw@@
ok deraadt@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.135 2014/12/17 14:40:03 deraadt Exp $ */
d78 1
a78 1
#include <sys/malloc.h>
a80 1
#include <sys/systm.h>
d82 1
a93 2
#include <powerpc/lock.h>

a164 1
void pmap_kremove_pg(vaddr_t va);
d193 1
d251 1
d777 1
a777 1
		pmap_kremove_pg(va); /* pted is reused */
a840 51

/*
 * remove kernel (pmap_kernel()) mapping, one page
 */
void
pmap_kremove_pg(vaddr_t va)
{
	struct pte_desc *pted;
	pmap_t pm;
	int s;

	pm = pmap_kernel();
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL)
		return;

	if (!PTED_VALID(pted))
		return; /* not mapped */

	s = splvm();

	pm->pm_stats.resident_count--;

	/*
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
	 */
	pmap_hash_remove(pted);

	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0)
			pm->pm_sr[sn] |= SR_NOEXEC;
	}

	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	/* invalidate pted; */
	if (ppc_proc_is_64b)
		pted->p.pted_pte64.pte_hi &= ~PTE_VALID_64;
	else
		pted->p.pted_pte32.pte_hi &= ~PTE_VALID_32;

	splx(s);

}
d848 1
a848 1
		pmap_kremove_pg(va);
d892 1
d894 1
d906 1
d909 1
d920 1
d923 1
d928 1
d931 1
d942 1
d945 1
d2349 4
a2352 2
	int sr, idx;
	int i, s;
d2379 1
d2385 1
a2385 1

d2395 1
d2398 1
d2406 1
d2412 1
d2422 1
d2425 1
d2429 3
a2432 1
busy:
d2436 1
d2442 1
d2484 1
d2487 1
d2496 4
a2499 2
	int sr, idx;
	int i, s;
d2527 1
d2533 1
d2543 1
d2546 1
d2554 1
d2560 1
d2570 1
d2573 1
d2577 3
a2580 1
busy:
d2583 1
d2589 1
d2621 1
d2624 1
@


1.135
log
@remove simplelocks use
ok kettenis mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.134 2014/11/25 10:45:07 mpi Exp $ */
d2212 2
a2213 1
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, 0, 0, "vp", NULL);
@


1.134
log
@Speed up page zeroing by using a loop of dcbz/dcbzl instead of bzero().

While here, use the direct map for pmap_copy_page() and remove the now
unused stolen page addresses.

No objection from the usual suspects, "it works, commit" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.133 2014/11/18 15:20:15 deraadt Exp $ */
a137 9
 * LOCKING structures.
 * This may not be correct, and doesn't do anything yet.
 */
#define pmap_simplelock_pm(pm)
#define pmap_simpleunlock_pm(pm)
#define pmap_simplelock_pv(pm)
#define pmap_simpleunlock_pv(pm)

/*
a354 2
	pmap_simplelock_pm(pm);

a378 2
	pmap_simpleunlock_pm(pm);

d662 1
a662 1
		
a1300 1
	/* simple_lock(&pmap->pm_obj.vmobjlock); */
a1301 1
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
a1312 1
	/* simple_lock(&pmap->pm_obj.vmobjlock); */
d1314 2
a1315 1
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
@


1.133
log
@make pmap_zero_page MP-safe, by using the directmap
mpi will investigate speedups after this.
ok mpi kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.132 2014/11/16 12:30:58 deraadt Exp $ */
a114 3
paddr_t zero_page;
paddr_t copy_src_page;
paddr_t copy_dst_page;
d1220 18
a1237 1
	bzero((void *)pmap_map_direct(pg), PAGE_SIZE);
d1241 1
a1241 1
 * copy the given physical page with zeros.
d1246 4
a1249 12
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
	/* simple_lock(&pmap_copy_page_lock); */

	pmap_kenter_pa(copy_src_page, srcpa, PROT_READ);
	pmap_kenter_pa(copy_dst_page, dstpa, PROT_READ | PROT_WRITE);

	bcopy((void *)copy_src_page, (void *)copy_dst_page, PAGE_SIZE);
	
	pmap_kremove_pg(copy_src_page);
	pmap_kremove_pg(copy_dst_page);
	/* simple_unlock(&pmap_copy_page_lock); */
a1710 6
	zero_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_src_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_dst_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
@


1.132
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.131 2014/11/02 00:11:32 kettenis Exp $ */
d1223 1
a1223 19
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#ifdef USE_DCBZ
	int i;
	paddr_t addr = zero_page;
#endif

	/* simple_lock(&pmap_zero_page_lock); */
	pmap_kenter_pa(zero_page, pa, PROT_READ | PROT_WRITE);
#ifdef USE_DCBZ
	for (i = PAGE_SIZE/CACHELINESIZE; i>0; i--) {
		__asm volatile ("dcbz 0,%0" :: "r"(addr));
		addr += CACHELINESIZE;
	}
#else
	bzero((void *)zero_page, PAGE_SIZE);
#endif
	pmap_kremove_pg(zero_page);
	
	/* simple_unlock(&pmap_zero_page_lock); */
@


1.131
log
@Only mark segment 0 as executable on 64-bit systems.  There it is harmless as
we have a proper X bit in the page tables.  On 32-bit systems kernel .text is
handled by an IBAT, so we don't need page table entries that are executable
in the kernel pmap.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.130 2014/10/27 19:16:38 kettenis Exp $ */
d619 1
a619 1
        if (prot & VM_PROT_EXECUTE) {
d628 1
a628 1
			if (prot & VM_PROT_WRITE)
d641 1
a641 1
		if ((prot & VM_PROT_WRITE) && (pg != NULL))
d797 1
a797 1
	if (prot & VM_PROT_WRITE) {
d835 1
a835 1
        if (prot & VM_PROT_EXECUTE) {
d1033 1
a1033 1
	if (prot & VM_PROT_WRITE)
d1040 1
a1040 1
	if (prot & VM_PROT_EXECUTE)
d1071 1
a1071 1
	if (prot & VM_PROT_WRITE)
d1079 1
a1079 1
	if (prot & VM_PROT_EXECUTE)
d1230 1
a1230 1
	pmap_kenter_pa(zero_page, pa, VM_PROT_READ|VM_PROT_WRITE);
d1254 2
a1255 2
	pmap_kenter_pa(copy_src_page, srcpa, VM_PROT_READ);
	pmap_kenter_pa(copy_dst_page, dstpa, VM_PROT_READ|VM_PROT_WRITE);
d2057 1
a2057 1
		if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) == VM_PROT_WRITE) {
d2067 1
a2067 1
	if ((prot & VM_PROT_EXECUTE) == 0)
d2116 1
a2116 1
		if ((prot & (VM_PROT_WRITE|VM_PROT_EXECUTE)) == VM_PROT_WRITE) {
d2173 1
a2173 1
	if (prot == VM_PROT_NONE) {
d2197 1
a2197 1
	if (prot & (VM_PROT_READ | VM_PROT_EXECUTE)) {
d2305 1
a2305 1
		vm_prot_t prot = VM_PROT_READ | VM_PROT_WRITE;
d2313 1
a2313 1
			prot |= VM_PROT_EXECUTE;
@


1.130
log
@Remove execute permission from most pages in the kernel pmap.  This is a first
step towards W^X in the kernel, even though it is only effective on machines
with a G5 processor.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.129 2014/05/09 18:16:15 miod Exp $ */
d1745 4
a1748 3
	/* first segment contains executable pages */
	pmap_kernel()->pm_exec[0]++;
	pmap_kernel()->pm_sr[0] &= ~SR_NOEXEC;
a1749 1
	if (ppc_proc_is_64b) {
@


1.129
log
@Format string fixes and removal of -Wno-format for *ppc kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.128 2014/04/26 14:19:04 mpi Exp $ */
d1745 4
a1749 3
		for(i = 0; i < 0x10000; i++)
			pmap_kenter_cache(ptoa(i), ptoa(i), VM_PROT_ALL,
			    PMAP_CACHE_WB);
d2302 1
a2302 1

d2305 10
a2314 3
		pted =  &pted_store;
		/* 0 - physmaxaddr mapped 1-1 */
		/* XXX - no WRX control */
d2319 1
a2319 2
			    pted, VM_PROT_READ | VM_PROT_WRITE |
			    VM_PROT_EXECUTE, 0, PMAP_CACHE_WB);
d2324 1
a2324 2
			    &pted_store, VM_PROT_READ | VM_PROT_WRITE |
			    VM_PROT_EXECUTE, 0, PMAP_CACHE_WB);
@


1.128
log
@Allow to compile with DEBUG_PMAP defined.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.127 2014/04/01 20:42:39 mpi Exp $ */
d1554 1
a1554 1
	panic ("unable to allocate region with size %x align %x",
@


1.127
log
@Remove the almost unused abstraction around "struct firmware" and use
instead a single function ppc_mem_regions() required by the ppc pmap.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.126 2014/03/31 18:58:41 mpi Exp $ */
d2669 1
a2669 1
	struct pte *ptp;
d2674 1
a2674 1
	ptp = pmap_ptable + idx  * 8;
d2683 1
a2683 1
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
d2712 1
a2712 1
			    pmap_ptable +
d2714 1
a2714 1
			    pmap_ptable +
d2722 1
a2722 1
			    pmap_ptable +
d2724 1
a2724 1
			    pmap_ptable +
@


1.126
log
@Including <uvm/uvm_extern.h> is enough, no need for <uvm/uvm.h> or more.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.125 2014/02/09 11:25:58 mpi Exp $ */
d87 1
a87 1
#include <machine/powerpc.h>
d96 3
d1404 1
a1404 1
	(fw->mem_regions) (&pmap_mem, &pmap_avail);
@


1.125
log
@Use syncicache() instead of rerolling an almost identical version.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.124 2014/02/08 23:49:20 miod Exp $ */
d84 1
a84 1
#include <uvm/uvm.h>
@


1.124
log
@Do not bzero() the available memory in pmap_bootstrap(); allocations in
pmap_bootstrap explicitely bzero them, and there is no need to clear the
remaining memory.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.123 2014/02/08 13:17:40 miod Exp $ */
a2013 1
#define CACHELINESIZE   32		/* For now XXX*/
d2017 1
a2017 1
	vaddr_t p, start;
a2018 1
	int l;
a2029 12
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("dcbst 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("icbi 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);
d2031 1
@


1.123
log
@Some (if not all) G5 systems use a different layout for the physical memory
information (property `reg' of the `/memory' node). Fortunately the available
physical memory information still uses the same format, so this only affects
the computation of physmem.

Detect this case and parse the information correctly, converting to the format
expected by pmap, ignoring physical memory beyond 4GB.

Compute physmem from all the physical memory information, even memory not
usable by the kernel. Let pmap not recompute physmem in pmap_bootstrap() if
physmem is != 0 upon entry.

This should allow G5 systems fitted with more than 2GB of physical memory to
report the correct amount of memory, even though the kernel will only use
the lower 2GB.

Prompted by a dmesg@@ submission by Greg Marsh, owner of a 3.5GB G5

help and tweaks kettenis@@, ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.122 2013/12/29 19:09:21 brad Exp $ */
d1654 1
d1658 1
@


1.122
log
@Remove excessive parentheses.

pmap.c:1061:13: error: equality comparison with extraneous parentheses [-Werror,-Wparentheses-equality]

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.121 2013/08/19 08:39:30 mpi Exp $ */
d1399 1
d1403 1
a1403 1
	physmem = 0;
d1407 1
a1407 1
		physmem += atop(mp->size);
d1411 3
@


1.121
log
@Mark all the C functions called in real mode as non instrumented and
remove the check for address relocation from MCOUNT_ENTER.

This fix kernel profiling on powerpc architectures, broken since the
buffers are per cpu.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.120 2013/08/07 08:19:05 kettenis Exp $ */
d1023 1
a1023 1
	if ((cache == PMAP_CACHE_WB))
d1025 1
a1025 1
	else if ((cache == PMAP_CACHE_WT))
d1061 1
a1061 1
	if ((cache == PMAP_CACHE_WB))
d1063 1
a1063 1
	else if ((cache == PMAP_CACHE_WT))
@


1.120
log
@Managed device mappings should be uncached by default.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.119 2012/08/30 18:14:26 mpi Exp $ */
d146 4
d155 1
a155 1
struct pte_desc *pmap_vp_lookup(pmap_t pm, vaddr_t va);
d163 7
a169 7
void pte_insert32(struct pte_desc *pted);
void pte_insert64(struct pte_desc *pted);
void pmap_hash_remove(struct pte_desc *pted);
void pmap_fill_pte64(pmap_t pm, vaddr_t va, paddr_t pa,
    struct pte_desc *pted, vm_prot_t prot, int flags, int cache);
void pmap_fill_pte32(pmap_t pm, vaddr_t va, paddr_t pa,
    struct pte_desc *pted, vm_prot_t prot, int flags, int cache);
d185 1
a185 2
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type,
    int exec_fault);
d207 2
a208 2
void pmap_hash_unlock(int entry);
int pmap_hash_lock_try(int entry);
@


1.119
log
@Add the possibility to map DMA memory non-cached, based on the i386/amd64
implementation. For the moment only the BUS_DMA_NOCACHE macro is required
to build drm on macppc but it will be used soon.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.118 2011/05/30 22:25:22 oga Exp $ */
d804 3
a806 2
		if (PHYS_TO_VM_PAGE(pa) != NULL)
			cache = PMAP_CACHE_WB; /* managed memory is cacheable */
@


1.118
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.117 2010/08/07 03:50:01 krw Exp $ */
d548 1
d585 2
d589 1
a589 1
	if (pg != NULL)
@


1.117
log
@No "\n" needed at the end of panic() strings.

Bogus chunks pointed out by matthew@@ and miod@@. No cookies for
marco@@ and jasper@@.

ok deraadt@@ miod@@ matthew@@ jasper@@ macro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116 2010/07/16 06:22:31 kettenis Exp $ */
d1751 1
a1751 2
		    atop(mp->start), atop(mp->start+mp->size),
		    VM_FREELIST_DEFAULT);
@


1.116
log
@We never create or destroy pmaps from interrupt context, so wrapping the
associated pool calls in splvm()/splx() is unnecessary and confusing.

ok deraadt@@, drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.115 2010/06/26 23:24:44 guenther Exp $ */
d246 1
a246 1
				panic("unable to obtain lock on entry %d\n",
d796 1
a796 1
		panic("pted not preallocated in pmap_kernel() va %lx pa %lx\n",
@


1.115
log
@Don't #include <sys/user.h> into files that don't need the stuff
it defines.  In some cases, this means pulling in uvm.h or pcb.h
instead, but most of the inclusions were just noise.  Tested on
alpha, amd64, armish, hppa, i386, macpcc, sgi, sparc64, and vax,
mostly by krw and naddy.
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2010/04/24 17:56:06 kettenis Exp $ */
a1303 1
	int s;
a1304 1
	s = splvm();
a1305 1
	splx(s);
a1328 1
	int s;
a1339 1
	s = splvm();
a1340 1
	splx(s);
@


1.114
log
@There is no reason to protect the pmap_vp_pool with splvm().  The only pmap
that gets manipulated in interrupt context is the kernel pmap, and we fully
populate its VP mappings during pmap_bootstrap().  Gets rid of the excessive
spl's at pmap_destroy() time noticed by deraadt@@

ok deraadt@@, drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.113 2010/04/15 21:30:29 deraadt Exp $ */
a80 1
#include <sys/user.h>
@


1.113
log
@two missing splx in error path; ok drahn
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.112 2010/04/09 17:36:08 drahn Exp $ */
a360 1
	int s;
a365 1
		s = splvm();
a366 1
		splx(s);
a376 1
		s = splvm();
a377 1
		splx(s);
a1374 1
	int s;
a1387 1
			s = splvm();
a1388 1
			splx(s);
a1390 1
		s = splvm();
a1391 1
		splx(s);
@


1.112
log
@Prevent an interrupt from causing recursion while holding the pmap hash lock,
Otherwise a pmap_remove from a completed I/O may deadlock.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.111 2010/04/02 18:04:42 deraadt Exp $ */
d576 2
a577 1
			if ((flags & PMAP_CANFAIL) == 0)
d579 1
d585 1
@


1.111
log
@fix an ugly construct
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.110 2010/04/02 16:35:31 drahn Exp $ */
d953 1
d965 1
d978 1
d983 1
d996 1
d2430 1
a2430 1
	int i;
d2457 3
a2459 1
		if (pmap_hash_lock_try(i) == 0)
d2461 1
d2473 1
d2481 3
a2483 1
		if (pmap_hash_lock_try(i) == 0)
d2485 1
d2496 1
d2505 3
a2507 1
	if (pmap_hash_lock_try(off & 7) == 0)
d2509 1
d2552 1
d2562 1
a2562 1
	int i;
d2590 3
a2592 1
		if (pmap_hash_lock_try(i) == 0)
d2594 1
d2605 1
d2613 3
a2615 1
		if (pmap_hash_lock_try(i) == 0)
d2617 1
d2628 1
d2636 3
a2638 1
	if (pmap_hash_lock_try(off & 7) == 0)
d2640 1
d2673 1
@


1.110
log
@Clear the PG_PMAP_EXE flags whenever writable mappings are created.
ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2010/03/31 21:02:42 drahn Exp $ */
d121 3
a123 3
	struct pte_32 pted_pte32;
	struct pte_64 pted_pte64;
	}p;
@


1.109
log
@More carefully manage PG_PMAP_EXE bit and cache flushing on pmap_protect
operations, where X or W is taken away. ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.108 2009/07/21 22:34:02 kettenis Exp $ */
d622 6
a627 1
			atomic_setbits_int(&pg->pg_flags, PG_PMAP_EXE);
d777 1
d791 5
@


1.108
log
@Make pmap_enter respect the PMAP_CANFAIL flag.  With and essential
memory leak plug from drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.107 2008/10/17 14:04:07 drahn Exp $ */
d33 44
d136 1
a136 1
void pmap_page_ro32(pmap_t pm, vaddr_t va);
d2043 1
d2050 9
d2096 1
d2098 1
a2098 1
pmap_page_ro32(pmap_t pm, vaddr_t va)
d2102 1
d2109 9
d2183 1
a2183 1
			pmap_page_ro32(pted->pted_pmap, pted->pted_va);
d2201 1
a2201 1
				pmap_page_ro32(pm, sva);
@


1.107
log
@Handle pool allocation failures slightly better. ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.106 2008/09/13 18:18:25 drahn Exp $ */
d105 1
a105 1
void pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted);
d312 2
a313 2
void
pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted)
d326 5
a330 2
		if (vp1 == NULL)
			panic("pmap_vp_enter: failed to allocate vp1");
d339 5
a343 2
		if (vp2 == NULL)
			panic("pmap_vp_enter: failed to allocate vp2");
d350 2
d513 1
d531 3
a533 1
		if (pted == NULL)
d535 6
a540 1
		pmap_vp_enter(pm, va, pted);
@


1.106
log
@Kernel map is supposed to only allocate from the limited kernel addresses,
panic if the kernel attempts to map an improper address.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.105 2008/06/14 10:55:20 mk Exp $ */
d326 2
d336 2
d522 2
@


1.105
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.104 2008/04/26 22:37:41 drahn Exp $ */
d722 1
a722 2
		/* XXX - future panic? */
		printf("pted not preallocated in pmap_kernel() va %lx pa %lx\n",
a723 2
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT | PR_ZERO);
		pmap_vp_enter(pm, va, pted);
@


1.104
log
@Changes to get closer to SMP.
add biglock before interrupt calls into the kernel.
switch the clock to using cpuinfo variables instead of globals
move cpu_switchto into C code so that on multiprocessor the FPU
and Altivec can be saved before switching CPUs.
add a lock into pmap when modifying the hash table.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2007/11/04 13:43:39 martin Exp $ */
d324 1
a324 1
		vp1 = pool_get(&pmap_vp_pool, PR_NOWAIT);
a325 1
		bzero(vp1, sizeof (struct pmapvp));
d332 1
a332 1
		vp2 = pool_get(&pmap_vp_pool, PR_NOWAIT);
a333 1
		bzero(vp2, sizeof (struct pmapvp));
d517 1
a517 2
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
d725 1
a725 2
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
@


1.103
log
@replace even more ctob/btoc with ptoa/atop
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.102 2007/09/15 14:28:17 krw Exp $ */
d51 2
d159 58
d891 1
d893 2
a894 1
		ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */
d905 1
d907 1
d909 2
a910 1
		ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */
d921 1
a2335 1
	/* HASH lock? */
d2361 2
d2372 2
d2381 2
d2391 2
d2397 1
d2400 4
d2443 2
a2455 2
	/* HASH lock? */

d2482 2
d2492 2
d2501 2
d2511 2
d2517 1
d2520 3
d2553 1
@


1.102
log
@[fF]uther -> [fF]urther in comments and man page. First one spotted on
tech@@ by Jung.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2007/05/27 15:46:02 drahn Exp $ */
d1272 1
a1272 1
		physmem += btoc(mp->size);
d1529 1
a1529 1
		while (HTABSIZE_32 < (ctob(physmem) >> 7))
d1604 1
a1604 1
			pmap_kenter_cache(ctob(i), ctob(i), VM_PROT_ALL,
@


1.101
log
@Move powerpc to vm_page_md, 'throw it in' kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2007/05/13 03:45:59 drahn Exp $ */
d372 1
a372 1
 * Futher notes: It seems that the PV table is only used for pmap_protect
@


1.100
log
@Ansi prototypes, not K&R. no binary difference.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2007/05/03 18:40:21 miod Exp $ */
d4 1
a4 1
 * Copyright (c) 2001, 2002 Dale Rahn.
d36 1
a51 1
static int npgs;
a56 3

void * pmap_pvh;
void * pmap_attrib;
a70 3
/* P->V table */
LIST_HEAD(pted_pv_head, pte_desc);

d109 1
a109 1
int pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *);
a156 2
#define ATTRSHIFT	4

a283 25
/* 
 * HELPER FUNCTIONS 
 */
static inline struct pted_pv_head *
pmap_find_pvh(paddr_t pa)
{
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.pvent[off];
	}
	return NULL;
}

static inline char *
pmap_find_attr(paddr_t pa)
{
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.attrs[off];
	}
	return NULL;
}

d389 2
a390 2
int
pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *pvh)
a391 2
	int first;

d393 1
a393 4
		return 0;
	}
	if (pvh == NULL) {
		return 0;
d396 1
a396 2
	first = LIST_EMPTY(pvh);
	LIST_INSERT_HEAD(pvh, pted, pted_pv_list);
a397 1
	return first;
d406 17
d426 1
a426 2
	int bank, pg;
	u_int8_t *attr;
d428 2
a429 2
	bank = vm_physseg_find(atop(pa), &pg);
	if (bank == -1)
d431 2
a432 2
	attr = &vm_physmem[bank].pmseg.attrs[pg];
	*attr |= (u_int8_t)(bits >> ATTRSHIFT);
d439 1
a439 1
	struct pted_pv_head *pvh;
a442 2
	u_int8_t *pattr = NULL;
	int first_map = 0;
d465 2
a466 2
	pvh = pmap_find_pvh(pa);
	if (pvh != NULL)
d476 2
a477 20
	if (pvh != NULL) {
		pattr = pmap_find_attr(pa); /* pattr only for managed mem */
		first_map = pmap_enter_pv(pted, pvh); /* only managed mem */
	}

	/* 
	 * We want to flush for executable pages which are not managed???
	 * Always flush for the first mapping if it is executable.
	 * If previous mappings exist, but this is the first EXE, sync.
	 */

	if (prot & VM_PROT_EXECUTE) {
		need_sync = 1;
		if (pvh != NULL) {
			if (!first_map)
				need_sync =
				    (*pattr & (PTE_EXE_32 >> ATTRSHIFT)) == 0;
			else if (pattr != NULL)
				*pattr = 0;
		}
d497 5
a501 2
		if (pattr != NULL)
			*pattr |= (PTE_EXE_32 >> ATTRSHIFT);
d507 2
a508 2
		if ((prot & VM_PROT_WRITE) && (pattr != NULL))
			*pattr &= ~(PTE_EXE_32 >> ATTRSHIFT);
d674 1
a674 1
		if (pmap_find_pvh(pa) != NULL)
d942 1
a942 1
pteclrbits(paddr_t pa, u_int bit, u_int clear)
a943 1
	char *pattr;
d947 1
a947 1
	struct pted_pv_head *pvh;
d950 1
a950 9
	/* PTE_REG_32 == PTE_REG_64 */

	pattr = pmap_find_attr(pa);

	/* check if managed memory */
	if (pattr == NULL)
		return 0;

	pvh = pmap_find_pvh(pa);
d955 2
a956 2
	bits = (*pattr << ATTRSHIFT) & bit;
	if ((bits == bit) && (clear == 0))
d967 1
a967 1
	LIST_FOREACH(pted, pvh, pted_pv_list) {
d994 1
a994 1
				bits |=	ptp64->pte_lo & bit;
d1000 1
a1000 1
					ptp64->pte_lo &= ~bit;
d1003 1
a1003 1
				} else if (bits == bit)
d1022 1
a1022 1
				bits |=	ptp32->pte_lo & bit;
d1028 1
a1028 1
					ptp32->pte_lo &= ~bit;
d1031 1
a1031 1
				} else if (bits == bit)
d1037 9
a1045 4
	if (clear)
		*pattr &= ~(bit >> ATTRSHIFT);
	else
		*pattr |= (bits >> ATTRSHIFT);
a1616 9
	npgs = 0;
	for (mp = pmap_avail; mp->size; mp++) {
		npgs += btoc(mp->size);
	}
	/* Ok we lose a few pages from this allocation, but hopefully
	 * not too many 
	 */
	pmap_pvh = pmap_steal_avail(sizeof(struct pted_pv_head *) * npgs, 4);
	pmap_attrib = pmap_steal_avail(sizeof(char) * npgs, 1);
a2015 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a2017 1
	struct pted_pv_head *pvh;
a2020 7
	pvh = pmap_find_pvh(pa);

	/* nothing to do if not a managed page */
	if (pvh == NULL) {
		splx(s);
		return;
	}
d2023 2
a2024 2
		while (!LIST_EMPTY(pvh)) {
			pted = LIST_FIRST(pvh);
d2027 2
d2033 1
a2033 1
	LIST_FOREACH(pted, pvh, pted_pv_list) {
a2091 5
	vsize_t sz;
	struct pted_pv_head *pvh;
	char *attr;
	int i, bank;

a2099 16
	/* pmap_pvh and pmap_attr must be allocated 1-1 so that pmap_save_attr
	 * is callable from pte_spill_r (with vm disabled)
	 */
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (i = npgs; i > 0; i--)
		LIST_INIT(pvh++);
	attr = pmap_attrib;
	bzero(pmap_attrib, npgs);
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		sz = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pvh;
		vm_physmem[bank].pmseg.attrs = attr;
		pvh += sz;
		attr += sz;
	}
d2307 1
a2307 1
	/* first just try fill of secondary hash */
d2351 1
a2351 1
		    ptp64->pte_lo & (PTE_REF_32|PTE_CHG_32));
d2414 1
a2414 1
	/* first just try fill of secondary hash */
a2636 1
	struct pted_pv_head *pvh;
d2638 4
a2641 2
	pvh = pmap_find_pvh(pa);
	if (pvh == NULL) {
d2644 1
a2644 1
		LIST_FOREACH(pted, pvh, pted_pv_list) {
@


1.99
log
@Implement pmap_steal_memory() on powerpc. With some help from art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2007/04/13 18:12:16 miod Exp $ */
d460 1
a460 6
pmap_enter(pm, va, pa, prot, flags)
	pmap_t pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
a747 1

d1753 1
a1753 4
copyin(udaddr, kaddr, len)
	const void *udaddr;
	void *kaddr;
	size_t len;
d1783 1
a1783 4
copyout(kaddr, udaddr, len)
	const void *kaddr;
	void *udaddr;
	size_t len;
@


1.98
log
@Relax the cache flags logic in pmap_kenter_pa, to make sure that mappings
entered before vm_physmem[] are initialized will be cached. This is a
temporary measure until this pmap implements pmap_steal_memory().

Help and ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2007/02/22 20:34:46 thib Exp $ */
d720 1
a720 1
		if (pa < 0x80000000 || pmap_find_pvh(pa) != NULL)
d1468 66
d1679 4
a2159 10
}

/*
 * How much virtual space is available to the kernel?
 */
void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	*start = VM_MIN_KERNEL_ADDRESS;
	*end = VM_MAX_KERNEL_ADDRESS;
@


1.97
log
@Dont pass seemingly random numbers down as the flag
parameter of pool_init()

ok drahn@@
no objections miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2005/12/29 23:54:49 kettenis Exp $ */
a696 1
	struct pted_pv_head *pvh;
a718 1
	pvh = pmap_find_pvh(pa);
d720 1
a720 1
		if (pvh != NULL)
@


1.96
log
@W^X for G5
ok drahn@@, totally groovy deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2005/12/17 07:31:27 miod Exp $ */
d2112 1
a2112 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 20, "pmap", NULL);
d2114 1
a2114 1
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, 0, 150, "vp", NULL);
d2116 1
a2116 1
	pool_init(&pmap_pted_pool, sizeof(struct pte_desc), 0, 0, 150, "pted",
@


1.95
log
@Get rid of deprecated vm_{offset,size}_t types for good, use {p,v}{addr,size}_t
instead; looked at millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2005/11/13 03:56:26 brad Exp $ */
d95 1
a95 1
void pmap_page_ro64(pmap_t pm, vaddr_t va);
d520 1
a520 1
		 if (pvh != NULL) {
a940 1
	/* XXX Per-page execution control. */
d943 2
d1912 1
a1912 1
pmap_page_ro64(pmap_t pm, vaddr_t va)
d1925 3
d2040 1
a2040 1
			pmap_page_ro64(pted->pted_pmap, pted->pted_va);
d2051 1
a2051 1
	if (prot & VM_PROT_READ) {
d2055 1
a2055 1
				pmap_page_ro64(pm, sva);
@


1.94
log
@splimp -> splvm

ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2005/10/09 14:01:11 drahn Exp $ */
d2070 1
a2070 1
pmap_real_memory(paddr_t *start, vm_size_t *size)
@


1.93
log
@Nearly functional crashdump support for macppc. Because savecore
does not recognize the resulting crashdumps, the writing has been disabled.
Better here than in my forest of trees.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2005/10/08 06:03:00 drahn Exp $ */
d258 1
a258 1
 * Should this be called under splimp?
d271 1
a271 1
		s = splimp();
d280 1
a280 1
		s = splimp();
d477 1
a477 1
	s = splimp();
d641 1
a641 1
	s = splimp();
d702 1
a702 1
	s = splimp();
d789 1
a789 1
	s = splimp();
d1022 1
a1022 1
	s = splimp();
d1192 1
a1192 1
			s = splimp();
d1220 1
a1220 1
	s = splimp();
d1258 1
a1258 1
	s = splimp();
d1279 1
a1279 1
	s = splimp();
d1302 1
a1302 1
			s = splimp();
d1307 1
a1307 1
		s = splimp();
d2016 1
a2016 1
	s = splimp();
d2048 1
a2048 1
		s = splimp();
@


1.92
log
@64 bit compat. clean up the 32/64 code paths so less decision points exist
allow more than 256MB ram on G5, (still 2G limit) by creating PTE entries
dynamically for all physical memory.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2005/10/03 04:47:30 drahn Exp $ */
d1322 2
a1323 1
	for (mp = pmap_mem; mp->size !=0; mp++)
d1325 3
d1572 2
@


1.91
log
@Handle segment register restore at context enter/exit instead of
deep in the kernel. Based on code from two years ago, now necessary
for G5. removes the 1GB ram limit on 32bit processors, temporarily
sets a 256MB limit on G5.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2005/10/03 02:18:50 drahn Exp $ */
d95 2
a96 1
void pmap_page_ro(pmap_t pm, vaddr_t va);
d123 3
a125 1
void pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa,
d336 2
a337 7
	if (ppc_proc_is_64b)
		for (va = 0; va < 0x10000000; va += 0x00001000)
			tlbie(va);
	else
		for (va = 0; va < 0x00040000; va += 0x00001000)
			tlbie(va);

d502 4
a505 1
	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);
d729 4
a732 1
	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);
d912 1
a912 1
pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
a915 1
	struct pte_32 *pte32;
d919 6
a924 2
	if (ppc_proc_is_64b) {
		pte64 = &pted->p.pted_pte64;
a925 4
		pte64->pte_hi = (((u_int64_t)sr & SR_VSID) <<
		   PTE_VSID_SHIFT_64) |
		    ((va >> ADDR_API_SHIFT_64) & PTE_API_64) | PTE_VALID_64;
		pte64->pte_lo = (pa & PTE_RPGN_64);
d927 6
d934 4
a937 6
		if ((cache == PMAP_CACHE_WB))
			pte64->pte_lo |= PTE_M_64;
		else if ((cache == PMAP_CACHE_WT))
			pte64->pte_lo |= (PTE_W_64 | PTE_M_64);
		else
			pte64->pte_lo |= (PTE_M_64 | PTE_I_64 | PTE_G_64);
d939 1
a939 6
		if (prot & VM_PROT_WRITE)
			pte64->pte_lo |= PTE_RW_64;
		else
			pte64->pte_lo |= PTE_RO_64;
	} else {
		pte32 = &pted->p.pted_pte32;
d941 3
a943 3
		pte32->pte_hi = ((sr & SR_VSID) << PTE_VSID_SHIFT_32) |
		    ((va >> ADDR_API_SHIFT_32) & PTE_API_32) | PTE_VALID_32;
		pte32->pte_lo = (pa & PTE_RPGN_32);
d945 11
d957 13
a969 6
		if ((cache == PMAP_CACHE_WB))
			pte32->pte_lo |= PTE_M_32;
		else if ((cache == PMAP_CACHE_WT))
			pte32->pte_lo |= (PTE_W_32 | PTE_M_32);
		else
			pte32->pte_lo |= (PTE_M_32 | PTE_I_32 | PTE_G_32);
d971 4
a974 5
		if (prot & VM_PROT_WRITE)
			pte32->pte_lo |= PTE_RW_32;
		else
			pte32->pte_lo |= PTE_RO_32;
	}
d1203 1
a1203 1
			for (k = 0; k < 16; k++) {
a1204 1
			}
d1325 3
a1327 30
	if (ppc_proc_is_64b) { 
		/* limit to 256MB available, for now -XXXGRR */
#define MEMMAX 0x10000000
		for (mp = pmap_avail; mp->size !=0 ; /* increment in loop */) {
			if (mp->start + mp->size > MEMMAX) {
				int rm_start;
				int rm_end;
				if (mp->start > MEMMAX) {
					rm_start = mp->start;
					rm_end = mp->start+mp->size;
				} else {
					rm_start = MEMMAX;
					rm_end = mp->start+mp->size;
				}
				pmap_remove_avail(rm_start, rm_end);

				/* whack physmem, since we ignore more than
				 * 256MB
				 */
				physmem = btoc(MEMMAX);

				/*
				 * start over at top, make sure not to
				 * skip any
				 */
				mp = pmap_avail;
				continue;
			}
			mp++;
		}
d1511 1
a1511 1
		while ((HTABSIZE_32 << 7) < ctob(physmem))
a1903 3
/*
 * Change a page to readonly
 */
d1905 1
a1905 1
pmap_page_ro(pmap_t pm, vaddr_t va)
a1907 1
	struct pte_32 *ptp32;
d1915 2
a1916 7
	if (ppc_proc_is_64b) {
		pted->p.pted_pte64.pte_lo &= ~PTE_PP_64;
		pted->p.pted_pte64.pte_lo |= PTE_RO_64;
	} else {
		pted->p.pted_pte32.pte_lo &= ~PTE_PP_32;
		pted->p.pted_pte32.pte_lo |= PTE_RO_32;
	}
d1921 4
a1924 5
	if (ppc_proc_is_64b) {
		/* determine which pteg mapping is present in */
		ptp64 = pmap_ptable64 +
		    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		ptp64 += PTED_PTEGIDX(pted); /* increment by index into pteg */
d1926 15
a1940 21
		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->p.pted_pte64.pte_hi | (PTED_HID(pted) ? PTE_HID_64 : 0))
		    == ptp64->pte_hi) {
			ptp64->pte_hi &= ~PTE_VALID_64;
			__asm__ volatile ("sync");
			tlbie(va);
			tlbsync();
			if (PTED_MANAGED(pted)) { /* XXX */
				pmap_attr_save(ptp64->pte_lo & PTE_RPGN_64,
				    ptp64->pte_lo & (PTE_REF_64|PTE_CHG_64));
			}
			ptp64->pte_lo &= ~PTE_CHG_64;
			ptp64->pte_lo &= ~PTE_PP_64;
			ptp64->pte_lo |= PTE_RO_64;
			__asm__ volatile ("sync");
			ptp64->pte_hi |= PTE_VALID_64;
d1942 28
a1969 5
	} else {
		/* determine which pteg mapping is present in */
		ptp32 = pmap_ptable32 +
		    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		ptp32 += PTED_PTEGIDX(pted); /* increment by index into pteg */
d1971 15
a1985 21
		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->p.pted_pte32.pte_hi | (PTED_HID(pted) ? PTE_HID_32 : 0))
		    == ptp32->pte_hi) {
			ptp32->pte_hi &= ~PTE_VALID_32;
			__asm__ volatile ("sync");
			tlbie(va);
			tlbsync();
			if (PTED_MANAGED(pted)) { /* XXX */
				pmap_attr_save(ptp32->pte_lo & PTE_RPGN_32,
				    ptp32->pte_lo & (PTE_REF_32|PTE_CHG_32));
			}
			ptp32->pte_lo &= ~PTE_CHG_32;
			ptp32->pte_lo &= ~PTE_PP_32;
			ptp32->pte_lo |= PTE_RO_32;
			__asm__ volatile ("sync");
			ptp32->pte_hi |= PTE_VALID_32;
d1987 5
d2029 4
a2032 1
		pmap_page_ro(pted->pted_pmap, pted->pted_va);
d2043 10
a2052 3
		while (sva < eva) {
			pmap_page_ro(pm, sva);
			sva += PAGE_SIZE;
d2161 3
d2168 23
a2190 4
	if (!(msr & PSL_PR)) {
		/* lookup is done physical to prevent faults */
		if (VP_SR(va) == PPC_USER_SR) {
			return 0;
d2192 5
a2196 1
			pm = pmap_kernel();
d2198 1
a2198 2
	} else {
		return 0;
d2218 6
d2229 6
a2235 9
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executable page */
		return 0;
	}
	if (ppc_proc_is_64b)
		pte_insert64(pted);
	else
		pte_insert32(pted);
@


1.90
log
@G5 pmap support, most of this G5 work has been done by kettenis@@
without his forging ahead, it would barely be started.
Again this is one step of many, but needs to be tested, this is
independant of the locore change just committed which kettenis@@ and
deraadt@@ significantly wrote.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2005/05/02 19:03:35 kettenis Exp $ */
d158 1
d542 1
a542 1
		if (pm->pm_sr[sn] & SR_NOEXEC) {
a544 8
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
			 */
			if (sn != PPC_USER_SR && sn != PPC_KERNEL_SR &&
			    curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d663 1
a663 1
		if (pm->pm_exec[sn] == 0) {
a664 9
			
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
			 */
			if (sn != PPC_USER_SR && sn != PPC_KERNEL_SR &&
			    curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d746 1
a746 1
		if (pm->pm_sr[sn] & SR_NOEXEC) {
a747 9

			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
			 */
			if (sn != PPC_USER_SR && sn != PPC_KERNEL_SR &&
			    curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d801 1
a801 1
		if (pm->pm_exec[sn] == 0) {
a802 9

			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
			 */
			if (sn != PPC_USER_SR && sn != PPC_KERNEL_SR &&
			    curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d1307 27
a1333 12
	/* limit to 1GB available, for now -XXXGRR */
#define MEMMAX 0x40000000
	for (mp = pmap_avail; mp->size !=0 ; /* increment in loop */) {
		if (mp->start + mp->size > MEMMAX) {
			int rm_start;
			int rm_end;
			if (mp->start > MEMMAX) {
				rm_start = mp->start;
				rm_end = mp->start+mp->size;
			} else {
				rm_start = MEMMAX;
				rm_end = mp->start+mp->size;
d1335 1
a1335 8
			pmap_remove_avail(rm_start, rm_end);

			/* whack physmem, since we ignore more than 256MB */
			physmem = btoc(MEMMAX);

			/* start over at top, make sure not to skip any */
			mp = pmap_avail;
			continue;
a1336 1
		mp++;
d1338 1
@


1.89
log
@Avoid infite loop.
ok drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2004/06/24 22:35:56 drahn Exp $ */
d60 2
a61 1
struct pte  *pmap_ptable;
d64 4
a67 1
#define HTABSIZE (pmap_ptab_cnt * 64)
d80 4
a83 1
	struct pte pted_pte;
d92 1
a92 1
static inline void tlbia(void);
d119 2
a120 1
void pte_insert(struct pte_desc *pted);
d146 1
a146 1
void pte_zap(struct pte *ptp, struct pte_desc *pted);
d326 1
a326 1
static inline void
d332 6
a337 2
	for (va = 0; va < 0x00040000; va += 0x00001000)
		tlbie(va);
d389 4
a392 1
	return (pted->pted_pte.pte_hi & PTE_VALID);
d521 1
a521 1
				    (*pattr & (PTE_EXE >> ATTRSHIFT)) == 0;
d532 4
a535 1
	pte_insert(pted);
d553 1
a553 1
			*pattr |= (PTE_EXE >> ATTRSHIFT);
d560 1
a560 1
			*pattr &= ~(PTE_EXE >> ATTRSHIFT);
d683 4
a686 1
	pted->pted_pte.pte_hi &= ~PTE_VALID;
d751 5
a755 1
	pte_insert(pted);
d843 4
a846 1
	pted->pted_pte.pte_hi &= ~PTE_VALID;
d862 1
a862 1
pte_zap(struct pte *ptp, struct pte_desc *pted)
d864 19
a882 6
		ptp->pte_hi &= ~PTE_VALID;
		__asm volatile ("sync");
		tlbie(pted->pted_va);
		__asm volatile ("sync");
		tlbsync();
		__asm volatile ("sync");
d884 3
a886 2
			pmap_attr_save(pted->pted_pte.pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
d899 2
a900 1
	struct pte *ptp;
a907 2
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
d909 26
a934 8
	/*
	 * We now have the pointer to where it will be, if it is currently
	 * mapped. If the mapping was thrown away in exchange for another
	 * page mapping, then this page is not currently in the HASH.
	 */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp, pted);
d946 2
a947 1
	struct pte *pte;
d950 8
a957 4
	pte = &pted->pted_pte;
	pte->pte_hi = ((sr & SR_VSID) << PTE_VSID_SHIFT) |
	    ((va >> ADDR_API_SHIFT) & PTE_API) | PTE_VALID;
	pte->pte_lo = (pa & PTE_RPGN);
d959 6
d966 18
a983 6
	if ((cache == PMAP_CACHE_WB))
		pte->pte_lo |= PTE_M;
	else if ((cache == PMAP_CACHE_WT))
		pte->pte_lo |= (PTE_W | PTE_M);
	else
		pte->pte_lo |= (PTE_M | PTE_I | PTE_G);
d985 5
a989 4
	if (prot & VM_PROT_WRITE)
		pte->pte_lo |= PTE_RW;
	else
		pte->pte_lo |= PTE_RO;
d993 1
d1013 3
d1042 2
a1043 1
		struct pte *ptp;
d1050 56
a1105 26
		ptp = pmap_ptable +
			(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 *
		 * if we are not clearing bits, and have found all of the
		 * bits we want, we can stop
		 */
		if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
		    == ptp->pte_hi) {
			bits |=	ptp->pte_lo & bit;
			if (clear) {
				ptp->pte_hi &= ~PTE_VALID;
				__asm__ volatile ("sync");
				tlbie(va);
				tlbsync();
				ptp->pte_lo &= ~bit;
				__asm__ volatile ("sync");
				ptp->pte_hi |= PTE_VALID;
			} else if (bits == bit)
				break;
d1515 2
d1537 11
a1547 6
#ifndef  HTABENTS
#define HTABENTS 1024
#endif
	pmap_ptab_cnt = HTABENTS;
	while ((HTABSIZE << 7) < ctob(physmem)) {
		pmap_ptab_cnt <<= 1;
d1552 9
a1560 3
	pmap_ptable = pmap_steal_avail(HTABSIZE, HTABSIZE);
	bzero((void *)pmap_ptable, HTABSIZE);
	pmap_ptab_mask = pmap_ptab_cnt - 1;
a1568 1

d1580 17
d1616 10
a1625 2
	asm volatile ("sync; mtsdr1 %0; isync"
	    :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));
d1686 6
a1691 1
	*pa = (pted->pted_pte.pte_lo & PTE_RPGN) | (va & ~PTE_RPGN);
d1945 2
a1946 1
	struct pte *ptp;
d1954 7
a1960 2
	pted->pted_pte.pte_lo &= ~PTE_PP;
	pted->pted_pte.pte_lo |= PTE_RO;
d1965 33
a1997 3
	/* determine which pteg mapping is present in */
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
d1999 22
a2020 21
	/*
	 * We now have the pointer to where it will be, if it is
	 * currently mapped. If the mapping was thrown away in
	 * exchange for another page mapping, then this page is
	 * not currently in the HASH.
	 */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		ptp->pte_hi &= ~PTE_VALID;
		__asm__ volatile ("sync");
		tlbie(va);
		tlbsync();
		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(ptp->pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
		}
		ptp->pte_lo &= ~PTE_CHG;
		ptp->pte_lo &= ~PTE_PP;
		ptp->pte_lo |= PTE_RO;
		__asm__ volatile ("sync");
		ptp->pte_hi |= PTE_VALID;
d2206 11
a2216 3
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
		/* write fault and we have a readonly mapping */
		return 0;
d2223 4
a2226 1
	pte_insert(pted);
d2248 10
a2257 3
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
		/* write fault and we have a readonly mapping */
		return 0;
d2264 4
a2267 1
	pte_insert(pted);
d2278 105
d2385 1
a2385 1
pte_insert(struct pte_desc *pted)
d2389 1
a2389 1
	struct pte *ptp;
d2399 6
a2404 6
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp,pted);
	}
d2417 1
a2417 1
	ptp = pmap_ptable + (idx) * 8;
d2419 1
a2419 1
		if (ptp[i].pte_hi & PTE_VALID)
a2422 1
/* printf("inserting in primary idx %x, i %x\n", idx, i); */
d2424 2
a2425 2
		ptp[i].pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
d2427 1
a2427 1
		ptp[i].pte_hi |= PTE_VALID;
d2432 1
a2432 1
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
d2434 1
a2434 1
		if (ptp[i].pte_hi & PTE_VALID)
d2438 3
a2440 2
		ptp[i].pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
d2442 1
a2442 1
		ptp[i].pte_hi |= PTE_VALID;
d2452 1
a2452 1
	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));
d2454 3
a2456 3
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if (ptp->pte_hi & PTE_VALID) {
d2458 1
a2458 1
		ptp->pte_hi &= ~PTE_VALID;
d2461 3
a2463 3
		va = ((ptp->pte_hi & PTE_API) << ADDR_API_SHIFT) |
		     ((((ptp->pte_hi >> PTE_VSID_SHIFT) & SR_VSID)
			^(idx ^ ((ptp->pte_hi & PTE_HID) ? 0x3ff : 0)))
d2468 2
a2469 2
		pmap_attr_save(ptp->pte_lo & PTE_RPGN,
		    ptp->pte_lo & (PTE_REF|PTE_CHG));
a2470 1

d2472 2
a2473 1
		ptp->pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
d2475 4
a2478 1
		ptp->pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
a2479 3
	ptp->pte_lo = pted->pted_pte.pte_lo;
	__asm__ volatile ("sync");
	ptp->pte_hi |= PTE_VALID;
d2526 21
a2546 8
		print("ptehi %x ptelo %x ptp %x Aptp %x\n",
		    pted->pted_pte.pte_hi, pted->pted_pte.pte_lo,
		    pmap_ptable +
			8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
		    pmap_ptable +
			8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
			    ^ pmap_ptab_mask)
		    );
@


1.88
log
@Do a better job at containing powerpc specific #defines to PPC_...
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2004/05/20 09:20:42 kettenis Exp $ */
d1958 2
a1959 1
		clen = round_page(addr) - addr;
@


1.87
log
@Properly flush instruction cache for ptrace(PT_WRTIE_{DI}, ...) on powerpc
and m68k.
ok drahn@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2004/01/25 13:22:10 miod Exp $ */
d529 2
a530 1
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
d658 2
a659 1
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
d743 2
a744 1
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
d807 2
a808 1
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
d1437 1
a1437 1
	pmap_kernel()->pm_vp[KERNEL_SR] = vp1;
d1461 3
a1463 3
#if NPMAPS >= KERNEL_SEGMENT / 16
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
d1466 2
a1467 2
		pmap_kernel()->pm_sr[i] = (KERNEL_SEG0 + i) | SR_NOEXEC;
		ppc_mtsrin(KERNEL_SEG0 + i, i << ADDR_SR_SHIFT);
d1545 1
a1545 1
	    : "=r" (oldsr): "n"(USER_SR));
d1547 1
a1547 1
	    :: "n"(USER_SR), "r"(sr));
d1555 1
a1555 1
	    :: "n"(USER_SR), "r"(sr));
d1571 2
a1572 2
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
d1604 2
a1605 2
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
d1640 2
a1641 2
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
d1696 2
a1697 2
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
d1750 2
a1751 1
		start = ((u_int)USER_ADDR + ((u_int)va & ~SEGMENT_MASK));
d1988 1
a1988 1
		if (VP_SR(va) == USER_SR) {
@


1.86
log
@Various typos in comments.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2004/01/03 00:57:06 pvalchev Exp $ */
d1944 20
@


1.85
log
@backout segment register restore diff which causes reproducible hangs; ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2003/12/20 22:40:28 miod Exp $ */
d190 1
a190 1
 * otherwise bad race conditions can appear.
d246 1
a246 1
 * should this be called under splimp?
d382 1
a382 1
 * manpulate the physical to virtual translations for the entire system.
d384 1
a384 1
 * QUESTION: should all mapped memory be stored in PV tables? or
d390 1
a390 1
 * mappings.
d405 1
a405 1
 * have too noticable unnecssary ram consumption.
d484 1
a484 1
		cache = PMAP_CACHE_WB; /* managed memory is cachable */
d514 1
a514 1
	 * we were told to map the page, probably called from vm_fault,
d526 2
a527 2
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
d654 2
a655 2
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
d715 1
a715 1
			cache = PMAP_CACHE_WB; /* managed memory is cachable */
d725 1
a725 1
	 * we were told to map the page, probably called from vm_fault,
d738 2
a739 2
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
d801 2
a802 2
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
d1009 1
a1009 1
	 * Is there pool functions which could be called
d1075 1
a1075 1
	 * try not to reuse pmap ids, to spread the hash table usage.
d1284 1
a1284 1
		 * Check if this region hold all of the region
d1477 1
a1477 1
	/* Ok we loose a few pages from this allocation, but hopefully
d1989 1
a1989 1
		/* attempted to execute non-executeable page */
d2020 1
a2020 1
		/* attempted to execute non-executeable page */
@


1.84
log
@Pass -Wformat
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2003/10/31 03:06:16 drahn Exp $ */
d523 1
a523 1
		if (pm->pm_sr[sn] & SR_NOEXEC)
d526 7
d651 1
a651 1
		if (pm->pm_exec[sn] == 0)
d653 8
d735 1
a735 1
		if (pm->pm_sr[sn] & SR_NOEXEC)
d737 8
d798 1
a798 1
		if (pm->pm_exec[sn] == 0)
d800 8
d1096 1
a1096 1
			for (k = 0; k < 16; k++)
d1098 1
d1219 24
d1409 4
d1417 1
a1417 1
	while (HTABSIZE < (ctob(physmem) >> 7)) {
d1927 1
a1927 1
	/* pmap_pvh and pmap_attr must be allocated 1-1 so that pmap_attr_save
@


1.83
log
@Fix ppc segment register restores, this fixes the 1GB ram limit and
cleans up pieces in the pmap code.
tested otto, brad, miod, pval.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2003/07/02 21:30:12 drahn Exp $ */
d690 1
a690 1
		printf("pted not preallocated in pmap_kernel() va %x pa %x \n",
@


1.82
log
@Reduce the amount of asm code in powerpc/macppc by replacing it with
inlined functions, helps improve readability and fix a couple of bugs.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2003/06/03 01:35:30 drahn Exp $ */
d523 1
a523 1
		if (pm->pm_sr[sn] & SR_NOEXEC) {
a525 7
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d644 1
a644 1
		if (pm->pm_exec[sn] == 0) {
a645 8
			
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d720 1
a720 1
		if (pm->pm_sr[sn] & SR_NOEXEC) {
a721 8

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d775 1
a775 1
		if (pm->pm_exec[sn] == 0) {
a776 8

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
		}
d1065 1
a1065 1
			for (k = 0; k < 16; k++) {
a1066 1
			}
a1186 24
	/* limit to 1GB available, for now -XXXGRR */
#define MEMMAX 0x40000000
	for (mp = pmap_avail; mp->size !=0 ; /* increment in loop */) {
		if (mp->start + mp->size > MEMMAX) {
			int rm_start;
			int rm_end;
			if (mp->start > MEMMAX) {
				rm_start = mp->start;
				rm_end = mp->start+mp->size;
			} else {
				rm_start = MEMMAX;
				rm_end = mp->start+mp->size;
			}
			pmap_remove_avail(rm_start, rm_end);

			/* whack physmem, since we ignore more than 256MB */
			physmem = btoc(MEMMAX);

			/* start over at top, make sure not to skip any */
			mp = pmap_avail;
			continue;
		}
		mp++;
	}
a1352 4
	for (mp = pmap_avail; mp->size; mp++) {
		bzero((void *)mp->start, mp->size);
	}

d1357 1
a1357 1
	while ((HTABSIZE << 7) < ctob(physmem)) {
d1867 1
a1867 1
	/* pmap_pvh and pmap_attr must be allocated 1-1 so that pmap_save_attr
@


1.81
log
@kill clause 3 and 4 from several of my copyrights, cleanup.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2003/02/26 21:54:44 drahn Exp $ */
d530 2
a531 3
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
d658 2
a659 3
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
d742 2
a743 3
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
d805 2
a806 3
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
d1463 1
a1463 2
		asm volatile ("mtsrin %0,%1"
		    :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
@


1.80
log
@Remove an unnecessary structure copy from useage of setfault(), call
by reference, not by value, ok matthieu#, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2003/01/30 15:38:09 drahn Exp $ */
d4 2
a5 1
 * Copyright (c) 2001, 2002 Dale Rahn. All rights reserved.
a15 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Dale Rahn.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.79
log
@Track if a physical page has been previously mapped executable. If it
has not been previously mapped EXE, flush it. If a writeable mapping
which is not executable occurs for the page, clear this bit.
Solves a problem where an executable page is double mapped, first without
EXE then accessed for execute at a different physical page, the cache
will behave properly.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2002/11/06 00:17:27 art Exp $ */
d1581 1
a1581 1
		if (setfault(env)) {
d1614 1
a1614 1
		if (setfault(env)) {
d1652 1
a1652 1
		if (setfault(env)) {
d1708 1
a1708 1
		if (setfault(env)) {
@


1.78
log
@Eliminate the use of KERN_SUCCESS outside of uvm/

Also uvm_map returns KERN_* codes that are directly mapped to
errnos, so we can return them instead of doing some attempt to
translation.

drahn@@ "I see no problem" pval@@ "makes sense"
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2002/10/13 18:26:12 krw Exp $ */
d460 1
a460 1
	int need_sync;
d462 2
d494 21
a514 1
	need_sync = pmap_enter_pv(pted, pvh);
d538 9
d552 1
a552 1
	if (need_sync && (prot & VM_PROT_EXECUTE))
d975 3
d981 1
a981 1
			bits |=	ptp->pte_lo & (PTE_REF|PTE_CHG);
d990 2
a991 1
			}
a994 1
	bits |= (*pattr << ATTRSHIFT) & bit;
@


1.77
log
@Remove more '\n's from panic() statements.  From Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2002/09/15 09:01:59 deraadt Exp $ */
d525 1
a525 1
	return KERN_SUCCESS;
@


1.76
log
@backout premature
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2002/09/10 18:29:43 art Exp $ */
d1348 1
a1348 1
	panic ("unable to allocate region with size %x align %x\n",
@


1.75
log
@KNF
@
text
@d6 1
d35 1
a35 1
 */
d61 1
d103 1
d114 1
d176 1
a176 1
#if VP_IDX1_SIZE != VP_IDX2_SIZE
d183 1
d192 1
a192 1
 * initial pmap initialization. This is so that memory allocation
d249 1
a249 1
 *
d284 2
a285 2
/*
 * HELPER FUNCTIONS
d340 1
a340 1
static inline int
d357 1
a357 1
	return (pted->pted_va & PTED_VA_HID_M);
d363 1
a363 1
	return (pted->pted_va & PTED_VA_PTEGIDX_M);
d369 1
a369 1
	return (pted->pted_va & PTED_VA_MANAGED_M);
d375 1
a375 1
	return (pted->pted_va & PTED_VA_WIRED_M);
d387 1
a387 1
 *
d407 1
a407 1
 * wasted for every page which does not map ram (device mappings), this
d478 1
a478 1
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);
d501 1
a501 1
	if (prot & VM_PROT_EXECUTE) {
d504 1
a504 1
		pm->pm_exec[sn]++;
d514 1
a514 1
				    "r"(sn << ADDR_SR_SHIFT) );
d528 1
a528 1
/*
d540 1
a540 1
	/* I suspect that if this loop were unrolled better
d551 1
a551 1

d560 1
a560 1
			e_vp1 = VP_IDX1_SIZE-1;
d575 1
a575 1
				e_vp2 = VP_IDX2_SIZE;
d608 1
a608 1
		}
d627 1
a627 1

d634 1
a634 1
				    "r"(sn << ADDR_SR_SHIFT) );
d652 1
a652 1
 *
d656 1
a656 1
 *
d682 1
a682 1
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);
d706 1
a706 1
	if (prot & VM_PROT_EXECUTE) {
d709 1
a709 1
		pm->pm_exec[sn]++;
d719 1
a719 1
				    "r"(sn << ADDR_SR_SHIFT) );
d739 1
d783 1
a783 1
				    "r"(sn << ADDR_SR_SHIFT) );
d846 2
a847 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0)) ==
	    ptp->pte_hi) {
d868 1
d919 1
a919 1
	 * page, copying bits to the attribute cache
d936 1
a936 1
		    (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d945 2
a946 2
		if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0)) ==
		    ptp->pte_hi) {
d971 1
a971 1
 * Garbage collects the physical map system for pages which are
d979 1
a979 1
	/* This could return unused v->p table layers which
d1011 1
a1011 1

d1029 1
a1029 1

d1065 1
a1065 1
			usedsr[tblidx] |= (1 << tbloff);
d1079 1
a1079 1
/*
d1082 1
a1082 1
pmap_t
d1169 1
a1169 1

d1197 2
a1198 2
			int rm_start, rm_end;

d1229 1
a1229 1
	while (mp->size !=0) {
d1326 1
a1326 1
		mp->size = end - base;
d1341 1
a1341 1
			remsize = mp->size - (start - mp->start);
d1358 1
a1358 1
 */
d1427 1
d1432 2
a1433 2
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)] |=
	    1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
d1445 1
d1453 1
a1453 1
	 * not too many
d1484 1
a1484 1
/*
d1486 1
a1486 1
 */
d1600 1
a1600 1
	u_char *kp = kaddr;
d1636 1
a1636 1
			}
d1656 1
a1656 1
	const u_char *kp = kaddr;
d1692 1
a1692 1
			}
d1742 1
d1779 2
a1780 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0)) ==
	    ptp->pte_hi) {
d1829 1
a1829 1

d1861 3
a1863 2
		if (((*start + *size) > mp->start) &&
		    (*start < (mp->start + mp->size))) {
d1921 1
a1921 1
/*
d1933 1
a1933 1
	/*
d1957 1
a1957 1
	}
d1962 2
a1963 2
	if (exec_fault != 0 &&
	    (pted->pted_va & PTED_VA_EXEC_M) == 0) {
d1993 2
a1994 2
	if (exec_fault != 0 &&
	    (pted->pted_va & PTED_VA_EXEC_M) == 0) {
d2002 1
d2027 2
a2028 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0)) ||
	    ptp->pte_hi) {
d2088 3
a2090 3
		    ((((ptp->pte_hi >> PTE_VSID_SHIFT) & SR_VSID)
		    ^(idx ^ ((ptp->pte_hi & PTE_HID) ? 0x3ff : 0)))
		    & 0x3ff) << PAGE_SHIFT;
d2136 1
d2154 6
a2159 3
		    pmap_ptable + 8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
		    pmap_ptable + 8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va) ^
		    pmap_ptab_mask));
a2163 1

d2191 1
d2265 1
a2265 1
pmap_show_mappings(paddr_t pa)
@


1.74
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2002/07/24 02:19:28 drahn Exp $ */
a5 1
 *   
d34 1
a34 1
 */  
a59 1

a100 1

a110 1

d172 1
a172 1
#if VP_IDX1_SIZE != VP_IDX2_SIZE 
a178 1

d187 1
a187 1
 * initial pmap initialization. This is so that memory allocation 
d244 1
a244 1
 * 
d279 2
a280 2
/* 
 * HELPER FUNCTIONS 
d335 1
a335 1
static inline int 
d352 1
a352 1
	return (pted->pted_va & PTED_VA_HID_M); 
d358 1
a358 1
	return (pted->pted_va & PTED_VA_PTEGIDX_M); 
d364 1
a364 1
	return (pted->pted_va & PTED_VA_MANAGED_M); 
d370 1
a370 1
	return (pted->pted_va & PTED_VA_WIRED_M); 
d382 1
a382 1
 * 
d402 1
a402 1
 * wasted for every page which does not map ram (device mappings), this 
d473 1
a473 1
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
d496 1
a496 1
        if (prot & VM_PROT_EXECUTE) {
d499 1
a499 1
        	pm->pm_exec[sn]++;
d509 1
a509 1
				       "r"(sn << ADDR_SR_SHIFT) );
d523 1
a523 1
/* 
d535 1
a535 1
	/* I suspect that if this loop were unrolled better 
d546 1
a546 1
		
d555 1
a555 1
			e_vp1 = VP_IDX1_SIZE-1; 
d570 1
a570 1
				e_vp2 = VP_IDX2_SIZE; 
d603 1
a603 1
		} 
d622 1
a622 1
			
d629 1
a629 1
				       "r"(sn << ADDR_SR_SHIFT) );
d647 1
a647 1
 * 
d651 1
a651 1
 * 
d677 1
a677 1
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
d701 1
a701 1
        if (prot & VM_PROT_EXECUTE) {
d704 1
a704 1
        	pm->pm_exec[sn]++;
d714 1
a714 1
				       "r"(sn << ADDR_SR_SHIFT) );
a733 1

d777 1
a777 1
				       "r"(sn << ADDR_SR_SHIFT) );
d840 2
a841 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
a861 1

d912 1
a912 1
	 * page, copying bits to the attribute cache 
d929 1
a929 1
			(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d938 2
a939 2
		if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
		    == ptp->pte_hi) {
d964 1
a964 1
 * Garbage collects the physical map system for pages which are 
d972 1
a972 1
	/* This could return unused v->p table layers which 
d1004 1
a1004 1
	
d1022 1
a1022 1
	
d1058 1
a1058 1
			usedsr[tblidx] |= (1 << tbloff); 
d1072 1
a1072 1
/* 
d1075 1
a1075 1
pmap_t 
d1162 1
a1162 1
			
d1190 2
a1191 2
			int rm_start;
			int rm_end;
d1222 1
a1222 1
	while(mp->size !=0) {
d1319 1
a1319 1
		mp->size  = end - base;
d1334 1
a1334 1
			remsize = mp->size - (start - mp->start); 
d1351 1
a1351 1
 */ 
a1419 1

d1424 2
a1425 2
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
a1436 1

d1444 1
a1444 1
	 * not too many 
d1475 1
a1475 1
/* 
d1477 1
a1477 1
 */ 
d1591 1
a1591 1
	u_char *kp    = kaddr;
d1627 1
a1627 1
			} 
d1647 1
a1647 1
	const u_char *kp    = kaddr;
d1683 1
a1683 1
			} 
a1732 1

d1769 2
a1770 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
d1819 1
a1819 1
	
d1851 2
a1852 3
		if (((*start + *size) > mp->start)
			&& (*start < (mp->start + mp->size)))
		{
d1910 1
a1910 1
/* 
d1922 1
a1922 1
	/* 
d1946 1
a1946 1
	} 
d1951 2
a1952 2
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
d1982 2
a1983 2
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
a1990 1

d2015 2
a2016 2
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
d2076 3
a2078 3
		     ((((ptp->pte_hi >> PTE_VSID_SHIFT) & SR_VSID)
			^(idx ^ ((ptp->pte_hi & PTE_HID) ? 0x3ff : 0)))
			    & 0x3ff) << PAGE_SHIFT;
a2123 1

d2141 3
a2143 6
		    pmap_ptable +
			8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
		    pmap_ptable +
			8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
			    ^ pmap_ptab_mask)
		    );
d2148 1
a2175 1

d2249 1
a2249 1
pmap_show_mappings(paddr_t pa) 
@


1.73
log
@- change pte_spill_X() to take an extra parameter to determine if
  the fault is a EXE fault or R/W fault.

- mask/or the SR_NOEXEC bit into the segment register value
  when the number of executable pages becomes 0/non-zero.

- create segments with SR_NOEXEC set, will be cleared when first
  exec mapping in the segment is created.

- allow pte_spill_X() to deal with a new type of fault, page mapped
  but non executable, when execute was requested.

Adds up to - non-exec stack support for powerpc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2002/07/15 17:01:25 drahn Exp $ */
d992 1
a992 1
pmap_zero_page(paddr_t pa)
d994 1
d1019 1
a1019 1
pmap_copy_page(paddr_t srcpa, paddr_t dstpa)
d1021 2
@


1.72
log
@Perform accounting for executable pages on powerpc, prepare for
non-executeable stack.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2002/07/12 20:28:55 drahn Exp $ */
d135 2
a136 1
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type);
d501 16
a516 2
        if (prot & VM_PROT_EXECUTE)
        	pm->pm_exec[va >> ADDR_SR_SHIFT]++;
d621 2
a622 1
		pm->pm_exec[pted->pted_va >> ADDR_SR_SHIFT]--;
d624 12
d706 16
a721 2
        if (prot & VM_PROT_EXECUTE)
        	pm->pm_exec[va >> ADDR_SR_SHIFT]++;
d770 2
a771 1
		pm->pm_exec[pted->pted_va >> ADDR_SR_SHIFT]--;
d773 12
d1068 1
a1068 1
				pm->pm_sr[k] = seg + k;
d1139 1
a1139 1
	i = pm->pm_sr[0] >> 4;
d1433 1
a1433 1
		pmap_kernel()->pm_sr[i] = KERNEL_SEG0 + i;
d1925 1
a1925 1
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr)
a1928 1
	int retcode = 0;
a1933 1
		/* PSL_PR is clear for supervisor, right? - XXXXX */
d1945 18
a1962 13
	if (pted != NULL) {
		/* if the current mapping is RO and the access was a write
		 * we return 0
		 */
		if (!PTED_VALID(pted) ||
		    ((dsisr & (1 << (31-6)))
		    && (pted->pted_pte.pte_lo & 0x1))) {
			/* write fault and we have a readonly mapping */
			retcode = 0;
		} else {
			retcode = 1;
			pte_insert(pted);
		}
d1964 1
d1966 1
a1966 1
	return retcode;
d1970 1
a1970 1
pte_spill_v(pmap_t pm, u_int32_t va, u_int32_t dsisr)
d1983 4
a1986 2
	if (!PTED_VALID(pted) ||
	    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1))) {
d1988 5
a1992 2
		if (PTED_VALID(pted))
			pmap_hash_remove(pted);
@


1.71
log
@Cleanup: use less _t typedefs, use the structure itself.

pmap_t is the exception, it is required by the MI code so pmap_t will
be used instead of using 'struct pmap *' in the code.  (consistency)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2002/07/12 20:26:10 drahn Exp $ */
d347 5
a351 4
#define PTED_VA_PTEGIDX_M 0x07
#define PTED_VA_HID_M	  0x08
#define PTED_VA_MANAGED_M 0x10
#define PTED_VA_WIRED_M   0x20
d500 3
d605 5
d678 3
d727 5
d827 4
@


1.70
log
@argh, no last minute changes...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2002/06/10 00:06:47 drahn Exp $ */
d64 1
a64 1
pte_t  *pmap_ptable;
d81 1
a81 1
	struct pmap *pted_pmap;
d85 1
a85 1
void print_pteg(struct pmap *pm, vaddr_t va);
d92 1
a92 2
void pmap_page_ro(struct pmap *pm, vaddr_t va);

d118 1
a118 1
void pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa,
d121 1
a121 1
void pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va);
d125 1
a125 1
void pmap_remove_pg(struct pmap *pm, vaddr_t va);
d137 1
a137 1
u_int32_t pmap_setusr(struct pmap *pm, vaddr_t va);
d140 3
d198 2
a199 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d223 2
a224 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d254 2
a255 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d513 1
a513 1
pmap_remove(struct pmap *pm, vaddr_t va, vaddr_t endva)
d518 2
a519 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d573 1
a573 1
pmap_remove_pg(struct pmap *pm, vaddr_t va)
d626 1
a626 1
	struct pmap *pm;
d693 1
a693 1
	struct pmap *pm;
a733 1
void pte_zap(pte_t *ptp, struct pte_desc *pted);
d735 1
a735 1
pte_zap(pte_t *ptp, struct pte_desc *pted)
d757 2
a758 2
	struct pmap *pm = pted->pted_pmap;
	pte_t *ptp;
d784 1
a784 1
pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
d851 2
a852 2
		struct pmap *pm = pted->pted_pmap;
		pte_t *ptp;
d901 1
a901 1
pmap_collect(struct pmap *pm)
d959 1
a959 1
pmap_pinit(struct pmap *pm)
d1003 1
a1003 1
struct pmap *
d1006 1
a1006 1
	struct pmap *pmap;
d1020 1
a1020 1
pmap_reference(struct pmap *pm)
d1032 1
a1032 1
pmap_destroy(struct pmap *pm)
d1057 1
a1057 1
pmap_release(struct pmap *pm)
d1074 1
a1074 1
pmap_vp_destroy(struct pmap *pm)
d1078 2
a1079 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d1285 2
a1286 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d1409 1
a1409 1
pmap_extract(struct pmap *pm, vaddr_t va, paddr_t *pa)
d1428 1
a1428 1
pmap_setusr(struct pmap *pm, vaddr_t va)
d1635 1
a1635 1
pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va)
d1674 1
a1674 1
pmap_page_ro(struct pmap *pm, vaddr_t va)
d1676 1
a1676 1
	pte_t *ptp;
d1758 1
a1758 1
pmap_protect(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d1851 1
a1851 1
	struct pmap *pm;
d1890 1
a1890 1
pte_spill_v(struct pmap *pm, u_int32_t va, u_int32_t dsisr)
d2022 1
a2022 1
print_pteg(struct pmap *pm, vaddr_t va)
d2111 1
a2111 1
	struct pmap *pm;
d2138 3
a2140 3
	struct pmap *pm;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
@


1.69
log
@pmap cleanup and KNF.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2002/06/07 21:49:35 drahn Exp $ */
d1307 1
a1307 1
#ifdef  HTABENTS
@


1.68
log
@This doesn't happen, and Debugger should not be used here anyway...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2002/06/04 00:09:08 deraadt Exp $ */
a65 4
#ifdef USE_WTABLE
/* Wired entries in pmap_ptable */
Uint8_t *pmap_wtable;
#endif /* USE_WTABLE */
d133 1
a133 1
void * pmap_steal_avail(size_t size, int align);
a278 2

	return;
d282 1
a282 2
 * WHERE DO HELPER FUNCTIONS GO?
 * XXX - need pvent filled out and changed.
d294 1
d312 1
d327 1
a328 1

d336 1
d349 1
d355 1
d361 1
d367 1
d373 1
a407 1

d481 3
a483 4
	if (pvh != NULL) {
		/* managed memory is cachable */
		cache = PMAP_CACHE_WB;
	} else {
a484 1
	}
d500 1
a500 1
	if (need_sync && (prot & VM_PROT_EXECUTE)) {
a501 1
	}
d504 1
a504 1
	return 0;
d531 1
a531 1
		if (i_sr == s_sr) {
d533 1
a533 1
		} else {
d535 2
a536 2
		}
		if (i_sr == e_sr) {
d538 1
a538 1
		} else {
d540 1
a540 1
		}
d546 1
a546 1
			if ((i_sr == s_sr) && (i_vp1 == s_vp1)) {
d548 1
a548 1
			} else {
d550 2
a551 2
			}
			if ((i_sr == e_sr) && (i_vp1 == e_vp1)) {
d553 1
a553 1
			} else {
d555 1
a555 1
			}
d604 1
a604 1
	if (pm != pmap_kernel()) {
a605 1
	}
d633 2
a634 4
	if (pted && PTED_VALID(pted)) {
		/* pted is reused */
		pmap_kremove_pg(va);
	}
d650 3
a652 4
		if (pvh != NULL) {
		/* managed memory is cachable */
			cache = PMAP_CACHE_WB;
		} else {
a653 1
		}
d696 1
a696 2
	if (pted == NULL) {
		/* XXX */
d698 4
a701 5
	}
	if (!PTED_VALID(pted)) {
		/* not mapped */
		return;
	}
d728 1
a728 1
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE) {
a729 1
	}
d742 1
a742 1
		if (PTED_MANAGED(pted)) { /* XXX */
a744 1
		}
d746 1
a775 3
#ifdef USE_WTABLE
		pmap_wtable [idx] &= ~(1 << PTED_PTEGIDX(pted)); 
#endif /* USE_WTABLE */
a778 1

d796 1
a796 1
	if ((cache == PMAP_CACHE_WB)) {
d798 1
a798 1
	} else if ((cache == PMAP_CACHE_WT)) {
d800 1
a800 1
	} else {
a801 1
	}
d803 1
a803 1
	if (prot & VM_PROT_WRITE) {
d805 1
a805 1
	} else {
a806 1
	}
d884 1
a884 1
	if (clear) {
d886 1
a886 1
	} else {
d888 1
a888 1
	}
d936 1
d956 1
d1039 1
a1039 1
	if (refs > 0) {
d1041 1
a1041 1
	}
a1075 3
#ifdef CHECK_IDX2_ENTRIES
	int k;
#endif
d1082 1
a1082 1
		if (vp1 == NULL) {
d1084 1
a1084 1
		}
a1089 19
			if (pm->pm_stats.resident_count != 0) 
#ifdef CHECK_IDX2_ENTRIES
/* This is ifdefed because it should not happen, and has not been occuring */
				for (k = 0; k < VP_IDX2_SIZE; k++) {
					if (vp2->vp[k] != NULL) {
						printf("PMAP NOT EMPTY"
						    " pm %x va %x\n",
						    pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
						pmap_remove_pg(pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
					}
				}
#endif
			/* vp1->vp[j] = NULL; */
d1094 1
a1094 1
		/* pm->pm_vp[i] = NULL; */
d1110 1
a1110 2

	for (mp = pmap_mem; mp->size !=0; mp++) {
a1111 1
	}
d1127 1
d1130 1
d1137 1
a1137 1
	for (mp = pmap_avail; mp->size !=0; mp++) {
a1138 1
	}
a1140 1

d1185 3
a1187 4
			if (end <= mp->start) {
				/* region not present -??? */
				break;
			}
d1192 2
a1193 3
					i < pmap_cnt_avail;
					i++)
				{
d1210 2
a1211 3
					i > (mp - pmap_avail);
					i--)
				{
d1229 1
a1229 1
			/* lenghten */
d1231 1
a1231 2
				i--)
			{
d1308 2
a1310 2
#else /* HTABENTS */
	pmap_ptab_cnt = 1024;
a1313 1
#endif /* HTABENTS */
a1320 4
#ifdef USE_WTABLE
	pmap_wtable = pmap_steal_avail(pmap_ptab_cnt, 4);
#endif /* USE_WTABLE */

d1358 1
a1358 1
			      :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
d1361 1
a1361 1
		      :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));
d1436 1
a1436 1
		      : "=r" (oldsr): "n"(USER_SR));
d1438 1
a1438 1
		      :: "n"(USER_SR), "r"(sr));
d1446 1
a1446 1
		      :: "n"(USER_SR), "r"(sr));
d1539 1
a1539 1
			if (done != NULL) {
d1541 1
a1541 1
			}
d1550 1
a1550 1
				if (done != NULL) {
d1552 1
a1552 1
				}
d1566 1
a1566 1
	if (done != NULL) {
d1568 1
a1568 1
	}
d1595 1
a1595 1
			if (done != NULL) {
d1597 1
a1597 1
			}
d1606 1
a1606 1
				if (done != NULL) {
d1608 1
a1608 1
				}
d1622 1
a1622 1
	if (done != NULL) {
d1624 1
a1624 1
	}
d1680 1
a1680 1
	if (pted == NULL || !PTED_VALID(pted)) {
d1682 1
a1682 1
	}
d1690 1
a1690 2
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d1875 2
a1876 2
		    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
		{
d1903 1
a1903 2
	    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
	{
d1905 1
a1905 1
		if (PTED_VALID(pted)) {
a1906 1
		}
d1936 1
a1936 2
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d1956 1
a1956 1
		if (ptp[i].pte_hi & PTE_VALID) {
d1958 1
a1958 1
		}
d1972 1
a1972 1
		if (ptp[i].pte_hi & PTE_VALID) {
d1974 1
a1974 2
		}
/* printf("inserting in secondary idx %x, i %x\n", idx, i); */
a1990 5
#ifdef USE_WTABLE
	if (PTED_WIRED(pted)) {
		pmap_wtable[idx] &= ~(1 << PTED_PTEGIDX(pted)); 
	}
#endif /* USE_WTABLE */
d2008 2
a2009 1
	if (secondary) {
d2011 1
a2011 1
	} else {
d2013 1
a2013 1
	}
@


1.67
log
@spelling; raj@@cerias.purdue.edu
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2002/05/18 20:11:06 drahn Exp $ */
a263 6
		if (pm == pmap_kernel()) {
			printf("mapping kernel va%x pted%x \n",
				va, pted);
				
			Debugger();
		}
@


1.66
log
@Optimize pmap_remove(). It frequently is called with no mappings to
remove eg mmap() ok miod@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2002/03/22 21:01:07 drahn Exp $ */
d465 1
a465 1
	/* MP - Aquire lock for this pmap */
@


1.65
log
@Attribute table must be allocated 1-1 because it is accessed from
pte_spill_r(). ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2002/03/21 05:42:43 drahn Exp $ */
d522 5
a526 1
	vaddr_t addr;
d528 3
a530 4
	/*
	 * Should this be optimized for unmapped regions
	 * rather than perform all of the vp lookups?
	 * Not yet, still much faster than old version
d532 42
a573 2
	for (addr = va; addr < endva; addr += PAGE_SIZE) {
		pmap_remove_pg(pm, addr);
d577 1
a577 2
 * remove a single mapping, any process besides pmap_kernel()
 * notice that this code is O(1)
@


1.64
log
@Remove dead code, ifdef code which should be dead, KNF. Cleanup.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2002/03/21 05:39:22 drahn Exp $ */
d62 2
d1383 8
a1822 1
	vaddr_t addr;
d1824 1
a1824 1
	char *pmap_attrib, *attr;
a1826 2
	sz = (sizeof (struct pted_pv_head)+1) * npgs;
	sz = round_page(sz);
d1835 4
a1838 2
	addr = uvm_km_zalloc(kernel_map, sz);
	pvh = (struct pted_pv_head *)addr;
a1840 1
	pmap_attrib = (char *)pvh; /* attrib was allocated at the end of pv */
d1843 1
a1843 1
	pvh = (struct pted_pv_head *)addr;
@


1.63
log
@Be more consistant about pted zeroing (the whole structure)
Fix missing 'attr' initialization.
Zero available memory.
Raise available memory limit from 256MB to 1GB. This code has only been
tested up to 512MB, but should be fine to 1GB. Ram modules are not avail
to the developers to test out the machines up to their 1.5GB physical limit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2002/03/14 18:43:51 drahn Exp $ */
a118 1
int pte_insert_path; /* DEBUGGING, where pte_insert was called from */
a138 4
#if 0
u_int32_t kvm_enable(void);
void kvm_disable(struct pmap *pm, u_int32_t val);
#endif
d406 3
a408 8
 * wasted for every page mapped except for pmap_kernel() mappings, all other
 * processes. What is the cost of that, it is reasonable to change the
 * structure to save that memory?
 *
 * The point of the overflow was to have lookups for wired entries,
 * if all of vmspace is mapped with physical address entries,
 * it is possible to perform the lookup in the table, 
 * thus no linked list for overflows is necessary.
a448 1
int stop_on_kernel=0;
a462 11
#if 0
printf("kernel pmap %x\n", pmap_kernel());
printf("pmap_enter pm %x va %x pa %x prot %x flags %x\n",
    pm, va, pa, prot, flags);
if (pm != pmap_kernel()
printf("pmap_enter pm %x va %x pa %x prot %x flags %x\n",
    pm, va, pa, prot, flags);
if (pm == pmap_kernel() && stop_on_kernel) 
	Debugger();
#endif

a475 1

a500 1
	pte_insert_path = 1;
d521 1
d527 1
a527 1
	for (addr = va; addr < endva; addr+= PAGE_SIZE) {
a561 5
#if 0
	printf("about to remove %x:%x %x %x %x from hash pted %p\n",
	    va, pted->pted_va, pted->pted_pte.pte_hi, pted->pted_pte.pte_lo,
	    pted->pted_pmap, pted);
#endif
a591 3
#if NEED_TO_SYNC_ON_KERNEL_MAP
	int need_sync;
#endif
a628 10
#if 0
	/*
	 * Is it necessary to track kernel entries in pv table?
	 */
	if (pmap_enter_pv(pted, pvh)) {
#if NEED_TO_SYNC_ON_KERNEL_MAP
		need_sync = 1;
#endif
	}
#endif
a633 1
	pte_insert_path = 2;
a638 4
#if NEED_TO_SYNC_ON_KERNEL_MAP
	if (need_sync)
		pmap_syncicache_user_virt(pm, va);
#endif
a682 1

d686 1
a686 1
		pmap_remove_pv(pted); /* XXX - necessary? */
a751 35
#if 0
	} else {
		int i;
		int found = 0;
		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);
		ptp = pmap_ptable + (idx * 8);
		for (i = 0; i < 8; i++) {
			if (ptp[i].pte_hi ==
			    (pted->pted_pte.pte_hi & ~ PTE_HID)) {
				printf("found match at entry HID 0 %d"
				    " hi %x lo %x\n", i, ptp[i].pte_hi,
				    ptp[i].pte_lo );
				found = 1;
				pte_zap(&ptp[i], pted);
			}
		}

		idx = idx ^ pmap_ptab_mask;
		ptp = pmap_ptable + (idx * 8);
		for (i = 0; i < 8; i++) {
			if (ptp[i].pte_hi ==
			    (pted->pted_pte.pte_hi | PTE_HID)) {
				printf("found match at entry HID 1 %d"
				    " hi %x lo %x\n", i, ptp[i].pte_hi,
				    ptp[i].pte_lo );
				found = 1;
				pte_zap(&ptp[i], pted);
			}
		}
		if (found == 1) {
			printf("pmap_hash_remove: found at non-matching idx");
			pmap_print_pted(pted, printf);
		}
#endif
d786 2
a787 1
	pted->pted_va = va;	/* mask? - XXX */
d946 1
a946 1
	 * try not to reuse pmap ids, to spread the hash table useage.
d958 1
a958 1
				/* DAMN, entry was stolen out from under us */
d1049 2
a1050 1
void pmap_vp_destroy(struct pmap *pm)
d1052 4
a1055 1
	int i, j, k;
d1071 2
d1087 1
a1140 16
void pmap_dump_mem(void);
void
pmap_dump_mem()
{
	struct mem_region *mp;
		 
	printf("avail: %x\n", pmap_cnt_avail );
	for (mp = pmap_avail; mp->size != 0; mp++) {
		printf("%08x %08x\n", mp->start, mp->start + mp->size);
	}
	printf("alloc:\n");
	for (mp = pmap_allocated; mp->size != 0; mp++) {
		printf("%08x %08x\n", mp->start, mp->start + mp->size);
	}
}

d1167 1
a1276 1

a1348 20
#if 0
	/* this code is here for when BATs are removed */
	/* map 32 meg (way too much I know) for kernel and data structures */
	vp1 = pmap_steal_avail(sizeof (struct pmapvp), 4);
	bzero (vp1, sizeof (struct pmapvp));
	pmap_kernel()->pm_vp[0] = vp1;
	for (i = 0; i < ((32 * 1024 *1024) >> 20); i++) {
		vp2 = vp1->vp[i] = pmap_steal_avail(sizeof (struct pmapvp), 4);
		bzero (vp2, sizeof (struct pmapvp));
		for (k = 0; k < VP_IDX2_SIZE; k++) {
			struct pte_desc *pted;
			pted = pmap_steal_avail((sizeof (struct pte_desc)), 4);
			vp2->vp[k] = pted;
			bzero (pted, sizeof (struct pte_desc));
		}
	}
	for (i = (SEGMENT_LENGTH >> 20); i < VP_IDX1_SIZE; i++) {
		vp1->vp[i] = NULL;
	}
#endif
a1373 1
	pmap_dump_mem();
a1374 25
	/* what about all mapping all of "allocated" memory at this point
	 * so that it is mapped 1-1? 
	 * Previously this is done with BATs, __BAD__ should use the hash
	 * like the rest of vm
	 */
	for (mp = pmap_allocated; mp->size !=0; mp++) {
		vaddr_t addr;
		vm_prot_t prot;
		for (addr = mp->start ; addr < (vaddr_t)(mp->start+mp->size);
			addr += PAGE_SIZE)
		{
			extern char _etext;
			prot = VM_PROT_READ;
			if (addr < 0x10000) {
				prot |= VM_PROT_EXECUTE|VM_PROT_WRITE;
			}
			if (addr >= (((u_long)&_etext) & ~PAGE_MASK)) {
				prot |= VM_PROT_EXECUTE|VM_PROT_WRITE;
			}

			/* - XXX not yet. must preallocated mappings first
			pmap_kenter_pa(addr, addr, prot);
			*/
		}
	}
a1380 1
		printf("loading %x-%x \n", mp->start, mp->start+mp->size);
d1396 1
d1436 1
a1436 1
	/* user address range lock */
d1670 1
d1774 1
d1798 1
a1885 1
			pte_insert_path = 3;
a1889 1

a1892 1
int extra_debug = 0;
a1915 1
	pte_insert_path = 4;
a1916 8

 if (extra_debug) {
	printf("pte_spill_v pm %x va %x dsisr %x, pa %x pted %x ptedpm %x"
	   " pted va %x\n",
		pm, va, dsisr, pted->pted_pte.pte_lo, pted, pted->pted_pmap,
		pted->pted_va);
 }

a1947 9
#if 0
		if (pte_insert_path != 3) {
		    printf("pte_insert: mapping already present pm %x va %x"
			" pted %x path %x hi %x lo %x\n",
			pted->pted_pmap, pted->pted_va, pted, pte_insert_path,
			pted->pted_pte.pte_hi, pted->pted_pte.pte_lo );
			 Debugger();
		}
#endif
a1974 1
		pte_insert_path = 0;
a1989 1
		pte_insert_path = 0;
a2029 1
	pte_insert_path = 0;
d2032 1
a2032 1
#if 1
a2058 1
#endif
d2203 1
@


1.62
log
@Do not include headers twice. Pointed out by Dries Schellekens.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2002/03/14 03:15:59 millert Exp $ */
d502 1
a502 1
		pted->pted_pte.pte_hi = NULL;
d643 1
d1188 2
a1189 2
	/* limit to 256MB available, for now -XXXGRR */
#define MEMMAX 0x10000000
d1398 4
d1966 1
@


1.61
log
@Final __P removal plus some cosmetic fixups
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2002/03/13 18:27:36 drahn Exp $ */
a40 2
#include <sys/systm.h>
#include <sys/pool.h>
@


1.60
log
@Complete rewrite of the powerpc pmap handling, Instead of keeping
the spill list for each PTEG, the V->P translations are stored in
trees for each pmap. All valid kernel mappings are preallocated
in 1-1 memory so that tlb spill/loads for kernel accesses can be
looked up while physical, user mappings are not guaranteed to
be 1-1 mapped, thus the kernel must go virtual to look up user
mappings. While this is more expensive, the tree search is much
lower cost than the long linked list search. Also on each pmap_remove()
it was necessary to search the linked lists for each possible mapping,
now it just looks up the entry in the tree.
This change gives a 25-36% speedup in 'make build' time. What was
around 2:50 is now around 1:55 on a 733MHz G4.

This change causes a likely existing bug to appear quite often,
it deals with the segment register invalidation in kernel mode.
Because of that problem, currently this change limits the physical
memory used to 256MB. This limitation will be fixed soon, it is not
an error in the pmap code.

 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d1008 1
a1008 1
	bcopy((void*)copy_src_page, (void*)copy_dst_page, PAGE_SIZE);
d2251 1
a2251 1
		err = copyin((void*)va, &read1, 1);
d2256 1
a2256 1
		err = copyin((void*)va, &read2, 2);
d2261 1
a2261 1
		err = copyin((void*)va, &read4, 4);
@


1.59
log
@Move the message buffer out of low memory, Openfirmware clears the area
on reboot. perhaps OF uses it at other times?
Since OF always use the same memory addresses, this should always allocate
the same ram to the msgbuf, and allow it to be preserved across reboot.
@
text
@d1 1
a1 2
/*	$OpenBSD: pmap.c,v 1.58 2002/01/25 04:04:55 drahn Exp $	*/
/*	$NetBSD: pmap.c,v 1.1 1996/09/30 16:34:52 ws Exp $	*/
d4 1
a4 3
 * Copyright (C) 1995, 1996 Wolfgang Solfrank.
 * Copyright (C) 1995, 1996 TooLs GmbH.
 * All rights reserved.
d6 1
d17 2
a18 2
 *	This product includes software developed by TooLs GmbH.
 * 4. The name of TooLs GmbH may not be used to endorse or promote products
d21 1
a21 1
 * THIS SOFTWARE IS PROVIDED BY TOOLS GMBH ``AS IS'' AND ANY EXPRESS OR
d24 13
a36 8
 * IN NO EVENT SHALL TOOLS GMBH BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
d41 2
a42 1
#include <sys/queue.h>
d50 13
d64 23
a86 8
pte_t *ptable;
int ptab_cnt;
u_int ptab_mask;
#define	HTABSIZE	(ptab_cnt * 64)

struct pte_ovfl {
	LIST_ENTRY(pte_ovfl) po_list;	/* Linked list of overflow entries */
	struct pte po_pte;		/* PTE for this mapping */
d89 1
a89 1
LIST_HEAD(pte_ovtab, pte_ovfl) *potable; /* Overflow entries for ptable */
d91 3
a93 10
/* free lists for potable entries, it is not valid to pool_put
 * in the pte spill handler.
 * pofreebusy variable is a flag to indicate that the 
 * higher level is maniuplating list0 and that spill cannot touch
 * it, the higher level can only be touching one at a time.
 * if list0 is busy list1 cannot be busy.
 */
LIST_HEAD(,pte_ovfl) pofreetable0 = LIST_HEAD_INITIALIZER(pofreetable0);
LIST_HEAD(,pte_ovfl) pofreetable1 = LIST_HEAD_INITIALIZER(pofreetable1);
volatile int pofreebusy;
d95 2
a96 1
struct pmap kernel_pmap_;
a97 3
int physmem;
static int npgs;
static u_int nextavail;
a98 1
static struct mem_region *mem, *avail;
d100 39
a138 2
#define USE_PMAP_VP
*/
d140 2
d143 2
a144 20
void
dump_avail()
{
	int cnt;
	struct mem_region *mp;
	extern struct mem_region *avail;
	
	printf("memory %x\n", mem);
	for (cnt = 0, mp = mem; mp->size; mp++) {
		printf("memory region %x: start %x, size %x\n",
				cnt, mp->start, mp->size);
		cnt++;
	}
	printf("available %x\n", avail);
	for (cnt = 0, mp = avail; mp->size; mp++) {
		printf("avail region %x: start %x, size %x\n",
				cnt, mp->start, mp->size);
		cnt++;
	}
}
d147 8
a154 1
#ifdef USE_PMAP_VP
d156 4
d161 1
a161 4
int pmap_vp_valid(pmap_t pm, vaddr_t va);
void pmap_vp_enter(pmap_t pm, vaddr_t va, paddr_t pa);
int pmap_vp_remove(pmap_t pm, vaddr_t va);
void pmap_vp_destroy(pmap_t pm);
d163 1
a163 1
/* virtual to physical map */
d165 1
a165 1
VP_SR(paddr_t va)
d167 1
a167 1
	return (va >> VP_SR_POS) & VP_SR_MASK;
d169 1
d171 1
a171 1
VP_IDX1(paddr_t va)
d177 1
a177 1
VP_IDX2(paddr_t va)
d182 22
a203 2
int
pmap_vp_valid(pmap_t pm, vaddr_t va)
d205 12
a216 4
	pmapv_t *vp1;
	vp1 = pm->vps[VP_SR(va)];
	if (vp1 != NULL) {
		return (vp1[VP_IDX1(va)] & (1 << VP_IDX2(va)));
d218 4
a221 1
	return 0;
d224 4
a227 1
int
d230 12
a241 13
	pmapv_t *vp1;
	int s;
	int retcode;
	retcode = 0;
	vp1 = pm->vps[VP_SR(va)];
#ifdef DEBUG
	printf("pmap_vp_remove: removing va %x pm %x", va, pm);
#endif
	if (vp1 != NULL) {
		s = splhigh();
		retcode = vp1[VP_IDX1(va)] & (1 << VP_IDX2(va));
		vp1[VP_IDX1(va)] &= ~(1 << VP_IDX2(va));
		splx(s);
d243 5
a247 4
#ifdef DEBUG
	printf(" ret %x\n", retcode);
#endif
	return retcode;
d249 9
d259 1
a259 1
pmap_vp_enter(pmap_t pm, vaddr_t va, paddr_t pa)
d261 2
a262 2
	pmapv_t *vp1;
	pmapv_t *mem1;
d264 4
a267 6
	int idx;
	idx = VP_SR(va);
	vp1 = pm->vps[idx];
#ifdef DEBUG
	printf("pmap_vp_enter: pm %x va %x vp1 %x idx %x ", pm, va, vp1, idx);
#endif
a268 3
#ifdef DEBUG
		printf("l1 entry idx %x ", idx);
#endif
d270 4
a273 1
			printf(" irk kernel allocating map?");
d276 1
a276 1
		mem1 = pool_get(&pmap_vp_pool, PR_NOWAIT);
d278 12
d291 1
a291 1
		bzero (mem1, PAGE_SIZE);
d293 1
a293 9
		pm->vps[idx] = mem1;
#ifdef DEBUG
		printf("got %x ", mem1);
#endif
		vp1 = mem1;
	}
#ifdef DEBUG
	printf("l2 idx %x\n", VP_IDX2(va));
#endif
a294 3
	s = splhigh();
	vp1[VP_IDX1(va)] |= (1 << VP_IDX2(va));
	splx(s);
d298 6
a303 3
void
pmap_vp_destroy(pm)
	pmap_t pm;
d305 4
a308 26
	pmapv_t *vp1;
	int s;
	int sr;
#ifdef SANITY
	int idx1;
#endif

	for (sr = 0; sr < 32; sr++) {
		vp1 = pm->vps[sr];
		if (vp1 == NULL) {
			continue;
		}
#ifdef SANITY
		for(idx1 = 0; idx1 < 1024; idx1++) {
			if (vp1[idx1] != 0) {
				printf("mapped page at %x \n"
					0); /* XXX what page was this... */
				vp1[idx2] = 0;

			}
		}
#endif
		s = splimp();
		pool_put(&pmap_vp_pool, vp1);
		splx(s);
		pm->vps[sr] = 0;
d310 1
d312 2
a313 6
static int vp_page0[1024];
static int vp_page1[1024];
void pmap_vp_preinit(void);

void
pmap_vp_preinit()
d315 6
a320 4
	pmap_t pm = pmap_kernel();
	/* magic addresses are 0xe0000000, 0xe8000000 */
	pm->vps[VP_SR(0xe0000000)] = vp_page0;
	pm->vps[VP_SR(0xe8000000)] = vp_page1;
a321 24
#endif


/*
 * This is a cache of referenced/modified bits.
 * Bits herein are shifted by ATTRSHFT.
 */
static char *pmap_attrib;
#define	ATTRSHFT	4

struct pv_entry {
	struct pv_entry *pv_next;	/* Linked list of mappings */
	int pv_idx;			/* Index into ptable */
	vm_offset_t pv_va;		/* virtual address of mapping */
	struct pmap *pv_pmap;		/* pmap associated with this map */
};

struct pool pmap_pv_pool;
struct pv_entry *pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *));

struct pool pmap_po_pool;
struct pte_ovfl *poalloc __P((void));
void pofree __P((struct pte_ovfl *, int));
d323 1
a323 30
static u_int usedsr[NPMAPS / sizeof(u_int) / 8];

static int pmap_initialized;

static inline void tlbie(vm_offset_t ea);
static inline void tlbsync(void);
static inline void tlbia(void);
static inline int ptesr(sr_t *sr, vm_offset_t addr);
static inline int pteidx(sr_t sr, vm_offset_t addr);
static inline int ptematch( pte_t *ptp, sr_t sr, vm_offset_t va, int which);
int pte_insert(int idx, pte_t *pt);
int pte_spill(vm_offset_t addr);
int pmap_page_index(vm_offset_t pa);
u_int pmap_free_pages(void);
int pmap_next_page(vm_offset_t *paddr);
struct pv_entry *pmap_alloc_pv(void);
void pmap_free_pv(struct pv_entry *pv);
struct pte_ovfl *poalloc(void);
static inline int pmap_enter_pv(struct pmap *pm, int pteidx, vm_offset_t va,
	vm_offset_t pa);
void pmap_remove_pv(struct pmap *pm, int pteidx, vm_offset_t va,
	u_int32_t pte_lo);
pte_t * pte_find(struct pmap *pm, vm_offset_t va);

void addbatmap(u_int32_t vaddr, u_int32_t raddr, u_int32_t wimg);

/*
 * These small routines may have to be replaced,
 * if/when we support processors other that the 604.
 */
d325 1
a325 1
tlbie(vm_offset_t ea)
d327 1
a327 1
	asm volatile ("tlbie %0" :: "r"(ea));
a328 1

d330 1
a330 1
tlbsync()
d332 1
a332 1
	asm volatile ("sync; tlbsync; sync");
d335 1
a335 1
static void
d338 5
a342 5
	vm_offset_t i;
	
	asm volatile ("sync");
	for (i = 0; i < 0x00040000; i += 0x00001000)
		tlbie(i);
d344 1
d348 1
a348 3
ptesr(sr, addr)
	sr_t *sr;
	vm_offset_t addr;
d350 1
a350 1
	return sr[(u_int)addr >> ADDR_SR_SHFT];
d352 2
a353 5

static inline int
pteidx(sr, addr)
	sr_t sr;
	vm_offset_t addr;
d356 2
a357 3
	
	hash = (sr & SR_VSID) ^ (((u_int)addr & ADDR_PIDX) >> ADDR_PIDX_SHFT);
	return hash & ptab_mask;
d360 26
a385 6
static inline int
ptematch(ptp, sr, va, which)
	pte_t *ptp;
	sr_t sr;
	vm_offset_t va;
	int which;
d387 1
a387 4
	return ptp->pte_hi
		== (((sr & SR_VSID) << PTE_VSID_SHFT)
		    | (((u_int)va >> ADDR_API_SHFT) & PTE_API)
		    | which);
d391 25
a415 1
 * Try to insert page table entry *pt into the ptable at idx.
d417 4
a420 2
 * Note: *pt mustn't have PTE_VALID set.
 * This is done here as required by Book III, 4.12.
d422 40
d463 6
a468 3
pte_insert(idx, pt)
	int idx;
	pte_t *pt;
d470 51
a520 3
	pte_t *ptp;
	int i;
	
d522 3
a524 1
	 * First try primary hash.
d526 11
a536 17
	for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++)
		if (!(ptp->pte_hi & PTE_VALID)) {
			*ptp = *pt;
			ptp->pte_hi &= ~PTE_HID;
			asm volatile ("sync");
			ptp->pte_hi |= PTE_VALID;
			return 1;
		}
	idx ^= ptab_mask;
	for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++)
		if (!(ptp->pte_hi & PTE_VALID)) {
			*ptp = *pt;
			ptp->pte_hi |= PTE_HID;
			asm volatile ("sync");
			ptp->pte_hi |= PTE_VALID;
			return 1;
		}
d540 2
a541 6
/*
 * Spill handler.
 *
 * Tries to spill a page table entry from the overflow area.
 * Note that this routine runs in real mode on a separate stack,
 * with interrupts disabled.
d543 2
a544 3
int
pte_spill(addr)
	vm_offset_t addr;
d546 9
a554 53
	int idx, i;
	sr_t sr;
	struct pte_ovfl *po;
	pte_t ps;
	pte_t *pt;
	vm_offset_t va;

	asm ("mfsrin %0,%1" : "=r"(sr) : "r"(addr));
	idx = pteidx(sr, addr);
	for (po = potable[idx].lh_first; po; po = po->po_list.le_next)
		if (ptematch(&po->po_pte, sr, addr, 0)) {
			/*
			 * Now found an entry to be spilled into the real ptable.
			 */
			if (pte_insert(idx, &po->po_pte)) {
				LIST_REMOVE(po, po_list);
				pofree(po, 0);
				return 1;
			}
			/*
			 * Have to substitute some entry. Use the primary hash for this.
			 *
			 * Use low bits of timebase as random generator
			 */
			asm ("mftb %0" : "=r"(i));
			pt = ptable + idx * 8 + (i & 7);
			pt->pte_hi &= ~PTE_VALID;
			ps = *pt;
			asm volatile ("sync");
			/* calculate the va of the address being removed */
			va = ((pt->pte_hi & PTE_API) << ADDR_API_SHFT) |
			    ((((pt->pte_hi >> PTE_VSID_SHFT) & SR_VSID)
				^(idx ^ ((pt->pte_hi & PTE_HID) ? 0x3ff : 0)))
				    & 0x3ff) << PAGE_SHIFT;
			tlbie(va);
			tlbsync();
			*pt = po->po_pte;
			asm volatile ("sync");
			pt->pte_hi |= PTE_VALID;
			po->po_pte = ps;
			if (ps.pte_hi & PTE_HID) {
				/*
				 * We took an entry that was on the alternate hash
				 * chain, so move it to it's original chain.
				 */
				po->po_pte.pte_hi &= ~PTE_HID;
				LIST_REMOVE(po, po_list);
				LIST_INSERT_HEAD(potable + (idx ^ ptab_mask),
						 po, po_list);
			}
			return 1;
		}
	return 0;
a555 5

void *msgbuf_addr;	 /* memory for msgbuf, physical, mapped with BATs */

int avail_start;
int avail_end;
d557 2
a558 1
 * This is called during initppc, before the system is really initialized.
d561 1
a561 2
pmap_bootstrap(kernelstart, kernelend)
	u_int kernelstart, kernelend;
d563 2
a564 3
	struct mem_region *mp, *mp1;
	int cnt, i;
	u_int s, sz;
d567 3
a569 1
	 * Get memory.
d571 13
a583 4
	(fw->mem_regions)(&mem, &avail);
	physmem = 0;
	for (mp = mem; mp->size; mp++) {
		physmem += btoc(mp->size);
d585 1
d587 8
a594 5
	/*
	 * Count the number of available entries.
	 */
	for (cnt = 0, mp = avail; mp->size; mp++)
		cnt++;
d596 2
a597 7
	/*
	 * Page align all regions.
	 * Non-page memory isn't very interesting to us.
	 * Also, sort the entries for ascending addresses.
	 */
	kernelstart &= ~PGOFSET;
	kernelend = (kernelend + PGOFSET) & ~PGOFSET;
d599 2
a600 10
	/* make certain that each section is page aligned for base and size */
	for (mp = avail; mp->size; mp++) {
		u_int32_t end;
		s = mp->start - round_page(mp->start);
		if (s != 0) {
			mp->start = round_page(mp->start);
			end = trunc_page(mp->size + mp->start);
			mp->size = end - mp->start;
		}
		mp->size = trunc_page(mp->size);
d602 33
a634 60
	for (mp = avail; mp->size; mp++) {
		/*
		 * Check whether this region holds all of the kernel.
		 */
		s = mp->start + mp->size;
		if (mp->start < kernelstart && s > kernelend) {
			avail[cnt].start = kernelend;
			avail[cnt++].size = s - kernelend;
			mp->size = kernelstart - mp->start;
		}
		/*
		 * Look whether this regions starts within the kernel.
		 */
		if (mp->start >= kernelstart && mp->start < kernelend) {
			s = kernelend - mp->start;
			if (mp->size > s)
				mp->size -= s;
			else
				mp->size = 0;
			mp->start = kernelend;
		}
		/*
		 * Now look whether this region ends within the kernel.
		 */
		s = mp->start + mp->size;
		if (s > kernelstart && s < kernelend)
			mp->size -= s - kernelstart;
		/*
		 * Now page align the start of the region.
		 */
		s = mp->start % NBPG;
		if (mp->size >= s) {
			mp->size -= s;
			mp->start += s;
		}
		/*
		 * And now align the size of the region.
		 */
		mp->size -= mp->size % NBPG;
		/*
		 * Check whether some memory is left here.
		 */
		if (mp->size == 0) {
			bcopy(mp + 1, mp,
			      (cnt - (mp - avail)) * sizeof *mp);
			cnt--;
			mp--;
			continue;
		}
		s = mp->start;
		sz = mp->size;
		npgs += btoc(sz);
		for (mp1 = avail; mp1 < mp; mp1++)
			if (s < mp1->start)
				break;
		if (mp1 < mp) {
			bcopy(mp1, mp1 + 1, (void *)mp - (void *)mp1);
			mp1->start = s;
			mp1->size = sz;
		}
a635 20
	/*
	 * grab first available memory for msgbuf
	 */
	for (mp = avail; mp->size; mp++) {
		if (mp->size >= MSGBUFSIZE) {
			mp->size -= MSGBUFSIZE;
			msgbuf_addr = (void *)mp->start;
			mp->start += MSGBUFSIZE;
			if (mp->size == 0) {
				bcopy(mp + 1, mp,
				      (cnt - (mp - avail)) * sizeof *mp);
				cnt--;
			}
			break;
		}
	}
#if 0
avail_start = 0;
avail_end = npgs * NBPG;
#endif
d637 9
a645 6
#ifdef  HTABENTS
	ptab_cnt = HTABENTS;
#else /* HTABENTS */
	ptab_cnt = 1024;
	while ((HTABSIZE << 7) < ctob(physmem)) {
		ptab_cnt <<= 1;
a646 1
#endif /* HTABENTS */
d648 5
a652 71
	/*
	 * Find suitably aligned memory for HTAB.
	 */
	for (mp = avail; mp->size; mp++) {
		if (mp->start % HTABSIZE == 0) {
			s = 0;
		} else {
			s = HTABSIZE - (mp->start % HTABSIZE) ;
		}
		if (mp->size < s + HTABSIZE)
			continue;
		ptable = (pte_t *)(mp->start + s);
		if (mp->size == s + HTABSIZE) {
			if (s)
				mp->size = s;
			else {
				bcopy(mp + 1, mp,
				      (cnt - (mp - avail)) * sizeof *mp);
				mp = avail;
			}
			break;
		}
		if (s != 0) {
			bcopy(mp, mp + 1,
			      (cnt - (mp - avail)) * sizeof *mp);
			mp++->size = s;
		}
		mp->start += s + HTABSIZE;
		mp->size -= s + HTABSIZE;
		break;
	}
	if (!mp->size)
		panic("not enough memory?");
	bzero((void *)ptable, HTABSIZE);
	ptab_mask = ptab_cnt - 1;
	
	/*
	 * We cannot do vm_bootstrap_steal_memory here,
	 * since we don't run with translation enabled yet.
	 */
	s = sizeof(struct pte_ovtab) * ptab_cnt;
	sz = round_page(s);
	for (mp = avail; mp->size; mp++)
		if (mp->size >= sz)
			break;
	if (!mp->size)
		panic("not enough memory?");
	potable = (struct pte_ovtab *)mp->start;
	mp->size -= sz;
	mp->start += sz;
	if (mp->size == 0)
		bcopy(mp + 1, mp, (cnt - (mp - avail)) * sizeof *mp);
	for (i = 0; i < ptab_cnt; i++)
		LIST_INIT(potable + i);

	/* use only one memory list */
	{ 
		u_int32_t size;
		struct mem_region *curmp;
		size = 0;
		curmp = NULL;
		for (mp = avail; mp->size; mp++) {
			if (mp->size > size) {
				size = mp->size;
				curmp=mp;
			}
		}
		mp = avail;
		if (curmp == mp) {
			++mp;
			mp->size = 0; /* lose the rest of memory */
d654 1
a654 3
			*mp = *curmp;
			++mp;
			mp->size = 0; /* lose the rest of memory */
a656 6
	
	for (mp = avail; mp->size; mp++) {
		uvm_page_physload(atop(mp->start), atop(mp->start + mp->size),
			atop(mp->start), atop(mp->start + mp->size),
			VM_FREELIST_DEFAULT);
	}
d658 4
d663 1
a663 1
	 * Initialize kernel pmap and hardware.
d665 3
a667 3
#if NPMAPS >= KERNEL_SEGMENT / 16
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
a668 4
	for (i = 0; i < 16; i++) {
		pmap_kernel()->pm_sr[i] = EMPTY_SEGMENT;
		asm volatile ("mtsrin %0,%1"
			      :: "r"(EMPTY_SEGMENT), "r"(i << ADDR_SR_SHFT) );
d670 16
a685 10
	pmap_kernel()->pm_sr[KERNEL_SR] = KERNEL_SEGMENT;
	asm volatile ("mtsr %0,%1"
		      :: "n"(KERNEL_SR), "r"(KERNEL_SEGMENT));
	asm volatile ("sync; mtsdr1 %0; isync"
		      :: "r"((u_int)ptable | (ptab_mask >> 10)));
	tlbia();
#ifdef USE_PMAP_VP
	pmap_vp_preinit();
#endif /*  USE_PMAP_VP */
	nextavail = avail->start;
a687 3
/*
 * Restrict given range to physical memory
 */
d689 1
a689 3
pmap_real_memory(start, size)
	vm_offset_t *start;
	vm_size_t *size;
d691 1
a691 15
	struct mem_region *mp;
	
	for (mp = mem; mp->size; mp++) {
		if (*start + *size > mp->start
		    && *start < mp->start + mp->size) {
			if (*start < mp->start) {
				*size -= mp->start - *start;
				*start = mp->start;
			}
			if (*start + *size > mp->start + mp->size)
				*size = mp->start + mp->size - *start;
			return;
		}
	}
	*size = 0;
a693 4
/*
 * Initialize anything else for pmap handling.
 * Called during vm_init().
 */
d695 1
a695 1
pmap_init()
d697 1
a697 34
	struct pv_entry *pv;
	vsize_t sz;
	vaddr_t addr;
	int i, s;
	int bank;
	char *attr;
	
	sz = (vm_size_t)((sizeof(struct pv_entry) + 1) * npgs);
	sz = round_page(sz);
	addr = uvm_km_zalloc(kernel_map, sz);
	s = splimp();
	pv = (struct pv_entry *)addr;
	for (i = npgs; --i >= 0;)
		pv++->pv_idx = -1;
#ifdef USE_PMAP_VP
	pool_init(&pmap_vp_pool, PAGE_SIZE, 0, 0, 0, "ppvl", NULL);
#endif
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
            NULL);
	pool_init(&pmap_po_pool, sizeof(struct pte_ovfl), 0, 0, 0, "popl",
            NULL);
	pmap_attrib = (char *)pv;
	bzero(pv, npgs);
	pv = (struct pv_entry *)addr;
	attr = pmap_attrib;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		sz = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += sz;
		attr += sz;
	}
	pmap_initialized = 1;
	splx(s);
d700 1
d702 1
a702 1
 * Return the index of the given page in terms of pmap_next_page() calls.
d704 2
a705 3
int
pmap_page_index(pa)
	vm_offset_t pa;
d707 13
a719 9
	struct mem_region *mp;
	vm_size_t pre;
	
	pa &= ~PGOFSET;
	for (pre = 0, mp = avail; mp->size; mp++) {
		if (pa >= mp->start
		    && pa < mp->start + mp->size)
			return btoc(pre + (pa - mp->start));
		pre += mp->size;
d721 11
a731 6
	return -1;
}
static inline struct pv_entry *
pmap_find_pv(paddr_t pa)
{
	int bank, off;
d733 2
a734 10
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.pvent[off];
	} 
	return NULL;
}
static inline char *
pmap_find_attr(paddr_t pa)
{
	int bank, off;
d736 2
a737 6
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.attrs[off];
	} 
	return NULL;
}
d739 1
a739 1
vm_offset_t ppc_kvm_size = VM_KERN_ADDR_SIZE_DEF;
d741 1
d743 1
a743 1
 * How much virtual space is available to the kernel?
d746 1
a746 2
pmap_virtual_space(start, end)
	vm_offset_t *start, *end;
d748 3
a750 5
	/*
	 * Reserve one segment for kernel virtual memory
	 */
	*start = (vm_offset_t)(KERNEL_SR << ADDR_SR_SHFT);
	*end = *start + VM_KERN_ADDRESS_SIZE;
d753 15
d769 3
a771 2
 * Return the number of possible page indices returned
 * from pmap_page_index for any page provided by pmap_next_page.
d773 2
a774 2
u_int
pmap_free_pages()
d776 60
a835 1
	return npgs;
d838 1
d840 1
a840 3
 * If there are still physical pages available, put the address of
 * the next available one at paddr and return TRUE.  Otherwise,
 * return FALSE to indicate that there are no more free pages.
d842 3
a844 3
int
pmap_next_page(paddr)
	vm_offset_t *paddr;
d846 16
a861 4
	static int lastidx = -1;
	
	if (lastidx == -1) {
		nextavail = avail->start;
d863 5
a867 5
	if (lastidx < 0
	    || nextavail >= avail[lastidx].start + avail[lastidx].size) {
		if (avail[++lastidx].size == 0)
			return FALSE;
		nextavail = avail[lastidx].start;
d869 2
a870 3
	*paddr = nextavail;
	nextavail += NBPG;
	return TRUE;
d874 2
a875 1
 * Create and return a physical map.
d877 2
a878 2
struct pmap *
pmap_create()
d880 13
a892 7
	struct pmap *pm;
	
	pm = (struct pmap *)malloc(sizeof *pm, M_VMPMAP, M_WAITOK);
	bzero((caddr_t)pm, sizeof *pm);
	pmap_pinit(pm);
	return pm;
}
a893 10
/*
 * Initialize a preallocated and zeroed pmap structure.
 */
void
pmap_pinit(pm)
	struct pmap *pm;
{
	int i, j, k;
	int s, seg;
	
d895 10
a904 1
	 * Allocate some segment registers for this pmap.
d906 1
d908 33
a940 10
	pm->pm_refs = 1;
	for (i = 0; i < sizeof usedsr / sizeof usedsr[0]; i++)
		if (usedsr[i] != 0xffffffff) {
			j = ffs(~usedsr[i]) - 1;
			usedsr[i] |= 1 << j;
			seg = (i * sizeof usedsr[0] * 8 + j) * 16;
			for (k = 0; k < 16; k++)
				pm->pm_sr[k] = seg + k;
			splx(s);
			return;
d942 8
d951 1
a951 1
	panic("out of segments");
d955 4
a958 1
 * Add a reference to the given pmap.
d961 1
a961 2
pmap_reference(pm)
	struct pmap *pm;
d963 7
a969 1
	pm->pm_refs++;
d973 1
a973 2
 * Retire the given pmap from service.
 * Should only be called if the map contains no valid mappings.
d976 1
a976 2
pmap_destroy(pm)
	struct pmap *pm;
d978 11
a988 3
	if (--pm->pm_refs == 0) {
		pmap_release(pm);
		free((caddr_t)pm, M_VMPMAP);
d990 6
a996 1

d998 1
a998 2
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
d1001 1
a1001 2
pmap_release(pm)
	struct pmap *pm;
d1003 4
a1006 2
	int i, j;
	int s;
d1008 5
a1012 11
#ifdef USE_PMAP_VP
	pmap_vp_destroy(pm);
#endif /*  USE_PMAP_VP */
	if (!pm->pm_sr[0])
		panic("pmap_release");
	i = pm->pm_sr[0] / 16;
	j = i % (sizeof usedsr[0] * 8);
	i /= sizeof usedsr[0] * 8;
	s = splimp();
	usedsr[i] &= ~(1 << j);
	splx(s);
d1015 1
a1015 7
/*
 * Copy the range specified by src_addr/len
 * from the source map to the range dst_addr/len
 * in the destination map.
 *
 * This routine is only advisory and need not do anything.
 */
d1017 1
a1017 4
pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	struct pmap *dst_pmap, *src_pmap;
	vm_offset_t dst_addr, src_addr;
	vm_size_t len;
d1019 6
a1024 1
}
d1026 34
a1059 7
/*
 * Garbage collects the physical map system for
 * pages which are no longer used.
 * Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but
 * others may be collected.
 * Called by the pageout daemon when pages are scarce.
d1061 2
a1062 3
void
pmap_collect(pm)
	struct pmap *pm;
d1064 8
d1075 1
a1075 1
 * Fill the given physical page with zeroes.
d1078 1
a1078 2
pmap_zero_page(pa)
	vm_offset_t pa;
d1080 3
a1082 10
#if 0
	bzero((caddr_t)pa, NBPG);
#else
        int i;
                 
        for (i = NBPG/CACHELINESIZE; i > 0; i--) {
                __asm __volatile ("dcbz 0,%0" :: "r"(pa));
                pa += CACHELINESIZE;
        }
#endif
d1086 2
a1087 1
 * Copy the given physical source page to its destination.
d1090 1
a1090 8
pmap_copy_page(src, dst)
	vm_offset_t src, dst;
{
	bcopy((caddr_t)src, (caddr_t)dst, NBPG);
}

struct pv_entry *
pmap_alloc_pv()
d1092 1
a1092 1
	struct pv_entry *pv;
d1095 6
d1102 1
a1102 7
	 * XXX - this splimp can go away once we have PMAP_NEW and
	 *       a correct implementation of pmap_kenter.
	 */
	/*
	 * Note that it's completly ok to use a pool here because it will
	 * never map anything or call pmap_enter because we have
	 * PMAP_MAP_POOLPAGE.
d1104 1
d1106 1
a1106 1
	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);
a1107 9
	/*
	 * XXX - some day we might want to implement pv stealing, or
	 *       to pass down flags from pmap_enter about allowed failure.
	 *	 Right now - just panic.
	 */
	if (pv == NULL)
		panic("pmap_alloc_pv: failed to allocate pv");

	return pv;
d1110 4
d1115 1
a1115 2
pmap_free_pv(pv)
	struct pv_entry *pv;
d1117 1
d1120 6
a1125 1
	/* XXX - see pmap_alloc_pv */
d1127 1
a1127 1
	pool_put(&pmap_pv_pool, pv);
d1131 1
a1131 7
/*
 * We really hope that we don't need overflow entries
 * before the VM system is initialized!							XXX
 * XXX - see pmap_alloc_pv
 */
struct pte_ovfl *
poalloc()
d1133 1
a1133 1
	struct pte_ovfl *po;
d1135 37
a1171 19
	
#ifdef DIAGNOSTIC
	if (!pmap_initialized)
		panic("poalloc");
#endif
	pofreebusy = 1;
	if (!LIST_EMPTY(&pofreetable0)) {
		po = LIST_FIRST(&pofreetable0);
		LIST_REMOVE(po,po_list);
		pofreebusy = 0;
		return po;
	}
	pofreebusy = 0;

	if (!LIST_EMPTY(&pofreetable1)) {
		po = LIST_FIRST(&pofreetable1);
		LIST_REMOVE(po,po_list);
		pofreebusy = 0;
		return po;
a1172 7

	s = splimp();
	po = pool_get(&pmap_po_pool, PR_NOWAIT);
	splx(s);
	if (po == NULL)
		panic("poalloc: failed to alloc po");
	return po;
d1176 1
a1176 3
pofree(po, freepage)
	struct pte_ovfl *po;
	int freepage;
d1178 10
a1187 12
	int s;
	if (freepage) {
		s = splimp();
		pool_put(&pmap_po_pool, po);
		splx(s);
		while (!LIST_EMPTY(&pofreetable1)) {
			po = LIST_FIRST(&pofreetable1);
			LIST_REMOVE(po, po_list);
			s = splimp();
			pool_put(&pmap_po_pool, po);
			splx(s);
		}
d1189 19
a1207 7
		pofreebusy = 1;
		while (!LIST_EMPTY(&pofreetable0)) {
			po = LIST_FIRST(&pofreetable0);
			LIST_REMOVE(po, po_list);
			s = splimp();
			pool_put(&pmap_po_pool, po);
			splx(s);
d1209 6
a1214 1
		pofreebusy = 0;
d1216 13
a1228 5
	} else {
		if (pofreebusy == 0)
			LIST_INSERT_HEAD(&pofreetable0, po, po_list);
		else
			LIST_INSERT_HEAD(&pofreetable1, po, po_list);
a1231 16
/*
 * This returns whether this is the first mapping of a page.
 */
static inline int
pmap_enter_pv(pm, pteidx, va, pa)
	struct pmap *pm;
	int pteidx;
	vm_offset_t va;
	vm_offset_t pa;
{
	struct pv_entry *npv;
	int s, first;
	struct pv_entry *pv;
	
	if (!pmap_initialized)
		return 0;
d1233 6
a1238 3
	pv = pmap_find_pv( pa );
	if (pv == NULL) 
		return 0;
d1240 16
a1255 21
	s = splimp();

	if ((first = pv->pv_idx) == -1) {
		/*
		 * No entries yet, use header as the first entry.
		 */
		pv->pv_va = va;
		pv->pv_idx = pteidx;
		pv->pv_pmap = pm;
		pv->pv_next = NULL;
	} else {
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		npv = pmap_alloc_pv();
		npv->pv_va = va;
		npv->pv_idx = pteidx;
		npv->pv_pmap = pm;
		npv->pv_next = pv->pv_next;
		pv->pv_next = npv;
a1256 2
	splx(s);
	return first;
d1258 1
a1258 1

d1260 5
a1264 19
pmap_remove_pv(pm, pteidx, va, pte_lo)
	struct pmap *pm;                                            
	int pteidx;
	vm_offset_t va;
	u_int32_t pte_lo;
{
	struct pv_entry *pv, *npv;

        int bank, pg;
	vm_offset_t pa;
	char *attr;

	pa = pte_lo & ~PGOFSET;
                   
        bank = vm_physseg_find(atop(pa), &pg);
        if (bank == -1)
                return;
        pv =   &vm_physmem[bank].pmseg.pvent[pg];
        attr = &vm_physmem[bank].pmseg.attrs[pg];
d1266 29
a1294 18
	/*
	 * First transfer reference/change bits to cache.
	 */
	*attr |= (pte_lo & (PTE_REF | PTE_CHG)) >> ATTRSHFT;
	
	/*
	 * Remove from the PV table.
	 *
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (va == pv->pv_va && pm == pv->pv_pmap) {
		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
d1296 18
a1313 2
			pv->pv_pmap = 0;
			pv->pv_idx = -1;
d1315 11
a1325 3
	} else {
		for (; (npv = pv->pv_next) != NULL; pv = npv) {
			if (va == npv->pv_va && pm == npv->pv_pmap)
d1327 1
a1327 1
				break;
d1329 4
d1334 3
a1336 3
		if (npv) {
			pv->pv_next = npv->pv_next;
			pmap_free_pv(npv);
d1338 23
a1360 7
#if 1
#ifdef	DIAGNOSTIC
		else {
			printf("pmap_remove_pv: not on list\n");
			/*
			panic("pmap_remove_pv: not on list");
			*/
a1361 2
#endif
#endif
d1363 2
d1367 2
a1368 3
int
pmap_enter_c_pv(struct pmap *pm, vm_offset_t va, vm_offset_t pa,
	vm_prot_t prot, int flags, int cacheable, int pv);
d1371 6
a1376 20
 * Insert physical page at pa into the given pmap at virtual address va.
 */
int
pmap_enter(pm, va, pa, prot, flags)
	struct pmap *pm;
	vm_offset_t va, pa;
	vm_prot_t prot;
	int flags;
{
	return pmap_enter_c_pv(pm, va, pa, prot, flags, PMAP_CACHE_DEFAULT,
		TRUE);
}
int
pmap_enter_c_pv(pm, va, pa, prot, flags, cacheable, pv)
	struct pmap *pm;
	vm_offset_t va, pa;
	vm_prot_t prot;
	int flags;
	int cacheable;
	int pv;
a1377 4
	sr_t sr;
	int idx, s;
	pte_t pte;
	struct pte_ovfl *po;
d1379 8
d1389 3
a1391 1
	 * Have to remove any existing mapping first.
d1393 3
a1395 1
	pmap_remove(pm, va, va + NBPG-1);
d1397 1
a1397 1
	pm->pm_stats.resident_count++;
d1399 8
a1406 4
#ifdef USE_PMAP_VP
	pmap_vp_enter(pm, va, pa);
#endif /*  USE_PMAP_VP */
	
d1408 1
a1408 1
	 * Compute the HTAB index.
d1410 40
a1449 16
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);
	/*
	 * Construct the PTE.
	 *
	 * Note: Don't set the valid bit for correct operation of tlb update.
	 */
	pte.pte_hi = ((sr & SR_VSID) << PTE_VSID_SHFT)
		| ((va & ADDR_PIDX) >> ADDR_API_SHFT);
	if (cacheable == PMAP_CACHE_DEFAULT) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_I | PTE_G;
		for (mp = mem; mp->size; mp++) {
			if (pa >= mp->start && pa < mp->start + mp->size) {
				pte.pte_lo &= ~(PTE_I | PTE_G);
				break;
			}
a1450 8
	} else if (cacheable == PMAP_CACHE_CI) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_I | PTE_G;
	} else if (cacheable == PMAP_CACHE_WT) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_W | PTE_G;
	} else if (cacheable == PMAP_CACHE_WB) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M;
	} else {
		panic("pmap_enter_c_pv: invalid cacheable %x\n", cacheable);
d1452 12
a1463 4
	if (prot & VM_PROT_WRITE)
		pte.pte_lo |= PTE_RW;
	else
		pte.pte_lo |= PTE_RO;
d1466 1
a1466 1
	 * Now record mapping for later back-translation.
d1468 38
a1505 6
	if (pv == TRUE) {
		if (pmap_enter_pv(pm, idx, va, pa)) {
			/* 
			 * Flush the real memory from the cache.
			 */
			syncicache((void *)pa, NBPG);
d1508 10
a1517 8
	
	s = splimp();
	/*
	 * Try to insert directly into HTAB.
	 */
	if (pte_insert(idx, &pte)) {
		splx(s);
		return (0);
a1518 12
	
	/*
	 * Have to allocate overflow entry.
	 *
	 * Note, that we must use real addresses for these.
	 */
	po = poalloc();
	po->po_pte = pte;
	LIST_INSERT_HEAD(potable + idx, po, po_list);
	splx(s);

	return (0);
d1521 5
a1525 2
#define KERN_MAP_PV TRUE

d1527 1
a1527 5
pmap_kenter_cache(va, pa, prot, cacheable)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int cacheable;
a1528 2
	pmap_enter_c_pv(pmap_kernel(), va, pa, prot, PMAP_WIRED, cacheable,
		KERN_MAP_PV);
d1530 4
d1535 1
a1535 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
a1536 2
	pmap_enter_c_pv(pmap_kernel(), va, pa, prot, PMAP_WIRED,
		PMAP_CACHE_DEFAULT, KERN_MAP_PV);
d1539 5
a1543 6
void pmap_remove_pvl( struct pmap *pm, vm_offset_t va, vm_offset_t endva,
	int pv);
void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
d1545 11
a1555 2
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove_pvl(pmap_kernel(), va, va + PAGE_SIZE, KERN_MAP_PV);
d1557 18
a1576 1
void pmap_pte_invalidate(vaddr_t va, pte_t *ptp);
d1578 1
a1578 1
pmap_pte_invalidate(vaddr_t va, pte_t *ptp)
d1580 2
a1581 4
	ptp->pte_hi &= ~PTE_VALID;
	asm volatile ("sync");
	tlbie(va);
	tlbsync();
d1584 32
d1617 32
a1648 9
/*
 * Remove the given range of mapping entries.
 */
void
pmap_remove(pm, va, endva)
	struct pmap *pm;
	vm_offset_t va, endva;
{
	pmap_remove_pvl(pm, va, endva, TRUE);
d1651 2
a1652 5
void
pmap_remove_pvl(pm, va, endva, pv)
	struct pmap *pm;
	vm_offset_t va, endva;
	int pv;
d1654 33
a1686 23
	int idx, i, s;
	int found; /* if found, we are done, only one mapping per va */
	sr_t sr;
	pte_t *ptp;
	struct pte_ovfl *po, *npo;
	
	s = splimp();
	for (; va < endva; va += NBPG) {
#ifdef USE_PMAP_VP
		if (0 == pmap_vp_remove(pm, va)) {
			/* no mapping */
			continue;
		}
#endif /*  USE_PMAP_VP */
		found = 0;
		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);
		for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++)
			if (ptematch(ptp, sr, va, PTE_VALID)) {
				pmap_pte_invalidate(va, ptp);
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						ptp->pte_lo);
d1688 55
a1742 13
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
		if (found) {
			continue;
		}
		for (ptp = ptable + (idx ^ ptab_mask) * 8, i = 8; --i >= 0; ptp++)
			if (ptematch(ptp, sr, va, PTE_VALID | PTE_HID)) {
				pmap_pte_invalidate(va, ptp);
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						ptp->pte_lo);
d1744 9
a1752 19
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
		if (found) {
			continue;
		}
		for (po = potable[idx].lh_first; po; po = npo) {
			npo = po->po_list.le_next;
			if (ptematch(&po->po_pte, sr, va, 0)) {
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						po->po_pte.pte_lo);
				}
				LIST_REMOVE(po, po_list);
				pofree(po, 1);
				pm->pm_stats.resident_count--;
				break;
			}
d1754 5
d1760 1
a1760 1
	splx(s);
d1763 45
a1807 4
pte_t *
pte_find(pm, va)
	struct pmap *pm;
	vm_offset_t va;
a1808 2
	int idx, i;
	sr_t sr;
d1810 9
a1818 1
	struct pte_ovfl *po;
d1822 28
a1849 10
	for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++)
		if (ptematch(ptp, sr, va, PTE_VALID))
			return ptp;
	for (ptp = ptable + (idx ^ ptab_mask) * 8, i = 8; --i >= 0; ptp++)
		if (ptematch(ptp, sr, va, PTE_VALID | PTE_HID))
			return ptp;
	for (po = potable[idx].lh_first; po; po = po->po_list.le_next)
		if (ptematch(&po->po_pte, sr, va, 0))
			return &po->po_pte;
	return 0;
d1853 4
a1856 1
 * Get the physical page address for the given pmap/virtual address.
d1858 2
a1859 5
boolean_t
pmap_extract(pm, va, pap)
	struct pmap *pm;
	vaddr_t va;
	paddr_t *pap;
d1861 19
a1879 11
	pte_t *ptp;
	int s = splimp();
	boolean_t ret;
	
	if (!(ptp = pte_find(pm, va))) {
		/* return address 0 if not mapped??? */
		ret = FALSE;
		if (pm == pmap_kernel() && va < 0x80000000){
			/* if in kernel, va==pa for 0 - 0x80000000 */
			*pap = va;
			ret = TRUE;
d1882 5
a1886 1
		return ret;
a1887 1
	*pap = (ptp->pte_lo & PTE_RPGN) | (va & ADDR_POFF);
a1888 1
	return TRUE;
a1890 6
/*
 * Lower the protection on the specified range of this pmap.
 *
 * There are only two cases: either the protection is going to 0,
 * or it is going to read-only.
 */
d1892 1
a1892 4
pmap_protect(pm, sva, eva, prot)
	struct pmap *pm;
	vm_offset_t sva, eva;
	vm_prot_t prot;
d1894 1
a1894 3
	pte_t *ptp;
	int valid, s;
	
d1898 2
a1899 12
			if ((ptp = pte_find(pm, sva))) {
				valid = ptp->pte_hi & PTE_VALID;
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(sva);
				tlbsync();
				ptp->pte_lo &= ~PTE_PP;
				ptp->pte_lo |= PTE_RO;
				asm volatile ("sync");
				ptp->pte_hi |= valid;
			}
			sva += NBPG;
d1906 32
d1939 44
a1982 5
boolean_t
ptemodify(pa, mask, val)
	paddr_t pa;
	u_int mask;
	u_int val;
a1983 10
	struct pv_entry *pv;
	pte_t *ptp;
	struct pte_ovfl *po;
	int i, s;
	char * pattr;
	boolean_t ret;
	u_int32_t pte_hi;
	int found;
	vaddr_t va;
	sr_t sr;
d1985 2
d1988 2
a1989 9
	ret = ptebits(pa, mask);
	
	pv = pmap_find_pv(pa);
	if (pv == NULL) 
		return (ret);
	pattr = pmap_find_attr(pa);

	/*
	 * First modify bits in cache.
d1991 11
a2001 5
	*pattr &= ~mask >> ATTRSHFT;
	*pattr |= val >> ATTRSHFT;
	
	if (pv->pv_idx < 0)
		return (ret);
d2003 14
a2016 45
	s = splimp();
	for (; pv; pv = pv->pv_next) {
		va = pv->pv_va;
		pm = pv->pv_pmap;
		sr = ptesr(pm->pm_sr, va);
		pte_hi = ((sr & SR_VSID) << PTE_VSID_SHFT)
		    | ((va & ADDR_PIDX) >> ADDR_API_SHFT);
		found = 0;
		for (ptp = ptable + pv->pv_idx * 8, i = 8; --i >= 0; ptp++)
			if ((pte_hi | PTE_VALID) == ptp->pte_hi) {
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(pv->pv_va);
				tlbsync();
				ptp->pte_lo &= ~mask;
				ptp->pte_lo |= val;
				asm volatile ("sync");
				ptp->pte_hi |= PTE_VALID;
				found = 1;
				break;
			}
		if (found)
			continue;
		for (ptp = ptable + (pv->pv_idx ^ ptab_mask) * 8, i = 8;
		    --i >= 0; ptp++) {
			if ((pte_hi | PTE_VALID | PTE_HID) == ptp->pte_hi) {
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(pv->pv_va);
				tlbsync();
				ptp->pte_lo &= ~mask;
				ptp->pte_lo |= val;
				asm volatile ("sync");
				ptp->pte_hi |= PTE_VALID;
				found = 1;
			}
		}
		if (found)
			continue;
		for (po = potable[pv->pv_idx].lh_first;
		    po; po = po->po_list.le_next) {
			if (pte_hi == po->po_pte.pte_hi) {
				po->po_pte.pte_lo &= ~mask;
				po->po_pte.pte_lo |= val;
			}
a2018 1
	splx(s);
d2020 2
a2021 1
	return (ret);
d2024 1
d2026 1
a2026 3
ptebits(pa, bit)
	vm_offset_t pa;
	int bit;
d2028 1
a2028 5
	struct pv_entry *pv;
	pte_t *ptp;
	struct pte_ovfl *po;
	int i, s, bits = 0;
	char *pattr;
d2030 2
a2031 2
	pv = pmap_find_pv(pa);
	if (pv == NULL)
d2033 1
a2033 1
	pattr = pmap_find_attr(pa);
d2036 2
a2037 1
	 * First try the cache.
d2039 7
a2045 5
	bits |= ((*pattr) << ATTRSHFT) & bit;
	if (bits == bit)
		return bits;

	if (pv->pv_idx < 0)
a2046 29
	
	s = splimp();
	for (; pv; pv = pv->pv_next) {
		for (ptp = ptable + pv->pv_idx * 8, i = 8; --i >= 0; ptp++)
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
				bits |= ptp->pte_lo & bit;
				if (bits == bit) {
					splx(s);
					return bits;
				}
			}
		for (ptp = ptable + (pv->pv_idx ^ ptab_mask) * 8, i = 8; --i >= 0; ptp++)
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
				bits |= ptp->pte_lo & bit;
				if (bits == bit) {
					splx(s);
					return bits;
				}
			}
		for (po = potable[pv->pv_idx].lh_first; po; po = po->po_list.le_next)
			if ((po->po_pte.pte_lo & PTE_RPGN) == pa) {
				bits |= po->po_pte.pte_lo & bit;
				if (bits == bit) {
					splx(s);
					return bits;
				}
			}
d2048 11
a2058 2
	splx(s);
	return bits;
d2061 1
d2063 4
a2066 4
 * Lower the protection on the specified physical page.
 *
 * There are only two cases: either the protection is going to 0,
 * or it is going to read-only.
d2068 1
d2070 1
a2070 3
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
d2072 27
a2098 16
	vm_offset_t pa = VM_PAGE_TO_PHYS(pg);
	vm_offset_t va;
	int s;
	struct pmap *pm;
	struct pv_entry *pv, *npv;
	int idx, i;
	sr_t sr;
	pte_t *ptp;
	struct pte_ovfl *po, *npo;
	int found;
	char *pattr;
	
	pa &= ~ADDR_POFF;
	if (prot & VM_PROT_READ) {
		ptemodify(pa, PTE_PP, PTE_RO);
		return;
d2101 1
a2101 4
	pattr = pmap_find_attr(pa);
	pv = pmap_find_pv(pa);
	if (pv == NULL) 
		return;
d2103 7
a2109 16
	s = splimp();
	while (pv->pv_idx != -1) {
		va = pv->pv_va;
		pm = pv->pv_pmap;
#ifdef USE_PMAP_VP
		pmap_vp_remove(pm, va);
#endif /*  USE_PMAP_VP */

		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
		} else {
			pv->pv_pmap = 0;
			pv->pv_idx = -1;
		}
d2111 4
a2114 15
		/* now remove this entry from the table */
		found = 0;
		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);
		for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++) {
			if (ptematch(ptp, sr, va, PTE_VALID)) {
				pmap_pte_invalidate(va, ptp);
				*pattr |= (ptp->pte_lo & (PTE_REF | PTE_CHG))
					>> ATTRSHFT;
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
		}
		if (found)
a2115 11
		for (ptp = ptable + (idx ^ ptab_mask) * 8, i = 8; --i >= 0;
			ptp++)
		{
			if (ptematch(ptp, sr, va, PTE_VALID | PTE_HID)) {
				pmap_pte_invalidate(va, ptp);
				*pattr |= (ptp->pte_lo & (PTE_REF | PTE_CHG))
					>> ATTRSHFT;
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
d2117 15
a2131 1
		if (found)
a2132 8
		for (po = potable[idx].lh_first; po; po = npo) {
			npo = po->po_list.le_next;
			if (ptematch(&po->po_pte, sr, va, 0)) {
				LIST_REMOVE(po, po_list);
				pofree(po, 1);
				pm->pm_stats.resident_count--;
				break;
			}
d2134 10
d2145 28
a2172 1
		
d2174 9
a2182 1
	splx(s);
a2183 5
/*
 * this code to manipulate the BAT tables was added here
 * because it is closely related to the vm system.
 * --dsr
 */
d2185 1
a2185 13
#include <machine/bat.h>

/* one major problem of mapping IO with bats, is that it
 * is not possible to use caching on any level of granularity 
 * that is reasonable.
 * This is just enhancing an existing design (that should be
 * replaced in my opinion).
 *
 * Current design only allow mapping of 256 MB block. (normally 1-1)
 * but not necessarily (in the case of PCI io at 0xf0000000 where
 * it might be desireable to map it elsewhere because that is
 * where the stack is?)
 */
d2187 1
a2187 1
addbatmap(u_int32_t vaddr, u_int32_t raddr, u_int32_t wimg)
d2189 22
a2210 4
	u_int32_t segment;
	segment = vaddr >> (32 - 4);
	battable[segment].batu = BATU(vaddr);
	battable[segment].batl = BATL(raddr, wimg);
d2212 5
a2217 1
/* ??? */
d2219 57
a2275 1
pmap_activate(struct proc *p)
d2277 19
a2295 3
#if 0
	struct pcb *pcb = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;
d2297 2
a2298 14
	/*
	 * XXX Normally performed in cpu_fork();
	 */
	if (pcb->pcb_pm != pmap) {
		pcb->pcb_pm = pmap;
		(void) pmap_extract(pmap_kernel(), (vaddr_t)pcb->pcb_pm,
		    (paddr_t *)&pcb->pcb_pmreal);
	}
	curpcb=pcb;
	if (p == curproc) {
		/* Disable interrupts while switching. */
		__asm __volatile("mfmsr %0" : "=r"(psl) :);
		psl &= ~PSL_EE;
		__asm __volatile("mtmsr %0" :: "r"(psl));
d2300 8
a2307 2
		/* Store pointer to new current pmap. */
		curpm = pcb->pcb_pmreal;
d2309 4
a2312 2
		/* Save kernel SR. */
		__asm __volatile("mfsr %0,14" : "=r"(ksr) :);
d2314 3
a2316 9
		/*
		 * Set new segment registers.  We use the pmap's real
		 * address to avoid accessibility problems.
		 */
		rpm = pcb->pcb_pmreal;
		for (i = 0; i < 16; i++) {
			seg = rpm->pm_sr[i];
			__asm __volatile("mtsrin %0,%1"
			    :: "r"(seg), "r"(i << ADDR_SR_SHFT));
d2318 11
d2330 5
a2334 6
		/* Restore kernel SR. */
		__asm __volatile("mtsr 14,%0" :: "r"(ksr));

		/* Interrupts are OK again. */
		psl |= PSL_EE;
		__asm __volatile("mtmsr %0" :: "r"(psl));
d2336 3
a2338 2
#endif
	return;
d2340 4
a2343 3
/* ??? */
void
pmap_deactivate(struct proc *p)
d2345 11
a2355 1
	return;
@


1.58
log
@Instead of finding pages matching this physical page, match this specific
entry. Also terminate the search as soon as this entry is found.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2002/01/25 03:55:23 drahn Exp $	*/
d461 2
d569 16
@


1.57
log
@How did this work? It used to attempt the tlb entry for a mapping it is
removing by using the va it is replacing it with, NO!.
Calculate the va of the mapping by inverting the pte_hi calculation
producing bits 4-19 of the address. This is enough to correctly invalidate
the tlb entry for the mapping being removed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2002/01/23 00:39:47 art Exp $	*/
d1568 5
d1592 6
d1599 1
a1599 2
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
d1608 2
d1611 5
a1615 3
		for (ptp = ptable + (pv->pv_idx ^ ptab_mask) * 8, i = 8; --i >= 0; ptp++)
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
d1624 1
d1626 6
a1631 2
		for (po = potable[pv->pv_idx].lh_first; po; po = po->po_list.le_next)
			if ((po->po_pte.pte_lo & PTE_RPGN) == pa) {
d1635 1
@


1.56
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2002/01/13 05:27:40 drahn Exp $	*/
d411 1
d435 6
a440 1
			tlbie(addr);
@


1.55
log
@Regress this one additional change, with this change more systems
successfully ran 'make build'.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.54 2002/01/06 06:02:59 drahn Exp $	*/
d732 1
a732 2
	pool_init(&pmap_vp_pool, PAGE_SIZE, 0, 0, 0, "ppvl",
            0, NULL, NULL, M_VMPMAP);
d735 1
a735 1
            0, NULL, NULL, M_VMPMAP);
d737 1
a737 1
            0, NULL, NULL, M_VMPMAP);
@


1.54
log
@That was no fix, that broke things. If the pte entry is currently found
in the po lists, it will NOT have the PTE_VALID bit set. Thus valid
mappings could be ignored if enough mappings existed for that PTEG pair.
This explains the bus_dma panics.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2002/01/06 04:04:48 drahn Exp $	*/
d1337 2
d1347 1
a1347 1
		FALSE);
d1356 1
a1356 1
		PMAP_CACHE_DEFAULT, FALSE);
d1367 1
a1367 1
		pmap_remove_pvl(pmap_kernel(), va, va + PAGE_SIZE, FALSE);
@


1.53
log
@pte_spill() is executed on a special stack in real mode (vm not enabled).
It is not valid to call pool_put() from that context.
If called from that context, put the freed item on one of two lists
(race safe), poalloc() will attempt to fetch from there, and pofree()
will clean up if called from a normal context.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2001/12/13 19:14:41 drahn Exp $	*/
d1496 1
a1496 1
	if (!(ptp = pte_find(pm, va)) || (ptp->pte_hi & PTE_VALID) == 0) {
@


1.52
log
@Fix for pmap extract from art.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2001/11/29 16:43:42 drahn Exp $	*/
d59 11
d1052 16
d1082 28
a1109 3
	s = splimp();
	pool_put(&pmap_po_pool, po);
	splx(s);
@


1.52.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2002/01/25 04:04:55 drahn Exp $	*/
a58 11
/* free lists for potable entries, it is not valid to pool_put
 * in the pte spill handler.
 * pofreebusy variable is a flag to indicate that the 
 * higher level is maniuplating list0 and that spill cannot touch
 * it, the higher level can only be touching one at a time.
 * if list0 is busy list1 cannot be busy.
 */
LIST_HEAD(,pte_ovfl) pofreetable0 = LIST_HEAD_INITIALIZER(pofreetable0);
LIST_HEAD(,pte_ovfl) pofreetable1 = LIST_HEAD_INITIALIZER(pofreetable1);
volatile int pofreebusy;

a399 1
	vm_offset_t va;
d423 1
a423 6
			/* calculate the va of the address being removed */
			va = ((pt->pte_hi & PTE_API) << ADDR_API_SHFT) |
			    ((((pt->pte_hi >> PTE_VSID_SHFT) & SR_VSID)
				^(idx ^ ((pt->pte_hi & PTE_HID) ? 0x3ff : 0)))
				    & 0x3ff) << PAGE_SHIFT;
			tlbie(va);
d721 2
a722 1
	pool_init(&pmap_vp_pool, PAGE_SIZE, 0, 0, 0, "ppvl", NULL);
d725 1
a725 1
            NULL);
d727 1
a727 1
            NULL);
a1040 16
	pofreebusy = 1;
	if (!LIST_EMPTY(&pofreetable0)) {
		po = LIST_FIRST(&pofreetable0);
		LIST_REMOVE(po,po_list);
		pofreebusy = 0;
		return po;
	}
	pofreebusy = 0;

	if (!LIST_EMPTY(&pofreetable1)) {
		po = LIST_FIRST(&pofreetable1);
		LIST_REMOVE(po,po_list);
		pofreebusy = 0;
		return po;
	}

d1055 3
a1057 28
	if (freepage) {
		s = splimp();
		pool_put(&pmap_po_pool, po);
		splx(s);
		while (!LIST_EMPTY(&pofreetable1)) {
			po = LIST_FIRST(&pofreetable1);
			LIST_REMOVE(po, po_list);
			s = splimp();
			pool_put(&pmap_po_pool, po);
			splx(s);
		}

		pofreebusy = 1;
		while (!LIST_EMPTY(&pofreetable0)) {
			po = LIST_FIRST(&pofreetable0);
			LIST_REMOVE(po, po_list);
			s = splimp();
			pool_put(&pmap_po_pool, po);
			splx(s);
		}
		pofreebusy = 0;

	} else {
		if (pofreebusy == 0)
			LIST_INSERT_HEAD(&pofreetable0, po, po_list);
		else
			LIST_INSERT_HEAD(&pofreetable1, po, po_list);
	}
a1284 2
#define KERN_MAP_PV TRUE

d1293 1
a1293 1
		KERN_MAP_PV);
d1302 1
a1302 1
		PMAP_CACHE_DEFAULT, KERN_MAP_PV);
d1313 1
a1313 1
		pmap_remove_pvl(pmap_kernel(), va, va + PAGE_SIZE, KERN_MAP_PV);
d1444 1
a1444 1
	if (!(ptp = pte_find(pm, va))) {
a1508 5
	u_int32_t pte_hi;
	int found;
	vaddr_t va;
	sr_t sr;
	struct pmap *pm;
a1527 6
		va = pv->pv_va;
		pm = pv->pv_pmap;
		sr = ptesr(pm->pm_sr, va);
		pte_hi = ((sr & SR_VSID) << PTE_VSID_SHFT)
		    | ((va & ADDR_PIDX) >> ADDR_API_SHFT);
		found = 0;
d1529 2
a1530 1
			if ((pte_hi | PTE_VALID) == ptp->pte_hi) {
a1538 2
				found = 1;
				break;
d1540 3
a1542 5
		if (found)
			continue;
		for (ptp = ptable + (pv->pv_idx ^ ptab_mask) * 8, i = 8;
		    --i >= 0; ptp++) {
			if ((pte_hi | PTE_VALID | PTE_HID) == ptp->pte_hi) {
a1550 1
				found = 1;
d1552 2
a1553 6
		}
		if (found)
			continue;
		for (po = potable[pv->pv_idx].lh_first;
		    po; po = po->po_list.le_next) {
			if (pte_hi == po->po_pte.pte_hi) {
a1556 1
		}
@


1.52.2.2
log
@Sync UBC branch to -current
@
text
@d1 2
a2 1
/*	$OpenBSD$ */
d5 3
a7 1
 * Copyright (c) 2001, 2002 Dale Rahn. All rights reserved.
a8 1
 *   
d19 2
a20 2
 *	This product includes software developed by Dale Rahn.
 * 4. The name of the author may not be used to endorse or promote products
d23 1
a23 1
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
d26 8
a33 13
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 */  

d38 1
a45 5
#include <machine/pmap.h>

#include <machine/db_machdep.h>
#include <ddb/db_extern.h>
#include <ddb/db_output.h>
d47 8
a54 33
struct pmap kernel_pmap_;
static int npgs;
static struct mem_region *pmap_mem, *pmap_avail;
struct mem_region pmap_allocated[10];
int pmap_cnt_avail;
int pmap_cnt_allocated;


void * pmap_pvh;
void * pmap_attrib;
pte_t  *pmap_ptable;
int	pmap_ptab_cnt;
#ifdef USE_WTABLE
/* Wired entries in pmap_ptable */
Uint8_t *pmap_wtable;
#endif /* USE_WTABLE */
u_int	pmap_ptab_mask;
#define HTABSIZE (pmap_ptab_cnt * 64)

static u_int usedsr[NPMAPS / sizeof(u_int) / 8];
paddr_t zero_page;
paddr_t copy_src_page;
paddr_t copy_dst_page;

/* P->V table */
LIST_HEAD(pted_pv_head, pte_desc);

struct pte_desc {
	/* Linked list of phys -> virt entries */
	LIST_ENTRY(pte_desc) pted_pv_list;
	struct pte pted_pte;
	struct pmap *pted_pmap;
	vaddr_t pted_va;
d57 1
a57 1
void print_pteg(struct pmap *pm, vaddr_t va);
d59 10
a68 3
static inline void tlbsync(void);
static inline void tlbie(vaddr_t ea);
static inline void tlbia(void);
d70 1
a70 2
void pmap_attr_save(paddr_t pa, u_int32_t bits);
void pmap_page_ro(struct pmap *pm, vaddr_t va);
d72 3
d76 1
d78 2
a79 7
 * LOCKING structures.
 * This may not be correct, and doesn't do anything yet.
 */
#define pmap_simplelock_pm(pm)
#define pmap_simpleunlock_pm(pm)
#define pmap_simplelock_pv(pm)
#define pmap_simpleunlock_pv(pm)
d81 22
d104 1
a104 41
/* VP routines */
void pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted);
struct pte_desc *pmap_vp_remove(pmap_t pm, vaddr_t va);
void pmap_vp_destroy(pmap_t pm);
struct pte_desc *pmap_vp_lookup(pmap_t pm, vaddr_t va);

/* PV routines */
int pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *);
void pmap_remove_pv(struct pte_desc *pted);


/* pte hash table routines */
void pte_insert(struct pte_desc *pted);
void pmap_hash_remove(struct pte_desc *pted);
void pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa,
    struct pte_desc *pted, vm_prot_t prot, int flags, int cache);

void pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va);

void _pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags,
    int cache);
void pmap_remove_pg(struct pmap *pm, vaddr_t va);
void pmap_kremove_pg(vaddr_t va);

/* setup/initialization functions */
void pmap_avail_setup(void);
void pmap_avail_fixup(void);
void pmap_remove_avail(paddr_t base, paddr_t end);
void * pmap_steal_avail(size_t size, int align);

/* asm interface */
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type);

u_int32_t pmap_setusr(struct pmap *pm, vaddr_t va);
void pmap_popusr(u_int32_t oldsr);

/* debugging */
void pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...));

/* XXX - panic on pool get failures? */
struct pool pmap_pmap_pool;
a105 1
struct pool pmap_pted_pool;
d107 4
a110 2
int pmap_initialized = 0;
int physmem;
d112 1
a112 3
#define ATTRSHIFT	4

/* virtual to physical helpers */
d114 1
a114 1
VP_SR(vaddr_t va)
d116 1
a116 1
	return (va >>VP_SR_POS) & VP_SR_MASK;
a117 1

d119 1
a119 1
VP_IDX1(vaddr_t va)
d125 1
a125 1
VP_IDX2(vaddr_t va)
d130 2
a131 22
#if VP_IDX1_SIZE != VP_IDX2_SIZE 
#error pmap allocation code expects IDX1 and IDX2 size to be same
#endif
struct pmapvp {
	void *vp[VP_IDX1_SIZE];
};


/*
 * VP routines, virtual to physical translation information.
 * These data structures are based off of the pmap, per process.
 */

/*
 * This is used for pmap_kernel() mappings, they are not to be removed
 * from the vp table because they were statically initialized at the
 * initial pmap initialization. This is so that memory allocation 
 * is not necessary in the pmap_kernel() mappings.
 * otherwise bad race conditions can appear.
 */
struct pte_desc *
pmap_vp_lookup(pmap_t pm, vaddr_t va)
d133 4
a136 7
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
d138 2
d141 16
a156 3
	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
d158 4
a161 4

	pted = vp2->vp[VP_IDX2(va)];

	return pted;
d163 2
a164 6

/*
 * Remove, and return, pted at specified address, NULL if not present
 */
struct pte_desc *
pmap_vp_remove(pmap_t pm, vaddr_t va)
d166 19
a184 3
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;
d186 1
a186 4
	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
	}
d188 5
a192 3
	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
d194 3
d198 4
a201 4
	pted = vp2->vp[VP_IDX2(va)];
	vp2->vp[VP_IDX2(va)] = NULL;

	return pted;
a203 8
/*
 * Create a V -> P mapping for the given pmap and virtual address
 * with reference to the pte descriptor that is used to map the page.
 * This code should track allocations of vp table allocations
 * so they can be freed efficiently.
 * 
 * should this be called under splimp?
 */
d205 2
a206 1
pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted)
d208 1
a208 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d210 4
d215 11
a225 1
	pmap_simplelock_pm(pm);
d227 3
a229 2
	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
d231 1
a231 1
		vp1 = pool_get(&pmap_vp_pool, PR_NOWAIT);
d233 1
a233 2
		bzero(vp1, sizeof (struct pmapvp));
		pm->pm_vp[VP_SR(va)] = vp1;
d235 37
d273 1
a273 8
	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		s = splimp();
		vp2 = pool_get(&pmap_vp_pool, PR_NOWAIT);
		splx(s);
		bzero(vp2, sizeof (struct pmapvp));
		vp1->vp[VP_IDX1(va)] = vp2;
	}
d275 1
a275 1
	vp2->vp[VP_IDX2(va)] = pted;
d277 19
a295 1
	pmap_simpleunlock_pm(pm);
d297 1
a297 2
	return;
}
d299 3
a301 3
/* 
 * WHERE DO HELPER FUNCTIONS GO?
 * XXX - need pvent filled out and changed.
d303 2
a304 12
static inline struct pted_pv_head *
pmap_find_pvh(paddr_t pa)
{
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.pvent[off];
	}
	return NULL;
}
static inline char *
pmap_find_attr(paddr_t pa)
d306 1
a306 6
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.attrs[off];
	}
	return NULL;
a308 6
/* PTE manipulation/calculations */
static inline void
tlbie(vaddr_t va)
{
	__asm volatile ("tlbie %0" :: "r"(va));
}
d310 1
a310 1
tlbsync(void)
d312 1
a312 1
	__asm volatile ("sync; tlbsync; sync");
d315 1
a315 1
static inline void
d318 5
a322 5
	vaddr_t va;

	__asm volatile ("sync");
	for (va = 0; va < 0x00040000; va += 0x00001000)
		tlbie(va);
a323 1

d327 3
a329 1
ptesr(sr_t *sr, vaddr_t va)
d331 1
a331 1
	return sr[(u_int)va >> ADDR_SR_SHIFT];
d333 5
a337 2
static inline int 
pteidx(sr_t sr, vaddr_t va)
d340 3
a342 2
	hash = (sr & SR_VSID) ^ (((u_int)va & ADDR_PIDX) >> ADDR_PIDX_SHIFT);
	return hash & pmap_ptab_mask;
d345 6
a350 26
#define PTED_VA_PTEGIDX_M 0x07
#define PTED_VA_HID_M	  0x08
#define PTED_VA_MANAGED_M 0x10
#define PTED_VA_WIRED_M   0x20
static inline u_int32_t
PTED_HID(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_HID_M); 
}
static inline u_int32_t
PTED_PTEGIDX(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_PTEGIDX_M); 
}
static inline u_int32_t
PTED_MANAGED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_MANAGED_M); 
}
static inline u_int32_t
PTED_WIRED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_WIRED_M); 
}
static inline u_int32_t
PTED_VALID(struct pte_desc *pted)
d352 4
a355 1
	return (pted->pted_pte.pte_hi & PTE_VALID);
d359 1
a359 20
 * PV entries -
 * manpulate the physical to virtual translations for the entire system.
 * 
 * QUESTION: should all mapped memory be stored in PV tables? or
 * is it alright to only store "ram" memory. Currently device mappings
 * are not stored.
 * It makes sense to pre-allocate mappings for all of "ram" memory, since
 * it is likely that it will be mapped at some point, but would it also
 * make sense to use a tree/table like is use for pmap to store device
 * mappings.
 * Futher notes: It seems that the PV table is only used for pmap_protect
 * and other paging related operations. Given this, it is not necessary
 * to store any pmap_kernel() entries in PV tables and does not make
 * sense to store device mappings in PV either.
 *
 * Note: unlike other powerpc pmap designs, the array is only an array
 * of pointers. Since the same structure is used for holding information
 * in the VP table, the PV table, and for kernel mappings, the wired entries.
 * Allocate one data structure to hold all of the info, instead of replicating
 * it multiple times.
d361 2
a362 4
 * One issue of making this a single data structure is that two pointers are
 * wasted for every page which does not map ram (device mappings), this 
 * should be a low percentage of mapped pages in the system, so should not
 * have too noticable unnecssary ram consumption.
a363 2


d365 3
a367 1
pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *pvh)
d369 3
a371 81
	int first;

	if (__predict_false(!pmap_initialized)) {
		return 0;
	}
	if (pvh == NULL) {
		return 0;
	}

	first = LIST_EMPTY(pvh);
	LIST_INSERT_HEAD(pvh, pted, pted_pv_list);
	pted->pted_va |= PTED_VA_MANAGED_M;
	return first;
}

void
pmap_remove_pv(struct pte_desc *pted)
{
	LIST_REMOVE(pted, pted_pv_list);
}

void
pmap_attr_save(paddr_t pa, u_int32_t bits)
{
	int bank, pg;
	u_int8_t *attr;

	bank = vm_physseg_find(atop(pa), &pg);
	if (bank == -1)
		return;
	attr = &vm_physmem[bank].pmseg.attrs[pg];
	*attr |= (u_int8_t)(bits >> ATTRSHIFT);
}

int
pmap_enter(pm, va, pa, prot, flags)
	pmap_t pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	struct pte_desc *pted;
	struct pted_pv_head *pvh;
	int s;
	int need_sync;
	int cache;

	/* MP - Acquire lock for this pmap */

	s = splimp();
	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted)) {
		pmap_remove_pg(pm, va);
		/* we lost our pted if it was user */
		if (pm != pmap_kernel())
			pted = pmap_vp_lookup(pm, va);
	}

	pm->pm_stats.resident_count++;

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
		pmap_vp_enter(pm, va, pted);
	}

	/* Calculate PTE */
	pvh = pmap_find_pvh(pa);
	if (pvh != NULL) {
		/* managed memory is cachable */
		cache = PMAP_CACHE_WB;
	} else {
		cache = PMAP_CACHE_CI;
	}

	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);

	need_sync = pmap_enter_pv(pted, pvh);

d373 1
a373 3
	 * Insert into HTAB
	 * we were told to map the page, probably called from vm_fault,
	 * so map the page!
d375 17
a391 10
	pte_insert(pted);

	splx(s);

	/* only instruction sync executable pages */
	if (need_sync && (prot & VM_PROT_EXECUTE)) {
		pmap_syncicache_user_virt(pm, va);
	}

	/* MP - free pmap lock */
d395 6
a400 2
/* 
 * Remove the given range of mapping entries.
d402 3
a404 2
void
pmap_remove(struct pmap *pm, vaddr_t va, vaddr_t endva)
d406 49
a454 50
	int i_sr, s_sr, e_sr;
	int i_vp1, s_vp1, e_vp1;
	int i_vp2, s_vp2, e_vp2;
	pmapvp_t *vp1;
	pmapvp_t *vp2;

	/* I suspect that if this loop were unrolled better 
	 * it would have better performance, testing i_sr and i_vp1
	 * in the middle loop seems excessive
	 */

	s_sr = VP_SR(va);
	e_sr = VP_SR(endva);
	for (i_sr = s_sr; i_sr <= e_sr; i_sr++) {
		vp1 = pm->pm_vp[i_sr];
		if (vp1 == NULL)
			continue;
		
		if (i_sr == s_sr) {
			s_vp1 = VP_IDX1(va);
		} else {
			s_vp1 = 0;
		}
		if (i_sr == e_sr) {
			e_vp1 = VP_IDX1(endva);
		} else {
			e_vp1 = VP_IDX1_SIZE-1; 
		}
		for (i_vp1 = s_vp1; i_vp1 <= e_vp1; i_vp1++) {
			vp2 = vp1->vp[i_vp1];
			if (vp2 == NULL)
				continue;

			if ((i_sr == s_sr) && (i_vp1 == s_vp1)) {
				s_vp2 = VP_IDX2(va);
			} else {
				s_vp2 = 0;
			}
			if ((i_sr == e_sr) && (i_vp1 == e_vp1)) {
				e_vp2 = VP_IDX2(endva);
			} else {
				e_vp2 = VP_IDX2_SIZE; 
			}
			for (i_vp2 = s_vp2; i_vp2 < e_vp2; i_vp2++) {
				if (vp2->vp[i_vp2] != NULL) {
					pmap_remove_pg(pm,
					    (i_sr << VP_SR_POS) |
					    (i_vp1 << VP_IDX1_POS) |
					    (i_vp2 << VP_IDX2_POS));
				}
d456 1
d458 1
a458 1
	}
d460 3
d464 1
a464 1
 * remove a single mapping, notice that this code is O(1)
d467 2
a468 1
pmap_remove_pg(struct pmap *pm, vaddr_t va)
d470 3
a472 2
	struct pte_desc *pted;
	int s;
d475 1
a475 3
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
d477 4
a480 13
	s = splimp();
	if (pm == pmap_kernel()) {
		pted = pmap_vp_lookup(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			splx(s);
			return;
		} 
	} else {
		pted = pmap_vp_remove(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			splx(s);
			return;
		}
a481 1
	pm->pm_stats.resident_count--;
d483 5
a487 1
	pmap_hash_remove(pted);
d489 7
a495 1
	pted->pted_pte.pte_hi &= ~PTE_VALID;
d497 10
a506 5
	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	if (pm != pmap_kernel()) {
		pool_put(&pmap_pted_pool, pted);
d508 60
a567 30

	splx(s);
}

/*
 * Enter a kernel mapping for the given page.
 * kernel mappings have a larger set of prerequisites than normal mappings.
 * 
 * 1. no memory should be allocated to create a kernel mapping.
 * 2. a vp mapping should already exist, even if invalid. (see 1)
 * 3. all vp tree mappings should already exist (see 1)
 * 
 */
void
_pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags, int cache)
{
	struct pte_desc *pted;
	int s;
	struct pmap *pm;
	struct pted_pv_head *pvh;

	pm = pmap_kernel();

	/* MP - lock pmap. */
	s = splimp();

	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted)) {
		/* pted is reused */
		pmap_kremove_pg(va);
d569 4
d574 6
a579 10
	pm->pm_stats.resident_count++;

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		/* XXX - future panic? */
		printf("pted not preallocated in pmap_kernel() va %x pa %x \n",
		    va, pa);
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
		pmap_vp_enter(pm, va, pted);
d581 1
d583 71
a653 5
	pvh = pmap_find_pvh(pa);
	if (cache == PMAP_CACHE_DEFAULT) {
		if (pvh != NULL) {
		/* managed memory is cachable */
			cache = PMAP_CACHE_WB;
d655 3
a657 1
			cache = PMAP_CACHE_CI;
d660 6
a665 3

	/* Calculate PTE */
	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);
d668 1
a668 3
	 * Insert into HTAB
	 * we were told to map the page, probably called from vm_fault,
	 * so map the page!
d670 19
a688 5
	pte_insert(pted);
	pted->pted_va |= PTED_VA_WIRED_M;

	splx(s);

d691 3
d695 3
a697 1
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
d699 15
a713 7
	_pmap_kenter_pa(va, pa, prot, 0, PMAP_CACHE_DEFAULT);
}

void
pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable)
{
	_pmap_kenter_pa(va, pa, prot, 0, cacheable);
a715 1

d717 2
a718 1
 * remove kernel (pmap_kernel()) mapping, one page
d721 1
a721 1
pmap_kremove_pg(vaddr_t va)
d723 31
a753 9
	struct pte_desc *pted;
	struct pmap *pm;
	int s;

	pm = pmap_kernel();
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
		/* XXX */
		return;
d755 1
a755 21
	if (!PTED_VALID(pted)) {
		/* not mapped */
		return;
	}
	s = splimp();

	pm->pm_stats.resident_count--;

	/*
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
	 */
	pmap_hash_remove(pted);

	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	/* invalidate pted; */
	pted->pted_pte.pte_hi &= ~PTE_VALID;

d757 1
a758 1
}
d760 1
a760 1
 * remove kernel (pmap_kernel()) mappings
d762 3
a764 2
void
pmap_kremove(vaddr_t va, vsize_t len)
d766 9
a774 2
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE) {
		pmap_kremove_pg(va);
d776 1
d778 4
d783 8
a790 3
void pte_zap(pte_t *ptp, struct pte_desc *pted);
void
pte_zap(pte_t *ptp, struct pte_desc *pted)
d792 7
a798 10
		ptp->pte_hi &= ~PTE_VALID;
		__asm volatile ("sync");
		tlbie(pted->pted_va);
		__asm volatile ("sync");
		tlbsync();
		__asm volatile ("sync");
		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(pted->pted_pte.pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
		}
d800 3
d804 1
a804 3
 * remove specified entry from hash table.
 * all information is present in pted to look up entry
 * LOCKS... should the caller lock?
d807 2
a808 1
pmap_hash_remove(struct pte_desc *pted)
a809 13
	vaddr_t va = pted->pted_va;
	struct pmap *pm = pted->pted_pmap;
	pte_t *ptp;
	int sr, idx;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);

	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));
	/* determine which pteg mapping is present in */
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

d811 1
a811 3
	 * We now have the pointer to where it will be, if it is currently
	 * mapped. If the mapping was thrown away in exchange for another
	 * page mapping, then this page is not currently in the HASH.
d813 2
a814 7
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp, pted);
#ifdef USE_WTABLE
		pmap_wtable [idx] &= ~(1 << PTED_PTEGIDX(pted)); 
#endif /* USE_WTABLE */
	}
a816 1

d818 2
a819 1
 * What about execution control? Even at only a segment granularity.
d821 2
a822 3
void
pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
	vm_prot_t prot, int flags, int cache)
d824 1
a824 26
	sr_t sr;
	struct pte *pte;

	sr = ptesr(pm->pm_sr, va);
	pte = &pted->pted_pte;
	pte->pte_hi = ((sr & SR_VSID) << PTE_VSID_SHIFT) |
	    ((va >> ADDR_API_SHIFT) & PTE_API) | PTE_VALID;
	pte->pte_lo = (pa & PTE_RPGN);


	if ((cache == PMAP_CACHE_WB)) {
		pte->pte_lo |= PTE_M;
	} else if ((cache == PMAP_CACHE_WT)) {
		pte->pte_lo |= (PTE_W | PTE_M);
	} else {
		pte->pte_lo |= (PTE_M | PTE_I | PTE_G);
	}

	if (prot & VM_PROT_WRITE) {
		pte->pte_lo |= PTE_RW;
	} else {
		pte->pte_lo |= PTE_RO;
	}

	pted->pted_va = va & ~PAGE_MASK;
	pted->pted_pmap = pm;
d828 3
a830 2
 * read/clear bits from pte/attr cache, for reference/change
 * ack, copied code in the pte flush code....
d833 2
a834 1
pteclrbits(paddr_t pa, u_int bit, u_int clear)
d836 4
a839 62
	char *pattr;
	u_int bits;
	int s;
	struct pte_desc *pted;
	struct pted_pv_head *pvh;

	pattr = pmap_find_attr(pa);

	/* check if managed memory */
	if (pattr == NULL)
		return 0;

	pvh = pmap_find_pvh(pa);

	/*
	 *  First try the attribute cache
	 */
	bits = (*pattr << ATTRSHIFT) & bit;
	if ((bits == bit) && (clear == 0))
		return bits;

	/* cache did not contain all necessary bits,
	 * need to walk thru pv table to collect all mappings for this
	 * page, copying bits to the attribute cache 
	 * then reread the attribute cache.
	 */
	/* need lock for this pv */
	s = splimp();

	LIST_FOREACH(pted, pvh, pted_pv_list) {
		vaddr_t va = pted->pted_va & PAGE_MASK;
		struct pmap *pm = pted->pted_pmap;
		pte_t *ptp;
		int sr, idx;

		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);

		/* determine which pteg mapping is present in */
		ptp = pmap_ptable +
			(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
		    == ptp->pte_hi) {
			bits |=	ptp->pte_lo & (PTE_REF|PTE_CHG);
			if (clear) {
				ptp->pte_hi &= ~PTE_VALID;
				__asm__ volatile ("sync");
				tlbie(va);
				tlbsync();
				ptp->pte_lo &= ~bit;
				__asm__ volatile ("sync");
				ptp->pte_hi |= PTE_VALID;
			}
		}
d841 5
a845 6

	bits |= (*pattr << ATTRSHIFT) & bit;
	if (clear) {
		*pattr &= ~(bit >> ATTRSHIFT);
	} else {
		*pattr |= (bits >> ATTRSHIFT);
d847 3
a849 2
	splx(s);
	return bits;
d853 1
a853 4
 * Garbage collects the physical map system for pages which are 
 * no longer used. Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but others may be collected
 * Called by the pageout daemon when pages are scarce.
d855 2
a856 2
void
pmap_collect(struct pmap *pm)
d858 6
a863 7
	/* This could return unused v->p table layers which 
	 * are empty.
	 * could malicious programs allocate memory and eat
	 * these wired pages? These are allocated via pool.
	 * Is there pool functions which could be called
	 * to lower the pool usage here?
	 */
d867 1
a867 1
 * Fill the given physical page with zeros.
d870 2
a871 1
pmap_zero_page(paddr_t pa)
d873 2
a874 31
#ifdef USE_DCBZ
	int i;
	paddr_t addr = zero_page;
#endif

	/* simple_lock(&pmap_zero_page_lock); */
	pmap_kenter_pa(zero_page, pa, VM_PROT_READ|VM_PROT_WRITE);
#ifdef USE_DCBZ
	for (i = PAGE_SIZE/CACHELINESIZE; i>0; i--) {
		__asm volatile ("dcbz 0,%0" :: "r"(addr));
		addr += CACHELINESIZE;
	}
#else
	bzero((void *)zero_page, PAGE_SIZE);
#endif
	pmap_kremove_pg(zero_page);
	
	/* simple_unlock(&pmap_zero_page_lock); */
}
/*
 * copy the given physical page with zeros.
 */
void
pmap_copy_page(paddr_t srcpa, paddr_t dstpa)
{
	/* simple_lock(&pmap_copy_page_lock); */

	pmap_kenter_pa(copy_src_page, srcpa, VM_PROT_READ);
	pmap_kenter_pa(copy_dst_page, dstpa, VM_PROT_READ|VM_PROT_WRITE);

	bcopy((void *)copy_src_page, (void *)copy_dst_page, PAGE_SIZE);
a875 16
	pmap_kremove_pg(copy_src_page);
	pmap_kremove_pg(copy_dst_page);
	/* simple_unlock(&pmap_copy_page_lock); */
}

int pmap_id_avail = 0;
void
pmap_pinit(struct pmap *pm)
{
	int i, k, try, tblidx, tbloff;
	int s, seg;

	bzero(pm, sizeof (struct pmap));

	pmap_reference(pm);

d877 1
a877 2
	 * Allocate segment registers for this pmap.
	 * try not to reuse pmap ids, to spread the hash table usage.
d879 8
a886 20
again:
	for (i = 0; i < NPMAPS; i++) {
		try = pmap_id_avail + i;
		try = try % NPMAPS; /* truncate back into bounds */
		tblidx = try / (8 * sizeof usedsr[0]);
		tbloff = try % (8 * sizeof usedsr[0]);
		if ((usedsr[tblidx] & (1 << tbloff)) == 0) {
			/* pmap create lock? */
			s = splimp();
			if ((usedsr[tblidx] & (1 << tbloff)) == 1) {
				/* entry was stolen out from under us, retry */
				splx(s); /* pmap create unlock */
				goto again;
			}
			usedsr[tblidx] |= (1 << tbloff); 
			pmap_id_avail = try + 1;
			splx(s); /* pmap create unlock */

			seg = try << 4;
			for (k = 0; k < 16; k++) {
d888 1
a888 1
			}
a890 15
	}
	panic("out of pmap slots");
}

/* 
 * Create and return a physical map.
 */
struct pmap *
pmap_create()
{
	struct pmap *pmap;
	int s;

	s = splimp();
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d892 1
a892 2
	pmap_pinit(pmap);
	return (pmap);
d896 1
a896 1
 * Add a reference to a given pmap.
d899 2
a900 1
pmap_reference(struct pmap *pm)
a901 1
	/* simple_lock(&pmap->pm_obj.vmobjlock); */
a902 1
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
d910 2
a911 1
pmap_destroy(struct pmap *pm)
d913 3
a915 8
	int refs;
	int s;

	/* simple_lock(&pmap->pm_obj.vmobjlock); */
	refs = --pm->pm_refs;
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
	if (refs > 0) {
		return;
a916 7
	/*
	 * reference count is zero, free pmap resources and free pmap.
	 */
	pmap_release(pm);
	s = splimp();
	pool_put(&pmap_pmap_pool, pm);
	splx(s);
d924 2
a925 1
pmap_release(struct pmap *pm)
d927 1
a927 1
	int i, tblidx, tbloff;
d930 1
d932 6
a937 5
	i = pm->pm_sr[0] >> 4;
	tblidx = i / (8  * sizeof usedsr[0]);
	tbloff = i % (8  * sizeof usedsr[0]);

	/* LOCK? */
d939 1
a939 1
	usedsr[tblidx] &= ~(1 << tbloff);
d943 7
d951 4
a954 1
pmap_vp_destroy(struct pmap *pm)
d956 1
a956 7
	int i, j;
#ifdef CHECK_IDX2_ENTRIES
	int k;
#endif
	int s;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d958 30
a987 27
	for (i = 0; i < VP_SR_SIZE; i++) {
		vp1 = pm->pm_vp[i];
		if (vp1 == NULL) {
			continue;
		}
		for (j = 0; j < VP_IDX1_SIZE; j++) {
			vp2 = vp1->vp[j];
			if (vp2 == NULL)
				continue;
			
			if (pm->pm_stats.resident_count != 0) 
#ifdef CHECK_IDX2_ENTRIES
/* This is ifdefed because it should not happen, and has not been occuring */
				for (k = 0; k < VP_IDX2_SIZE; k++) {
					if (vp2->vp[k] != NULL) {
						printf("PMAP NOT EMPTY"
						    " pm %x va %x\n",
						    pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
						pmap_remove_pg(pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
					}
				}
a988 10
			/* vp1->vp[j] = NULL; */
			s = splimp();
			pool_put(&pmap_vp_pool, vp2);
			splx(s);
		}
		/* pm->pm_vp[i] = NULL; */
		s = splimp();
		pool_put(&pmap_vp_pool, vp1);
		splx(s);
	}
d991 3
d995 2
a996 1
pmap_avail_setup(void)
d998 31
a1028 1
	struct mem_region *mp;
d1030 5
a1034 3
	(fw->mem_regions) (&pmap_mem, &pmap_avail);
	pmap_cnt_avail = 0;
	physmem = 0;
d1036 5
d1042 29
a1070 2
	for (mp = pmap_mem; mp->size !=0; mp++) {
		physmem += btoc(mp->size);
d1073 6
a1078 25
	/* limit to 1GB available, for now -XXXGRR */
#define MEMMAX 0x40000000
	for (mp = pmap_avail; mp->size !=0 ; /* increment in loop */) {
		if (mp->start + mp->size > MEMMAX) {
			int rm_start;
			int rm_end;
			if (mp->start > MEMMAX) {
				rm_start = mp->start;
				rm_end = mp->start+mp->size;
			} else {
				rm_start = MEMMAX;
				rm_end = mp->start+mp->size;
			}
			pmap_remove_avail(rm_start, rm_end);
			/* whack physmem, since we ignore more than 256MB */
			physmem = btoc(MEMMAX);
			/* start over at top, make sure not to skip any */
			mp = pmap_avail;
			continue;
		}
		mp++;
	}
	for (mp = pmap_avail; mp->size !=0; mp++) {
		pmap_cnt_avail += 1;
	}
a1080 1

d1082 3
a1084 1
pmap_avail_fixup(void)
d1086 12
a1097 3
	struct mem_region *mp;
	u_int32_t align;
	u_int32_t end;
d1099 7
a1105 7
	mp = pmap_avail;
	while(mp->size !=0) {
		align = round_page(mp->start);
		if (mp->start != align) {
			pmap_remove_avail(mp->start, align);
			mp = pmap_avail;
			continue;
d1107 7
a1113 8
		end = mp->start+mp->size;
		align = trunc_page(end);
		if (end != align) {
			pmap_remove_avail(align, end);
			mp = pmap_avail;
			continue;
		}
		mp++;
d1117 22
a1138 7
/* remove a given region from avail memory */
void
pmap_remove_avail(paddr_t base, paddr_t end)
{
	struct mem_region *mp;
	int i;
	int mpend;
d1140 1
a1140 2
	/* remove given region from available */
	for (mp = pmap_avail; mp->size; mp++) {
d1142 1
a1142 1
		 * Check if this region hold all of the region
d1144 15
a1158 72
		mpend = mp->start + mp->size;
		if (base > mpend) {
			continue;
		}
		if (base <= mp->start) {
			if (end <= mp->start) {
				/* region not present -??? */
				break;
			}
			if (end >= mpend) {
				/* covers whole region */
				/* shorten */
				for (i = mp - pmap_avail;
					i < pmap_cnt_avail;
					i++)
				{
					pmap_avail[i] = pmap_avail[i+1];
				}
				pmap_cnt_avail--;
				pmap_avail[pmap_cnt_avail].size = 0;
			} else {
				mp->start = end;
				mp->size = mpend - end;
			}
		} else {
			/* start after the beginning */
			if (end >= mpend) {
				/* just truncate */
				mp->size = base - mp->start;
			} else {
				/* split */
				for (i = pmap_cnt_avail;
					i > (mp - pmap_avail);
					i--)
				{
					pmap_avail[i] = pmap_avail[i - 1];
				}
				pmap_cnt_avail++;
				mp->size = base - mp->start;
				mp++;
				mp->start = end;
				mp->size = mpend - end;
			}
		}
	}
	for (mp = pmap_allocated; mp->size != 0; mp++) {
		if (base < mp->start) {
			if (end == mp->start) {
				mp->start = base;
				mp->size += end - base;
				break;
			}
			/* lenghten */
			for (i = pmap_cnt_allocated; i > (mp - pmap_allocated);
				i--)
			{
				pmap_allocated[i] = pmap_allocated[i - 1];
			}
			pmap_cnt_allocated++;
			mp->start = base;
			mp->size = end - base;
			return;
		}
		if (base == (mp->start + mp->size)) {
			mp->size += end - base;
			return;
		}
	}
	if (mp->size == 0) {
		mp->start = base;
		mp->size  = end - base;
		pmap_cnt_allocated++;
d1160 2
d1164 6
a1169 2
void *
pmap_steal_avail(size_t size, int align)
d1171 13
a1183 3
	struct mem_region *mp;
	int start;
	int remsize;
d1185 27
a1211 7
	for (mp = pmap_avail; mp->size; mp++) {
		if (mp->size > size) {
			start = (mp->start + (align -1)) & ~(align -1);
			remsize = mp->size - (start - mp->start); 
			if (remsize >= 0) {
				pmap_remove_avail(start, start+size);
				return (void *)start;
d1214 14
a1228 2
	panic ("unable to allocate region with size %x align %x\n",
	    size, align);
d1231 3
a1233 1
void *msgbuf_addr;
d1236 20
a1255 6
 * Initialize pmap setup.
 * ALL of the code which deals with avail needs rewritten as an actual
 * memory allocation.
 */ 
void
pmap_bootstrap(u_int kernelstart, u_int kernelend)
d1257 4
a1261 3
	int i, k;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d1264 1
a1264 1
	 * Get memory.
d1266 3
a1268 1
	pmap_avail_setup();
d1270 4
d1275 1
a1275 3
	 * Page align all regions.
	 * Non-page memory isn't very interesting to us.
	 * Also, sort the entries for ascending addresses.
d1277 25
a1301 8
	kernelstart = trunc_page(kernelstart);
	kernelend = round_page(kernelend);
	pmap_remove_avail(kernelstart, kernelend);

	msgbuf_addr = pmap_steal_avail(MSGBUFSIZE,4);

	for (mp = pmap_avail; mp->size; mp++) {
		bzero((void *)mp->start, mp->size);
d1303 4
a1307 8
#ifdef  HTABENTS
	pmap_ptab_cnt = HTABENTS;
#else /* HTABENTS */
	pmap_ptab_cnt = 1024;
	while ((HTABSIZE << 7) < ctob(physmem)) {
		pmap_ptab_cnt <<= 1;
	}
#endif /* HTABENTS */
d1309 1
a1309 1
	 * allocate suitably aligned memory for HTAB
d1311 6
a1316 24
	pmap_ptable = pmap_steal_avail(HTABSIZE, HTABSIZE);
	bzero((void *)pmap_ptable, HTABSIZE);
	pmap_ptab_mask = pmap_ptab_cnt - 1;

#ifdef USE_WTABLE
	pmap_wtable = pmap_steal_avail(pmap_ptab_cnt, 4);
#endif /* USE_WTABLE */

	/* allocate v->p mappings for pmap_kernel() */
	for (i = 0; i < VP_SR_SIZE; i++) {
		pmap_kernel()->pm_vp[i] = NULL;
	}
	vp1 = pmap_steal_avail(sizeof (struct pmapvp), 4);
	bzero (vp1, sizeof(struct pmapvp));
	pmap_kernel()->pm_vp[KERNEL_SR] = vp1;

	for (i = 0; i < VP_IDX1_SIZE; i++) {
		vp2 = vp1->vp[i] = pmap_steal_avail(sizeof (struct pmapvp), 4);
		bzero (vp2, sizeof(struct pmapvp));
		for (k = 0; k < VP_IDX2_SIZE; k++) {
			struct pte_desc *pted;
			pted = pmap_steal_avail(sizeof (struct pte_desc), 4);
			bzero (pted, sizeof (struct pte_desc));
			vp2->vp[k] = pted;
d1319 2
a1320 9

	zero_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_src_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_dst_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;


d1322 1
a1322 1
	 * Initialize kernel pmap and hardware.
d1324 3
a1326 8
#if NPMAPS >= KERNEL_SEGMENT / 16
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
#endif
	for (i = 0; i < 16; i++) {
		pmap_kernel()->pm_sr[i] = KERNEL_SEG0 + i;
		asm volatile ("mtsrin %0,%1"
			      :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
d1328 10
a1337 4
	asm volatile ("sync; mtsdr1 %0; isync"
		      :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));

	pmap_avail_fixup();
d1339 2
d1342 1
a1342 18
	tlbia();

	npgs = 0;
	for (mp = pmap_avail; mp->size; mp++) {
		npgs += btoc(mp->size);
	}
	/* Ok we loose a few pages from this allocation, but hopefully
	 * not too many 
	 */
	pmap_pvh = pmap_steal_avail(sizeof(struct pted_pv_head *) * npgs, 4);
	pmap_attrib = pmap_steal_avail(sizeof(char) * npgs, 1);
	pmap_avail_fixup();
	for (mp = pmap_avail; mp->size; mp++) {
		uvm_page_physload(atop(mp->start), atop(mp->start+mp->size),
		    atop(mp->start), atop(mp->start+mp->size),
		    VM_FREELIST_DEFAULT);
	}
}
a1343 5
/*
 * activate a pmap entry
 * NOOP on powerpc, all PTE entries exist in the same hash table.
 * Segment registers are filled on exit to user mode.
 */
d1345 5
a1349 1
pmap_activate(struct proc *p)
d1351 2
a1353 5

/*
 * deactivate a pmap entry
 * NOOP on powerpc
 */
d1355 4
a1358 1
pmap_deactivate(struct proc *p)
d1360 2
d1364 6
a1369 5
/* 
 * Get the physical page address for the given pmap/virtual address.
 */ 
boolean_t
pmap_extract(struct pmap *pm, vaddr_t va, paddr_t *pa)
d1371 2
a1372 11
	struct pte_desc *pted;

	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted)) {
		if (pm == pmap_kernel() && va < 0x80000000) {
			/* XXX - this is not true without BATs */
			/* if in kernel, va==pa for 0-0x80000000 */
			*pa = va;
			return TRUE;
		}
		return FALSE;
a1373 2
	*pa = (pted->pted_pte.pte_lo & PTE_RPGN) | (va & ~PTE_RPGN);
	return TRUE;
d1376 3
a1378 2
u_int32_t
pmap_setusr(struct pmap *pm, vaddr_t va)
d1380 5
a1384 2
	u_int32_t sr;
	u_int32_t oldsr;
a1385 9
	sr = pm->pm_sr[(u_int)va >> ADDR_SR_SHIFT];

	/* user address range lock?? */
	asm volatile ("mfsr %0,%1"
		      : "=r" (oldsr): "n"(USER_SR));
	asm volatile ("isync; mtsr %0,%1; isync"
		      :: "n"(USER_SR), "r"(sr));
	return oldsr;
}
d1387 3
d1391 3
a1393 1
pmap_popusr(u_int32_t sr)
d1395 1
a1395 2
	asm volatile ("isync; mtsr %0,%1; isync"
		      :: "n"(USER_SR), "r"(sr));
d1398 5
a1402 69
int
copyin(udaddr, kaddr, len)
	const void *udaddr;
	void *kaddr;
	size_t len;
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}
		bcopy(p, kaddr, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
}

int
copyout(kaddr, udaddr, len)
	const void *kaddr;
	void *udaddr;
	size_t len;
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}

		bcopy(kaddr, p, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
}

int
copyinstr(const void *udaddr, void *kaddr, size_t len, size_t *done)
d1404 37
a1440 33
	const u_char *uaddr = udaddr;
	u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(env)) {
			if (done != NULL) {
				*done =  cnt;
			}
			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *up;
			*kp = c;
			if (c == 0) {
				if (done != NULL) {
					*done = cnt + 1;
d1442 13
a1454 55
				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
		}
		pmap_popusr(oldsr);
	}
	curpcb->pcb_onfault = oldh;
	if (done != NULL) {
		*done = cnt;
	}
	return ENAMETOOLONG;
}

int
copyoutstr(const void *kaddr, void *udaddr, size_t len, size_t *done)
{
	u_char *uaddr = (void *)udaddr;
	const u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(env)) {
			if (done != NULL) {
				*done =  cnt;
			}
			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *kp;
			*up = c;
			if (c == 0) {
				if (done != NULL) {
					*done = cnt + 1;
d1456 5
a1460 9
				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
a1461 47
		pmap_popusr(oldsr);
	}
	curpcb->pcb_onfault = oldh;
	if (done != NULL) {
		*done = cnt;
	}
	return ENAMETOOLONG;
}

/*
 * sync instruction cache for user virtual address.
 * The address WAS JUST MAPPED, so we have a VALID USERSPACE mapping
 */
#define CACHELINESIZE   32		/* For now XXX*/
void
pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va)
{
	vaddr_t p, start;
	int oldsr;
	int l;

	if (pm != pmap_kernel()) {
		start = ((u_int)USER_ADDR + ((u_int)va & ~SEGMENT_MASK));
		/* will only ever be page size, will not cross segments */

		/* USER SEGMENT LOCK - MPXXX */
		oldsr = pmap_setusr(pm, va);
	} else {
		start = va; /* flush mapped page */
	}
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("dcbst 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("icbi 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);


	if (pm != pmap_kernel()) {
		pmap_popusr(oldsr);
		/* USER SEGMENT UNLOCK -MPXXX */
d1463 1
d1466 4
a1469 5
/*
 * Change a page to readonly
 */
void
pmap_page_ro(struct pmap *pm, vaddr_t va)
d1471 2
d1474 1
a1474 9
	struct pte_desc *pted;
	int sr, idx;

	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted)) {
		return;
	}
	pted->pted_pte.pte_lo &= ~PTE_PP;
	pted->pted_pte.pte_lo |= PTE_RO;
d1478 10
a1487 28

	/* determine which pteg mapping is present in */
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

	/*
	 * We now have the pointer to where it will be, if it is
	 * currently mapped. If the mapping was thrown away in
	 * exchange for another page mapping, then this page is
	 * not currently in the HASH.
	 */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		ptp->pte_hi &= ~PTE_VALID;
		__asm__ volatile ("sync");
		tlbie(va);
		tlbsync();
		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(ptp->pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
		}
		ptp->pte_lo &= ~PTE_CHG;
		ptp->pte_lo &= ~PTE_PP;
		ptp->pte_lo |= PTE_RO;
		__asm__ volatile ("sync");
		ptp->pte_hi |= PTE_VALID;
	}
d1491 1
a1491 4
 * Lower the protection on the specified physical page.
 *
 * There are only two cases, either the protection is going to 0,
 * or it is going to read-only.
d1493 5
a1497 2
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
d1499 11
a1509 19
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s;
	struct pte_desc *pted;
	struct pted_pv_head *pvh;

	/* need to lock for this pv */
	s = splimp();
	pvh = pmap_find_pvh(pa);

	/* nothing to do if not a managed page */
	if (pvh == NULL) {
		splx(s);
		return;
	}

	if (prot == VM_PROT_NONE) {
		while (!LIST_EMPTY(pvh)) {
			pted = LIST_FIRST(pvh);
			pmap_remove_pg(pted->pted_pmap, pted->pted_va);
d1512 1
a1512 5
		return;
	}
	
	LIST_FOREACH(pted, pvh, pted_pv_list) {
		pmap_page_ro(pted->pted_pmap, pted->pted_va);
d1514 1
d1516 1
d1519 6
d1526 4
a1529 1
pmap_protect(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d1531 3
a1533 1
	int s;
d1537 12
a1548 2
			pmap_page_ro(pm, sva);
			sva += PAGE_SIZE;
d1556 5
a1560 5
/*
 * Restrict given range to physical memory
 */
void
pmap_real_memory(paddr_t *start, vm_size_t *size)
d1562 11
a1572 1
	struct mem_region *mp;
d1574 6
a1579 15
	for (mp = pmap_mem; mp->size; mp++) {
		if (((*start + *size) > mp->start)
			&& (*start < (mp->start + mp->size)))
		{
			if (*start < mp->start) {
				*size -= mp->start - *start;
				*start = mp->start;
			}
			if ((*start + *size) > (mp->start + mp->size))
				*size = mp->start + mp->size - *start;
			return;
		}
	}
	*size = 0;
}
d1581 8
a1588 9
/*
 * How much virtual space is available to the kernel?
 */
void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	*start = VM_MIN_KERNEL_ADDRESS;
	*end = VM_MAX_KERNEL_ADDRESS;
}
d1590 36
a1625 58
void
pmap_init()
{
	vsize_t sz;
	struct pted_pv_head *pvh;
	char *attr;
	int i, bank;

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 20, "pmap", NULL);
	pool_setlowat(&pmap_pmap_pool, 2);
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, 0, 150, "vp", NULL);
	pool_setlowat(&pmap_vp_pool, 10);
	pool_init(&pmap_pted_pool, sizeof(struct pte_desc), 0, 0, 150, "pted",
	    NULL);
	pool_setlowat(&pmap_pted_pool, 20);

	/* pmap_pvh and pmap_attr must be allocated 1-1 so that pmap_save_attr
	 * is callable from pte_spill_r (with vm disabled)
	 */
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (i = npgs; i > 0; i--)
		LIST_INIT(pvh++);
	attr = pmap_attrib;
	bzero(pmap_attrib, npgs);
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		sz = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pvh;
		vm_physmem[bank].pmseg.attrs = attr;
		pvh += sz;
		attr += sz;
	}
	pmap_initialized = 1;
}

/* 
 * There are two routines, pte_spill_r and pte_spill_v
 * the _r version only handles kernel faults which are not user
 * accesses. The _v version handles all user faults and kernel copyin/copyout
 * "user" accesses.
 */
int
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr)
{
	struct pmap *pm;
	struct pte_desc *pted;
	int retcode = 0;

	/* 
	 * This function only handles kernel faults, not supervisor copyins.
	 */
	if (!(msr & PSL_PR)) {
		/* PSL_PR is clear for supervisor, right? - XXXXX */
		/* lookup is done physical to prevent faults */
		if (VP_SR(va) == USER_SR) {
			return 0;
		} else {
			pm = pmap_kernel();
d1627 8
a1634 17
	} else {
		return 0;
	}

	pted = pmap_vp_lookup(pm, va);
	if (pted != NULL) {
		/* if the current mapping is RO and the access was a write
		 * we return 0
		 */
		if (!PTED_VALID(pted) ||
		    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
		{
			/* write fault and we have a readonly mapping */
			retcode = 0;
		} else {
			retcode = 1;
			pte_insert(pted);
d1637 1
d1639 1
a1639 1
	return retcode;
d1643 3
a1645 1
pte_spill_v(struct pmap *pm, u_int32_t va, u_int32_t dsisr)
d1647 5
a1651 1
	struct pte_desc *pted;
d1653 2
a1654 2
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
d1656 1
a1656 1
	}
d1659 1
a1659 2
	 * if the current mapping is RO and the access was a write
	 * we return 0
d1661 5
a1665 7
	if (!PTED_VALID(pted) ||
	    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
	{
		/* write fault and we have a readonly mapping */
		if (PTED_VALID(pted)) {
			pmap_hash_remove(pted);
		}
d1667 29
d1697 2
a1698 2
	pte_insert(pted);
	return 1;
a1700 1

d1702 4
a1705 4
 * should pte_insert code avoid wired mappings?
 * is the stack safe?
 * is the pted safe? (physical)
 * -ugh
a1706 1

d1708 3
a1710 1
pte_insert(struct pte_desc *pted)
d1712 16
a1727 18
	int off;
	int secondary;
	struct pte *ptp;
	int sr, idx;
	int i;

	/* HASH lock? */

	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

	/* determine if ptp is already mapped */
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp,pted);
d1730 4
a1733 1
	pted->pted_va &= ~(PTED_VA_HID_M|PTED_VA_PTEGIDX_M);
d1735 16
a1750 7
	/*
	 * instead of starting at the beginning of each pteg,
	 * the code should pick a random location with in the primary
	 * then search all of the entries, then if not yet found,
	 * do the same for the secondary.
	 * this would reduce the frontloading of the pteg.
	 */
d1752 15
a1766 4
	/* first just try fill of primary hash */
	ptp = pmap_ptable + (idx) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp[i].pte_hi & PTE_VALID) {
d1768 11
d1780 1
a1780 14
		/* not valid, just load */
/* printf("inserting in primary idx %x, i %x\n", idx, i); */
		pted->pted_va |= i;
		ptp[i].pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
		__asm__ volatile ("sync");
		ptp[i].pte_hi |= PTE_VALID;
		__asm volatile ("sync");
		return;
	}
	/* first just try fill of secondary hash */
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp[i].pte_hi & PTE_VALID) {
d1782 8
a1790 9
/* printf("inserting in secondary idx %x, i %x\n", idx, i); */
		pted->pted_va |= (i | PTED_VA_HID_M);
		ptp[i].pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
		__asm__ volatile ("sync");
		ptp[i].pte_hi |= PTE_VALID;
		__asm volatile ("sync");
		return;
	}
d1792 1
a1792 28
	/* need decent replacement algorithm */
	__asm__ volatile ("mftb %0" : "=r"(off));
	secondary = off & 8;
	pted->pted_va |= off & (PTED_VA_PTEGIDX_M|PTED_VA_HID_M);

	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));

#ifdef USE_WTABLE
	if (PTED_WIRED(pted)) {
		pmap_wtable[idx] &= ~(1 << PTED_PTEGIDX(pted)); 
	}
#endif /* USE_WTABLE */
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if (ptp->pte_hi & PTE_VALID) {
		vaddr_t va;
		ptp->pte_hi &= ~PTE_VALID;
		__asm volatile ("sync");

		va = ((ptp->pte_hi & PTE_API) << ADDR_API_SHIFT) |
		     ((((ptp->pte_hi >> PTE_VSID_SHIFT) & SR_VSID)
			^(idx ^ ((ptp->pte_hi & PTE_HID) ? 0x3ff : 0)))
			    & 0x3ff) << PAGE_SHIFT;
		tlbie(va);

		tlbsync();
		pmap_attr_save(ptp->pte_lo & PTE_RPGN,
		    ptp->pte_lo & (PTE_REF|PTE_CHG));
d1794 1
a1794 8
	if (secondary) {
		ptp->pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
	} else {
		ptp->pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
	}
	ptp->pte_lo = pted->pted_pte.pte_lo;
	__asm__ volatile ("sync");
	ptp->pte_hi |= PTE_VALID;
d1796 5
d1802 13
a1814 1
#ifdef DEBUG_PMAP
d1816 1
a1816 1
print_pteg(struct pmap *pm, vaddr_t va)
d1818 4
a1821 22
	int sr, idx;
	struct pte *ptp;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr,  va);

	ptp = pmap_ptable + idx  * 8;
	db_printf("va %x, sr %x, idx %x\n", va, sr, idx);

	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
d1824 1
a1824 4

/* debugger assist function */
int pmap_prtrans(u_int pid, vaddr_t va);

d1826 1
a1826 1
pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...))
d1828 3
a1830 19
	vaddr_t va;
	va = pted->pted_va & ~PAGE_MASK;
	print("\n pted %x", pted);
	if (PTED_VALID(pted)) {
		print(" va %x:", pted->pted_va & ~PAGE_MASK);
		print(" HID %d", PTED_HID(pted) ? 1: 0);
		print(" PTEGIDX %x", PTED_PTEGIDX(pted));
		print(" MANAGED %d", PTED_MANAGED(pted) ? 1: 0);
		print(" WIRED %d\n", PTED_WIRED(pted) ? 1: 0);
		print("ptehi %x ptelo %x ptp %x Aptp %x\n",
		    pted->pted_pte.pte_hi, pted->pted_pte.pte_lo,
		    pmap_ptable +
			8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
		    pmap_ptable +
			8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
			    ^ pmap_ptab_mask)
		    );
	}
}
d1832 14
a1845 27
int pmap_user_read(int size, vaddr_t va);
int
pmap_user_read(int size, vaddr_t va)
{
	unsigned char  read1;
	unsigned short read2;
	unsigned int   read4;
	int err;

	if (size == 1) {
		err = copyin((void *)va, &read1, 1);
		if (err == 0) {
			db_printf("byte read %x\n", read1);
		}
	} else if (size == 2) {
		err = copyin((void *)va, &read2, 2);
		if (err == 0) {
			db_printf("short read %x\n", read2);
		}
	} else if (size == 4) {
		err = copyin((void *)va, &read4, 4);
		if (err == 0) {
			db_printf("int read %x\n", read4);
		}
	} else {
		return 1;
	}
d1847 2
d1850 2
a1851 13
	return 0;
}

int pmap_dump_pmap(u_int pid);
int
pmap_dump_pmap(u_int pid)
{
	struct pmap *pm;
	struct proc *p;
	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);
d1853 9
a1861 3
		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
a1862 20
		pm = p->p_vmspace->vm_map.pmap;
	}
	printf("pmap %x:\n", pm);
	printf("segid %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x",
	    pm->pm_sr[0], pm->pm_sr[1], pm->pm_sr[2], pm->pm_sr[3],
	    pm->pm_sr[4], pm->pm_sr[5], pm->pm_sr[6], pm->pm_sr[7],
	    pm->pm_sr[8], pm->pm_sr[9], pm->pm_sr[10], pm->pm_sr[11],
	    pm->pm_sr[12], pm->pm_sr[13], pm->pm_sr[14], pm->pm_sr[15]);

	return 0;
}

int
pmap_prtrans(u_int pid, vaddr_t va)
{
	struct proc *p;
	struct pmap *pm;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;
d1864 6
a1869 10
	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);

		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
		}
		pm = p->p_vmspace->vm_map.pmap;
d1871 2
a1872 19

	db_printf(" pid %d, va 0x%x pmap %x\n", pid, va, pm);
	vp1 = pm->pm_vp[VP_SR(va)];
	db_printf("sr %x id %x vp1 %x", VP_SR(va), pm->pm_sr[VP_SR(va)],
	    vp1);

	if (vp1) {
		vp2 = vp1->vp[VP_IDX1(va)];
		db_printf(" vp2 %x", vp2);

		if (vp2) {
			pted = vp2->vp[VP_IDX2(va)];
			pmap_print_pted(pted, db_printf);

		}
	}
	print_pteg(pm, va);

	return 0;
d1874 3
a1876 4
int pmap_show_mappings(paddr_t pa);

int
pmap_show_mappings(paddr_t pa) 
d1878 1
a1878 11
	struct pted_pv_head *pvh;
	struct pte_desc *pted;
	pvh = pmap_find_pvh(pa);
	if (pvh == NULL) {
		db_printf("pa %x: unmanaged\n");
	} else {
		LIST_FOREACH(pted, pvh, pted_pv_list) {
			pmap_print_pted(pted, db_printf);
		}
	}
	return 0;
a1879 1
#endif
@


1.52.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52.2.2 2002/06/11 03:37:28 art Exp $ */
d64 1
a64 1
struct pte  *pmap_ptable;
d66 4
d85 1
a85 1
	pmap_t pted_pmap;
d89 1
a89 1
void print_pteg(pmap_t pm, vaddr_t va);
d96 2
a97 1
void pmap_page_ro(pmap_t pm, vaddr_t va);
d123 1
a123 1
void pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa,
d126 1
a126 1
void pmap_syncicache_user_virt(pmap_t pm, vaddr_t va);
d130 1
a130 1
void pmap_remove_pg(pmap_t pm, vaddr_t va);
d137 1
a137 1
void *pmap_steal_avail(size_t size, int align);
d140 1
a140 2
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type,
    int exec_fault);
d142 1
a142 1
u_int32_t pmap_setusr(pmap_t pm, vaddr_t va);
a144 3
/* pte invalidation */
void pte_zap(struct pte *ptp, struct pte_desc *pted);

d200 2
a201 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d225 2
a226 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d256 2
a257 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d283 2
d288 2
a289 1
 * HELPER FUNCTIONS 
a300 1

a317 1

d332 1
a333 1
	tlbsync();
a340 1

d349 4
a352 6
#define PTED_VA_PTEGIDX_M	0x07
#define PTED_VA_HID_M		0x08
#define PTED_VA_MANAGED_M	0x10
#define PTED_VA_WIRED_M		0x20
#define PTED_VA_EXEC_M		0x40

a357 1

a362 1

a367 1

a372 1

d407 1
d481 4
a484 3
	if (pvh != NULL)
		cache = PMAP_CACHE_WB; /* managed memory is cachable */
	else
d486 1
a498 17
        if (prot & VM_PROT_EXECUTE) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC) {
			pm->pm_sr[sn] &= ~SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d502 1
a502 1
	if (need_sync && (prot & VM_PROT_EXECUTE))
d504 1
d507 1
a507 1
	return KERN_SUCCESS;
d514 1
a514 1
pmap_remove(pmap_t pm, vaddr_t va, vaddr_t endva)
d519 2
a520 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d534 1
a534 1
		if (i_sr == s_sr)
d536 1
a536 1
		else
d538 2
a539 2

		if (i_sr == e_sr)
d541 1
a541 1
		else
d543 1
a543 1

d549 1
a549 1
			if ((i_sr == s_sr) && (i_vp1 == s_vp1))
d551 1
a551 1
			else
d553 2
a554 2

			if ((i_sr == e_sr) && (i_vp1 == e_vp1))
d556 1
a556 1
			else
d558 1
a558 1

d574 1
a574 1
pmap_remove_pg(pmap_t pm, vaddr_t va)
a601 18
	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0) {
			pm->pm_sr[sn] |= SR_NOEXEC;
			
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d607 1
a607 1
	if (pm != pmap_kernel())
d609 1
d628 1
a628 1
	pmap_t pm;
d637 4
a640 2
	if (pted && PTED_VALID(pted))
		pmap_kremove_pg(va); /* pted is reused */
d656 4
a659 3
		if (pvh != NULL)
			cache = PMAP_CACHE_WB; /* managed memory is cachable */
		else
d661 1
a674 17
        if (prot & VM_PROT_EXECUTE) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC) {
			pm->pm_sr[sn] &= ~SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d699 1
a699 1
	pmap_t pm;
d704 2
a705 1
	if (pted == NULL)
d707 5
a711 4

	if (!PTED_VALID(pted))
		return; /* not mapped */

a722 18
	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0) {
			pm->pm_sr[sn] |= SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d738 1
a738 1
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE)
d740 1
d743 1
d745 1
a745 1
pte_zap(struct pte *ptp, struct pte_desc *pted)
d753 1
a753 1
		if (PTED_MANAGED(pted))
d756 1
a757 1

d767 2
a768 2
	pmap_t pm = pted->pted_pmap;
	struct pte *ptp;
d787 3
d793 1
d798 1
a798 1
pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
d811 1
a811 1
	if ((cache == PMAP_CACHE_WB))
d813 1
a813 1
	else if ((cache == PMAP_CACHE_WT))
d815 1
a815 1
	else
d817 1
d819 1
a819 1
	if (prot & VM_PROT_WRITE)
d821 1
a821 1
	else
d823 1
a825 4

	if (prot & VM_PROT_EXECUTE)
		pted->pted_va  |= PTED_VA_EXEC_M;

d867 2
a868 2
		pmap_t pm = pted->pted_pmap;
		struct pte *ptp;
d901 1
a901 1
	if (clear)
d903 1
a903 1
	else
d905 1
a905 1

d917 1
a917 1
pmap_collect(pmap_t pm)
d932 1
a932 1
pmap_zero_page(struct vm_page *pg)
a933 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a952 1

d957 1
a957 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a958 2
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
a971 1

d973 1
a973 1
pmap_pinit(pmap_t pm)
d1006 1
a1006 1
				pm->pm_sr[k] = (seg + k) | SR_NOEXEC;
d1017 1
a1017 1
pmap_t 
d1020 1
a1020 1
	pmap_t pmap;
d1034 1
a1034 1
pmap_reference(pmap_t pm)
d1046 1
a1046 1
pmap_destroy(pmap_t pm)
d1054 1
a1054 1
	if (refs > 0)
d1056 1
a1056 1

d1071 1
a1071 1
pmap_release(pmap_t pm)
d1077 1
a1077 1
	i = (pm->pm_sr[0] & SR_VSID) >> 4;
d1088 1
a1088 1
pmap_vp_destroy(pmap_t pm)
d1091 3
d1095 2
a1096 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d1100 1
a1100 1
		if (vp1 == NULL)
d1102 1
a1102 1

d1108 19
d1131 1
a1131 1
		pm->pm_vp[i] = NULL;
d1147 2
a1148 1
	for (mp = pmap_mem; mp->size !=0; mp++)
d1150 1
a1165 1

a1167 1

d1174 1
a1174 1
	for (mp = pmap_avail; mp->size !=0; mp++)
d1176 1
d1179 1
d1224 4
a1227 3
			if (end <= mp->start)
				break; /* region not present -??? */

d1232 3
a1234 2
				    i < pmap_cnt_avail;
				    i++) {
d1251 3
a1253 2
				    i > (mp - pmap_avail);
				    i--) {
d1271 1
a1271 1
			/* lengthen */
d1273 2
a1274 1
			    i--) {
d1311 1
a1311 1
	panic ("unable to allocate region with size %x align %x",
d1327 2
a1328 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d1350 1
a1350 3
#ifndef  HTABENTS
#define HTABENTS 1024
#endif
d1352 2
d1357 1
d1365 4
d1404 1
a1404 1
		pmap_kernel()->pm_sr[i] = (KERNEL_SEG0 + i) | SR_NOEXEC;
d1406 1
a1406 1
		    :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
d1409 1
a1409 1
	    :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));
d1456 1
a1456 1
pmap_extract(pmap_t pm, vaddr_t va, paddr_t *pa)
d1475 1
a1475 1
pmap_setusr(pmap_t pm, vaddr_t va)
d1484 1
a1484 1
	    : "=r" (oldsr): "n"(USER_SR));
d1486 1
a1486 1
	    :: "n"(USER_SR), "r"(sr));
d1494 1
a1494 1
	    :: "n"(USER_SR), "r"(sr));
d1587 1
a1587 1
			if (done != NULL)
d1589 1
a1589 1

d1598 1
a1598 1
				if (done != NULL)
d1600 1
a1600 1

d1614 1
a1614 1
	if (done != NULL)
d1616 1
a1616 1

d1643 1
a1643 1
			if (done != NULL)
d1645 1
a1645 1

d1654 1
a1654 1
				if (done != NULL)
d1656 1
a1656 1

d1670 1
a1670 1
	if (done != NULL)
d1672 1
a1672 1

d1682 1
a1682 1
pmap_syncicache_user_virt(pmap_t pm, vaddr_t va)
d1721 1
a1721 1
pmap_page_ro(pmap_t pm, vaddr_t va)
d1723 1
a1723 1
	struct pte *ptp;
d1728 1
a1728 1
	if (pted == NULL || !PTED_VALID(pted))
d1730 1
a1730 1

d1738 2
a1739 1
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d1806 1
a1806 1
pmap_protect(pmap_t pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d1897 1
a1897 1
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr, int exec_fault)
d1899 1
a1899 1
	pmap_t pm;
d1901 1
d1907 1
d1919 13
a1931 18
	if (pted == NULL) {
		return 0;
	}

	/* if the current mapping is RO and the access was a write
	 * we return 0
	 */
	if (!PTED_VALID(pted)) {
		return 0;
	} 
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
		/* write fault and we have a readonly mapping */
		return 0;
	}
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executeable page */
		return 0;
a1932 1
	pte_insert(pted);
d1934 1
a1934 1
	return 1;
d1938 1
a1938 1
pte_spill_v(pmap_t pm, u_int32_t va, u_int32_t dsisr, int exec_fault)
d1951 3
a1953 4
	if (!PTED_VALID(pted)) {
		return 0;
	}
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
d1955 3
a1957 5
		return 0;
	}
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executeable page */
d1987 2
a1988 1
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d2008 1
a2008 1
		if (ptp[i].pte_hi & PTE_VALID)
d2010 1
a2010 1

d2024 1
a2024 1
		if (ptp[i].pte_hi & PTE_VALID)
d2026 2
a2027 1

d2044 5
d2066 1
a2066 2

	if (secondary)
d2068 1
a2068 1
	else
d2070 1
a2070 1

d2078 1
a2078 1
print_pteg(pmap_t pm, vaddr_t va)
d2167 1
a2167 1
	pmap_t pm;
d2194 3
a2196 3
	pmap_t pm;
	struct pmapvp *vp1;
	struct pmapvp *vp2;
@


1.52.2.4
log
@allow UBC branch to build on powerpc(macppc).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52.2.3 2002/10/29 00:28:08 art Exp $ */
d525 1
a525 1
	return 0;
@


1.52.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d460 1
a460 1
	int need_sync = 0;
a461 2
	u_int8_t *pattr = NULL;
	int first_map = 0;
d492 1
a492 21
	if (pvh != NULL) {
		pattr = pmap_find_attr(pa); /* pattr only for managed mem */
		first_map = pmap_enter_pv(pted, pvh); /* only managed mem */
	}

	/* 
	 * We want to flush for executable pages which are not managed???
	 * Always flush for the first mapping if it is executable.
	 * If previous mappings exist, but this is the first EXE, sync.
	 */

	if (prot & VM_PROT_EXECUTE) {
		need_sync = 1;
		 if (pvh != NULL) {
			if (!first_map)
				need_sync =
				    (*pattr & (PTE_EXE >> ATTRSHIFT)) == 0;
			else if (pattr != NULL)
				*pattr = 0;
		}
	}
a515 9
		if (pattr != NULL)
			*pattr |= (PTE_EXE >> ATTRSHIFT);
	} else {
		/*
		 * Should we be paranoid about writeable non-exec 
		 * mappings ? if so, clear the exec tag
		 */
		if ((prot & VM_PROT_WRITE) && (pattr != NULL))
			*pattr &= ~(PTE_EXE >> ATTRSHIFT);
d521 1
a521 1
	if (need_sync)
a943 3
		 *
		 * if we are not clearing bits, and have found all of the
		 * bits we want, we can stop
d947 1
a947 1
			bits |=	ptp->pte_lo & bit;
d956 1
a956 2
			} else if (bits == bit)
				break;
d960 1
d1547 1
a1547 1
		if (setfault(&env)) {
d1580 1
a1580 1
		if (setfault(&env)) {
d1618 1
a1618 1
		if (setfault(&env)) {
d1674 1
a1674 1
		if (setfault(&env)) {
@


1.51
log
@Kernel mappings (pmap_kenter_pa) should not be entered into the pv list.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2001/11/28 15:34:16 art Exp $	*/
d1444 1
a1444 1
	if (!(ptp = pte_find(pm, va))) {
@


1.50
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2001/11/28 14:13:06 art Exp $	*/
a1284 2
#define KERN_MAP_PV TRUE

d1293 1
a1293 1
		KERN_MAP_PV);
d1302 1
a1302 1
		PMAP_CACHE_DEFAULT, KERN_MAP_PV);
d1313 1
a1313 1
		pmap_remove_pvl(pmap_kernel(), va, va + PAGE_SIZE, KERN_MAP_PV);
@


1.49
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2001/11/28 13:47:39 art Exp $	*/
a938 9
{
}

/*
 * Require that all active physical maps contain no
 * incorrect entries NOW.
 */
void
pmap_update()
@


1.48
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2001/11/06 19:53:16 miod Exp $	*/
a1313 15
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter_c_pv(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED,
				PMAP_CACHE_DEFAULT, KERN_MAP_PV);
	}
@


1.47
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/11/06 02:53:57 art Exp $	*/
d1278 1
a1278 1
		return (KERN_SUCCESS);
d1291 1
a1291 1
	return (KERN_SUCCESS);
@


1.46
log
@No need for those prototypes here.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2001/09/19 20:50:57 mickey Exp $	*/
a40 2

#include <vm/vm.h>
@


1.45
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2001/09/18 13:59:23 drahn Exp $	*/
a287 3
/* XXX */
void pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs);
void pmap_kremove(vaddr_t va, vsize_t len);
@


1.44
log
@Changing the way the pmap code works again.
Changes to the pmap_enter code so that the pmap_kenter/pmap_kremove
has a method to create mappings without adding them to the _pv lists
(part of the point of pmap_k* functions). Also adds an interface
so that device mappings can be created with cacheable attributes.
So that devices such as display memory can be mapped writethru
greatly increasing their speed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2001/09/15 13:21:06 drahn Exp $	*/
a42 1
#include <vm/vm_kern.h>
@


1.43
log
@Rewrite of powerpc pmap_page_protect(), the old version had a couple of
possible bugs in it which could cause the code to spin indefinately
attempting to remove all mappings for a page.

This is now able to survive a paging death program and additional other
testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2001/09/03 21:45:08 drahn Exp $	*/
d1189 4
d1203 12
d1244 7
a1250 5
	pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_I | PTE_G;
	for (mp = mem; mp->size; mp++) {
		if (pa >= mp->start && pa < mp->start + mp->size) {
			pte.pte_lo &= ~(PTE_I | PTE_G);
			break;
d1252 8
d1269 7
a1275 5
	if (pmap_enter_pv(pm, idx, va, pa)) {
		/* 
		 * Flush the real memory from the cache.
		 */
		syncicache((void *)pa, NBPG);
d1300 12
d1318 2
a1319 1
	pmap_enter(pmap_kernel(), va, pa, prot, PMAP_WIRED);
d1331 3
a1333 2
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED);
d1337 2
d1345 1
a1345 1
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
d1368 9
d1397 4
a1400 1
				pmap_remove_pv(pm, idx, va, ptp->pte_lo);
d1411 4
a1414 1
				pmap_remove_pv(pm, idx, va, ptp->pte_lo);
d1425 4
a1428 1
				pmap_remove_pv(pm, idx, va, po->po_pte.pte_lo);
@


1.42
log
@Zero pages before handing them over to the VM layer.
This seems to improve the reliablity of the system.
Thanks to those who tested this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2001/08/25 05:06:29 drahn Exp $	*/
d69 3
d178 1
a178 1
		bzero (mem1, NBPG);
a650 1
		bzero((void*)mp->start, mp->size);
d727 1
a727 1
	pool_init(&pmap_vp_pool, NBPG, 0, 0, 0, "ppvl",
d1305 10
d1343 1
a1343 4
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(va);
				tlbsync();
d1354 1
a1354 4
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(va);
				tlbsync();
d1607 7
a1613 1
	struct pv_entry *pv;
d1621 1
d1630 11
a1640 2
		if ((va >=uvm.pager_sva) && (va < uvm.pager_eva)) {
				continue;
d1642 42
a1683 1
		pmap_remove(pm, va, va + NBPG);
@


1.41
log
@The VP cache code, while giving the powerpc port a signficant speed
increase is contributing to the instability of the port.
This ifdef's the code, disabling it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/08/18 06:18:49 drahn Exp $	*/
d648 1
@


1.40
log
@remove pv_table, it is not referenced any longer.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2001/08/10 15:05:48 drahn Exp $	*/
d93 1
d116 1
a116 1
	return (va >> VP_IDX2_POS) & VP_SR_MASK;
d175 2
d237 1
d671 1
d673 1
d723 1
d726 1
d917 1
d919 1
d1209 1
d1211 1
d1319 1
d1324 1
@


1.39
log
@convert V->P table memory allocations to pool.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2001/08/06 19:03:33 drahn Exp $	*/
a249 2
struct pv_entry *pv_table;

d714 1
a714 1
	pv = pv_table = (struct pv_entry *)addr;
d725 1
a725 1
	pv = pv_table;
@


1.38
log
@Correct misinitialization of a variable. This worked before?
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2001/07/25 13:25:33 art Exp $	*/
d93 2
a168 3
		} else {
			if (!(mem1 = (pmapv_t *)uvm_km_zalloc(kernel_map, NBPG)))
				panic("pmap_vp_enter: uvm_km_zalloc() failed");
d170 4
d195 1
d216 3
a218 1
		uvm_km_free(kernel_map, (vaddr_t)vp1, NBPG);
d719 2
@


1.37
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2001/07/18 10:47:05 art Exp $	*/
a1528 1
	pv = pv_table;
@


1.36
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2001/07/09 02:14:05 mickey Exp $	*/
d1173 2
a1174 2
void
pmap_enter(pm, va, pa, prot, wired, access_type)
d1178 1
a1178 2
	int wired;
	vm_prot_t access_type;
d1235 1
a1235 1
		return;
d1247 2
d1257 1
a1257 1
	pmap_enter(pmap_kernel(), va, pa, prot, 1, 0);
d1270 1
a1270 1
				VM_PROT_READ|VM_PROT_WRITE, 1, 0);
@


1.35
log
@more spacees, includes, protos
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2001/06/27 06:19:54 art Exp $	*/
a827 1
#if defined(PMAP_NEW)
a829 5
#else 
struct pmap *
pmap_create(size)
	vsize_t size;
#endif
d1439 1
a1439 1
void
d1441 1
a1441 1
	vm_offset_t pa;
d1450 3
d1456 1
a1456 1
		return;
d1466 1
a1466 1
		return;
d1501 2
a1570 1
#if defined(PMAP_NEW)
a1574 6
#else
void
pmap_page_protect(pa, prot)
	vm_offset_t pa;
	vm_prot_t prot;
#endif
a1575 1
#if defined(PMAP_NEW)
a1576 1
#endif
@


1.34
log
@MNN is no longer an option.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2001/06/27 04:37:21 art Exp $	*/
a278 1
void pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot);
@


1.33
log
@kill old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2001/06/24 23:29:36 drahn Exp $	*/
a639 1
#ifdef MACHINE_NEW_NONCONTIG	
a644 1
#endif
a703 1
#ifdef MACHINE_NEW_NONCONTIG
a705 1
#endif
a719 1
#ifdef MACHINE_NEW_NONCONTIG
a728 1
#endif
a751 1
#ifdef MACHINE_NEW_NONCONTIG
a773 1
#endif
@


1.32
log
@-Warn cleanups for powerpc, still not done.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2001/06/10 15:20:16 drahn Exp $	*/
a44 1
#ifdef UVM
a45 1
#endif
a69 4
#ifndef UVM
	extern vm_offset_t pager_sva, pager_eva;
#endif

a167 1
#ifdef UVM
a169 4
#else
			if (!(mem1 = (pmapv_t *)kmem_alloc(kernel_map, NBPG)))
				panic("pmap_vp_enter: kmem_alloc() failed");
#endif
a211 1
#ifdef UVM
a212 3
#else
		kmem_free(kernel_map, (vm_offset_t)vp1, NBPG);
#endif
a641 1
#ifdef UVM
a644 4
#else
		vm_page_physload(atop(mp->start), atop(mp->start + mp->size),
			atop(mp->start), atop(mp->start + mp->size));
#endif
a712 1
#ifdef UVM
a713 3
#else
	addr = kmem_alloc(kernel_map, sz);
#endif
a1614 1
#ifdef UVM
a1617 5
#else
		if (va >= pager_sva && va < pager_eva) {
				continue;
		}
#endif
@


1.31
log
@Post pmap_extract() changes cleanup.
bus_addr_t vs vaddr_t/paddr_t
Return correct value for poalloc();
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2001/06/08 08:09:22 art Exp $	*/
d99 4
d106 1
a106 2
VP_SR(va)
	paddr_t va;
d111 1
a111 2
VP_IDX1(va)
	paddr_t va;
d117 1
a117 2
VP_IDX2(va)
	paddr_t va;
d123 1
a123 3
pmap_vp_valid(pm, va)
	pmap_t pm;
	vaddr_t va;
d132 1
d134 1
a134 3
pmap_vp_remove(pm, va)
	pmap_t pm;
	vaddr_t va;
d156 1
a156 4
pmap_vp_enter(pm, va, pa)
	pmap_t pm;
	vaddr_t va;
	paddr_t pa;
d203 4
a206 1
	int sr, idx1;
d233 2
d262 2
a263 2
static struct pv_entry *pmap_alloc_pv __P((void));
static void pmap_free_pv __P((struct pv_entry *));
d266 2
a267 2
static struct pte_ovfl *poalloc __P((void));
static void pofree __P((struct pte_ovfl *, int));
d273 26
d304 1
a304 2
tlbie(ea)
	caddr_t ea;
d318 1
a318 1
	caddr_t i;
d321 1
a321 1
	for (i = 0; i < (caddr_t)0x00040000; i += 0x00001000)
d364 1
a364 1
static int
d783 1
a783 1
static __inline struct pv_entry *
d794 1
a794 1
static __inline char *
d1020 1
a1020 1
static struct pv_entry *
d1049 1
a1049 1
static void
d1066 1
a1066 1
static struct pte_ovfl *
d1084 1
a1084 1
static void
d1118 1
a1118 1
	if (first = pv->pv_idx == -1) {
d1142 1
a1142 1
static void
d1186 1
a1186 1
		for (; npv = pv->pv_next; pv = npv) {
d1221 1
a1221 1
	int idx, i, s;
a1335 1
	struct pv_entry *pv;
d1388 1
a1388 1
static pte_t *
d1459 1
a1459 1
			if (ptp = pte_find(pm, sva)) {
d1621 1
a1621 3
	pte_t *ptp;
	struct pte_ovfl *po, *npo;
	int i, s, pind, idx;
@


1.30
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/05/09 15:31:26 art Exp $	*/
d1056 1
@


1.29
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/05/05 21:26:40 art Exp $	*/
d40 1
d261 1
a261 14
struct pv_page;
struct pv_page_info {
	LIST_ENTRY(pv_page) pgi_list;
	struct pv_entry *pgi_freelist;
	int pgi_nfree;
};
#define	NPVPPG	((NBPG - sizeof(struct pv_page_info)) / sizeof(struct pv_entry))
struct pv_page {
	struct pv_page_info pvp_pgi;
	struct pv_entry pvp_pv[NPVPPG];
};
LIST_HEAD(pv_page_list, pv_page) pv_page_freelist;
int pv_nfree;
int pv_pcnt;
d265 1
a265 15
struct po_page;
struct po_page_info {
	LIST_ENTRY(po_page) pgi_list;
	vm_page_t pgi_page;
	LIST_HEAD(po_freelist, pte_ovfl) pgi_freelist;
	int pgi_nfree;
};
#define	NPOPPG	((NBPG - sizeof(struct po_page_info)) / sizeof(struct pte_ovfl))
struct po_page {
	struct po_page_info pop_pgi;
	struct pte_ovfl pop_po[NPOPPG];
};
LIST_HEAD(po_page_list, po_page) po_page_freelist;
int po_nfree;
int po_pcnt;
a605 1
	LIST_INIT(&pv_page_freelist);
d717 4
a720 1
	LIST_INIT(&pv_page_freelist);
a997 1
	struct pv_page *pvp;
d999 22
a1020 26
	int i;
	
	if (pv_nfree == 0) {
#ifdef UVM
		if (!(pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG)))
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
#else
		if (!(pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG)))
			panic("pmap_alloc_pv: kmem_alloc() failed");
#endif
		pv_pcnt++;
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; --i >= 0; pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		LIST_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = pvp->pvp_pv;
	} else {
		pv_nfree--;
		pvp = pv_page_freelist.lh_first;
		if (--pvp->pvp_pgi.pgi_nfree <= 0)
			LIST_REMOVE(pvp, pvp_pgi.pgi_list);
		pv = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
d1028 6
a1033 22
	struct pv_page *pvp;
	
	pvp = (struct pv_page *)trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		LIST_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		pv_nfree++;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		pv_pcnt--;
		LIST_REMOVE(pvp, pvp_pgi.pgi_list);
#ifdef UVM
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
#endif
		break;
	}
d1039 1
a1043 1
	struct po_page *pop;
d1045 1
a1045 2
	vm_page_t mem;
	int i;
d1047 1
a1049 10
	
	if (po_nfree == 0) {
		/*
		 * Since we cannot use maps for potable allocation,
		 * we have to steal some memory from the VM system.			XXX
		 */
#ifdef UVM
		mem = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
#else
		mem = vm_page_alloc(NULL, NULL);
d1051 5
a1055 18
		po_pcnt++;
		pop = (struct po_page *)VM_PAGE_TO_PHYS(mem);
		pop->pop_pgi.pgi_page = mem;
		LIST_INIT(&pop->pop_pgi.pgi_freelist);
		for (i = NPOPPG - 1, po = pop->pop_po + 1; --i >= 0; po++)
			LIST_INSERT_HEAD(&pop->pop_pgi.pgi_freelist, po, po_list);
		po_nfree += pop->pop_pgi.pgi_nfree = NPOPPG - 1;
		LIST_INSERT_HEAD(&po_page_freelist, pop, pop_pgi.pgi_list);
		po = pop->pop_po;
	} else {
		po_nfree--;
		pop = po_page_freelist.lh_first;
		if (--pop->pop_pgi.pgi_nfree <= 0)
			LIST_REMOVE(pop, pop_pgi.pgi_list);
		po = pop->pop_pgi.pgi_freelist.lh_first;
		LIST_REMOVE(po, po_list);
	}
	return po;
d1063 4
a1066 23
	struct po_page *pop;
	
	pop = (struct po_page *)trunc_page((vaddr_t)po);
	switch (++pop->pop_pgi.pgi_nfree) {
	case NPOPPG:
		if (!freepage)
			break;
		po_nfree -= NPOPPG - 1;
		po_pcnt--;
		LIST_REMOVE(pop, pop_pgi.pgi_list);
#ifdef UVM
		uvm_pagefree(pop->pop_pgi.pgi_page);
#else
		vm_page_free(pop->pop_pgi.pgi_page);
#endif
		return;
	case 1:
		LIST_INSERT_HEAD(&po_page_freelist, pop, pop_pgi.pgi_list);
	default:
		break;
	}
	LIST_INSERT_HEAD(&pop->pop_pgi.pgi_freelist, po, po_list);
	po_nfree++;
d1390 2
a1391 2
vm_offset_t
pmap_extract(pm, va)
d1393 2
a1394 1
	vm_offset_t va;
a1396 1
	vm_offset_t o;
d1398 1
d1402 1
a1402 1
		o = 0;
d1405 2
a1406 1
			o = va;
d1409 1
a1409 1
		return o;
d1411 1
a1411 1
	o = (ptp->pte_lo & PTE_RPGN) | (va & ADDR_POFF);
d1413 1
a1413 1
	return o;
@


1.28
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/04/18 06:26:12 drahn Exp $	*/
a986 13
{
}

/*
 * Make the specified pages pageable or not as requested.
 *
 * This routine is merely advisory.
 */
void
pmap_pageable(pm, start, end, pageable)
	struct pmap *pm;
	vm_offset_t start, end;
	int pageable;
@


1.27
log
@Remove last change to powerpc pmap. This has a slight chance of being the
cause of a frequent, but not easy to reproduce crash.
The reason for making this change is to support functionality that will
not be in 2.9.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/04/18 06:25:29 drahn Exp $	*/
d1072 1
a1072 1
	pvp = (struct pv_page *)trunc_page(pv);
d1146 1
a1146 1
	pop = (struct po_page *)trunc_page(po);
@


1.26
log
@If in pmap_kernel() allow vtop for all 1-1 mapped pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 2001/03/03 05:14:56 drahn Exp $	*/
d1502 2
a1503 2
		if (pm == pmap_kernel() && va < 0xa0000000){
			/* if in kernel, va==pa for 0 - 0xa0000000 */
@


1.25
log
@Quiet down pmap code, this is mostly shaken out now, remove some of
the debug code.
@
text
@d1502 2
a1503 2
		if (pm == pmap_kernel() && va < 0x80000000){
			/* if in kernel, va==pa for 0 - 0x80000000 */
@


1.24
log
@Improve the page mapped check algorithm in the powerpc pmap module,
before it was looking through two arrays of 8 and a linked list of
undetermined size, before deciding that a mapping was not valid.
Now it allocates a data structure and caches that data.

This improves both pmap_enter and pmap_remove because both check
to see if a mapping is valid before taking the appropriate actions.

Also in pmap_remove, if the va mapping is found, stop searching for
it in the rest of this array, the alternate array and the linked list.
only one valid mapping of each va is allowed.

This change improved lat_mmap (from lmbench) from 1300 to 720
and fork+exit from 7320 to 2724 microseconds.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2001/02/20 04:29:08 drahn Exp $	*/
d75 1
a75 1
#if 1
a589 1
dump_avail();
a606 1
dump_avail();
@


1.23
log
@Adhere to VM/UVM pager requirements, do not unmap pager mappings.
recognize pager_sva/pager_eva. Does not seem to change anything under UVM
but is recommended, may have been the cause for the "pmap" bug under VM.
Test compiled for VM, but not run.
Ok'd by art.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2001/02/16 05:18:06 drahn Exp $	*/
d98 146
d690 1
d943 1
d1308 2
d1408 1
d1415 6
a1420 1
	while (va < endva) {
d1431 2
d1434 3
d1445 2
d1448 3
d1458 1
a1460 1
		va += NBPG;
a1698 1
	int found;
@


1.22
log
@Allow siop driver to work on powerpc.
pmap_extract should work for all accessable memory.
Since powerpc maps the kernel va=pa without using the pte table,
these addresses need to be handled seperately.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2001/01/24 21:23:05 drahn Exp $	*/
d71 4
d1551 9
@


1.21
log
@Attempt to update powerpc pmap module to MACHINE_NEW_NONCONTIG
configuration. modernize for UVM. Does not yet work with UVM,
but does seem more stable than older version with old VM.
This may be in part due to a kludge that only uses the largest
memory region instead of all of the memory regions. a bug in
the MD MNN code is suspected.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2000/10/24 02:05:36 drahn Exp $	*/
d1332 6
d1339 1
a1339 1
		return 0;
@


1.20
log
@Verify that memory regions are always page aligned and multiple of page size.
The kernel does not want to deal with memory that is not page aligned.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2000/07/28 13:02:12 rahnds Exp $	*/
d71 1
a71 1
#if 0
d82 1
a82 1
				cnt, mp->size, mp->start);
d88 1
a88 1
				cnt, mp->size, mp->start);
a322 13
#if 0
	/* work around repeated entries bug */
	for (mp = mem; mp->size; mp++) {
		if (mp[1].start == mp[0].start) {
			int i;
			i = 0;
			do {
				mp[0+i].size = mp[1+i].size;
				mp[0+i].start = mp[1+i].start;
			} while ( mp[0+(i++)].size != 0);
		}
	}
#endif
a422 1
	#if 0
a425 1
	#endif
a431 1
		s = mp->start % HTABSIZE;
d440 1
d458 1
d485 23
d509 2
a511 1
	for (mp = avail; mp->size; mp++) {
d515 4
d521 1
d579 1
a579 1
#ifdef UVM
d598 1
a598 1
#ifdef UVM
d632 24
d694 3
d711 4
d717 2
a718 1
	vm_size_t size;
d735 2
a736 2
	int i, j;
	int s;
d747 3
a749 3
			pm->pm_sr[0] = (i * sizeof usedsr[0] * 8 + j) * 16;
			for (i = 1; i < 16; i++)
				pm->pm_sr[i] = pm->pm_sr[i - 1] + 1;
d790 2
a791 1
	
d797 1
d799 1
d860 1
d862 8
d1023 1
a1023 1
pmap_enter_pv(pm, pteidx, va, pind)
d1027 1
a1027 1
	u_int pind;
d1029 1
a1029 1
	struct pv_entry *pv, *npv;
d1031 1
d1036 4
a1041 1
	pv = &pv_table[pind];
d1067 1
a1067 1
pmap_remove_pv(pm, pteidx, va, pind, pte)
d1071 1
a1071 2
	int pind;
	struct pte *pte;
d1075 11
a1085 2
	if (pind < 0)
		return;
d1090 1
a1090 1
	pmap_attrib[pind] |= (pte->pte_lo & (PTE_REF | PTE_CHG)) >> ATTRSHFT;
d1094 1
a1094 4
	 */
	pv = &pv_table[pind];
	
	/*
d1100 1
a1100 1
	if (pteidx == pv->pv_idx && va == pv->pv_va && pm == pv->pv_pmap) {
d1106 1
d1111 1
a1111 2
			if (pteidx == npv->pv_idx && va == npv->pv_va &&
				pm == npv->pv_pmap)
d1153 1
a1153 1
	pmap_remove(pm, va, va + NBPG);
d1160 2
a1161 1
	idx = pteidx(sr = ptesr(pm->pm_sr, va), va);
d1184 6
a1189 7
	if (pmap_initialized && (i = pmap_page_index(pa)) != -1)
		if (pmap_enter_pv(pm, idx, va, i)) {
			/* 
			 * Flush the real memory from the cache.
			 */
			syncicache((void *)pa, NBPG);
		}
d1211 34
d1257 1
d1261 2
a1262 1
		idx = pteidx(sr = ptesr(pm->pm_sr, va), va);
a1264 2
				pmap_remove_pv(pm, idx, va,
					pmap_page_index(ptp->pte_lo), ptp);
d1269 1
a1273 2
				pmap_remove_pv(pm, idx, va,
					pmap_page_index(ptp->pte_lo), ptp);
d1278 1
d1284 1
a1284 3
				pmap_remove_pv(pm, idx, va,
					pmap_page_index(po->po_pte.pte_lo),
					       &po->po_pte);
d1305 2
a1306 1
	idx = pteidx(sr = ptesr(pm->pm_sr, va), va);
d1387 1
d1389 2
a1390 2
	i = pmap_page_index(pa);
	if (i < 0)
d1392 1
d1397 2
a1398 2
	pmap_attrib[i] &= ~mask >> ATTRSHFT;
	pmap_attrib[i] |= val >> ATTRSHFT;
a1399 1
	pv = pv_table + i;
d1447 1
d1449 2
a1450 2
	i = pmap_page_index(pa);
	if (i < 0)
d1452 1
d1457 1
a1457 1
	bits |= (pmap_attrib[i] << ATTRSHFT) & bit;
d1461 1
a1461 1
	pv = pv_table + i;
d1504 6
d1514 1
d1516 3
d1533 2
a1534 2
	pind = pmap_page_index(pa);
	if (pind < 0)
a1536 1
	pv = &pv_table[pind];
d1586 2
d1590 30
@


1.19
log
@size htab according to system memory size, not constant in header file.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2000/07/28 14:00:55 drahn Exp $	*/
d354 12
d494 1
a494 1
	if (mp->size <= 0)
@


1.18
log
@Rework some code in the powerpc pmap module.
Previously it was possible to remove multiple entries on a single pass thru
the pv deletion code in pmap_page_protect. Also when it did this, it
was not properly tracking of decrementing pm->pm_stats.resident_count.

By storing an additional piece of information in the pv structure,
the pmap pointer it is possible to call pmap_remove on the
entries rather than duplicating the pte removal code (again).

This fixes a problem seen where the system panics or hangs in
pmap_remove_pv due to the item not being on the list. Now
it is not possible for mix entries.

Because the pv entry only contained the pteidx, it was possible
to remove an incorrect entry due to ambiguity. multiple pmaps
having mappings at the same va of the same pa. Multipe pmap containing similar
entries will occur frequenty with shared libaries. Because of the hash
entries that have the same result of (sr & ptab_mask) will alias in the pv
list..

Since the pv_idx is now recomputable, should it be removed?
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2000/06/15 03:20:53 rahnds Exp $	*/
d52 1
a52 1
int ptab_cnt = HTABENTS;
d424 1
d428 1
d1043 1
a1043 1
			printf("pmap_remove_pv: not on list");
d1073 1
a1073 1
	pmap_remove(pm, va, va + NBPG - 1);
@


1.17
log
@Clean up one of the screwy things about the powerpc pmap, it could
not properly track the count of mapped pages. Fix the count
at a higher level. From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 2000/03/23 03:52:55 rahnds Exp $	*/
d105 1
d675 1
d680 1
d689 1
d692 1
d950 2
a951 1
pmap_enter_pv(pteidx, va, pind)
d971 1
d981 1
d1018 1
a1018 1
	if (pteidx == pv->pv_idx && va == pv->pv_va) {
d1027 4
a1030 2
		for (; npv = pv->pv_next; pv = npv)
			if (pteidx == npv->pv_idx && va == npv->pv_va)
d1032 2
d1038 1
a1038 1
#if 0
d1040 3
a1042 1
		else
d1044 2
d1102 1
a1102 1
		if (pmap_enter_pv(idx, va, i)) {
d1395 3
d1409 1
d1411 4
a1414 29
	while (pv_table[pind].pv_idx >= 0) {
		idx = pv_table[pind].pv_idx;
		va = pv_table[pind].pv_va;
		for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++)
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
				pmap_remove_pv(NULL, idx, va, pind, ptp);
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(va);
				tlbsync();
			}
		for (ptp = ptable + (idx ^ ptab_mask) * 8, i = 8; --i >= 0; ptp++)
			if ((ptp->pte_hi & PTE_VALID)
			    && (ptp->pte_lo & PTE_RPGN) == pa) {
				pmap_remove_pv(NULL, idx, va, pind, ptp);
				ptp->pte_hi &= ~PTE_VALID;
				asm volatile ("sync");
				tlbie(va);
				tlbsync();
			}
		for (po = potable[idx].lh_first; po; po = npo) {
			npo = po->po_list.le_next;
			if ((po->po_pte.pte_lo & PTE_RPGN) == pa) {
				pmap_remove_pv(NULL,idx, va, pind, &po->po_pte);
				LIST_REMOVE(po, po_list);
				pofree(po, 1);
			}
		}
@


1.16
log
@comment out memory region debugging.
make certain that physmem is initialized.
improve readablity of code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 2000/03/20 07:05:53 rahnds Exp $	*/
a1017 7
		if (pm != NULL) {
			/* if called from pmap_page_protect,
			 * we don't know what pmap it was removed from.
			 * BAD DESIGN.
			 */
			pm->pm_stats.resident_count--;
		}
a1024 3
			if (pm != NULL) {
				pm->pm_stats.resident_count--;
			}
d1137 1
d1147 1
d1157 1
@


1.15
log
@add first version of bus_dma for powerpc.
changes to trap handler to print out better information for jump to 0 bugs.
changes to pmap.c and machdep.c to debug a duplicate memory region
bug occasionally observed on imac with compressed kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 2000/01/14 05:42:17 rahnds Exp $	*/
d71 1
a71 1
#if 1
d79 7
d87 1
a87 1
		printf("memory region %x: start %x, size %x\n",
d322 1
d334 2
d423 3
a425 2
	while ((HTABSIZE << 7) < ctob(physmem))
	ptab_cnt <<= 1;
@


1.14
log
@

UVM changes mainly. As of this checkin UVM is still not working for powerpc
it has a copyin bug after device configuration. However to get these diffs
out of my tree.

All of the UVM code is currently inside ifdef UVM the kernel works fine
without option UVM. Config files have been left without UVM for now.

Prelimiary changes for busdma, (what UVM was wanted for).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1999/11/09 00:20:42 rahnds Exp $	*/
d71 16
d315 12
a326 1
	for (mp = mem; mp->size; mp++)
d328 1
d1451 1
d1462 1
@


1.14.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a70 23
#if 0
void
dump_avail()
{
	int cnt;
	struct mem_region *mp;
	extern struct mem_region *avail;
	
	printf("memory %x\n", mem);
	for (cnt = 0, mp = mem; mp->size; mp++) {
		printf("memory region %x: start %x, size %x\n",
				cnt, mp->size, mp->start);
		cnt++;
	}
	printf("available %x\n", avail);
	for (cnt = 0, mp = avail; mp->size; mp++) {
		printf("avail region %x: start %x, size %x\n",
				cnt, mp->size, mp->start);
		cnt++;
	}
}
#endif

d299 1
a299 15
#if 0
	/* work around repeated entries bug */
	for (mp = mem; mp->size; mp++) {
		if (mp[1].start == mp[0].start) {
			int i;
			i = 0;
			do {
				mp[0+i].size = mp[1+i].size;
				mp[0+i].start = mp[1+i].start;
			} while ( mp[0+(i++)].size != 0);
		}
	}
#endif
	physmem = 0;
	for (mp = mem; mp->size; mp++) {
a300 1
	}
d385 2
a386 3
	while ((HTABSIZE << 7) < ctob(physmem)) {
		ptab_cnt <<= 1;
	}
a1422 1
#if 0
a1432 1
#endif
@


1.14.2.2
log
@Continue the aborted merge of current just before 2.9 was cut into the
SMP branch.  Note that this will not make any progress of SMP functionality,
it is just merging of new code from the trunk into the old branch.
Please do not ask me questions about SMP status because of this mail,
instead go read the archives of smp@@openbsd.org, where I mailed about
these commits some week ago.  Another note: I am doing this in chunks now,
so as to not lock too much of the tree for long times
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/04/18 06:26:12 drahn Exp $	*/
d52 1
a52 1
int ptab_cnt;
a70 4
#ifndef UVM
	extern vm_offset_t pager_sva, pager_eva;
#endif

d82 1
a82 1
				cnt, mp->start, mp->size);
d88 1
a88 1
				cnt, mp->start, mp->size);
a93 146

/* virtual to physical map */
static inline int
VP_SR(va)
	paddr_t va;
{
	return (va >> VP_SR_POS) & VP_SR_MASK;
}
static inline int
VP_IDX1(va)
	paddr_t va;
{
	return (va >> VP_IDX1_POS) & VP_IDX1_MASK;
}

static inline int
VP_IDX2(va)
	paddr_t va;
{
	return (va >> VP_IDX2_POS) & VP_SR_MASK;
}

int
pmap_vp_valid(pm, va)
	pmap_t pm;
	vaddr_t va;
{
	pmapv_t *vp1;
	vp1 = pm->vps[VP_SR(va)];
	if (vp1 != NULL) {
		return (vp1[VP_IDX1(va)] & (1 << VP_IDX2(va)));
	}
	return 0;
}
int
pmap_vp_remove(pm, va)
	pmap_t pm;
	vaddr_t va;
{
	pmapv_t *vp1;
	int s;
	int retcode;
	retcode = 0;
	vp1 = pm->vps[VP_SR(va)];
#ifdef DEBUG
	printf("pmap_vp_remove: removing va %x pm %x", va, pm);
#endif
	if (vp1 != NULL) {
		s = splhigh();
		retcode = vp1[VP_IDX1(va)] & (1 << VP_IDX2(va));
		vp1[VP_IDX1(va)] &= ~(1 << VP_IDX2(va));
		splx(s);
	}
#ifdef DEBUG
	printf(" ret %x\n", retcode);
#endif
	return retcode;
}
void
pmap_vp_enter(pm, va, pa)
	pmap_t pm;
	vaddr_t va;
	paddr_t pa;
{
	pmapv_t *vp1;
	pmapv_t *mem1;
	int s;
	int idx;
	idx = VP_SR(va);
	vp1 = pm->vps[idx];
#ifdef DEBUG
	printf("pmap_vp_enter: pm %x va %x vp1 %x idx %x ", pm, va, vp1, idx);
#endif
	if (vp1 == NULL) {
#ifdef DEBUG
		printf("l1 entry idx %x ", idx);
#endif
		if (pm == pmap_kernel()) {
			printf(" irk kernel allocating map?");
		} else {
#ifdef UVM
			if (!(mem1 = (pmapv_t *)uvm_km_zalloc(kernel_map, NBPG)))
				panic("pmap_vp_enter: uvm_km_zalloc() failed");
#else
			if (!(mem1 = (pmapv_t *)kmem_alloc(kernel_map, NBPG)))
				panic("pmap_vp_enter: kmem_alloc() failed");
#endif
		}
		pm->vps[idx] = mem1;
#ifdef DEBUG
		printf("got %x ", mem1);
#endif
		vp1 = mem1;
	}
#ifdef DEBUG
	printf("l2 idx %x\n", VP_IDX2(va));
#endif

	s = splhigh();
	vp1[VP_IDX1(va)] |= (1 << VP_IDX2(va));
	splx(s);
	return;
}

void
pmap_vp_destroy(pm)
	pmap_t pm;
{
	pmapv_t *vp1;
	int sr, idx1;

	for (sr = 0; sr < 32; sr++) {
		vp1 = pm->vps[sr];
		if (vp1 == NULL) {
			continue;
		}
#ifdef SANITY
		for(idx1 = 0; idx1 < 1024; idx1++) {
			if (vp1[idx1] != 0) {
				printf("mapped page at %x \n"
					0); /* XXX what page was this... */
				vp1[idx2] = 0;

			}
		}
#endif
#ifdef UVM
		uvm_km_free(kernel_map, (vaddr_t)vp1, NBPG);
#else
		kmem_free(kernel_map, (vm_offset_t)vp1, NBPG);
#endif
		pm->vps[sr] = 0;
	}
}
static int vp_page0[1024];
static int vp_page1[1024];
void
pmap_vp_preinit()
{
	pmap_t pm = pmap_kernel();
	/* magic addresses are 0xe0000000, 0xe8000000 */
	pm->vps[VP_SR(0xe0000000)] = vp_page0;
	pm->vps[VP_SR(0xe8000000)] = vp_page1;
}


a104 1
	struct pmap *pv_pmap;		/* pmap associated with this map */
d322 13
a352 12

	/* make certain that each section is page aligned for base and size */
	for (mp = avail; mp->size; mp++) {
		u_int32_t end;
		s = mp->start - round_page(mp->start);
		if (s != 0) {
			mp->start = round_page(mp->start);
			end = trunc_page(mp->size + mp->start);
			mp->size = end - mp->start;
		}
		mp->size = trunc_page(mp->size);
	}
d432 1
d479 1
a479 1
	if (mp->size == 0)
a483 23

	/* use only one memory list */
	{ 
		u_int32_t size;
		struct mem_region *curmp;
		size = 0;
		curmp = NULL;
		for (mp = avail; mp->size; mp++) {
			if (mp->size > size) {
				size = mp->size;
				curmp=mp;
			}
		}
		mp = avail;
		if (curmp == mp) {
			++mp;
			mp->size = 0; /* lose the rest of memory */
		} else {
			*mp = *curmp;
			++mp;
			mp->size = 0; /* lose the rest of memory */
		}
	}
d485 1
a485 1
#ifdef MACHINE_NEW_NONCONTIG	
a486 1
#ifdef UVM
a489 4
#else
		vm_page_physload(atop(mp->start), atop(mp->start + mp->size),
			atop(mp->start), atop(mp->start + mp->size));
#endif
a491 1

a509 1
	pmap_vp_preinit();
d549 1
a549 1
#ifdef MACHINE_NEW_NONCONTIG
d568 1
a568 1
#ifdef MACHINE_NEW_NONCONTIG
a601 24
#ifdef MACHINE_NEW_NONCONTIG
static __inline struct pv_entry *
pmap_find_pv(paddr_t pa)
{
	int bank, off;

	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.pvent[off];
	} 
	return NULL;
}
static __inline char *
pmap_find_attr(paddr_t pa)
{
	int bank, off;

	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.attrs[off];
	} 
	return NULL;
}
#endif
a639 3
	if (lastidx == -1) {
		nextavail = avail->start;
	}
a653 4
#if defined(PMAP_NEW)
struct pmap *
pmap_create()
#else 
d656 1
a656 2
	vsize_t size;
#endif
d673 1
a673 2
	int i, j, k;
	int s, seg;
a677 1
	s = splimp();
d683 3
a685 4
			seg = (i * sizeof usedsr[0] * 8 + j) * 16;
			for (k = 0; k < 16; k++)
				pm->pm_sr[k] = seg + k;
			splx(s);
a687 1
	splx(s);
d724 1
a724 3
	int s;

	pmap_vp_destroy(pm);
a729 1
	s = splimp();
a730 1
	splx(s);
a790 1
#if 0
a791 8
#else
        int i;
                 
        for (i = NBPG/CACHELINESIZE; i > 0; i--) {
                __asm __volatile ("dcbz 0,%0" :: "r"(pa));
                pa += CACHELINESIZE;
        }
#endif
d945 1
a945 2
pmap_enter_pv(pm, pteidx, va, pa)
	struct pmap *pm;
d948 1
a948 1
	vm_offset_t pa;
d950 1
a950 1
	struct pv_entry *npv;
a951 1
	struct pv_entry *pv;
a955 4
	pv = pmap_find_pv( pa );
	if (pv == NULL) 
		return 0;

d958 1
a964 1
		pv->pv_pmap = pm;
a973 1
		npv->pv_pmap = pm;
d982 1
a982 1
pmap_remove_pv(pm, pteidx, va, pte_lo)
d986 2
a987 1
	u_int32_t pte_lo;
d991 2
a992 11
        int bank, pg;
	vm_offset_t pa;
	char *attr;

	pa = pte_lo & ~PGOFSET;
                   
        bank = vm_physseg_find(atop(pa), &pg);
        if (bank == -1)
                return;
        pv =   &vm_physmem[bank].pmseg.pvent[pg];
        attr = &vm_physmem[bank].pmseg.attrs[pg];
d997 1
a997 1
	*attr |= (pte_lo & (PTE_REF | PTE_CHG)) >> ATTRSHFT;
d1001 4
a1004 1
	 *
d1010 1
a1010 1
	if (va == pv->pv_va && pm == pv->pv_pmap) {
a1015 1
			pv->pv_pmap = 0;
d1018 7
d1026 2
a1027 3
		for (; npv = pv->pv_next; pv = npv) {
			if (va == npv->pv_va && pm == npv->pv_pmap)
			{
a1028 2
			}
		}
d1032 3
d1036 1
a1036 1
#if 1
d1038 1
a1038 3
		else {
			printf("pmap_remove_pv: not on list\n");
			/*
a1039 2
			*/
		}
d1065 1
a1065 1
	pmap_remove(pm, va, va + NBPG-1);
a1067 2

	pmap_vp_enter(pm, va, pa);
d1072 1
a1072 2
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);
d1095 7
a1101 6
	if (pmap_enter_pv(pm, idx, va, pa)) {
		/* 
		 * Flush the real memory from the cache.
		 */
		syncicache((void *)pa, NBPG);
	}
a1122 34
void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
{
	pmap_enter(pmap_kernel(), va, pa, prot, 1, 0);
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	struct vm_page **pgs;
	int npgs;
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, 1, 0);
	}
}

void
pmap_kremove(va, len)
	vaddr_t va;
	vsize_t len;
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}


a1131 1
	int found; /* if found, we are done, only one mapping per va */
a1134 1
	struct pv_entry *pv;
d1137 2
a1138 8
	for (; va < endva; va += NBPG) {
		if (0 == pmap_vp_remove(pm, va)) {
			/* no mapping */
			continue;
		}
		found = 0;
		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);
d1141 2
a1146 4
				pmap_remove_pv(pm, idx, va, ptp->pte_lo);
				pm->pm_stats.resident_count--;
				found = 1;
				break;
a1147 3
		if (found) {
			continue;
		}
d1150 2
a1155 4
				pmap_remove_pv(pm, idx, va, ptp->pte_lo);
				pm->pm_stats.resident_count--;
				found = 1;
				break;
a1156 3
		if (found) {
			continue;
		}
d1160 3
a1162 1
				pmap_remove_pv(pm, idx, va, po->po_pte.pte_lo);
a1164 2
				pm->pm_stats.resident_count--;
				break;
d1167 1
d1182 1
a1182 2
	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);
a1207 6
		/* return address 0 if not mapped??? */
		o = 0;
		if (pm == pmap_kernel() && va < 0x80000000){
			/* if in kernel, va==pa for 0 - 0x80000000 */
			o = va;
		}
d1209 1
a1209 1
		return o;
a1262 1
	char * pattr;
d1264 2
a1265 2
	pv = pmap_find_pv(pa);
	if (pv == NULL) 
a1266 1
	pattr = pmap_find_attr(pa);
d1271 2
a1272 2
	*pattr &= ~mask >> ATTRSHFT;
	*pattr |= val >> ATTRSHFT;
d1274 1
a1321 1
	char *pattr;
d1323 2
a1324 2
	pv = pmap_find_pv(pa);
	if (pv == NULL)
a1325 1
	pattr = pmap_find_attr(pa);
d1330 1
a1330 1
	bits |= ((*pattr) << ATTRSHFT) & bit;
d1334 1
a1334 1
	pv = pv_table;
a1376 6
#if defined(PMAP_NEW)
void
pmap_page_protect(pg, prot)
	struct vm_page *pg;
	vm_prot_t prot;
#else
a1380 1
#endif
a1381 3
#if defined(PMAP_NEW)
	vm_offset_t pa = VM_PAGE_TO_PHYS(pg);
#endif
a1385 2
	struct pmap *pm;
	struct pv_entry *pv;
d1393 2
a1394 2
	pv = pmap_find_pv(pa);
	if (pv == NULL) 
d1398 28
a1425 10
	while (pv->pv_idx != -1) {
		va = pv->pv_va;
		pm = pv->pv_pmap;
#ifdef UVM
		if ((va >=uvm.pager_sva) && (va < uvm.pager_eva)) {
				continue;
		}
#else
		if (va >= pager_sva && va < pager_eva) {
				continue;
a1426 2
#endif
		pmap_remove(pm, va, va + NBPG);
a1470 2
		(void) pmap_extract(pmap_kernel(), (vaddr_t)pcb->pcb_pm,
		    (paddr_t *)&pcb->pcb_pmreal);
a1472 30
	if (p == curproc) {
		/* Disable interrupts while switching. */
		__asm __volatile("mfmsr %0" : "=r"(psl) :);
		psl &= ~PSL_EE;
		__asm __volatile("mtmsr %0" :: "r"(psl));

		/* Store pointer to new current pmap. */
		curpm = pcb->pcb_pmreal;

		/* Save kernel SR. */
		__asm __volatile("mfsr %0,14" : "=r"(ksr) :);

		/*
		 * Set new segment registers.  We use the pmap's real
		 * address to avoid accessibility problems.
		 */
		rpm = pcb->pcb_pmreal;
		for (i = 0; i < 16; i++) {
			seg = rpm->pm_sr[i];
			__asm __volatile("mtsrin %0,%1"
			    :: "r"(seg), "r"(i << ADDR_SR_SHFT));
		}

		/* Restore kernel SR. */
		__asm __volatile("mtsr 14,%0" :: "r"(ksr));

		/* Interrupts are OK again. */
		psl |= PSL_EE;
		__asm __volatile("mtmsr %0" :: "r"(psl));
	}
@


1.14.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.2.2 2001/05/14 21:36:57 niklas Exp $	*/
a39 1
#include <sys/pool.h>
d44 1
d46 1
d71 4
a97 4
int pmap_vp_valid(pmap_t pm, vaddr_t va);
void pmap_vp_enter(pmap_t pm, vaddr_t va, paddr_t pa);
int pmap_vp_remove(pmap_t pm, vaddr_t va);
void pmap_vp_destroy(pmap_t pm);
d101 2
a102 1
VP_SR(paddr_t va)
d107 2
a108 1
VP_IDX1(paddr_t va)
d114 2
a115 1
VP_IDX2(paddr_t va)
d121 3
a123 1
pmap_vp_valid(pmap_t pm, vaddr_t va)
a131 1

d133 3
a135 1
pmap_vp_remove(pmap_t pm, vaddr_t va)
d157 4
a160 1
pmap_vp_enter(pmap_t pm, vaddr_t va, paddr_t pa)
d178 1
d181 4
d207 1
a207 4
	int sr;
#ifdef SANITY
	int idx1;
#endif
d224 1
d226 3
a233 2
void pmap_vp_preinit(void);

d260 34
a293 7
struct pool pmap_pv_pool;
struct pv_entry *pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *));

struct pool pmap_po_pool;
struct pte_ovfl *poalloc __P((void));
void pofree __P((struct pte_ovfl *, int));
a298 26
static inline void tlbie(vm_offset_t ea);
static inline void tlbsync(void);
static inline void tlbia(void);
static inline int ptesr(sr_t *sr, vm_offset_t addr);
static inline int pteidx(sr_t sr, vm_offset_t addr);
static inline int ptematch( pte_t *ptp, sr_t sr, vm_offset_t va, int which);
int pte_insert(int idx, pte_t *pt);
int pte_spill(vm_offset_t addr);
int pmap_page_index(vm_offset_t pa);
u_int pmap_free_pages(void);
int pmap_next_page(vm_offset_t *paddr);
struct pv_entry *pmap_alloc_pv(void);
void pmap_free_pv(struct pv_entry *pv);
struct pte_ovfl *poalloc(void);
static inline int pmap_enter_pv(struct pmap *pm, int pteidx, vm_offset_t va,
	vm_offset_t pa);
void pmap_remove_pv(struct pmap *pm, int pteidx, vm_offset_t va,
	u_int32_t pte_lo);
pte_t * pte_find(struct pmap *pm, vm_offset_t va);

/* XXX */
void pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot);
void pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs);
void pmap_kremove(vaddr_t va, vsize_t len);
void addbatmap(u_int32_t vaddr, u_int32_t raddr, u_int32_t wimg);

d304 2
a305 1
tlbie(vm_offset_t ea)
d319 1
a319 1
	vm_offset_t i;
d322 1
a322 1
	for (i = 0; i < 0x00040000; i += 0x00001000)
d365 1
a365 1
int
d632 1
d657 1
d659 1
d663 4
d668 1
d728 1
d731 1
d735 1
d737 3
d744 1
a744 4
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pvpl",
            0, NULL, NULL, M_VMPMAP);
	pool_init(&pmap_po_pool, sizeof(struct pte_ovfl), 0, 0, 0, "popl",
            0, NULL, NULL, M_VMPMAP);
d747 1
d757 1
d781 2
a782 1
static inline struct pv_entry *
d793 1
a793 1
static inline char *
d804 1
d991 13
d1032 1
a1032 1
struct pv_entry *
d1035 1
d1037 26
a1062 22
	int s;

	/*
	 * XXX - this splimp can go away once we have PMAP_NEW and
	 *       a correct implementation of pmap_kenter.
	 */
	/*
	 * Note that it's completly ok to use a pool here because it will
	 * never map anything or call pmap_enter because we have
	 * PMAP_MAP_POOLPAGE.
	 */
	s = splimp();
	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);
	splx(s);
	/*
	 * XXX - some day we might want to implement pv stealing, or
	 *       to pass down flags from pmap_enter about allowed failure.
	 *	 Right now - just panic.
	 */
	if (pv == NULL)
		panic("pmap_alloc_pv: failed to allocate pv");

d1066 1
a1066 1
void
d1070 22
a1091 6
	int s;

	/* XXX - see pmap_alloc_pv */
	s = splimp();
	pool_put(&pmap_pv_pool, pv);
	splx(s);
a1096 1
 * XXX - see pmap_alloc_pv
d1098 1
a1098 1
struct pte_ovfl *
d1101 1
d1103 2
a1104 1
	int s;
a1105 1
#ifdef DIAGNOSTIC
d1108 10
d1119 17
a1135 5
	s = splimp();
	po = pool_get(&pmap_po_pool, PR_NOWAIT);
	splx(s);
	if (po == NULL)
		panic("poalloc: failed to alloc po");
d1139 1
a1139 1
void
d1144 23
a1166 4
	int s;
	s = splimp();
	pool_put(&pmap_po_pool, po);
	splx(s);
d1192 1
a1192 1
	if ((first = pv->pv_idx) == -1) {
d1216 1
a1216 1
void
d1260 1
a1260 1
		for (; (npv = pv->pv_next) != NULL; pv = npv) {
d1295 1
a1295 1
	int idx, s;
d1410 1
d1463 1
a1463 1
pte_t *
d1490 2
a1491 2
boolean_t
pmap_extract(pm, va, pap)
d1493 1
a1493 2
	vaddr_t va;
	paddr_t *pap;
d1496 1
a1497 1
	boolean_t ret;
d1501 1
a1501 1
		ret = FALSE;
d1504 1
a1504 2
			*pap = va;
			ret = TRUE;
d1507 1
a1507 1
		return ret;
d1509 1
a1509 1
	*pap = (ptp->pte_lo & PTE_RPGN) | (va & ADDR_POFF);
d1511 1
a1511 1
	return TRUE;
d1532 1
a1532 1
			if ((ptp = pte_find(pm, sva))) {
d1694 3
a1696 1
	int s;
d1714 1
d1718 5
@


1.14.2.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.2.3 2001/07/04 10:23:01 niklas Exp $	*/
d43 1
a68 3
/*
#define USE_PMAP_VP
*/
a92 3
#ifdef USE_PMAP_VP
struct pool pmap_vp_pool;

d113 1
a113 1
	return (va >> VP_IDX2_POS) & VP_IDX2_MASK;
d167 3
a170 6
		s = splimp();
		mem1 = pool_get(&pmap_vp_pool, PR_NOWAIT);
		splx(s);

		bzero (mem1, PAGE_SIZE);

a191 1
	int s;
d212 1
a212 3
		s = splimp();
		pool_put(&pmap_vp_pool, vp1);
		splx(s);
a227 1
#endif
d244 2
d279 1
a663 1
#ifdef USE_PMAP_VP
a664 1
#endif /*  USE_PMAP_VP */
d711 1
a711 1
	pv = (struct pv_entry *)addr;
a713 4
#ifdef USE_PMAP_VP
	pool_init(&pmap_vp_pool, PAGE_SIZE, 0, 0, 0, "ppvl",
            0, NULL, NULL, M_VMPMAP);
#endif
d720 1
a720 1
	pv = (struct pv_entry *)addr;
d829 1
d832 5
a909 1
#ifdef USE_PMAP_VP
a910 1
#endif /*  USE_PMAP_VP */
a1176 4
int
pmap_enter_c_pv(struct pmap *pm, vm_offset_t va, vm_offset_t pa,
	vm_prot_t prot, int flags, int cacheable, int pv);

d1180 2
a1181 2
int
pmap_enter(pm, va, pa, prot, flags)
d1185 2
a1186 13
	int flags;
{
	return pmap_enter_c_pv(pm, va, pa, prot, flags, PMAP_CACHE_DEFAULT,
		TRUE);
}
int
pmap_enter_c_pv(pm, va, pa, prot, flags, cacheable, pv)
	struct pmap *pm;
	vm_offset_t va, pa;
	vm_prot_t prot;
	int flags;
	int cacheable;
	int pv;
a1200 1
#ifdef USE_PMAP_VP
a1201 1
#endif /*  USE_PMAP_VP */
d1215 5
a1219 7
	if (cacheable == PMAP_CACHE_DEFAULT) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_I | PTE_G;
		for (mp = mem; mp->size; mp++) {
			if (pa >= mp->start && pa < mp->start + mp->size) {
				pte.pte_lo &= ~(PTE_I | PTE_G);
				break;
			}
a1220 8
	} else if (cacheable == PMAP_CACHE_CI) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_I | PTE_G;
	} else if (cacheable == PMAP_CACHE_WT) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M | PTE_W | PTE_G;
	} else if (cacheable == PMAP_CACHE_WB) {
		pte.pte_lo = (pa & PTE_RPGN) | PTE_M;
	} else {
		panic("pmap_enter_c_pv: invalid cacheable %x\n", cacheable);
d1230 5
a1234 7
	if (pv == TRUE) {
		if (pmap_enter_pv(pm, idx, va, pa)) {
			/* 
			 * Flush the real memory from the cache.
			 */
			syncicache((void *)pa, NBPG);
		}
d1243 1
a1243 1
		return (KERN_SUCCESS);
a1254 2

	return (KERN_SUCCESS);
a1256 12
#define KERN_MAP_PV TRUE

void
pmap_kenter_cache(va, pa, prot, cacheable)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int cacheable;
{
	pmap_enter_c_pv(pmap_kernel(), va, pa, prot, PMAP_WIRED, cacheable,
		KERN_MAP_PV);
}
d1263 1
a1263 2
	pmap_enter_c_pv(pmap_kernel(), va, pa, prot, PMAP_WIRED,
		PMAP_CACHE_DEFAULT, KERN_MAP_PV);
d1275 2
a1276 3
		pmap_enter_c_pv(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
				VM_PROT_READ|VM_PROT_WRITE, PMAP_WIRED,
				PMAP_CACHE_DEFAULT, KERN_MAP_PV);
a1279 2
void pmap_remove_pvl( struct pmap *pm, vm_offset_t va, vm_offset_t endva,
	int pv);
d1286 1
a1286 1
		pmap_remove_pvl(pmap_kernel(), va, va + PAGE_SIZE, KERN_MAP_PV);
a1289 10
void pmap_pte_invalidate(vaddr_t va, pte_t *ptp);
void
pmap_pte_invalidate(vaddr_t va, pte_t *ptp)
{
	ptp->pte_hi &= ~PTE_VALID;
	asm volatile ("sync");
	tlbie(va);
	tlbsync();
}

a1298 9
	pmap_remove_pvl(pm, va, endva, TRUE);
}

void
pmap_remove_pvl(pm, va, endva, pv)
	struct pmap *pm;
	vm_offset_t va, endva;
	int pv;
{
a1306 1
#ifdef USE_PMAP_VP
a1310 1
#endif /*  USE_PMAP_VP */
d1316 5
a1320 5
				pmap_pte_invalidate(va, ptp);
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						ptp->pte_lo);
				}
d1330 5
a1334 5
				pmap_pte_invalidate(va, ptp);
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						ptp->pte_lo);
				}
d1345 1
a1345 4
				if (pv == TRUE) {
					pmap_remove_pv(pm, idx, va,
						po->po_pte.pte_lo);
				}
d1446 1
a1446 1
boolean_t
d1448 1
a1448 1
	paddr_t pa;
a1456 3
	boolean_t ret;

	ret = ptebits(pa, mask);
d1460 1
a1460 1
		return (ret);
d1470 1
a1470 1
		return (ret);
a1504 2

	return (ret);
d1530 1
d1573 1
d1578 6
d1585 1
d1587 1
d1591 1
a1591 7
	struct pv_entry *pv, *npv;
	int idx, i;
	sr_t sr;
	pte_t *ptp;
	struct pte_ovfl *po, *npo;
	int found;
	char *pattr;
a1598 1
	pattr = pmap_find_attr(pa);
d1607 2
a1608 40
#ifdef USE_PMAP_VP
		pmap_vp_remove(pm, va);
#endif /*  USE_PMAP_VP */

		npv = pv->pv_next;
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
		} else {
			pv->pv_pmap = 0;
			pv->pv_idx = -1;
		}

		/* now remove this entry from the table */
		found = 0;
		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);
		for (ptp = ptable + idx * 8, i = 8; --i >= 0; ptp++) {
			if (ptematch(ptp, sr, va, PTE_VALID)) {
				pmap_pte_invalidate(va, ptp);
				*pattr |= (ptp->pte_lo & (PTE_REF | PTE_CHG))
					>> ATTRSHFT;
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
		}
		if (found)
			continue;
		for (ptp = ptable + (idx ^ ptab_mask) * 8, i = 8; --i >= 0;
			ptp++)
		{
			if (ptematch(ptp, sr, va, PTE_VALID | PTE_HID)) {
				pmap_pte_invalidate(va, ptp);
				*pattr |= (ptp->pte_lo & (PTE_REF | PTE_CHG))
					>> ATTRSHFT;
				pm->pm_stats.resident_count--;
				found = 1;
				break;
			}
d1610 1
a1610 13
		if (found)
			continue;
		for (po = potable[idx].lh_first; po; po = npo) {
			npo = po->po_list.le_next;
			if (ptematch(&po->po_pte, sr, va, 0)) {
				LIST_REMOVE(po, po_list);
				pofree(po, 1);
				pm->pm_stats.resident_count--;
				break;
			}
		}

		
@


1.14.2.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d42 2
d288 3
@


1.14.2.6
log
@repair
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.2.5 2001/11/13 21:04:16 niklas Exp $	*/
@


1.14.2.7
log
@Re-add missing pieces
@
text
@d1 2
a2 1
/*	$OpenBSD: pmap.c,v 1.65 2002/03/22 21:01:07 drahn Exp $ */
d5 3
a7 1
 * Copyright (c) 2001, 2002 Dale Rahn. All rights reserved.
a8 1
 *   
d19 2
a20 2
 *	This product includes software developed by Dale Rahn.
 * 4. The name of the author may not be used to endorse or promote products
d23 1
a23 1
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
d26 8
a33 13
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * Effort sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F30602-01-2-0537.
 */  

d38 1
a45 5
#include <machine/pmap.h>

#include <machine/db_machdep.h>
#include <ddb/db_extern.h>
#include <ddb/db_output.h>
d47 8
a54 33
struct pmap kernel_pmap_;
static int npgs;
static struct mem_region *pmap_mem, *pmap_avail;
struct mem_region pmap_allocated[10];
int pmap_cnt_avail;
int pmap_cnt_allocated;


void * pmap_pvh;
void * pmap_attrib;
pte_t  *pmap_ptable;
int	pmap_ptab_cnt;
#ifdef USE_WTABLE
/* Wired entries in pmap_ptable */
Uint8_t *pmap_wtable;
#endif /* USE_WTABLE */
u_int	pmap_ptab_mask;
#define HTABSIZE (pmap_ptab_cnt * 64)

static u_int usedsr[NPMAPS / sizeof(u_int) / 8];
paddr_t zero_page;
paddr_t copy_src_page;
paddr_t copy_dst_page;

/* P->V table */
LIST_HEAD(pted_pv_head, pte_desc);

struct pte_desc {
	/* Linked list of phys -> virt entries */
	LIST_ENTRY(pte_desc) pted_pv_list;
	struct pte pted_pte;
	struct pmap *pted_pmap;
	vaddr_t pted_va;
d57 1
a57 1
void print_pteg(struct pmap *pm, vaddr_t va);
d59 1
a59 6
static inline void tlbsync(void);
static inline void tlbie(vaddr_t ea);
static inline void tlbia(void);

void pmap_attr_save(paddr_t pa, u_int32_t bits);
void pmap_page_ro(struct pmap *pm, vaddr_t va);
d61 3
d65 1
d67 2
a68 7
 * LOCKING structures.
 * This may not be correct, and doesn't do anything yet.
 */
#define pmap_simplelock_pm(pm)
#define pmap_simpleunlock_pm(pm)
#define pmap_simplelock_pv(pm)
#define pmap_simpleunlock_pv(pm)
d70 22
d93 1
a93 41
/* VP routines */
void pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted);
struct pte_desc *pmap_vp_remove(pmap_t pm, vaddr_t va);
void pmap_vp_destroy(pmap_t pm);
struct pte_desc *pmap_vp_lookup(pmap_t pm, vaddr_t va);

/* PV routines */
int pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *);
void pmap_remove_pv(struct pte_desc *pted);


/* pte hash table routines */
void pte_insert(struct pte_desc *pted);
void pmap_hash_remove(struct pte_desc *pted);
void pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa,
    struct pte_desc *pted, vm_prot_t prot, int flags, int cache);

void pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va);

void _pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags,
    int cache);
void pmap_remove_pg(struct pmap *pm, vaddr_t va);
void pmap_kremove_pg(vaddr_t va);

/* setup/initialization functions */
void pmap_avail_setup(void);
void pmap_avail_fixup(void);
void pmap_remove_avail(paddr_t base, paddr_t end);
void * pmap_steal_avail(size_t size, int align);

/* asm interface */
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type);

u_int32_t pmap_setusr(struct pmap *pm, vaddr_t va);
void pmap_popusr(u_int32_t oldsr);

/* debugging */
void pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...));

/* XXX - panic on pool get failures? */
struct pool pmap_pmap_pool;
a94 1
struct pool pmap_pted_pool;
d96 4
a99 4
int pmap_initialized = 0;
int physmem;

#define ATTRSHIFT	4
d101 1
a101 1
/* virtual to physical helpers */
d103 1
a103 1
VP_SR(vaddr_t va)
d105 1
a105 1
	return (va >>VP_SR_POS) & VP_SR_MASK;
a106 1

d108 1
a108 1
VP_IDX1(vaddr_t va)
d114 1
a114 1
VP_IDX2(vaddr_t va)
d119 2
a120 22
#if VP_IDX1_SIZE != VP_IDX2_SIZE 
#error pmap allocation code expects IDX1 and IDX2 size to be same
#endif
struct pmapvp {
	void *vp[VP_IDX1_SIZE];
};


/*
 * VP routines, virtual to physical translation information.
 * These data structures are based off of the pmap, per process.
 */

/*
 * This is used for pmap_kernel() mappings, they are not to be removed
 * from the vp table because they were statically initialized at the
 * initial pmap initialization. This is so that memory allocation 
 * is not necessary in the pmap_kernel() mappings.
 * otherwise bad race conditions can appear.
 */
struct pte_desc *
pmap_vp_lookup(pmap_t pm, vaddr_t va)
d122 4
a125 7
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
d127 1
a127 9

	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
	}

	pted = vp2->vp[VP_IDX2(va)];

	return pted;
d130 1
a130 4
/*
 * Remove, and return, pted at specified address, NULL if not present
 */
struct pte_desc *
d133 13
a145 7
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;

	vp1 = pm->pm_vp[VP_SR(va)];
	if (vp1 == NULL) {
		return NULL;
d147 4
a150 10

	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		return NULL;
	}

	pted = vp2->vp[VP_IDX2(va)];
	vp2->vp[VP_IDX2(va)] = NULL;

	return pted;
a151 9

/*
 * Create a V -> P mapping for the given pmap and virtual address
 * with reference to the pte descriptor that is used to map the page.
 * This code should track allocations of vp table allocations
 * so they can be freed efficiently.
 * 
 * should this be called under splimp?
 */
d153 1
a153 1
pmap_vp_enter(pmap_t pm, vaddr_t va, struct pte_desc *pted)
d155 2
a156 2
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d158 6
a163 4

	pmap_simplelock_pm(pm);

	vp1 = pm->pm_vp[VP_SR(va)];
d165 3
d169 1
a169 4
			printf("mapping kernel va%x pted%x \n",
				va, pted);
				
			Debugger();
d172 1
a172 1
		vp1 = pool_get(&pmap_vp_pool, PR_NOWAIT);
a173 3
		bzero(vp1, sizeof (struct pmapvp));
		pm->pm_vp[VP_SR(va)] = vp1;
	}
d175 7
a181 7
	vp2 = vp1->vp[VP_IDX1(va)];
	if (vp2 == NULL) {
		s = splimp();
		vp2 = pool_get(&pmap_vp_pool, PR_NOWAIT);
		splx(s);
		bzero(vp2, sizeof (struct pmapvp));
		vp1->vp[VP_IDX1(va)] = vp2;
d183 3
d187 3
a189 4
	vp2->vp[VP_IDX2(va)] = pted;

	pmap_simpleunlock_pm(pm);

d193 3
a195 6
/* 
 * WHERE DO HELPER FUNCTIONS GO?
 * XXX - need pvent filled out and changed.
 */
static inline struct pted_pv_head *
pmap_find_pvh(paddr_t pa)
d197 26
a222 4
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.pvent[off];
a223 1
	return NULL;
d225 6
a230 2
static inline char *
pmap_find_attr(paddr_t pa)
d232 4
a235 6
	int bank, off;
	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1) {
		return &vm_physmem[bank].pmseg.attrs[off];
	}
	return NULL;
d237 50
d288 4
a291 1
/* PTE manipulation/calculations */
d293 1
a293 1
tlbie(vaddr_t va)
d295 1
a295 1
	__asm volatile ("tlbie %0" :: "r"(va));
d297 1
d299 1
a299 1
tlbsync(void)
d301 1
a301 1
	__asm volatile ("sync; tlbsync; sync");
d304 1
a304 1
static inline void
d307 5
a311 5
	vaddr_t va;

	__asm volatile ("sync");
	for (va = 0; va < 0x00040000; va += 0x00001000)
		tlbie(va);
a312 1

d316 3
a318 1
ptesr(sr_t *sr, vaddr_t va)
d320 1
a320 1
	return sr[(u_int)va >> ADDR_SR_SHIFT];
d322 5
a326 2
static inline int 
pteidx(sr_t sr, vaddr_t va)
d329 3
a331 2
	hash = (sr & SR_VSID) ^ (((u_int)va & ADDR_PIDX) >> ADDR_PIDX_SHIFT);
	return hash & pmap_ptab_mask;
d334 6
a339 26
#define PTED_VA_PTEGIDX_M 0x07
#define PTED_VA_HID_M	  0x08
#define PTED_VA_MANAGED_M 0x10
#define PTED_VA_WIRED_M   0x20
static inline u_int32_t
PTED_HID(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_HID_M); 
}
static inline u_int32_t
PTED_PTEGIDX(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_PTEGIDX_M); 
}
static inline u_int32_t
PTED_MANAGED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_MANAGED_M); 
}
static inline u_int32_t
PTED_WIRED(struct pte_desc *pted)
{
	return (pted->pted_va & PTED_VA_WIRED_M); 
}
static inline u_int32_t
PTED_VALID(struct pte_desc *pted)
d341 4
a344 1
	return (pted->pted_pte.pte_hi & PTE_VALID);
d348 1
a348 20
 * PV entries -
 * manpulate the physical to virtual translations for the entire system.
 * 
 * QUESTION: should all mapped memory be stored in PV tables? or
 * is it alright to only store "ram" memory. Currently device mappings
 * are not stored.
 * It makes sense to pre-allocate mappings for all of "ram" memory, since
 * it is likely that it will be mapped at some point, but would it also
 * make sense to use a tree/table like is use for pmap to store device
 * mappings.
 * Futher notes: It seems that the PV table is only used for pmap_protect
 * and other paging related operations. Given this, it is not necessary
 * to store any pmap_kernel() entries in PV tables and does not make
 * sense to store device mappings in PV either.
 *
 * Note: unlike other powerpc pmap designs, the array is only an array
 * of pointers. Since the same structure is used for holding information
 * in the VP table, the PV table, and for kernel mappings, the wired entries.
 * Allocate one data structure to hold all of the info, instead of replicating
 * it multiple times.
d350 2
a351 4
 * One issue of making this a single data structure is that two pointers are
 * wasted for every page which does not map ram (device mappings), this 
 * should be a low percentage of mapped pages in the system, so should not
 * have too noticable unnecssary ram consumption.
a352 2


d354 3
a356 1
pmap_enter_pv(struct pte_desc *pted, struct pted_pv_head *pvh)
d358 24
a381 32
	int first;

	if (__predict_false(!pmap_initialized)) {
		return 0;
	}
	if (pvh == NULL) {
		return 0;
	}

	first = LIST_EMPTY(pvh);
	LIST_INSERT_HEAD(pvh, pted, pted_pv_list);
	pted->pted_va |= PTED_VA_MANAGED_M;
	return first;
}

void
pmap_remove_pv(struct pte_desc *pted)
{
	LIST_REMOVE(pted, pted_pv_list);
}

void
pmap_attr_save(paddr_t pa, u_int32_t bits)
{
	int bank, pg;
	u_int8_t *attr;

	bank = vm_physseg_find(atop(pa), &pg);
	if (bank == -1)
		return;
	attr = &vm_physmem[bank].pmseg.attrs[pg];
	*attr |= (u_int8_t)(bits >> ATTRSHIFT);
d384 7
d392 2
a393 6
pmap_enter(pm, va, pa, prot, flags)
	pmap_t pm;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d395 46
a440 54
	struct pte_desc *pted;
	struct pted_pv_head *pvh;
	int s;
	int need_sync;
	int cache;

	/* MP - Aquire lock for this pmap */

	s = splimp();
	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted)) {
		pmap_remove_pg(pm, va);
		/* we lost our pted if it was user */
		if (pm != pmap_kernel())
			pted = pmap_vp_lookup(pm, va);
	}

	pm->pm_stats.resident_count++;

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
		pmap_vp_enter(pm, va, pted);
	}

	/* Calculate PTE */
	pvh = pmap_find_pvh(pa);
	if (pvh != NULL) {
		/* managed memory is cachable */
		cache = PMAP_CACHE_WB;
	} else {
		cache = PMAP_CACHE_CI;
	}

	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);

	need_sync = pmap_enter_pv(pted, pvh);

	/*
	 * Insert into HTAB
	 * we were told to map the page, probably called from vm_fault,
	 * so map the page!
	 */
	pte_insert(pted);

	splx(s);

	/* only instruction sync executable pages */
	if (need_sync && (prot & VM_PROT_EXECUTE)) {
		pmap_syncicache_user_virt(pm, va);
	}

	/* MP - free pmap lock */
d444 4
a447 2
/* 
 * Remove the given range of mapping entries.
d450 2
a451 1
pmap_remove(struct pmap *pm, vaddr_t va, vaddr_t endva)
d453 3
a455 1
	vaddr_t addr;
d458 1
a458 3
	 * Should this be optimized for unmapped regions
	 * rather than perform all of the vp lookups?
	 * Not yet, still much faster than old version
d460 4
a463 2
	for (addr = va; addr < endva; addr += PAGE_SIZE) {
		pmap_remove_pg(pm, addr);
a464 10
}
/*
 * remove a single mapping, any process besides pmap_kernel()
 * notice that this code is O(1)
 */
void
pmap_remove_pg(struct pmap *pm, vaddr_t va)
{
	struct pte_desc *pted;
	int s;
d467 1
a467 3
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
d469 2
a470 15
	s = splimp();
	if (pm == pmap_kernel()) {
		pted = pmap_vp_lookup(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			splx(s);
			return;
		} 
	} else {
		pted = pmap_vp_remove(pm, va);
		if (pted == NULL || !PTED_VALID(pted)) {
			splx(s);
			return;
		}
	}
	pm->pm_stats.resident_count--;
d472 7
a478 1
	pmap_hash_remove(pted);
d480 10
a489 7
	pted->pted_pte.pte_hi &= ~PTE_VALID;

	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	if (pm != pmap_kernel()) {
		pool_put(&pmap_pted_pool, pted);
d491 60
a550 30

	splx(s);
}

/*
 * Enter a kernel mapping for the given page.
 * kernel mappings have a larger set of prerequisites than normal mappings.
 * 
 * 1. no memory should be allocated to create a kernel mapping.
 * 2. a vp mapping should already exist, even if invalid. (see 1)
 * 3. all vp tree mappings should already exist (see 1)
 * 
 */
void
_pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot, int flags, int cache)
{
	struct pte_desc *pted;
	int s;
	struct pmap *pm;
	struct pted_pv_head *pvh;

	pm = pmap_kernel();

	/* MP - lock pmap. */
	s = splimp();

	pted = pmap_vp_lookup(pm, va);
	if (pted && PTED_VALID(pted)) {
		/* pted is reused */
		pmap_kremove_pg(va);
d552 4
d557 6
a562 10
	pm->pm_stats.resident_count++;

	/* Do not have pted for this, get one and put it in VP */
	if (pted == NULL) {
		/* XXX - future panic? */
		printf("pted not preallocated in pmap_kernel() va %x pa %x \n",
		    va, pa);
		pted = pool_get(&pmap_pted_pool, PR_NOWAIT);	
		bzero(pted, sizeof (*pted));
		pmap_vp_enter(pm, va, pted);
d564 1
d566 71
a636 5
	pvh = pmap_find_pvh(pa);
	if (cache == PMAP_CACHE_DEFAULT) {
		if (pvh != NULL) {
		/* managed memory is cachable */
			cache = PMAP_CACHE_WB;
d638 3
a640 1
			cache = PMAP_CACHE_CI;
d643 6
a648 3

	/* Calculate PTE */
	pmap_fill_pte(pm, va, pa, pted, prot, flags, cache);
d651 1
a651 3
	 * Insert into HTAB
	 * we were told to map the page, probably called from vm_fault,
	 * so map the page!
d653 19
a671 5
	pte_insert(pted);
	pted->pted_va |= PTED_VA_WIRED_M;

	splx(s);

d674 3
d678 3
a680 1
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
d682 15
a696 7
	_pmap_kenter_pa(va, pa, prot, 0, PMAP_CACHE_DEFAULT);
}

void
pmap_kenter_cache(vaddr_t va, paddr_t pa, vm_prot_t prot, int cacheable)
{
	_pmap_kenter_pa(va, pa, prot, 0, cacheable);
a698 1

d700 2
a701 1
 * remove kernel (pmap_kernel()) mapping, one page
d704 1
a704 1
pmap_kremove_pg(vaddr_t va)
d706 32
a737 9
	struct pte_desc *pted;
	struct pmap *pm;
	int s;

	pm = pmap_kernel();
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
		/* XXX */
		return;
d739 1
a739 21
	if (!PTED_VALID(pted)) {
		/* not mapped */
		return;
	}
	s = splimp();

	pm->pm_stats.resident_count--;

	/*
	 * HASH needs to be locked here as well as pmap, and pv list.
	 * so that we know the mapping information is either valid,
	 * or that the mapping is not present in the hash table.
	 */
	pmap_hash_remove(pted);

	if (PTED_MANAGED(pted))
		pmap_remove_pv(pted);

	/* invalidate pted; */
	pted->pted_pte.pte_hi &= ~PTE_VALID;

d741 1
a742 1
}
d744 1
a744 1
 * remove kernel (pmap_kernel()) mappings
d746 3
a748 2
void
pmap_kremove(vaddr_t va, vsize_t len)
d750 9
a758 2
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE) {
		pmap_kremove_pg(va);
d760 1
d762 4
d767 8
a774 3
void pte_zap(pte_t *ptp, struct pte_desc *pted);
void
pte_zap(pte_t *ptp, struct pte_desc *pted)
d776 7
a782 10
		ptp->pte_hi &= ~PTE_VALID;
		__asm volatile ("sync");
		tlbie(pted->pted_va);
		__asm volatile ("sync");
		tlbsync();
		__asm volatile ("sync");
		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(pted->pted_pte.pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
		}
d784 3
d788 1
a788 3
 * remove specified entry from hash table.
 * all information is present in pted to look up entry
 * LOCKS... should the caller lock?
d791 2
a792 1
pmap_hash_remove(struct pte_desc *pted)
a793 13
	vaddr_t va = pted->pted_va;
	struct pmap *pm = pted->pted_pmap;
	pte_t *ptp;
	int sr, idx;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);

	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));
	/* determine which pteg mapping is present in */
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

d795 1
a795 3
	 * We now have the pointer to where it will be, if it is currently
	 * mapped. If the mapping was thrown away in exchange for another
	 * page mapping, then this page is not currently in the HASH.
d797 2
a798 7
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp, pted);
#ifdef USE_WTABLE
		pmap_wtable [idx] &= ~(1 << PTED_PTEGIDX(pted)); 
#endif /* USE_WTABLE */
	}
a800 1

d802 2
a803 1
 * What about execution control? Even at only a segment granularity.
d805 2
a806 3
void
pmap_fill_pte(struct pmap *pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
	vm_prot_t prot, int flags, int cache)
d808 1
a808 26
	sr_t sr;
	struct pte *pte;

	sr = ptesr(pm->pm_sr, va);
	pte = &pted->pted_pte;
	pte->pte_hi = ((sr & SR_VSID) << PTE_VSID_SHIFT) |
	    ((va >> ADDR_API_SHIFT) & PTE_API) | PTE_VALID;
	pte->pte_lo = (pa & PTE_RPGN);


	if ((cache == PMAP_CACHE_WB)) {
		pte->pte_lo |= PTE_M;
	} else if ((cache == PMAP_CACHE_WT)) {
		pte->pte_lo |= (PTE_W | PTE_M);
	} else {
		pte->pte_lo |= (PTE_M | PTE_I | PTE_G);
	}

	if (prot & VM_PROT_WRITE) {
		pte->pte_lo |= PTE_RW;
	} else {
		pte->pte_lo |= PTE_RO;
	}

	pted->pted_va = va & ~PAGE_MASK;
	pted->pted_pmap = pm;
d812 3
a814 2
 * read/clear bits from pte/attr cache, for reference/change
 * ack, copied code in the pte flush code....
d817 2
a818 1
pteclrbits(paddr_t pa, u_int bit, u_int clear)
d820 4
a823 62
	char *pattr;
	u_int bits;
	int s;
	struct pte_desc *pted;
	struct pted_pv_head *pvh;

	pattr = pmap_find_attr(pa);

	/* check if managed memory */
	if (pattr == NULL)
		return 0;

	pvh = pmap_find_pvh(pa);

	/*
	 *  First try the attribute cache
	 */
	bits = (*pattr << ATTRSHIFT) & bit;
	if ((bits == bit) && (clear == 0))
		return bits;

	/* cache did not contain all necessary bits,
	 * need to walk thru pv table to collect all mappings for this
	 * page, copying bits to the attribute cache 
	 * then reread the attribute cache.
	 */
	/* need lock for this pv */
	s = splimp();

	LIST_FOREACH(pted, pvh, pted_pv_list) {
		vaddr_t va = pted->pted_va & PAGE_MASK;
		struct pmap *pm = pted->pted_pmap;
		pte_t *ptp;
		int sr, idx;

		sr = ptesr(pm->pm_sr, va);
		idx = pteidx(sr, va);

		/* determine which pteg mapping is present in */
		ptp = pmap_ptable +
			(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
		ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

		/*
		 * We now have the pointer to where it will be, if it is
		 * currently mapped. If the mapping was thrown away in
		 * exchange for another page mapping, then this page is
		 * not currently in the HASH.
		 */
		if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
		    == ptp->pte_hi) {
			bits |=	ptp->pte_lo & (PTE_REF|PTE_CHG);
			if (clear) {
				ptp->pte_hi &= ~PTE_VALID;
				__asm__ volatile ("sync");
				tlbie(va);
				tlbsync();
				ptp->pte_lo &= ~bit;
				__asm__ volatile ("sync");
				ptp->pte_hi |= PTE_VALID;
			}
		}
d825 5
a829 6

	bits |= (*pattr << ATTRSHIFT) & bit;
	if (clear) {
		*pattr &= ~(bit >> ATTRSHIFT);
	} else {
		*pattr |= (bits >> ATTRSHIFT);
d831 3
a833 2
	splx(s);
	return bits;
d837 1
a837 4
 * Garbage collects the physical map system for pages which are 
 * no longer used. Success need not be guaranteed -- that is, there
 * may well be pages which are not referenced, but others may be collected
 * Called by the pageout daemon when pages are scarce.
d839 2
a840 2
void
pmap_collect(struct pmap *pm)
d842 6
a847 7
	/* This could return unused v->p table layers which 
	 * are empty.
	 * could malicious programs allocate memory and eat
	 * these wired pages? These are allocated via pool.
	 * Is there pool functions which could be called
	 * to lower the pool usage here?
	 */
d851 1
a851 1
 * Fill the given physical page with zeros.
d854 2
a855 1
pmap_zero_page(paddr_t pa)
d857 2
a858 31
#ifdef USE_DCBZ
	int i;
	paddr_t addr = zero_page;
#endif

	/* simple_lock(&pmap_zero_page_lock); */
	pmap_kenter_pa(zero_page, pa, VM_PROT_READ|VM_PROT_WRITE);
#ifdef USE_DCBZ
	for (i = PAGE_SIZE/CACHELINESIZE; i>0; i--) {
		__asm volatile ("dcbz 0,%0" :: "r"(addr));
		addr += CACHELINESIZE;
	}
#else
	bzero((void *)zero_page, PAGE_SIZE);
#endif
	pmap_kremove_pg(zero_page);
	
	/* simple_unlock(&pmap_zero_page_lock); */
}
/*
 * copy the given physical page with zeros.
 */
void
pmap_copy_page(paddr_t srcpa, paddr_t dstpa)
{
	/* simple_lock(&pmap_copy_page_lock); */

	pmap_kenter_pa(copy_src_page, srcpa, VM_PROT_READ);
	pmap_kenter_pa(copy_dst_page, dstpa, VM_PROT_READ|VM_PROT_WRITE);

	bcopy((void *)copy_src_page, (void *)copy_dst_page, PAGE_SIZE);
a859 16
	pmap_kremove_pg(copy_src_page);
	pmap_kremove_pg(copy_dst_page);
	/* simple_unlock(&pmap_copy_page_lock); */
}

int pmap_id_avail = 0;
void
pmap_pinit(struct pmap *pm)
{
	int i, k, try, tblidx, tbloff;
	int s, seg;

	bzero(pm, sizeof (struct pmap));

	pmap_reference(pm);

d861 1
a861 2
	 * Allocate segment registers for this pmap.
	 * try not to reuse pmap ids, to spread the hash table usage.
d863 8
a870 20
again:
	for (i = 0; i < NPMAPS; i++) {
		try = pmap_id_avail + i;
		try = try % NPMAPS; /* truncate back into bounds */
		tblidx = try / (8 * sizeof usedsr[0]);
		tbloff = try % (8 * sizeof usedsr[0]);
		if ((usedsr[tblidx] & (1 << tbloff)) == 0) {
			/* pmap create lock? */
			s = splimp();
			if ((usedsr[tblidx] & (1 << tbloff)) == 1) {
				/* entry was stolen out from under us, retry */
				splx(s); /* pmap create unlock */
				goto again;
			}
			usedsr[tblidx] |= (1 << tbloff); 
			pmap_id_avail = try + 1;
			splx(s); /* pmap create unlock */

			seg = try << 4;
			for (k = 0; k < 16; k++) {
d872 1
a872 1
			}
a874 15
	}
	panic("out of pmap slots");
}

/* 
 * Create and return a physical map.
 */
struct pmap *
pmap_create()
{
	struct pmap *pmap;
	int s;

	s = splimp();
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d876 1
a876 2
	pmap_pinit(pmap);
	return (pmap);
d880 1
a880 1
 * Add a reference to a given pmap.
d883 2
a884 1
pmap_reference(struct pmap *pm)
a885 1
	/* simple_lock(&pmap->pm_obj.vmobjlock); */
a886 1
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
d894 2
a895 1
pmap_destroy(struct pmap *pm)
d897 3
a899 8
	int refs;
	int s;

	/* simple_lock(&pmap->pm_obj.vmobjlock); */
	refs = --pm->pm_refs;
	/* simple_unlock(&pmap->pm_obj.vmobjlock); */
	if (refs > 0) {
		return;
a900 7
	/*
	 * reference count is zero, free pmap resources and free pmap.
	 */
	pmap_release(pm);
	s = splimp();
	pool_put(&pmap_pmap_pool, pm);
	splx(s);
d908 2
a909 1
pmap_release(struct pmap *pm)
d911 1
a911 1
	int i, tblidx, tbloff;
d914 1
d916 6
a921 5
	i = pm->pm_sr[0] >> 4;
	tblidx = i / (8  * sizeof usedsr[0]);
	tbloff = i % (8  * sizeof usedsr[0]);

	/* LOCK? */
d923 1
a923 1
	usedsr[tblidx] &= ~(1 << tbloff);
d927 19
d947 1
a947 1
pmap_vp_destroy(struct pmap *pm)
d949 1
a949 7
	int i, j;
#ifdef CHECK_IDX2_ENTRIES
	int k;
#endif
	int s;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d951 30
a980 27
	for (i = 0; i < VP_SR_SIZE; i++) {
		vp1 = pm->pm_vp[i];
		if (vp1 == NULL) {
			continue;
		}
		for (j = 0; j < VP_IDX1_SIZE; j++) {
			vp2 = vp1->vp[j];
			if (vp2 == NULL)
				continue;
			
			if (pm->pm_stats.resident_count != 0) 
#ifdef CHECK_IDX2_ENTRIES
/* This is ifdefed because it should not happen, and has not been occuring */
				for (k = 0; k < VP_IDX2_SIZE; k++) {
					if (vp2->vp[k] != NULL) {
						printf("PMAP NOT EMPTY"
						    " pm %x va %x\n",
						    pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
						pmap_remove_pg(pm,
						    (i << VP_SR_POS) |
						    (j << VP_IDX1_POS) |
						    (k << VP_IDX2_POS));
					}
				}
a981 10
			/* vp1->vp[j] = NULL; */
			s = splimp();
			pool_put(&pmap_vp_pool, vp2);
			splx(s);
		}
		/* pm->pm_vp[i] = NULL; */
		s = splimp();
		pool_put(&pmap_vp_pool, vp1);
		splx(s);
	}
d984 3
d988 8
a995 1
pmap_avail_setup(void)
d997 2
a998 1
	struct mem_region *mp;
d1000 19
a1018 3
	(fw->mem_regions) (&pmap_mem, &pmap_avail);
	pmap_cnt_avail = 0;
	physmem = 0;
d1020 2
d1023 5
a1027 3
	for (mp = pmap_mem; mp->size !=0; mp++) {
		physmem += btoc(mp->size);
	}
d1029 4
a1032 25
	/* limit to 1GB available, for now -XXXGRR */
#define MEMMAX 0x40000000
	for (mp = pmap_avail; mp->size !=0 ; /* increment in loop */) {
		if (mp->start + mp->size > MEMMAX) {
			int rm_start;
			int rm_end;
			if (mp->start > MEMMAX) {
				rm_start = mp->start;
				rm_end = mp->start+mp->size;
			} else {
				rm_start = MEMMAX;
				rm_end = mp->start+mp->size;
			}
			pmap_remove_avail(rm_start, rm_end);
			/* whack physmem, since we ignore more than 256MB */
			physmem = btoc(MEMMAX);
			/* start over at top, make sure not to skip any */
			mp = pmap_avail;
			continue;
		}
		mp++;
	}
	for (mp = pmap_avail; mp->size !=0; mp++) {
		pmap_cnt_avail += 1;
	}
d1035 22
d1059 3
a1061 1
pmap_avail_fixup(void)
d1063 28
a1090 3
	struct mem_region *mp;
	u_int32_t align;
	u_int32_t end;
d1092 19
a1110 16
	mp = pmap_avail;
	while(mp->size !=0) {
		align = round_page(mp->start);
		if (mp->start != align) {
			pmap_remove_avail(mp->start, align);
			mp = pmap_avail;
			continue;
		}
		end = mp->start+mp->size;
		align = trunc_page(end);
		if (end != align) {
			pmap_remove_avail(align, end);
			mp = pmap_avail;
			continue;
		}
		mp++;
d1112 2
a1115 1
/* remove a given region from avail memory */
d1117 5
a1121 1
pmap_remove_avail(paddr_t base, paddr_t end)
d1123 13
a1135 3
	struct mem_region *mp;
	int i;
	int mpend;
d1137 18
a1154 29
	/* remove given region from available */
	for (mp = pmap_avail; mp->size; mp++) {
		/*
		 * Check if this region hold all of the region
		 */
		mpend = mp->start + mp->size;
		if (base > mpend) {
			continue;
		}
		if (base <= mp->start) {
			if (end <= mp->start) {
				/* region not present -??? */
				break;
			}
			if (end >= mpend) {
				/* covers whole region */
				/* shorten */
				for (i = mp - pmap_avail;
					i < pmap_cnt_avail;
					i++)
				{
					pmap_avail[i] = pmap_avail[i+1];
				}
				pmap_cnt_avail--;
				pmap_avail[pmap_cnt_avail].size = 0;
			} else {
				mp->start = end;
				mp->size = mpend - end;
			}
d1156 2
a1157 18
			/* start after the beginning */
			if (end >= mpend) {
				/* just truncate */
				mp->size = base - mp->start;
			} else {
				/* split */
				for (i = pmap_cnt_avail;
					i > (mp - pmap_avail);
					i--)
				{
					pmap_avail[i] = pmap_avail[i - 1];
				}
				pmap_cnt_avail++;
				mp->size = base - mp->start;
				mp++;
				mp->start = end;
				mp->size = mpend - end;
			}
d1159 4
a1162 6
	}
	for (mp = pmap_allocated; mp->size != 0; mp++) {
		if (base < mp->start) {
			if (end == mp->start) {
				mp->start = base;
				mp->size += end - base;
a1164 10
			/* lenghten */
			for (i = pmap_cnt_allocated; i > (mp - pmap_allocated);
				i--)
			{
				pmap_allocated[i] = pmap_allocated[i - 1];
			}
			pmap_cnt_allocated++;
			mp->start = base;
			mp->size = end - base;
			return;
d1166 11
a1176 3
		if (base == (mp->start + mp->size)) {
			mp->size += end - base;
			return;
d1178 2
a1179 5
	}
	if (mp->size == 0) {
		mp->start = base;
		mp->size  = end - base;
		pmap_cnt_allocated++;
d1183 13
a1195 2
void *
pmap_steal_avail(size_t size, int align)
d1197 2
a1198 16
	struct mem_region *mp;
	int start;
	int remsize;

	for (mp = pmap_avail; mp->size; mp++) {
		if (mp->size > size) {
			start = (mp->start + (align -1)) & ~(align -1);
			remsize = mp->size - (start - mp->start); 
			if (remsize >= 0) {
				pmap_remove_avail(start, start+size);
				return (void *)start;
			}
		}
	}
	panic ("unable to allocate region with size %x align %x\n",
	    size, align);
d1200 8
a1207 10

void *msgbuf_addr;

/*
 * Initialize pmap setup.
 * ALL of the code which deals with avail needs rewritten as an actual
 * memory allocation.
 */ 
void
pmap_bootstrap(u_int kernelstart, u_int kernelend)
d1209 4
a1213 3
	int i, k;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
d1216 1
a1216 1
	 * Get memory.
d1218 3
a1220 1
	pmap_avail_setup();
d1222 4
d1227 1
a1227 3
	 * Page align all regions.
	 * Non-page memory isn't very interesting to us.
	 * Also, sort the entries for ascending addresses.
d1229 25
a1253 8
	kernelstart = trunc_page(kernelstart);
	kernelend = round_page(kernelend);
	pmap_remove_avail(kernelstart, kernelend);

	msgbuf_addr = pmap_steal_avail(MSGBUFSIZE,4);

	for (mp = pmap_avail; mp->size; mp++) {
		bzero((void *)mp->start, mp->size);
d1255 4
a1259 8
#ifdef  HTABENTS
	pmap_ptab_cnt = HTABENTS;
#else /* HTABENTS */
	pmap_ptab_cnt = 1024;
	while ((HTABSIZE << 7) < ctob(physmem)) {
		pmap_ptab_cnt <<= 1;
	}
#endif /* HTABENTS */
d1261 1
a1261 1
	 * allocate suitably aligned memory for HTAB
d1263 6
a1268 24
	pmap_ptable = pmap_steal_avail(HTABSIZE, HTABSIZE);
	bzero((void *)pmap_ptable, HTABSIZE);
	pmap_ptab_mask = pmap_ptab_cnt - 1;

#ifdef USE_WTABLE
	pmap_wtable = pmap_steal_avail(pmap_ptab_cnt, 4);
#endif /* USE_WTABLE */

	/* allocate v->p mappings for pmap_kernel() */
	for (i = 0; i < VP_SR_SIZE; i++) {
		pmap_kernel()->pm_vp[i] = NULL;
	}
	vp1 = pmap_steal_avail(sizeof (struct pmapvp), 4);
	bzero (vp1, sizeof(struct pmapvp));
	pmap_kernel()->pm_vp[KERNEL_SR] = vp1;

	for (i = 0; i < VP_IDX1_SIZE; i++) {
		vp2 = vp1->vp[i] = pmap_steal_avail(sizeof (struct pmapvp), 4);
		bzero (vp2, sizeof(struct pmapvp));
		for (k = 0; k < VP_IDX2_SIZE; k++) {
			struct pte_desc *pted;
			pted = pmap_steal_avail(sizeof (struct pte_desc), 4);
			bzero (pted, sizeof (struct pte_desc));
			vp2->vp[k] = pted;
d1271 2
a1272 9

	zero_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_src_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;
	copy_dst_page = VM_MIN_KERNEL_ADDRESS + ppc_kvm_stolen;
	ppc_kvm_stolen += PAGE_SIZE;


d1274 1
a1274 1
	 * Initialize kernel pmap and hardware.
d1276 3
a1278 8
#if NPMAPS >= KERNEL_SEGMENT / 16
	usedsr[KERNEL_SEGMENT / 16 / (sizeof usedsr[0] * 8)]
		|= 1 << ((KERNEL_SEGMENT / 16) % (sizeof usedsr[0] * 8));
#endif
	for (i = 0; i < 16; i++) {
		pmap_kernel()->pm_sr[i] = KERNEL_SEG0 + i;
		asm volatile ("mtsrin %0,%1"
			      :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
d1280 10
a1289 2
	asm volatile ("sync; mtsdr1 %0; isync"
		      :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));
d1291 2
a1292 1
	pmap_avail_fixup();
d1294 1
a1295 24
	tlbia();

	npgs = 0;
	for (mp = pmap_avail; mp->size; mp++) {
		npgs += btoc(mp->size);
	}
	/* Ok we loose a few pages from this allocation, but hopefully
	 * not too many 
	 */
	pmap_pvh = pmap_steal_avail(sizeof(struct pted_pv_head *) * npgs, 4);
	pmap_attrib = pmap_steal_avail(sizeof(char) * npgs, 1);
	pmap_avail_fixup();
	for (mp = pmap_avail; mp->size; mp++) {
		uvm_page_physload(atop(mp->start), atop(mp->start+mp->size),
		    atop(mp->start), atop(mp->start+mp->size),
		    VM_FREELIST_DEFAULT);
	}
}

/*
 * activate a pmap entry
 * NOOP on powerpc, all PTE entries exist in the same hash table.
 * Segment registers are filled on exit to user mode.
 */
d1297 5
a1301 1
pmap_activate(struct proc *p)
d1303 2
a1305 5

/*
 * deactivate a pmap entry
 * NOOP on powerpc
 */
d1307 4
a1310 1
pmap_deactivate(struct proc *p)
d1312 2
d1316 5
a1320 5
/* 
 * Get the physical page address for the given pmap/virtual address.
 */ 
boolean_t
pmap_extract(struct pmap *pm, vaddr_t va, paddr_t *pa)
d1322 1
a1322 1
	struct pte_desc *pted;
d1324 4
a1327 9
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted)) {
		if (pm == pmap_kernel() && va < 0x80000000) {
			/* XXX - this is not true without BATs */
			/* if in kernel, va==pa for 0-0x80000000 */
			*pa = va;
			return TRUE;
		}
		return FALSE;
a1328 2
	*pa = (pted->pted_pte.pte_lo & PTE_RPGN) | (va & ~PTE_RPGN);
	return TRUE;
d1331 6
a1336 2
u_int32_t
pmap_setusr(struct pmap *pm, vaddr_t va)
d1338 3
a1340 11
	u_int32_t sr;
	u_int32_t oldsr;

	sr = pm->pm_sr[(u_int)va >> ADDR_SR_SHIFT];

	/* user address range lock?? */
	asm volatile ("mfsr %0,%1"
		      : "=r" (oldsr): "n"(USER_SR));
	asm volatile ("isync; mtsr %0,%1; isync"
		      :: "n"(USER_SR), "r"(sr));
	return oldsr;
d1343 1
d1345 1
a1345 1
pmap_popusr(u_int32_t sr)
d1347 4
a1350 2
	asm volatile ("isync; mtsr %0,%1; isync"
		      :: "n"(USER_SR), "r"(sr));
a1352 32
int
copyin(udaddr, kaddr, len)
	const void *udaddr;
	void *kaddr;
	size_t len;
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}
		bcopy(p, kaddr, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
}
d1354 9
a1362 32
int
copyout(kaddr, udaddr, len)
	const void *kaddr;
	void *udaddr;
	size_t len;
{
	void *p;
	size_t l;
	u_int32_t oldsr;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)udaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		if (l > len)
			l = len;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)udaddr);
		if (setfault(env)) {
			pmap_popusr(oldsr);
			curpcb->pcb_onfault = oldh;
			return EFAULT;
		}

		bcopy(kaddr, p, l);
		pmap_popusr(oldsr);
		udaddr += l;
		kaddr += l;
		len -= l;
	}
	curpcb->pcb_onfault = oldh;
	return 0;
d1365 5
a1369 2
int
copyinstr(const void *udaddr, void *kaddr, size_t len, size_t *done)
d1371 23
a1393 33
	const u_char *uaddr = udaddr;
	u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(env)) {
			if (done != NULL) {
				*done =  cnt;
			}
			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *up;
			*kp = c;
			if (c == 0) {
				if (done != NULL) {
					*done = cnt + 1;
d1395 27
a1421 55
				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
		}
		pmap_popusr(oldsr);
	}
	curpcb->pcb_onfault = oldh;
	if (done != NULL) {
		*done = cnt;
	}
	return ENAMETOOLONG;
}

int
copyoutstr(const void *kaddr, void *udaddr, size_t len, size_t *done)
{
	u_char *uaddr = (void *)udaddr;
	const u_char *kp    = kaddr;
	u_char *up;
	u_char c;
	void   *p;
	size_t	 l;
	u_int32_t oldsr;
	int cnt = 0;
	faultbuf env;
	void *oldh = curpcb->pcb_onfault;

	while (len > 0) {
		p = USER_ADDR + ((u_int)uaddr & ~SEGMENT_MASK);
		l = (USER_ADDR + SEGMENT_LENGTH) - p;
		up = p;
		if (l > len)
			l = len;
		len -= l;
		oldsr = pmap_setusr(curpcb->pcb_pm, (vaddr_t)uaddr);
		if (setfault(env)) {
			if (done != NULL) {
				*done =  cnt;
			}
			curpcb->pcb_onfault = oldh;
			pmap_popusr(oldsr);
			return EFAULT;
		}
		while (l > 0) {
			c = *kp;
			*up = c;
			if (c == 0) {
				if (done != NULL) {
					*done = cnt + 1;
d1423 5
a1427 9
				curpcb->pcb_onfault = oldh;
				pmap_popusr(oldsr);
				return 0;
			} 
			up++;
			kp++;
			l--;
			cnt++;
			uaddr++;
a1428 1
		pmap_popusr(oldsr);
d1430 1
a1430 5
	curpcb->pcb_onfault = oldh;
	if (done != NULL) {
		*done = cnt;
	}
	return ENAMETOOLONG;
d1433 4
a1436 7
/*
 * sync instruction cache for user virtual address.
 * The address WAS JUST MAPPED, so we have a VALID USERSPACE mapping
 */
#define CACHELINESIZE   32		/* For now XXX*/
void
pmap_syncicache_user_virt(struct pmap *pm, vaddr_t va)
d1438 4
a1441 3
	vaddr_t p, start;
	int oldsr;
	int l;
d1443 12
a1454 27
	if (pm != pmap_kernel()) {
		start = ((u_int)USER_ADDR + ((u_int)va & ~SEGMENT_MASK));
		/* will only ever be page size, will not cross segments */

		/* USER SEGMENT LOCK - MPXXX */
		oldsr = pmap_setusr(pm, va);
	} else {
		start = va; /* flush mapped page */
	}
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("dcbst 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);
	p = start;
	l = PAGE_SIZE;
	do {
		__asm__ __volatile__ ("icbi 0,%0" :: "r"(p));
		p += CACHELINESIZE;
	} while ((l -= CACHELINESIZE) > 0);


	if (pm != pmap_kernel()) {
		pmap_popusr(oldsr);
		/* USER SEGMENT UNLOCK -MPXXX */
	}
d1458 1
a1458 1
 * Change a page to readonly
d1460 5
a1464 2
void
pmap_page_ro(struct pmap *pm, vaddr_t va)
d1467 13
a1479 39
	struct pte_desc *pted;
	int sr, idx;

	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL || !PTED_VALID(pted)) {
		return;
	}
	pted->pted_pte.pte_lo &= ~PTE_PP;
	pted->pted_pte.pte_lo |= PTE_RO;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr, va);

	/* determine which pteg mapping is present in */
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */

	/*
	 * We now have the pointer to where it will be, if it is
	 * currently mapped. If the mapping was thrown away in
	 * exchange for another page mapping, then this page is
	 * not currently in the HASH.
	 */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		ptp->pte_hi &= ~PTE_VALID;
		__asm__ volatile ("sync");
		tlbie(va);
		tlbsync();
		if (PTED_MANAGED(pted)) { /* XXX */
			pmap_attr_save(ptp->pte_lo & PTE_RPGN,
			    ptp->pte_lo & (PTE_REF|PTE_CHG));
		}
		ptp->pte_lo &= ~PTE_CHG;
		ptp->pte_lo &= ~PTE_PP;
		ptp->pte_lo |= PTE_RO;
		__asm__ volatile ("sync");
		ptp->pte_hi |= PTE_VALID;
d1481 3
d1487 1
a1487 1
 * Lower the protection on the specified physical page.
d1489 1
a1489 1
 * There are only two cases, either the protection is going to 0,
d1493 4
a1496 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
d1498 2
a1499 23
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s;
	struct pte_desc *pted;
	struct pted_pv_head *pvh;

	/* need to lock for this pv */
	s = splimp();
	pvh = pmap_find_pvh(pa);

	/* nothing to do if not a managed page */
	if (pvh == NULL) {
		splx(s);
		return;
	}

	if (prot == VM_PROT_NONE) {
		while (!LIST_EMPTY(pvh)) {
			pted = LIST_FIRST(pvh);
			pmap_remove_pg(pted->pted_pmap, pted->pted_va);
		}
		splx(s);
		return;
	}
a1500 10
	LIST_FOREACH(pted, pvh, pted_pv_list) {
		pmap_page_ro(pted->pted_pmap, pted->pted_va);
	}
	splx(s);
}

void
pmap_protect(struct pmap *pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	int s;
d1504 12
a1515 2
			pmap_page_ro(pm, sva);
			sva += PAGE_SIZE;
d1523 5
a1527 5
/*
 * Restrict given range to physical memory
 */
void
pmap_real_memory(paddr_t *start, vm_size_t *size)
d1529 6
a1534 1
	struct mem_region *mp;
d1536 6
a1541 15
	for (mp = pmap_mem; mp->size; mp++) {
		if (((*start + *size) > mp->start)
			&& (*start < (mp->start + mp->size)))
		{
			if (*start < mp->start) {
				*size -= mp->start - *start;
				*start = mp->start;
			}
			if ((*start + *size) > (mp->start + mp->size))
				*size = mp->start + mp->size - *start;
			return;
		}
	}
	*size = 0;
}
d1543 2
a1544 60
/*
 * How much virtual space is available to the kernel?
 */
void
pmap_virtual_space(vaddr_t *start, vaddr_t *end)
{
	*start = VM_MIN_KERNEL_ADDRESS;
	*end = VM_MAX_KERNEL_ADDRESS;
}

void
pmap_init()
{
	vsize_t sz;
	struct pted_pv_head *pvh;
	char *attr;
	int i, bank;

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 20, "pmap", NULL);
	pool_setlowat(&pmap_pmap_pool, 2);
	pool_init(&pmap_vp_pool, sizeof(struct pmapvp), 0, 0, 150, "vp", NULL);
	pool_setlowat(&pmap_vp_pool, 10);
	pool_init(&pmap_pted_pool, sizeof(struct pte_desc), 0, 0, 150, "pted",
	    NULL);
	pool_setlowat(&pmap_pted_pool, 20);

	/* pmap_pvh and pmap_attr must be allocated 1-1 so that pmap_save_attr
	 * is callable from pte_spill_r (with vm disabled)
	 */
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (i = npgs; i > 0; i--)
		LIST_INIT(pvh++);
	attr = pmap_attrib;
	bzero(pmap_attrib, npgs);
	pvh = (struct pted_pv_head *)pmap_pvh;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		sz = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pvh;
		vm_physmem[bank].pmseg.attrs = attr;
		pvh += sz;
		attr += sz;
	}
	pmap_initialized = 1;
}

/* 
 * There are two routines, pte_spill_r and pte_spill_v
 * the _r version only handles kernel faults which are not user
 * accesses. The _v version handles all user faults and kernel copyin/copyout
 * "user" accesses.
 */
int
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr)
{
	struct pmap *pm;
	struct pte_desc *pted;
	int retcode = 0;

	/* 
	 * This function only handles kernel faults, not supervisor copyins.
d1546 5
a1550 11
	if (!(msr & PSL_PR)) {
		/* PSL_PR is clear for supervisor, right? - XXXXX */
		/* lookup is done physical to prevent faults */
		if (VP_SR(va) == USER_SR) {
			return 0;
		} else {
			pm = pmap_kernel();
		}
	} else {
		return 0;
	}
d1552 31
a1582 14
	pted = pmap_vp_lookup(pm, va);
	if (pted != NULL) {
		/* if the current mapping is RO and the access was a write
		 * we return 0
		 */
		if (!PTED_VALID(pted) ||
		    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
		{
			/* write fault and we have a readonly mapping */
			retcode = 0;
		} else {
			retcode = 1;
			pte_insert(pted);
		}
d1584 1
d1586 1
a1586 1
	return retcode;
d1590 3
a1592 1
pte_spill_v(struct pmap *pm, u_int32_t va, u_int32_t dsisr)
d1594 5
a1598 1
	struct pte_desc *pted;
d1600 2
a1601 2
	pted = pmap_vp_lookup(pm, va);
	if (pted == NULL) {
d1603 1
a1603 1
	}
d1606 1
a1606 2
	 * if the current mapping is RO and the access was a write
	 * we return 0
d1608 5
a1612 7
	if (!PTED_VALID(pted) ||
	    ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)))
	{
		/* write fault and we have a readonly mapping */
		if (PTED_VALID(pted)) {
			pmap_hash_remove(pted);
		}
d1614 29
d1644 2
a1645 2
	pte_insert(pted);
	return 1;
a1647 1

d1649 4
a1652 4
 * should pte_insert code avoid wired mappings?
 * is the stack safe?
 * is the pted safe? (physical)
 * -ugh
a1653 1

d1655 3
a1657 1
pte_insert(struct pte_desc *pted)
d1659 16
a1674 18
	int off;
	int secondary;
	struct pte *ptp;
	int sr, idx;
	int i;

	/* HASH lock? */

	sr = ptesr(pted->pted_pmap->pm_sr, pted->pted_va);
	idx = pteidx(sr, pted->pted_va);

	/* determine if ptp is already mapped */
	ptp = pmap_ptable +
		(idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if ((pted->pted_pte.pte_hi | (PTED_HID(pted) ? PTE_HID : 0))
	    == ptp->pte_hi) {
		pte_zap(ptp,pted);
d1677 4
a1680 1
	pted->pted_va &= ~(PTED_VA_HID_M|PTED_VA_PTEGIDX_M);
d1682 16
a1697 7
	/*
	 * instead of starting at the beginning of each pteg,
	 * the code should pick a random location with in the primary
	 * then search all of the entries, then if not yet found,
	 * do the same for the secondary.
	 * this would reduce the frontloading of the pteg.
	 */
d1699 15
a1713 4
	/* first just try fill of primary hash */
	ptp = pmap_ptable + (idx) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp[i].pte_hi & PTE_VALID) {
d1715 11
d1727 1
a1727 14
		/* not valid, just load */
/* printf("inserting in primary idx %x, i %x\n", idx, i); */
		pted->pted_va |= i;
		ptp[i].pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
		__asm__ volatile ("sync");
		ptp[i].pte_hi |= PTE_VALID;
		__asm volatile ("sync");
		return;
	}
	/* first just try fill of secondary hash */
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
	for (i = 0; i < 8; i++) {
		if (ptp[i].pte_hi & PTE_VALID) {
d1729 8
a1737 9
/* printf("inserting in secondary idx %x, i %x\n", idx, i); */
		pted->pted_va |= (i | PTED_VA_HID_M);
		ptp[i].pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
		ptp[i].pte_lo = pted->pted_pte.pte_lo;
		__asm__ volatile ("sync");
		ptp[i].pte_hi |= PTE_VALID;
		__asm volatile ("sync");
		return;
	}
d1739 1
a1739 28
	/* need decent replacement algorithm */
	__asm__ volatile ("mftb %0" : "=r"(off));
	secondary = off & 8;
	pted->pted_va |= off & (PTED_VA_PTEGIDX_M|PTED_VA_HID_M);

	idx =  (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0));

#ifdef USE_WTABLE
	if (PTED_WIRED(pted)) {
		pmap_wtable[idx] &= ~(1 << PTED_PTEGIDX(pted)); 
	}
#endif /* USE_WTABLE */
	ptp = pmap_ptable + (idx * 8);
	ptp += PTED_PTEGIDX(pted); /* increment by index into pteg */
	if (ptp->pte_hi & PTE_VALID) {
		vaddr_t va;
		ptp->pte_hi &= ~PTE_VALID;
		__asm volatile ("sync");

		va = ((ptp->pte_hi & PTE_API) << ADDR_API_SHIFT) |
		     ((((ptp->pte_hi >> PTE_VSID_SHIFT) & SR_VSID)
			^(idx ^ ((ptp->pte_hi & PTE_HID) ? 0x3ff : 0)))
			    & 0x3ff) << PAGE_SHIFT;
		tlbie(va);

		tlbsync();
		pmap_attr_save(ptp->pte_lo & PTE_RPGN,
		    ptp->pte_lo & (PTE_REF|PTE_CHG));
d1741 1
a1741 8
	if (secondary) {
		ptp->pte_hi = (pted->pted_pte.pte_hi | PTE_HID) & ~PTE_VALID;
	} else {
		ptp->pte_hi = pted->pted_pte.pte_hi & ~PTE_VALID;
	}
	ptp->pte_lo = pted->pted_pte.pte_lo;
	__asm__ volatile ("sync");
	ptp->pte_hi |= PTE_VALID;
d1743 5
d1749 13
a1761 1
#ifdef DEBUG_PMAP
d1763 1
a1763 1
print_pteg(struct pmap *pm, vaddr_t va)
d1765 4
a1768 22
	int sr, idx;
	struct pte *ptp;

	sr = ptesr(pm->pm_sr, va);
	idx = pteidx(sr,  va);

	ptp = pmap_ptable + idx  * 8;
	db_printf("va %x, sr %x, idx %x\n", va, sr, idx);

	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
	ptp = pmap_ptable + (idx ^ pmap_ptab_mask) * 8;
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_hi, ptp[1].pte_hi, ptp[2].pte_hi, ptp[3].pte_hi,
	    ptp[4].pte_hi, ptp[5].pte_hi, ptp[6].pte_hi, ptp[7].pte_hi);
	db_printf("%08x %08x %08x %08x  %08x %08x %08x %08x\n",
	    ptp[0].pte_lo, ptp[1].pte_lo, ptp[2].pte_lo, ptp[3].pte_lo,
	    ptp[4].pte_lo, ptp[5].pte_lo, ptp[6].pte_lo, ptp[7].pte_lo);
d1771 1
a1771 4

/* debugger assist function */
int pmap_prtrans(u_int pid, vaddr_t va);

d1773 1
a1773 1
pmap_print_pted(struct pte_desc *pted, int(*print)(const char *, ...))
d1775 3
a1777 19
	vaddr_t va;
	va = pted->pted_va & ~PAGE_MASK;
	print("\n pted %x", pted);
	if (PTED_VALID(pted)) {
		print(" va %x:", pted->pted_va & ~PAGE_MASK);
		print(" HID %d", PTED_HID(pted) ? 1: 0);
		print(" PTEGIDX %x", PTED_PTEGIDX(pted));
		print(" MANAGED %d", PTED_MANAGED(pted) ? 1: 0);
		print(" WIRED %d\n", PTED_WIRED(pted) ? 1: 0);
		print("ptehi %x ptelo %x ptp %x Aptp %x\n",
		    pted->pted_pte.pte_hi, pted->pted_pte.pte_lo,
		    pmap_ptable +
			8*pteidx(ptesr(pted->pted_pmap->pm_sr, va), va),
		    pmap_ptable +
			8*(pteidx(ptesr(pted->pted_pmap->pm_sr, va), va)
			    ^ pmap_ptab_mask)
		    );
	}
}
d1779 14
a1792 27
int pmap_user_read(int size, vaddr_t va);
int
pmap_user_read(int size, vaddr_t va)
{
	unsigned char  read1;
	unsigned short read2;
	unsigned int   read4;
	int err;

	if (size == 1) {
		err = copyin((void *)va, &read1, 1);
		if (err == 0) {
			db_printf("byte read %x\n", read1);
		}
	} else if (size == 2) {
		err = copyin((void *)va, &read2, 2);
		if (err == 0) {
			db_printf("short read %x\n", read2);
		}
	} else if (size == 4) {
		err = copyin((void *)va, &read4, 4);
		if (err == 0) {
			db_printf("int read %x\n", read4);
		}
	} else {
		return 1;
	}
d1794 2
d1797 2
a1798 2
	return 0;
}
d1800 9
a1808 14
int pmap_dump_pmap(u_int pid);
int
pmap_dump_pmap(u_int pid)
{
	struct pmap *pm;
	struct proc *p;
	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);

		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
a1809 8
		pm = p->p_vmspace->vm_map.pmap;
	}
	printf("pmap %x:\n", pm);
	printf("segid %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x %x",
	    pm->pm_sr[0], pm->pm_sr[1], pm->pm_sr[2], pm->pm_sr[3],
	    pm->pm_sr[4], pm->pm_sr[5], pm->pm_sr[6], pm->pm_sr[7],
	    pm->pm_sr[8], pm->pm_sr[9], pm->pm_sr[10], pm->pm_sr[11],
	    pm->pm_sr[12], pm->pm_sr[13], pm->pm_sr[14], pm->pm_sr[15]);
d1811 6
a1816 22
	return 0;
}

int
pmap_prtrans(u_int pid, vaddr_t va)
{
	struct proc *p;
	struct pmap *pm;
	pmapvp_t *vp1;
	pmapvp_t *vp2;
	struct pte_desc *pted;

	if (pid == 0) {
		pm = pmap_kernel();
	} else {
		p = pfind(pid);

		if (p == NULL) {
			db_printf("invalid pid %d", pid);
			return 1;
		}
		pm = p->p_vmspace->vm_map.pmap;
d1818 2
a1819 19

	db_printf(" pid %d, va 0x%x pmap %x\n", pid, va, pm);
	vp1 = pm->pm_vp[VP_SR(va)];
	db_printf("sr %x id %x vp1 %x", VP_SR(va), pm->pm_sr[VP_SR(va)],
	    vp1);

	if (vp1) {
		vp2 = vp1->vp[VP_IDX1(va)];
		db_printf(" vp2 %x", vp2);

		if (vp2) {
			pted = vp2->vp[VP_IDX2(va)];
			pmap_print_pted(pted, db_printf);

		}
	}
	print_pteg(pm, va);

	return 0;
d1821 3
a1823 4
int pmap_show_mappings(paddr_t pa);

int
pmap_show_mappings(paddr_t pa) 
d1825 1
a1825 11
	struct pted_pv_head *pvh;
	struct pte_desc *pted;
	pvh = pmap_find_pvh(pa);
	if (pvh == NULL) {
		db_printf("pa %x: unmanaged\n");
	} else {
		LIST_FOREACH(pted, pvh, pted_pv_list) {
			pmap_print_pted(pted, db_printf);
		}
	}
	return 0;
a1826 1
#endif
@


1.14.2.8
log
@manually merge stuff cvs missed long ago
@
text
@@


1.14.2.9
log
@Sync the SMP branch with 3.3
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d64 1
a64 1
struct pte  *pmap_ptable;
d66 4
d85 1
a85 1
	pmap_t pted_pmap;
d89 1
a89 1
void print_pteg(pmap_t pm, vaddr_t va);
d96 2
a97 1
void pmap_page_ro(pmap_t pm, vaddr_t va);
d123 1
a123 1
void pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa,
d126 1
a126 1
void pmap_syncicache_user_virt(pmap_t pm, vaddr_t va);
d130 1
a130 1
void pmap_remove_pg(pmap_t pm, vaddr_t va);
d137 1
a137 1
void *pmap_steal_avail(size_t size, int align);
d140 1
a140 2
int pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t access_type,
    int exec_fault);
d142 1
a142 1
u_int32_t pmap_setusr(pmap_t pm, vaddr_t va);
a144 3
/* pte invalidation */
void pte_zap(struct pte *ptp, struct pte_desc *pted);

d200 2
a201 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d225 2
a226 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d256 2
a257 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d264 6
d289 2
d294 2
a295 1
 * HELPER FUNCTIONS 
a306 1

a323 1

d338 1
a339 1
	tlbsync();
a346 1

d355 4
a358 6
#define PTED_VA_PTEGIDX_M	0x07
#define PTED_VA_HID_M		0x08
#define PTED_VA_MANAGED_M	0x10
#define PTED_VA_WIRED_M		0x20
#define PTED_VA_EXEC_M		0x40

a363 1

a368 1

a373 1

a378 1

d413 1
d462 1
a462 1
	int need_sync = 0;
a463 2
	u_int8_t *pattr = NULL;
	int first_map = 0;
d465 1
a465 1
	/* MP - Acquire lock for this pmap */
d487 4
a490 3
	if (pvh != NULL)
		cache = PMAP_CACHE_WB; /* managed memory is cachable */
	else
d492 1
d496 1
a496 21
	if (pvh != NULL) {
		pattr = pmap_find_attr(pa); /* pattr only for managed mem */
		first_map = pmap_enter_pv(pted, pvh); /* only managed mem */
	}

	/* 
	 * We want to flush for executable pages which are not managed???
	 * Always flush for the first mapping if it is executable.
	 * If previous mappings exist, but this is the first EXE, sync.
	 */

	if (prot & VM_PROT_EXECUTE) {
		need_sync = 1;
		 if (pvh != NULL) {
			if (!first_map)
				need_sync =
				    (*pattr & (PTE_EXE >> ATTRSHIFT)) == 0;
			else if (pattr != NULL)
				*pattr = 0;
		}
	}
a504 26
        if (prot & VM_PROT_EXECUTE) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC) {
			pm->pm_sr[sn] &= ~SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
		if (pattr != NULL)
			*pattr |= (PTE_EXE >> ATTRSHIFT);
	} else {
		/*
		 * Should we be paranoid about writeable non-exec 
		 * mappings ? if so, clear the exec tag
		 */
		if ((prot & VM_PROT_WRITE) && (pattr != NULL))
			*pattr &= ~(PTE_EXE >> ATTRSHIFT);
	}

d508 1
a508 1
	if (need_sync)
d510 1
d520 1
a520 1
pmap_remove(pmap_t pm, vaddr_t va, vaddr_t endva)
d522 1
a522 27
	int i_sr, s_sr, e_sr;
	int i_vp1, s_vp1, e_vp1;
	int i_vp2, s_vp2, e_vp2;
	struct pmapvp *vp1;
	struct pmapvp *vp2;

	/* I suspect that if this loop were unrolled better 
	 * it would have better performance, testing i_sr and i_vp1
	 * in the middle loop seems excessive
	 */

	s_sr = VP_SR(va);
	e_sr = VP_SR(endva);
	for (i_sr = s_sr; i_sr <= e_sr; i_sr++) {
		vp1 = pm->pm_vp[i_sr];
		if (vp1 == NULL)
			continue;
		
		if (i_sr == s_sr)
			s_vp1 = VP_IDX1(va);
		else
			s_vp1 = 0;

		if (i_sr == e_sr)
			e_vp1 = VP_IDX1(endva);
		else
			e_vp1 = VP_IDX1_SIZE-1; 
d524 7
a530 24
		for (i_vp1 = s_vp1; i_vp1 <= e_vp1; i_vp1++) {
			vp2 = vp1->vp[i_vp1];
			if (vp2 == NULL)
				continue;

			if ((i_sr == s_sr) && (i_vp1 == s_vp1))
				s_vp2 = VP_IDX2(va);
			else
				s_vp2 = 0;

			if ((i_sr == e_sr) && (i_vp1 == e_vp1))
				e_vp2 = VP_IDX2(endva);
			else
				e_vp2 = VP_IDX2_SIZE; 

			for (i_vp2 = s_vp2; i_vp2 < e_vp2; i_vp2++) {
				if (vp2->vp[i_vp2] != NULL) {
					pmap_remove_pg(pm,
					    (i_sr << VP_SR_POS) |
					    (i_vp1 << VP_IDX1_POS) |
					    (i_vp2 << VP_IDX2_POS));
				}
			}
		}
d534 2
a535 1
 * remove a single mapping, notice that this code is O(1)
d538 1
a538 1
pmap_remove_pg(pmap_t pm, vaddr_t va)
a565 18
	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0) {
			pm->pm_sr[sn] |= SR_NOEXEC;
			
			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d571 1
a571 1
	if (pm != pmap_kernel())
d573 1
d592 1
a592 1
	pmap_t pm;
d601 4
a604 2
	if (pted && PTED_VALID(pted))
		pmap_kremove_pg(va); /* pted is reused */
d620 4
a623 3
		if (pvh != NULL)
			cache = PMAP_CACHE_WB; /* managed memory is cachable */
		else
d625 1
a638 17
        if (prot & VM_PROT_EXECUTE) {
		u_int sn = VP_SR(va);

        	pm->pm_exec[sn]++;
		if (pm->pm_sr[sn] & SR_NOEXEC) {
			pm->pm_sr[sn] &= ~SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d663 1
a663 1
	pmap_t pm;
d668 2
a669 1
	if (pted == NULL)
d671 5
a675 4

	if (!PTED_VALID(pted))
		return; /* not mapped */

a686 18
	if (pted->pted_va & PTED_VA_EXEC_M) {
		u_int sn = VP_SR(va);

		pted->pted_va &= ~PTED_VA_EXEC_M;
		pm->pm_exec[sn]--;
		if (pm->pm_exec[sn] == 0) {
			pm->pm_sr[sn] |= SR_NOEXEC;

			/* set the current sr if not kernel used segemnts
			 * and this pmap is current active pmap
			 */
			if (sn != USER_SR && sn != KERNEL_SR && curpm == pm)
				asm volatile ("mtsrin %0,%1"
				    :: "r"(pm->pm_sr[sn]),
				       "r"(sn << ADDR_SR_SHIFT) );
		}
	}

d702 1
a702 1
	for (len >>= PAGE_SHIFT; len >0; len--, va += PAGE_SIZE)
d704 1
d707 1
d709 1
a709 1
pte_zap(struct pte *ptp, struct pte_desc *pted)
d717 1
a717 1
		if (PTED_MANAGED(pted))
d720 1
a721 1

d731 2
a732 2
	pmap_t pm = pted->pted_pmap;
	struct pte *ptp;
d751 3
d757 1
d762 1
a762 1
pmap_fill_pte(pmap_t pm, vaddr_t va, paddr_t pa, struct pte_desc *pted,
d775 1
a775 1
	if ((cache == PMAP_CACHE_WB))
d777 1
a777 1
	else if ((cache == PMAP_CACHE_WT))
d779 1
a779 1
	else
d781 1
d783 1
a783 1
	if (prot & VM_PROT_WRITE)
d785 1
a785 1
	else
d787 1
a789 4

	if (prot & VM_PROT_EXECUTE)
		pted->pted_va  |= PTED_VA_EXEC_M;

d831 2
a832 2
		pmap_t pm = pted->pted_pmap;
		struct pte *ptp;
a847 3
		 *
		 * if we are not clearing bits, and have found all of the
		 * bits we want, we can stop
d851 1
a851 1
			bits |=	ptp->pte_lo & bit;
d860 1
a860 2
			} else if (bits == bit)
				break;
d864 2
a865 1
	if (clear)
d867 1
a867 1
	else
d869 1
a869 1

d881 1
a881 1
pmap_collect(pmap_t pm)
d896 1
a896 1
pmap_zero_page(struct vm_page *pg)
a897 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
a916 1

d921 1
a921 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a922 2
	paddr_t srcpa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dstpa = VM_PAGE_TO_PHYS(dstpg);
a935 1

d937 1
a937 1
pmap_pinit(pmap_t pm)
d970 1
a970 1
				pm->pm_sr[k] = (seg + k) | SR_NOEXEC;
d981 1
a981 1
pmap_t 
d984 1
a984 1
	pmap_t pmap;
d998 1
a998 1
pmap_reference(pmap_t pm)
d1010 1
a1010 1
pmap_destroy(pmap_t pm)
d1018 1
a1018 1
	if (refs > 0)
d1020 1
a1020 1

d1035 1
a1035 1
pmap_release(pmap_t pm)
d1041 1
a1041 1
	i = (pm->pm_sr[0] & SR_VSID) >> 4;
d1052 1
a1052 1
pmap_vp_destroy(pmap_t pm)
d1055 3
d1059 2
a1060 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d1064 1
a1064 1
		if (vp1 == NULL)
d1066 1
a1066 1

d1072 19
d1095 1
a1095 1
		pm->pm_vp[i] = NULL;
d1111 2
a1112 1
	for (mp = pmap_mem; mp->size !=0; mp++)
d1114 1
a1129 1

a1131 1

d1138 1
a1138 1
	for (mp = pmap_avail; mp->size !=0; mp++)
d1140 1
d1143 1
d1188 4
a1191 3
			if (end <= mp->start)
				break; /* region not present -??? */

d1196 3
a1198 2
				    i < pmap_cnt_avail;
				    i++) {
d1215 3
a1217 2
				    i > (mp - pmap_avail);
				    i--) {
d1235 1
a1235 1
			/* lengthen */
d1237 2
a1238 1
			    i--) {
d1275 1
a1275 1
	panic ("unable to allocate region with size %x align %x",
d1291 2
a1292 2
	struct pmapvp *vp1;
	struct pmapvp *vp2;
d1314 1
a1314 3
#ifndef  HTABENTS
#define HTABENTS 1024
#endif
d1316 2
d1321 1
d1329 4
d1368 1
a1368 1
		pmap_kernel()->pm_sr[i] = (KERNEL_SEG0 + i) | SR_NOEXEC;
d1370 1
a1370 1
		    :: "r"( KERNEL_SEG0 + i), "r"(i << ADDR_SR_SHIFT) );
d1373 1
a1373 1
	    :: "r"((u_int)pmap_ptable | (pmap_ptab_mask >> 10)));
d1420 1
a1420 1
pmap_extract(pmap_t pm, vaddr_t va, paddr_t *pa)
d1439 1
a1439 1
pmap_setusr(pmap_t pm, vaddr_t va)
d1448 1
a1448 1
	    : "=r" (oldsr): "n"(USER_SR));
d1450 1
a1450 1
	    :: "n"(USER_SR), "r"(sr));
d1458 1
a1458 1
	    :: "n"(USER_SR), "r"(sr));
d1479 1
a1479 1
		if (setfault(&env)) {
d1512 1
a1512 1
		if (setfault(&env)) {
d1550 2
a1551 2
		if (setfault(&env)) {
			if (done != NULL)
d1553 1
a1553 1

d1562 1
a1562 1
				if (done != NULL)
d1564 1
a1564 1

d1578 1
a1578 1
	if (done != NULL)
d1580 1
a1580 1

d1606 2
a1607 2
		if (setfault(&env)) {
			if (done != NULL)
d1609 1
a1609 1

d1618 1
a1618 1
				if (done != NULL)
d1620 1
a1620 1

d1634 1
a1634 1
	if (done != NULL)
d1636 1
a1636 1

d1646 1
a1646 1
pmap_syncicache_user_virt(pmap_t pm, vaddr_t va)
d1685 1
a1685 1
pmap_page_ro(pmap_t pm, vaddr_t va)
d1687 1
a1687 1
	struct pte *ptp;
d1692 1
a1692 1
	if (pted == NULL || !PTED_VALID(pted))
d1694 1
a1694 1

d1702 2
a1703 1
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d1770 1
a1770 1
pmap_protect(pmap_t pm, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
d1861 1
a1861 1
pte_spill_r(u_int32_t va, u_int32_t msr, u_int32_t dsisr, int exec_fault)
d1863 1
a1863 1
	pmap_t pm;
d1865 1
d1871 1
d1883 13
a1895 18
	if (pted == NULL) {
		return 0;
	}

	/* if the current mapping is RO and the access was a write
	 * we return 0
	 */
	if (!PTED_VALID(pted)) {
		return 0;
	} 
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
		/* write fault and we have a readonly mapping */
		return 0;
	}
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executeable page */
		return 0;
a1896 1
	pte_insert(pted);
d1898 1
a1898 1
	return 1;
d1902 1
a1902 1
pte_spill_v(pmap_t pm, u_int32_t va, u_int32_t dsisr, int exec_fault)
d1915 3
a1917 4
	if (!PTED_VALID(pted)) {
		return 0;
	}
	if ((dsisr & (1 << (31-6))) && (pted->pted_pte.pte_lo & 0x1)) {
d1919 3
a1921 5
		return 0;
	}
	if ((exec_fault != 0)
	    && ((pted->pted_va & PTED_VA_EXEC_M) == 0)) {
		/* attempted to execute non-executeable page */
d1951 2
a1952 1
	ptp = pmap_ptable + (idx ^ (PTED_HID(pted) ? pmap_ptab_mask : 0)) * 8;
d1972 1
a1972 1
		if (ptp[i].pte_hi & PTE_VALID)
d1974 1
a1974 1

d1988 1
a1988 1
		if (ptp[i].pte_hi & PTE_VALID)
d1990 2
a1991 1

d2008 5
d2030 1
a2030 2

	if (secondary)
d2032 1
a2032 1
	else
d2034 1
a2034 1

d2042 1
a2042 1
print_pteg(pmap_t pm, vaddr_t va)
d2131 1
a2131 1
	pmap_t pm;
d2158 3
a2160 3
	pmap_t pm;
	struct pmapvp *vp1;
	struct pmapvp *vp2;
@


1.14.2.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14.2.9 2003/03/27 23:42:35 niklas Exp $ */
d4 1
a4 2
 * Copyright (c) 2001, 2002 Dale Rahn.
 * All rights reserved.
d15 5
@


1.14.2.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d190 1
a190 1
 * Otherwise bad race conditions can appear.
d246 1
a246 1
 * Should this be called under splimp?
d382 1
a382 1
 * manipulate the physical to virtual translations for the entire system.
d384 1
a384 1
 * QUESTION: should all mapped memory be stored in PV tables? Or
d390 1
a390 1
 * mappings?
d405 1
a405 1
 * have too noticable unnecessary ram consumption.
d484 1
a484 1
		cache = PMAP_CACHE_WB; /* managed memory is cacheable */
d514 1
a514 1
	 * We were told to map the page, probably called from vm_fault,
d526 2
a527 2
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
d530 3
a532 2
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
d655 2
a656 2
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
d659 3
a661 2
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
d707 1
a707 1
		printf("pted not preallocated in pmap_kernel() va %lx pa %lx\n",
d717 1
a717 1
			cache = PMAP_CACHE_WB; /* managed memory is cacheable */
d727 1
a727 1
	 * We were told to map the page, probably called from vm_fault,
d740 2
a741 2
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
d744 3
a746 2
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
d804 2
a805 2
			/* set the current sr if not kernel used segments
			 * and this pmap is the currently active pmap
d808 3
a810 2
				ppc_mtsrin(pm->pm_sr[sn],
				     sn << ADDR_SR_SHIFT);
d1013 1
a1013 1
	 * Are there pool functions which could be called
d1079 1
a1079 1
	 * Try not to reuse pmap ids, to spread the hash table usage.
d1288 1
a1288 1
		 * Check if this region holds all of the region
d1467 2
a1468 1
		ppc_mtsrin(KERNEL_SEG0 + i, i << ADDR_SR_SHIFT);
d1482 1
a1482 1
	/* Ok we lose a few pages from this allocation, but hopefully
d1994 1
a1994 1
		/* attempted to execute non-executable page */
d2025 1
a2025 1
		/* attempted to execute non-executable page */
@


1.14.2.12
log
@Merge with the trunk
@
text
@a1945 20
void
pmap_proc_iflush(struct proc *p, vaddr_t addr, vsize_t len)
{
	paddr_t pa;
	vsize_t clen;

	while (len > 0) {
		clen = round_page(addr) - addr;
		if (clen > len)
			clen = len;

		if (pmap_extract(p->p_vmspace->vm_map.pmap, addr, &pa)) {
			syncicache((void *)pa, clen);
		}

		len -= clen;
		addr += clen;
	}
}

@


1.13
log
@autoconf.c:
	calculate delay time for delay() before it is acutally used.
	add support for md_diskconf come closer to supporting crashdumps,
	eventually this code should be un if 0 ed and supported.
	add the wd device as a supported device, fix some comments.
clock.c:
	support calculation of delay loop earlier, do the spin loop correcly,
	unsigned math on the lower half, not signed math.
conf.c:
	addd support for wd driver, block major 0, char major 11.
machdep.c:
	bus_space_map becomes a real function, not just inlined function.
	Support devices that are not mapped with bats (most still currently
	are mapped with bats,...). BAT mapping does not allow proper
	mapping of cachable devices.
	mapiodev HACK, NEEDS TO BE REMOVED. added for quicker import
	of BROKEN mac drivers. the drivers NEED to be rewritten in
	a busified manner. it would FIX all of the endian swabbing
	done by each driver. (Is that emphasized enough?)

	bus_space_(read|write)_raw_multi as functions, should these
	be turned into inline functions and put in bus.h?
ofw_machdep.c:
	removed extranious variable.
openfirm.c:
	telling openfirmware to "boot" will put the system
	in somewhat of a strange state, try reset-all, but that
	typically fails, therefore, try OF_exit before spinning.
pmap.c:
	support stealing memory from kernel address space so that
	mappings can be created before vm is initalized.
vm_machdep.c:
	maybe the meaning of removing this will later become obvious. ???
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/10/28 04:28:03 rahnds Exp $	*/
d36 2
d44 4
d446 7
d507 2
a508 2
	vm_size_t sz;
	vm_offset_t addr;
d510 4
d517 5
a521 1
	addr = (vm_offset_t)kmem_alloc(kernel_map, sz);
d529 11
d773 4
d779 1
d818 3
d822 1
d847 3
d851 1
d887 3
d891 1
d1419 22
@


1.12
log
@clean up a global pointer/array reference for OF_buf.
fix typo someone made.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1999/09/03 18:01:50 art Exp $	*/
d532 2
d545 1
a545 1
	*end = *start + SEGMENT_LENGTH;
d1003 1
a1003 1
	
@


1.11
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1999/07/05 20:56:26 rahnds Exp $	*/
d959 1
a959 1
pmap_enter(pm, va, pa, prot, wired, acces_type)
@


1.10
log
@Several changes here:
(Some of these changes are work in progress and may change more later)
locore.S:
        rearranged to remove most of the direct openfirmware references in
        the attempt to move all of the openfirmware pieces into ofw_ files.
        This could allow other firmware type to be supported easier. Also
        this keeps the openfirmware code grouped in the same files.

        OF_buf is now statically allocated in the data/bss section instead
        of allocated during initialization.

machdep.c:
        change the order of vm initialization, Still considering removing
        the BATs from use. instead of calls directly to ppc_exit and ppc_boot
        these are now called via a firmware function pointer structure.
        Add iMac recognition to systems
ofw_machdep.c:
        function pointer structure to allow different firmware to supply
        specific system functionality, normally startup and reset,
        including a hook to notify when bsd is about to go virtual,
        in case firmware calls need to act different after that time.

        Allow BSD to handle the virtual memory operations for openfirmware.
        this idea was copied from NetBSD macppc, It is not fully implemented,
        among other problems, openfirmware does not have a mechanism to
        add new mappings.

ofwreal.S:
        Major rewrite of the firmware call code, It still copies
        a portion of the stack, but now does not restore exeception vectors.
        Modified to be similar in idea to NetBSD macppc with BSD handling
        the openfirmware VM faults/TLB misses.
        This still needs to be reviewed, Should be possible to not require
        any stack copy.

opendev.c:
        OF_bus is not a pointer to the buffer, but is the buffer itself now.

openfirm.c:
        OF_bus is not a pointer to the buffer, but is the buffer itself now.
        Dont panic if OF_boot fails, OF_boot can be called by panic.
        instead print and the hang in a spin loop.

pmap.c:
        call the firmware function to get memory regions.
        Scale the PowerPC hash table size by size of real memory.
        Properly align the hash table based on the start, not just
        the size.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1999/03/22 02:41:21 rahnds Exp $	*/
d959 1
a959 1
pmap_enter(pm, va, pa, prot, wired)
d964 1
@


1.9
log
@Remove diagnotic that could (was frequently) causing crashes.
this whole pmap could use replacing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1999/01/11 05:11:54 millert Exp $	*/
d292 1
a292 1
	mem_regions(&mem, &avail);
d370 1
d373 10
d387 6
a392 1
		s = mp->size % HTABSIZE;
@


1.8
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1998/08/22 18:32:00 rahnds Exp $	*/
d930 1
d934 1
@


1.7
log
@Various changes to allow mixing of ofw drivers and real drivers.
NCR driver seems to work.
Major changes are isa can be child of pci or mainbus.
ofroot is child of mainbus not root.
ofw bus configured before pci bus
Note that if a pci device configures accessing of driver will crash
the system. they need to be exclusive.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 1998/03/04 10:58:16 niklas Exp $	*/
d932 1
a932 1
			panic("pmap_remove_pv: not on list\n");
@


1.6
log
@Adapt comments to reality
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 1997/01/21 17:00:10 rahnds Exp $	*/
d1321 28
@


1.5
log
@Fix problems pointed out by Andrew Cagney,
These didn't seem to have adverse effects, but were wrong.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 1997/01/09 21:19:02 rahnds Exp $	*/
d405 1
a405 1
	 * We cannot do pmap_steal_memory here,
@


1.4
log
@Now that ELF symbols are working (not leading '_'), dont put them
in for these symbols either.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 1997/01/09 03:07:16 rahnds Exp $	*/
d434 1
a434 1
			      :: "r"(i << ADDR_SR_SHFT), "r"(EMPTY_SEGMENT));
@


1.3
log
@support resident page count.
Attempt at least, pmap doesn't allow
for exact tracking.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 1996/12/28 06:22:14 rahnds Exp $	*/
d276 2
a277 2
int avail_start __asm__ ("_avail_start");
int avail_end __asm__ ("_avail_end");
@


1.2
log
@Adding OpenBSD tags to files.
@
text
@d1 1
a1 1
/*	$OpenBSD:$	*/
d876 2
a877 1
pmap_remove_pv(pteidx, va, pind, pte)
d909 1
a909 1
		} else
d911 8
d926 3
d957 2
d1032 2
a1033 1
				pmap_remove_pv(idx, va, pmap_page_index(ptp->pte_lo), ptp);
d1041 2
a1042 1
				pmap_remove_pv(idx, va, pmap_page_index(ptp->pte_lo), ptp);
d1051 2
a1052 1
				pmap_remove_pv(idx, va, pmap_page_index(po->po_pte.pte_lo),
d1295 1
a1295 1
				pmap_remove_pv(idx, va, pind, ptp);
d1304 1
a1304 1
				pmap_remove_pv(idx, va, pind, ptp);
d1313 1
a1313 1
				pmap_remove_pv(idx, va, pind, &po->po_pte);
@


1.1
log
@Initial revision
@
text
@d1 1
@


1.1.1.1
log
@Check-in of powerpc kernel support.
NOTE: This will not work until the other pieces are checked in.
This is primarily the NetBSD powerpc port, with modifications
to support ELF. 
@
text
@@

