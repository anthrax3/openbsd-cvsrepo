head	1.11;
access;
symbols
	SMP_SYNC_A:1.11
	SMP_SYNC_B:1.11
	UBC_SYNC_A:1.11
	UBC_SYNC_B:1.11
	OPENBSD_2_9_BASE:1.3
	OPENBSD_2_9:1.3.0.4
	OPENBSD_2_8:1.3.0.2
	OPENBSD_2_8_BASE:1.3
	OPENBSD_2_7:1.2.0.2
	OPENBSD_2_7_BASE:1.2
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.11
date	2001.09.01.15.44.20;	author drahn;	state dead;
branches;
next	1.10;

1.10
date	2001.08.18.21.59.48;	author drahn;	state Exp;
branches;
next	1.9;

1.9
date	2001.08.01.23.53.09;	author pvalchev;	state Exp;
branches;
next	1.8;

1.8
date	2001.07.30.14.16.00;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.07.25.13.25.32;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2001.06.27.04.37.20;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2001.06.24.23.29.33;	author drahn;	state Exp;
branches;
next	1.4;

1.4
date	2001.06.08.08.09.21;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2000.08.03.03.02.50;	author rahnds;	state Exp;
branches;
next	1.2;

1.2
date	2000.03.31.04.12.58;	author rahnds;	state Exp;
branches;
next	1.1;

1.1
date	2000.03.20.07.05.52;	author rahnds;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2000.03.24.09.08.43;	author niklas;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2001.05.14.21.36.56;	author niklas;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2001.07.04.10.22.55;	author niklas;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2001.10.31.03.07.55;	author nate;	state dead;
branches;
next	1.1.2.5;

1.1.2.5
date	2001.11.13.21.04.16;	author niklas;	state Exp;
branches;
next	1.1.2.6;

1.1.2.6
date	2001.11.13.22.14.34;	author niklas;	state dead;
branches;
next	;


desc
@@


1.11
log
@The "powerpc" port which has supported the newer Apple Macintosh powerpc based
is being renamed to macppc. This is to allow sharing of common code
between different powerpc base platforms.

Most of the work involved in the renaming process was performed by miod@@

Files moved from powerpc/powerpc to macppc/macppc

This moves hardware specific files from the common directory to the
platform specific directory. This leaves common files.
With this change all of the debugger (db_) files have been moved to
the platform specific directory. The debugger should be reconsidered
and commonized.
@
text
@/*	$OpenBSD: dma.c,v 1.10 2001/08/18 21:59:48 drahn Exp $	*/
/*	$NetBSD: machdep.c,v 1.214 1996/11/10 03:16:17 thorpej Exp $	*/

/*-
 * Copyright (c) 1996, 1997 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/proc.h>
#include <sys/user.h>
#include <sys/extent.h>
#include <sys/buf.h>
#include <sys/device.h>
#include <sys/systm.h>
#include <sys/conf.h>
#include <sys/file.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/mount.h>

#include <vm/vm.h>
#include <vm/vm_kern.h>
#include <uvm/uvm.h>
#include <uvm/uvm_page.h>

#include <machine/bus.h>
int _dmamem_alloc_range( bus_dma_tag_t t, bus_size_t size,
	bus_size_t alignment, bus_size_t boundary, bus_dma_segment_t *segs,
	int nsegs, int *rsegs, int flags, vm_offset_t low, vm_offset_t high);

/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
int
_dmamap_create(t, size, nsegments, maxsegsz, boundary, flags, dmamp)
	bus_dma_tag_t t;
	bus_size_t size;
	int nsegments;
	bus_size_t maxsegsz;
	bus_size_t boundary;
	int flags;
	bus_dmamap_t *dmamp;
{
	struct powerpc_bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
	mapsize = sizeof(struct powerpc_bus_dmamap) +
	    (sizeof(bus_dma_segment_t) * (nsegments - 1));
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
		return (ENOMEM);

	bzero(mapstore, mapsize);
	map = (struct powerpc_bus_dmamap *)mapstore;
	map->_dm_size = size;
	map->_dm_segcnt = nsegments;
	map->_dm_maxsegsz = maxsegsz;
	map->_dm_boundary = boundary;
	map->_dm_flags = flags & ~(BUS_DMA_WAITOK|BUS_DMA_NOWAIT);
	map->dm_nsegs = 0;		/* no valid mappings */

	*dmamp = map;
	return (0);
}

/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
void
_dmamap_destroy(t, map)
	bus_dma_tag_t t;
	bus_dmamap_t map;
{

	free(map, M_DEVBUF);
}

/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
int
_dmamap_load(t, map, buf, buflen, p, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	void *buf;
	bus_size_t buflen;
	struct proc *p;
	int flags;
{
	bus_size_t sgsize;
	bus_addr_t curaddr, lastaddr, baddr, bmask;
	caddr_t vaddr = buf;
	int first, seg;
	pmap_t pmap;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = ~0;		/* XXX gcc */
	bmask  = ~(map->_dm_boundary - 1);

	for (first = 1, seg = 0; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 */
		pmap_extract(pmap, (vm_offset_t)vaddr, (paddr_t *)&curaddr);

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = PAGE_SIZE - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = (curaddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - curaddr))
				sgsize = (baddr - curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr = curaddr;
			map->dm_segs[seg].ds_len = sgsize;
			first = 0;
		} else {
			if (curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			     (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     (curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr = curaddr;
				map->dm_segs[seg].ds_len = sgsize;
			}
		}

		lastaddr = curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */

	map->dm_nsegs = seg + 1;
	return (0);
}

/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
int
_dmamap_load_mbuf(t, map, m, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct mbuf *m;
	int flags;
{

	panic("_bus_dmamap_load: not implemented");
}

/*
 * Like _bus_dmamap_load(), but for uios.
 */
int
_dmamap_load_uio(t, map, uio, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	struct uio *uio;
	int flags;
{

	panic("_bus_dmamap_load_uio: not implemented");
}

/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
int
_dmamap_load_raw(t, map, segs, nsegs, size, flags)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dma_segment_t *segs;
	int nsegs;
	bus_size_t size;
	int flags;
{

	panic("_bus_dmamap_load_raw: not implemented");
}

/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
void
_dmamap_unload(t, map)
	bus_dma_tag_t t;
	bus_dmamap_t map;
{

	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_nsegs = 0;
}

/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 */
void
_dmamap_sync(t, map, op)
	bus_dma_tag_t t;
	bus_dmamap_t map;
	bus_dmasync_op_t op;
{

	/* Nothing to do here. */
}

/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
int
_dmamem_alloc(t, size, alignment, boundary, segs, nsegs, rsegs, flags)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
{
	return (_dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags, 0, 0xf0000000));
}

/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
void
_dmamem_free(t, segs, nsegs)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
{
	vm_page_t m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE(addr);
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

	uvm_pglistfree(&mlist);
}

/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
int
_dmamem_map(t, segs, nsegs, size, kvap, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	size_t size;
	caddr_t *kvap;
	int flags;
{
	vm_offset_t va;
	bus_addr_t addr;
	int curseg;

	size = round_page(size);
	va = uvm_km_valloc(kmem_map, size);
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE, va += PAGE_SIZE, size -= PAGE_SIZE) {
			if (size == 0)
				panic("_bus_dmamem_map: size botch");
			pmap_enter(pmap_kernel(), va, addr,
			    VM_PROT_READ | VM_PROT_WRITE,
			    VM_PROT_READ | VM_PROT_WRITE | PMAP_WIRED);
		}
	}

	return (0);
}

/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
void
_dmamem_unmap(t, kva, size)
	bus_dma_tag_t t;
	caddr_t kva;
	size_t size;
{

#ifdef DIAGNOSTIC
	if ((u_long)kva & PGOFSET)
		panic("_bus_dmamem_unmap");
#endif

	size = round_page(size);
	uvm_km_free(kmem_map, (vm_offset_t)kva, size);
}

/*
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
paddr_t
_dmamem_mmap(t, segs, nsegs, off, prot, flags)
	bus_dma_tag_t t;
	bus_dma_segment_t *segs;
	int nsegs;
	off_t off;
	int prot, flags;
{
	int i;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PGOFSET)
			panic("_bus_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PGOFSET)
			panic("_bus_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PGOFSET)
			panic("_bus_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return (powerpc_btop((caddr_t)segs[i].ds_addr + off));
	}

	/* Page not found. */
	return (-1);
}

/**********************************************************************
 * DMA utility functions
 **********************************************************************/

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
int
_dmamem_alloc_range(t, size, alignment, boundary, segs, nsegs, rsegs,
    flags, low, high)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
	vm_offset_t low;
	vm_offset_t high;
{
	vm_offset_t curaddr, lastaddr;
	vm_page_t m;
	struct pglist mlist;
	int curseg, error;

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate pages from the VM system.
	 */
	TAILQ_INIT(&mlist);
	error = uvm_pglistalloc(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = mlist.tqh_first;
	curseg = 0;
	lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
	segs[curseg].ds_len = PAGE_SIZE;
	m = m->pageq.tqe_next;

	for (; m != NULL; m = m->pageq.tqe_next) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curaddr < low || curaddr >= high) {
			printf("vm_page_alloc_memory returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("dmamem_alloc_range");
		}
#endif
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
}
@


1.10
log
@Some NBPG to PAGE_SIZE conversions.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.9 2001/08/01 23:53:09 pvalchev Exp $	*/
@


1.9
log
@make this compile. ok drahn
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.8 2001/07/30 14:16:00 art Exp $	*/
d170 1
a170 1
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
d373 1
a373 1
		    addr += NBPG, va += NBPG, size -= NBPG) {
@


1.8
log
@Change:
int bus_dmamem_mmap(bus_dma_tag_t, bus_dma_segment_t, int, int, int, int);
to:
paddr_t bus_dmamem_mmap(bus_dma_tag_t, bus_dma_segment_t, int, int, off_t, int);

To allow mmaping offsets larger than INT_MAX. And to simply make more sense.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.7 2001/07/25 13:25:32 art Exp $	*/
d413 1
a413 1
	int nsegs, 
@


1.7
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.6 2001/06/27 04:37:20 art Exp $	*/
d409 1
a409 1
int
d413 3
a415 1
	int nsegs, off, prot, flags;
@


1.6
log
@kill old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.5 2001/06/24 23:29:33 drahn Exp $	*/
d377 2
a378 2
			    VM_PROT_READ | VM_PROT_WRITE, TRUE,
			    VM_PROT_READ | VM_PROT_WRITE);
@


1.5
log
@-Warn cleanups for powerpc, still not done.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.4 2001/06/08 08:09:21 art Exp $	*/
a55 1
#ifdef UVM
a57 2
#else
#endif
a342 1
#if defined(UVM)
a343 3
#else
	vm_page_free_memory(&mlist);
#endif
a363 1
#if defined(UVM)
a364 3
#else
	va = kmem_alloc_pageable(kmem_map, size);
#endif
a401 1
#if defined(UVM)
a402 3
#else
	kmem_free(kmem_map, (vm_offset_t)kva, size);
#endif
a470 1
#if defined(UVM)
a472 4
#else
	error = vm_page_alloc_memory(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
#endif
@


1.4
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.3 2000/08/03 03:02:50 rahnds Exp $	*/
d63 3
a313 2
	extern int avail_end;

@


1.3
log
@Fixes to bus dmamem code.
(Fixed by replacing the code with the version from the i386 port.)
@
text
@d1 1
a1 1
/*	$OpenBSD: machdep.c,v 1.135 2000/07/06 00:59:00 todd Exp $	*/
d165 1
a165 1
		curaddr = (bus_addr_t)pmap_extract(pmap, (vm_offset_t)vaddr);
@


1.2
log
@implement dmamap_load (stolen from i386) for powerpc.
This is required for USB support.
(with this a a bit more the root hub configures).
@
text
@d1 2
a2 1
/*	$OpenBSD$	*/
d4 2
a5 2
/*
 * Copyright (c) 1998 Michael Shalayeff
d8 4
d22 5
a26 3
 *	This product includes software developed by Michael Shalayeff.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
d28 11
a38 10
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
d64 4
d69 2
a70 2
_dmamap_create(v, size, nsegments, maxsegsz, boundary, flags, dmamp)
	void *v;
d78 3
a80 2
	register struct powerpc_bus_dmamap *map;
	register size_t mapsize;
d82 12
d96 2
a97 3
	MALLOC(map, struct powerpc_bus_dmamap *, mapsize, M_DEVBUF,
		(flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK);
	if (!map)
d100 2
a101 1
	bzero(map, mapsize);
d107 1
d113 4
d118 2
a119 2
_dmamap_destroy(v, map)
	void *v;
d122 1
d132 1
a132 1
	void * t;
d149 1
d222 3
d226 2
a227 2
_dmamap_load_mbuf(v, map, m, flags)
	void *v;
d232 2
a233 1
	panic("_dmamap_load_mbuf: not implemented");
d236 3
d240 2
a241 2
_dmamap_load_uio(v, map, uio, flags)
	void *v;
d246 2
a247 1
	panic("_dmamap_load_uio: not implemented");
d250 4
d255 2
a256 2
_dmamap_load_raw(v, map, segs, nsegs, size, flags)
	void *v;
d263 2
a264 1
	panic("_dmamap_load_raw: not implemented");
d267 4
d272 2
a273 2
_dmamap_unload(v, map)
	void *v;
d276 6
a281 1
	panic("_dmamap_unload: not implemented");
d284 4
d289 2
a290 2
_dmamap_sync(v, map, ops)
	void *v;
d292 1
a292 1
	bus_dmasync_op_t ops;
d294 2
a295 3
#if 0
	__asm __volatile ("syncdma");
#endif
d298 4
d303 2
a304 2
_dmamem_alloc(v, size, alignment, boundary, segs, nsegs, rsegs, flags)
	void *v;
d311 1
a311 31
	vaddr_t va;
	paddr_t spa, epa;

	size = round_page(size);

#if defined(UVM)
	va = uvm_pagealloc_contig(size, VM_MIN_KERNEL_ADDRESS,
					VM_MAX_KERNEL_ADDRESS, NBPG);
#else
# if 0
	vm_page_alloc_memory(size, VM_MIN_KERNEL_ADDRESS,
		VM_MAX_KERNEL_ADDRESS,
	    alignment, boundary, (void *)&va, nsegs, (flags & BUS_DMA_NOWAIT));
# else
	va = kmem_alloc_wait(phys_map, NBPG); /* XXX */
# endif
#endif
	if (va == NULL)
		return (ENOMEM);

	segs[0].ds_addr = va;
	segs[0].ds_len = size;
	*rsegs = nsegs;

#if 0
	/* XXX for now */
	for (epa = size + (spa = kvtop((caddr_t)va)); spa < epa; spa += NBPG)
		pmap_changebit(spa, TLB_UNCACHEABLE, 0);
#endif

	return 0;
d313 2
d317 4
d322 2
a323 2
_dmamem_free(v, segs, nsegs)
	void *v;
d327 20
a346 2
#if defined (UVM)
	uvm_km_free(kmem_map, segs[0].ds_addr, M_DEVBUF);
d348 1
a348 1
	kmem_free(kernel_map, segs[0].ds_addr, segs[0].ds_len);
d352 4
d357 2
a358 2
_dmamem_map(v, segs, nsegs, size, kvap, flags)
	void *v;
d365 28
a392 2
	*kvap = (caddr_t)segs[0].ds_addr;
	return 0;
d395 4
d400 2
a401 2
_dmamem_unmap(v, kva, size)
	void *v;
d405 12
d419 4
d424 2
a425 2
_dmamem_mmap(v, segs, nsegs, off, prot, flags)
	void *v;
d429 22
a450 1
	panic("_dmamem_mmap: not implemented");
d453 8
d462 29
a490 7
dma_cachectl(p, size)
	caddr_t p;
	int size;
{
#if 0
	fdcache(HPPA_SID_KERNEL, (vaddr_t)p, size);
	sync_caches();
d492 35
a526 1
	return 0;
@


1.1
log
@add first version of bus_dma for powerpc.
changes to trap handler to print out better information for jump to 0 bugs.
changes to pmap.c and machdep.c to debug a duplicate memory region
bug occasionally observed on imac with compressed kernels.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.5 1999/09/10 17:00:03 mickey Exp $	*/
d95 4
d100 2
a101 2
_dmamap_load(v, map, addr, size, p, flags)
	void *v;
d103 2
a104 2
	void *addr;
	bus_size_t size;
d108 80
a187 1
	panic("_dmamap_load: not implemented");
d259 1
d263 3
d272 1
a272 1
	*rsegs = 1;
@


1.1.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.1.2.2
log
@Continue the aborted merge of current just before 2.9 was cut into the
SMP branch.  Note that this will not make any progress of SMP functionality,
it is just merging of new code from the trunk into the old branch.
Please do not ask me questions about SMP status because of this mail,
instead go read the archives of smp@@openbsd.org, where I mailed about
these commits some week ago.  Another note: I am doing this in chunks now,
so as to not lock too much of the tree for long times
@
text
@a1 1
/*	$NetBSD: machdep.c,v 1.214 1996/11/10 03:16:17 thorpej Exp $	*/
d3 2
a4 2
/*-
 * Copyright (c) 1996, 1997 The NetBSD Foundation, Inc.
a6 4
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.
 *
d17 3
a19 5
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
d21 10
a30 11
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
a55 4
/*
 * Common function for DMA map creation.  May be called by bus-specific
 * DMA map creation functions.
 */
d57 2
a58 2
_dmamap_create(t, size, nsegments, maxsegsz, boundary, flags, dmamp)
	bus_dma_tag_t t;
d66 3
a68 16
	struct powerpc_bus_dmamap *map;
	void *mapstore;
	size_t mapsize;

	/*
	 * Allocate and initialize the DMA map.  The end of the map
	 * is a variable-sized array of segments, so we allocate enough
	 * room for them in one shot.
	 *
	 * Note we don't preserve the WAITOK or NOWAIT flags.  Preservation
	 * of ALLOCNOW notifies others that we've reserved these resources,
	 * and they are not to be freed.
	 *
	 * The bus_dmamap_t includes one bus_dma_segment_t, hence
	 * the (nsegments - 1).
	 */
d71 3
a73 2
	if ((mapstore = malloc(mapsize, M_DEVBUF,
	    (flags & BUS_DMA_NOWAIT) ? M_NOWAIT : M_WAITOK)) == NULL)
d76 1
a76 2
	bzero(mapstore, mapsize);
	map = (struct powerpc_bus_dmamap *)mapstore;
a81 1
	map->dm_nsegs = 0;		/* no valid mappings */
a86 4
/*
 * Common function for DMA map destruction.  May be called by bus-specific
 * DMA map destruction functions.
 */
d88 2
a89 2
_dmamap_destroy(t, map)
	bus_dma_tag_t t;
a91 1

a94 4
/*
 * Common function for loading a DMA map with a linear buffer.  May
 * be called by bus-specific DMA map load functions.
 */
d96 2
a97 2
_dmamap_load(t, map, buf, buflen, p, flags)
	bus_dma_tag_t t;
d99 2
a100 2
	void *buf;
	bus_size_t buflen;
d104 1
a104 81
	bus_size_t sgsize;
	bus_addr_t curaddr, lastaddr, baddr, bmask;
	caddr_t vaddr = buf;
	int first, seg;
	pmap_t pmap;

	/*
	 * Make sure that on error condition we return "no valid mappings".
	 */
	map->dm_nsegs = 0;

	if (buflen > map->_dm_size)
		return (EINVAL);

	if (p != NULL)
		pmap = p->p_vmspace->vm_map.pmap;
	else
		pmap = pmap_kernel();

	lastaddr = ~0;		/* XXX gcc */
	bmask  = ~(map->_dm_boundary - 1);

	for (first = 1, seg = 0; buflen > 0; ) {
		/*
		 * Get the physical address for this segment.
		 */
		curaddr = (bus_addr_t)pmap_extract(pmap, (vm_offset_t)vaddr);

		/*
		 * Compute the segment size, and adjust counts.
		 */
		sgsize = NBPG - ((u_long)vaddr & PGOFSET);
		if (buflen < sgsize)
			sgsize = buflen;

		/*
		 * Make sure we don't cross any boundaries.
		 */
		if (map->_dm_boundary > 0) {
			baddr = (curaddr + map->_dm_boundary) & bmask;
			if (sgsize > (baddr - curaddr))
				sgsize = (baddr - curaddr);
		}

		/*
		 * Insert chunk into a segment, coalescing with
		 * previous segment if possible.
		 */
		if (first) {
			map->dm_segs[seg].ds_addr = curaddr;
			map->dm_segs[seg].ds_len = sgsize;
			first = 0;
		} else {
			if (curaddr == lastaddr &&
			    (map->dm_segs[seg].ds_len + sgsize) <=
			     map->_dm_maxsegsz &&
			     (map->_dm_boundary == 0 ||
			     (map->dm_segs[seg].ds_addr & bmask) ==
			     (curaddr & bmask)))
				map->dm_segs[seg].ds_len += sgsize;
			else {
				if (++seg >= map->_dm_segcnt)
					break;
				map->dm_segs[seg].ds_addr = curaddr;
				map->dm_segs[seg].ds_len = sgsize;
			}
		}

		lastaddr = curaddr + sgsize;
		vaddr += sgsize;
		buflen -= sgsize;
	}

	/*
	 * Did we fit?
	 */
	if (buflen != 0)
		return (EFBIG);		/* XXX better return value here? */

	map->dm_nsegs = seg + 1;
	return (0);
a106 3
/*
 * Like _bus_dmamap_load(), but for mbufs.
 */
d108 2
a109 2
_dmamap_load_mbuf(t, map, m, flags)
	bus_dma_tag_t t;
d114 1
a114 2

	panic("_bus_dmamap_load: not implemented");
a116 3
/*
 * Like _bus_dmamap_load(), but for uios.
 */
d118 2
a119 2
_dmamap_load_uio(t, map, uio, flags)
	bus_dma_tag_t t;
d124 1
a124 2

	panic("_bus_dmamap_load_uio: not implemented");
a126 4
/*
 * Like _bus_dmamap_load(), but for raw memory allocated with
 * bus_dmamem_alloc().
 */
d128 2
a129 2
_dmamap_load_raw(t, map, segs, nsegs, size, flags)
	bus_dma_tag_t t;
d136 1
a136 2

	panic("_bus_dmamap_load_raw: not implemented");
a138 4
/*
 * Common function for unloading a DMA map.  May be called by
 * bus-specific DMA map unload functions.
 */
d140 2
a141 2
_dmamap_unload(t, map)
	bus_dma_tag_t t;
d144 1
a144 6

	/*
	 * No resources to free; just mark the mappings as
	 * invalid.
	 */
	map->dm_nsegs = 0;
a146 4
/*
 * Common function for DMA map synchronization.  May be called
 * by bus-specific DMA map synchronization functions.
 */
d148 2
a149 2
_dmamap_sync(t, map, op)
	bus_dma_tag_t t;
d151 1
a151 1
	bus_dmasync_op_t op;
d153 3
a155 2

	/* Nothing to do here. */
a157 4
/*
 * Common function for DMA-safe memory allocation.  May be called
 * by bus-specific DMA memory allocation functions.
 */
d159 2
a160 2
_dmamem_alloc(t, size, alignment, boundary, segs, nsegs, rsegs, flags)
	bus_dma_tag_t t;
d167 27
a193 1
	extern int avail_end;
a194 2
	return (_dmamem_alloc_range(t, size, alignment, boundary,
	    segs, nsegs, rsegs, flags, 0, 0xf0000000));
a196 4
/*
 * Common function for freeing DMA-safe memory.  May be called by
 * bus-specific DMA memory free functions.
 */
d198 2
a199 2
_dmamem_free(t, segs, nsegs)
	bus_dma_tag_t t;
d203 2
a204 20
	vm_page_t m;
	bus_addr_t addr;
	struct pglist mlist;
	int curseg;

	/*
	 * Build a list of pages to free back to the VM system.
	 */
	TAILQ_INIT(&mlist);
	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += PAGE_SIZE) {
			m = PHYS_TO_VM_PAGE(addr);
			TAILQ_INSERT_TAIL(&mlist, m, pageq);
		}
	}

#if defined(UVM)
	uvm_pglistfree(&mlist);
d206 1
a206 1
	vm_page_free_memory(&mlist);
a209 4
/*
 * Common function for mapping DMA-safe memory.  May be called by
 * bus-specific DMA memory map functions.
 */
d211 2
a212 2
_dmamem_map(t, segs, nsegs, size, kvap, flags)
	bus_dma_tag_t t;
d219 2
a220 28
	vm_offset_t va;
	bus_addr_t addr;
	int curseg;

	size = round_page(size);
#if defined(UVM)
	va = uvm_km_valloc(kmem_map, size);
#else
	va = kmem_alloc_pageable(kmem_map, size);
#endif
	if (va == 0)
		return (ENOMEM);

	*kvap = (caddr_t)va;

	for (curseg = 0; curseg < nsegs; curseg++) {
		for (addr = segs[curseg].ds_addr;
		    addr < (segs[curseg].ds_addr + segs[curseg].ds_len);
		    addr += NBPG, va += NBPG, size -= NBPG) {
			if (size == 0)
				panic("_bus_dmamem_map: size botch");
			pmap_enter(pmap_kernel(), va, addr,
			    VM_PROT_READ | VM_PROT_WRITE, TRUE,
			    VM_PROT_READ | VM_PROT_WRITE);
		}
	}

	return (0);
a222 4
/*
 * Common function for unmapping DMA-safe memory.  May be called by
 * bus-specific DMA memory unmapping functions.
 */
d224 2
a225 2
_dmamem_unmap(t, kva, size)
	bus_dma_tag_t t;
a228 12

#ifdef DIAGNOSTIC
	if ((u_long)kva & PGOFSET)
		panic("_bus_dmamem_unmap");
#endif

	size = round_page(size);
#if defined(UVM)
	uvm_km_free(kmem_map, (vm_offset_t)kva, size);
#else
	kmem_free(kmem_map, (vm_offset_t)kva, size);
#endif
a230 4
/*
 * Common functin for mmap(2)'ing DMA-safe memory.  May be called by
 * bus-specific DMA mmap(2)'ing functions.
 */
d232 2
a233 2
_dmamem_mmap(t, segs, nsegs, off, prot, flags)
	bus_dma_tag_t t;
d237 1
a237 22
	int i;

	for (i = 0; i < nsegs; i++) {
#ifdef DIAGNOSTIC
		if (off & PGOFSET)
			panic("_bus_dmamem_mmap: offset unaligned");
		if (segs[i].ds_addr & PGOFSET)
			panic("_bus_dmamem_mmap: segment unaligned");
		if (segs[i].ds_len & PGOFSET)
			panic("_bus_dmamem_mmap: segment size not multiple"
			    " of page size");
#endif
		if (off >= segs[i].ds_len) {
			off -= segs[i].ds_len;
			continue;
		}

		return (powerpc_btop((caddr_t)segs[i].ds_addr + off));
	}

	/* Page not found. */
	return (-1);
a239 8
/**********************************************************************
 * DMA utility functions
 **********************************************************************/

/*
 * Allocate physical memory from the given physical address range.
 * Called by DMA-safe memory allocation methods.
 */
d241 3
a243 10
_dmamem_alloc_range(t, size, alignment, boundary, segs, nsegs, rsegs,
    flags, low, high)
	bus_dma_tag_t t;
	bus_size_t size, alignment, boundary;
	bus_dma_segment_t *segs;
	int nsegs;
	int *rsegs;
	int flags;
	vm_offset_t low;
	vm_offset_t high;
d245 3
a247 18
	vm_offset_t curaddr, lastaddr;
	vm_page_t m;
	struct pglist mlist;
	int curseg, error;

	/* Always round the size. */
	size = round_page(size);

	/*
	 * Allocate pages from the VM system.
	 */
	TAILQ_INIT(&mlist);
#if defined(UVM)
	error = uvm_pglistalloc(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
#else
	error = vm_page_alloc_memory(size, low, high,
	    alignment, boundary, &mlist, nsegs, (flags & BUS_DMA_NOWAIT) == 0);
d249 1
a249 35
	if (error)
		return (error);

	/*
	 * Compute the location, size, and number of segments actually
	 * returned by the VM code.
	 */
	m = mlist.tqh_first;
	curseg = 0;
	lastaddr = segs[curseg].ds_addr = VM_PAGE_TO_PHYS(m);
	segs[curseg].ds_len = PAGE_SIZE;
	m = m->pageq.tqe_next;

	for (; m != NULL; m = m->pageq.tqe_next) {
		curaddr = VM_PAGE_TO_PHYS(m);
#ifdef DIAGNOSTIC
		if (curaddr < low || curaddr >= high) {
			printf("vm_page_alloc_memory returned non-sensical"
			    " address 0x%lx\n", curaddr);
			panic("dmamem_alloc_range");
		}
#endif
		if (curaddr == (lastaddr + PAGE_SIZE))
			segs[curseg].ds_len += PAGE_SIZE;
		else {
			curseg++;
			segs[curseg].ds_addr = curaddr;
			segs[curseg].ds_len = PAGE_SIZE;
		}
		lastaddr = curaddr;
	}

	*rsegs = curseg + 1;

	return (0);
@


1.1.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.1.2.2 2001/05/14 21:36:56 niklas Exp $	*/
d56 1
d59 2
a62 3
int _dmamem_alloc_range( bus_dma_tag_t t, bus_size_t size,
	bus_size_t alignment, bus_size_t boundary, bus_dma_segment_t *segs,
	int nsegs, int *rsegs, int flags, vm_offset_t low, vm_offset_t high);
d165 1
a165 1
		pmap_extract(pmap, (vm_offset_t)vaddr, (paddr_t *)&curaddr);
d311 2
d345 1
d347 3
d370 1
d372 3
d412 1
d414 3
d485 1
d488 4
@


1.1.2.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.1.2.3 2001/07/04 10:22:55 niklas Exp $	*/
@


1.1.2.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.1.2.6
log
@repair
@
text
@d1 1
a1 1
/*	$OpenBSD: dma.c,v 1.1.2.5 2001/11/13 21:04:16 niklas Exp $	*/
@


