head	1.15;
access;
symbols
	OPENBSD_6_1:1.15.0.6
	OPENBSD_6_1_BASE:1.15
	OPENBSD_6_0:1.15.0.4
	OPENBSD_6_0_BASE:1.15
	OPENBSD_5_9:1.15.0.2
	OPENBSD_5_9_BASE:1.15
	OPENBSD_5_8:1.14.0.4
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.13.0.2
	OPENBSD_5_7_BASE:1.13
	OPENBSD_5_6:1.13.0.4
	OPENBSD_5_6_BASE:1.13
	OPENBSD_5_5:1.12.0.4
	OPENBSD_5_5_BASE:1.12
	OPENBSD_5_4:1.11.0.10
	OPENBSD_5_4_BASE:1.11
	OPENBSD_5_3:1.11.0.8
	OPENBSD_5_3_BASE:1.11
	OPENBSD_5_2:1.11.0.6
	OPENBSD_5_2_BASE:1.11
	OPENBSD_5_1_BASE:1.11
	OPENBSD_5_1:1.11.0.4
	OPENBSD_5_0:1.11.0.2
	OPENBSD_5_0_BASE:1.11
	OPENBSD_4_9:1.9.0.2
	OPENBSD_4_9_BASE:1.9
	OPENBSD_4_8:1.8.0.2
	OPENBSD_4_8_BASE:1.8
	OPENBSD_4_7:1.7.0.2
	OPENBSD_4_7_BASE:1.7
	OPENBSD_4_6:1.4.0.4
	OPENBSD_4_6_BASE:1.4
	OPENBSD_4_5:1.1.0.8
	OPENBSD_4_5_BASE:1.1
	OPENBSD_4_4:1.1.0.6
	OPENBSD_4_4_BASE:1.1
	OPENBSD_4_3:1.1.0.4
	OPENBSD_4_3_BASE:1.1
	OPENBSD_4_2:1.1.0.2
	OPENBSD_4_2_BASE:1.1;
locks; strict;
comment	@ * @;


1.15
date	2015.09.20.19.19.03;	author kettenis;	state Exp;
branches;
next	1.14;
commitid	iDeQZkkbQR5aFK2F;

1.14
date	2015.05.02.10.59.47;	author dlg;	state Exp;
branches;
next	1.13;
commitid	cdsn2FXYvekLGlpq;

1.13
date	2014.06.17.15.43.27;	author guenther;	state Exp;
branches;
next	1.12;
commitid	SCQD2E633OoqoQMa;

1.12
date	2014.01.30.15.18.51;	author kettenis;	state Exp;
branches;
next	1.11;

1.11
date	2011.04.20.16.10.53;	author jsing;	state Exp;
branches;
next	1.10;

1.10
date	2011.04.03.18.46.40;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2010.09.28.20.27.54;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2010.06.26.00.45.05;	author jsing;	state Exp;
branches;
next	1.7;

1.7
date	2010.01.10.04.07.18;	author kettenis;	state Exp;
branches;
next	1.6;

1.6
date	2009.12.29.15.01.59;	author jsing;	state Exp;
branches;
next	1.5;

1.5
date	2009.08.13.13.24.55;	author weingart;	state Exp;
branches;
next	1.4;

1.4
date	2009.04.27.21.48.56;	author kettenis;	state Exp;
branches;
next	1.3;

1.3
date	2009.04.26.04.44.33;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2009.04.25.20.14.42;	author weingart;	state Exp;
branches;
next	1.1;

1.1
date	2007.05.05.12.00.55;	author miod;	state Exp;
branches;
next	;


desc
@@


1.15
log
@Put memory barriers in the appropriate place.

Discssed with geunther@@, tested by landry@@
@
text
@/*	$OpenBSD: mutex.c,v 1.14 2015/05/02 10:59:47 dlg Exp $	*/

/*
 * Copyright (c) 2004 Artur Grabowski <art@@openbsd.org>
 * All rights reserved. 
 *
 * Redistribution and use in source and binary forms, with or without 
 * modification, are permitted provided that the following conditions 
 * are met: 
 *
 * 1. Redistributions of source code must retain the above copyright 
 *    notice, this list of conditions and the following disclaimer. 
 * 2. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission. 
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
 * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
 * AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL
 * THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL  DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
 */

#include <sys/param.h>
#include <sys/mutex.h>
#include <sys/systm.h>
#include <sys/atomic.h>

#include <machine/intr.h>

#include <ddb/db_output.h>

int __mtx_enter_try(struct mutex *);

#ifdef MULTIPROCESSOR
/* Note: lock must be 16-byte aligned. */
#define __mtx_lock(mtx) ((int *)(((vaddr_t)mtx->mtx_lock + 0xf) & ~0xf))
#endif

void
__mtx_init(struct mutex *mtx, int wantipl)
{
#ifdef MULTIPROCESSOR
	mtx->mtx_lock[0] = 1;
	mtx->mtx_lock[1] = 1;
	mtx->mtx_lock[2] = 1;
	mtx->mtx_lock[3] = 1;
#endif
	mtx->mtx_wantipl = wantipl;
	mtx->mtx_oldipl = IPL_NONE;
	mtx->mtx_owner = NULL;
}

#ifdef MULTIPROCESSOR
void
mtx_enter(struct mutex *mtx)
{
	while (mtx_enter_try(mtx) == 0)
		;
}

int
mtx_enter_try(struct mutex *mtx)
{
	struct cpu_info *ci = curcpu();
	volatile int *lock = __mtx_lock(mtx);
	int ret;
	int s;

 	if (mtx->mtx_wantipl != IPL_NONE)
		s = splraise(mtx->mtx_wantipl);

#ifdef DIAGNOSTIC
	if (__predict_false(mtx->mtx_owner == ci))
		panic("mtx %p: locking against myself", mtx);
#endif

	asm volatile (
		"ldcws      0(%2), %0"
		: "=&r" (ret), "+m" (lock)
		: "r" (lock)
	);

	if (ret) {
		membar_enter();
		mtx->mtx_owner = ci;
		if (mtx->mtx_wantipl != IPL_NONE)
			mtx->mtx_oldipl = s;
#ifdef DIAGNOSTIC
		ci->ci_mutex_level++;
#endif

		return (1);
	}

	if (mtx->mtx_wantipl != IPL_NONE)
		splx(s);

	return (0);
}
#else
void
mtx_enter(struct mutex *mtx)
{
	struct cpu_info *ci = curcpu();

#ifdef DIAGNOSTIC
	if (__predict_false(mtx->mtx_owner == ci))
		panic("mtx %p: locking against myself", mtx);
#endif

	if (mtx->mtx_wantipl != IPL_NONE)
		mtx->mtx_oldipl = splraise(mtx->mtx_wantipl);

	mtx->mtx_owner = ci;

#ifdef DIAGNOSTIC
	ci->ci_mutex_level++;
#endif
}

int
mtx_enter_try(struct mutex *mtx)
{
	mtx_enter(mtx);
	return (1);
}
#endif

void
mtx_leave(struct mutex *mtx)
{
#ifdef MULTIPROCESSOR
	volatile int *lock = __mtx_lock(mtx);
#endif
	int s;

	MUTEX_ASSERT_LOCKED(mtx);

#ifdef DIAGNOSTIC
	curcpu()->ci_mutex_level--;
#endif
	s = mtx->mtx_oldipl;
	mtx->mtx_owner = NULL;
#ifdef MULTIPROCESSOR
	membar_exit();
	*lock = 1;
#endif

	if (mtx->mtx_wantipl != IPL_NONE)
		splx(s);
}
@


1.14
log
@rework hppa mutexes.

this is largely based on src/sys/arch/alpha/alpha/mutex.c r1.14 and
src/sys/arch/sgi/sgi/mutex.c r1.15

always and explicitely record which cpu owns the lock (or NULL if
noone owns it). improve the mutex diagnostics/asserts so they operate
on the mtx_owner field rather than mtx_lock. previously the asserts
would assume the lock cpu owns the lock if any of them own the lock,
which blows up badly.

hppa hasnt got good atomic cpu opcodes, so this still relies on
ldcws to serialise access to the lock.

while im here i also shuffled the code. on MULTIPROCESSOR systems
instead of duplicating code between mtx_enter and mtx_enter_try,
mtx_enter simply loops on mtx_enter_try until it succeeds.

this also provides an alternative implementation of mutexes on
!MULTIPROCESSOR systems that avoids interlocking opcodes. mutexes
wont contend on UP boxes, theyre basically wrappers around spls.
we can just do the splraise, stash the owner as a guard value for
DIAGNOSTIC and return. similarly, mtx_enter_try on UP will never
fail, so we can just call mtx_enter and return 1.

tested by and ok kettenis@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.13 2014/06/17 15:43:27 guenther Exp $	*/
d89 1
a95 1
		membar_enter();
d150 1
a151 1
	membar_exit();
@


1.13
log
@Add membars to guarantee mtx_oldipl is written after locking and
read before unlocking.  Believed to fix some spl problems on MP
that have had landry and tobaisu seeing red.

suggestion to use membar API from matthew@@
ok matthew@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.12 2014/01/30 15:18:51 kettenis Exp $	*/
d37 1
a37 5
static inline int
try_lock(struct mutex *mtx)
{
	volatile int *lock = (int *)(((vaddr_t)mtx->mtx_lock + 0xf) & ~0xf);
	volatile register_t ret = 0;
d39 4
a42 9
	/* Note: lock must be 16-byte aligned. */
	asm volatile (
		"ldcws      0(%2), %0"
		: "=&r" (ret), "+m" (lock)
		: "r" (lock)
	);

	return ret;
}
d47 1
d52 1
d55 1
d58 1
d62 2
a63 18
	int s;

	for (;;) {
		if (mtx->mtx_wantipl != IPL_NONE)
			s = splraise(mtx->mtx_wantipl);
		if (try_lock(mtx)) {
			membar_enter();
			if (mtx->mtx_wantipl != IPL_NONE)
				mtx->mtx_oldipl = s;
			mtx->mtx_owner = curcpu();
#ifdef DIAGNOSTIC
			curcpu()->ci_mutex_level++;
#endif
			return;
		}
		if (mtx->mtx_wantipl != IPL_NONE)
			splx(s);
	}
d69 3
d73 1
a73 1
	
d76 14
a89 2
	if (try_lock(mtx)) {
		membar_enter();
a91 1
		mtx->mtx_owner = curcpu();
d93 1
a93 1
		curcpu()->ci_mutex_level++;
d95 3
a97 1
		return 1;
d99 1
d103 1
a103 1
	return 0;
d105 28
d137 3
d149 2
d152 1
a152 5

	mtx->mtx_lock[0] = 1;
	mtx->mtx_lock[1] = 1;
	mtx->mtx_lock[2] = 1;
	mtx->mtx_lock[3] = 1;
@


1.12
log
@To prevent lock ordering problems with the kernel lock, we need to make sure
we block all interrupts that can grab the kernel lock.  The simplest way to
achieve this is to make sure mutexes always raise the ipl to the highest
level that has interrupts that grab the kernel lock.  This will allow us
to have "mpsafe" interrupt handlers at lower priority levels.

No change for non-MULTIPROCESSOR kernels.

Tested by juanfra@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.11 2011/04/20 16:10:53 jsing Exp $	*/
d31 1
d73 1
d95 1
d122 1
@


1.11
log
@Back out r1.10 of mutex.c as this breaks serial on hppa (at least for MP).
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.8 2010/06/26 00:45:05 jsing Exp $	*/
d53 1
a53 1
mtx_init(struct mutex *mtx, int wantipl)
@


1.10
log
@Remove the `skip splraise/splx for IPL_NONE mutexes' optimizations. It is not
always gaining anything, and msleep() implementation depends upon mtx_leave()
invoking splx().
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.9 2010/09/28 20:27:54 miod Exp $	*/
d69 2
a70 1
		s = splraise(mtx->mtx_wantipl);
d72 2
a73 1
			mtx->mtx_oldipl = s;
d80 2
a81 1
		splx(s);
d90 2
a91 1
	s = splraise(mtx->mtx_wantipl);
d93 2
a94 1
		mtx->mtx_oldipl = s;
d101 2
a102 1
	splx(s);
d125 2
a126 1
	splx(s);
@


1.9
log
@Implement a per-cpu held mutex counter if DIAGNOSTIC on all non-x86 platforms,
to complete matthew@@'s commit of a few days ago, and drop __HAVE_CPU_MUTEX_LEVEL
define. With help from, and ok deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.8 2010/06/26 00:45:05 jsing Exp $	*/
d69 1
a69 2
		if (mtx->mtx_wantipl != IPL_NONE)
			s = splraise(mtx->mtx_wantipl);
d71 1
a71 2
			if (mtx->mtx_wantipl != IPL_NONE)
				mtx->mtx_oldipl = s;
d78 1
a78 2
		if (mtx->mtx_wantipl != IPL_NONE)
			splx(s);
d87 1
a87 2
 	if (mtx->mtx_wantipl != IPL_NONE)
		s = splraise(mtx->mtx_wantipl);
d89 1
a89 2
		if (mtx->mtx_wantipl != IPL_NONE)
			mtx->mtx_oldipl = s;
d96 1
a96 2
	if (mtx->mtx_wantipl != IPL_NONE)
		splx(s);
d119 1
a119 2
	if (mtx->mtx_wantipl != IPL_NONE)
		splx(s);
@


1.8
log
@Avoid a potential race when unlocking a mutex.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.7 2010/01/10 04:07:18 kettenis Exp $	*/
d75 3
d96 3
d114 3
@


1.7
log
@GCC doesn't respect the aligned attribute for automatic variables.  So
having mutexes on the stack, like dlg@@ added recently to the scsi code,
doesn't work on hppa.  So instead of relying on mutexes being properly
alligned just reserve 4 words and use the one that has the proper alignment.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.6 2009/12/29 15:01:59 jsing Exp $	*/
d104 2
d107 4
d115 1
d117 1
a117 2
		splx(mtx->mtx_oldipl);
	mtx->mtx_owner = NULL;
@


1.6
log
@Implement MP safe mutexes for hppa.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.7 2009/10/22 22:08:54 miod Exp $	*/
d39 1
a41 7
#ifdef DIAGNOSTIC
	if (((u_int32_t)(&mtx->mtx_lock) & 0xf) != 0) {
		db_printf("mtx_lock is not 16-byte aligned\n");
		Debugger();
	}
#endif

d45 2
a46 2
		: "=&r" (ret), "+m" (mtx->mtx_lock)
		: "r" (&mtx->mtx_lock)
d48 1
a48 1
	
d55 4
a58 1
	mtx->mtx_lock = MUTEX_UNLOCKED;
d105 4
a108 1
	mtx->mtx_lock = MUTEX_UNLOCKED;
@


1.5
log
@A new(er) mtx_enter_try().

Ok oga@@, "the time is now" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.4 2009/04/27 21:48:56 kettenis Exp $	*/
d34 12
a45 2
#ifdef MULTIPROCESSOR
#error This code needs more work
d48 10
a57 4
/*
 * Single processor systems don't need any mutexes, but they need the spl
 * raising semantics of the mutexes.
 */
d61 1
a61 1
	mtx->mtx_oldipl = 0;
d63 1
a63 1
	mtx->mtx_lock = 0;
d69 14
a82 4
	if (mtx->mtx_wantipl != IPL_NONE)
		mtx->mtx_oldipl = splraise(mtx->mtx_wantipl);
	MUTEX_ASSERT_UNLOCKED(mtx);
	mtx->mtx_lock = 1;
d88 10
d99 1
a99 3
		mtx->mtx_oldipl = splraise(mtx->mtx_wantipl);
	MUTEX_ASSERT_UNLOCKED(mtx);
	mtx->mtx_lock = 1;
d101 1
a101 1
	return 1;
d108 1
a108 1
	mtx->mtx_lock = 0;
d111 1
@


1.4
log
@Revert mtx_enter_try.  It didn't compile on hppa, it doesn't compile on
landisk, and the sparc implementation is obviously wrong.  That's where I
stopped looking, so who knows what else was broken.  A simple comparison of
the existing mtx_enter with the new mtx_enter_try would have told anybody.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.1 2007/05/05 12:00:55 miod Exp $	*/
d57 11
@


1.3
log
@fix compile
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.2 2009/04/25 20:14:42 weingart Exp $	*/
a56 11
}

int
mtx_enter_try(struct mutex *mtx)
{
	if (mtx->mtx_wantipl != IPL_NONE)
		mtx->mtx_oldipl = splraise(mtx->mtx_wantipl);
	MUTEX_ASSERT_UNLOCKED(mtx);
	mtx->mtx_lock = 1;

	return 1;
@


1.2
log
@Enter mtx_enter_try.  In part for completeness, things may start
using this soon(ish).  Ok oga@@, sorta yes kettenis@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: mutex.c,v 1.1 2007/05/05 12:00:55 miod Exp $	*/
d63 1
a63 1
		mtx->mtx_oldipl = _splraise(mtx->mtx_wantipl);
@


1.1
log
@Simple single-processor only mutex implementation.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d57 11
@

