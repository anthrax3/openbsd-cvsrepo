head	1.172;
access;
symbols
	OPENBSD_6_1:1.172.0.2
	OPENBSD_6_1_BASE:1.172
	OPENBSD_6_0:1.169.0.2
	OPENBSD_6_0_BASE:1.169
	OPENBSD_5_9:1.168.0.2
	OPENBSD_5_9_BASE:1.168
	OPENBSD_5_8:1.167.0.4
	OPENBSD_5_8_BASE:1.167
	OPENBSD_5_7:1.165.0.2
	OPENBSD_5_7_BASE:1.165
	OPENBSD_5_6:1.162.0.4
	OPENBSD_5_6_BASE:1.162
	OPENBSD_5_5:1.160.0.10
	OPENBSD_5_5_BASE:1.160
	OPENBSD_5_4:1.160.0.6
	OPENBSD_5_4_BASE:1.160
	OPENBSD_5_3:1.160.0.4
	OPENBSD_5_3_BASE:1.160
	OPENBSD_5_2:1.160.0.2
	OPENBSD_5_2_BASE:1.160
	OPENBSD_5_1_BASE:1.158
	OPENBSD_5_1:1.158.0.4
	OPENBSD_5_0:1.158.0.2
	OPENBSD_5_0_BASE:1.158
	OPENBSD_4_9:1.155.0.4
	OPENBSD_4_9_BASE:1.155
	OPENBSD_4_8:1.155.0.2
	OPENBSD_4_8_BASE:1.155
	OPENBSD_4_7:1.148.0.2
	OPENBSD_4_7_BASE:1.148
	OPENBSD_4_6:1.137.0.4
	OPENBSD_4_6_BASE:1.137
	OPENBSD_4_5:1.134.0.2
	OPENBSD_4_5_BASE:1.134
	OPENBSD_4_4:1.132.0.2
	OPENBSD_4_4_BASE:1.132
	OPENBSD_4_3:1.131.0.2
	OPENBSD_4_3_BASE:1.131
	OPENBSD_4_2:1.130.0.2
	OPENBSD_4_2_BASE:1.130
	OPENBSD_4_1:1.128.0.6
	OPENBSD_4_1_BASE:1.128
	OPENBSD_4_0:1.128.0.4
	OPENBSD_4_0_BASE:1.128
	OPENBSD_3_9:1.128.0.2
	OPENBSD_3_9_BASE:1.128
	OPENBSD_3_8:1.126.0.2
	OPENBSD_3_8_BASE:1.126
	OPENBSD_3_7:1.123.0.2
	OPENBSD_3_7_BASE:1.123
	OPENBSD_3_6:1.115.0.2
	OPENBSD_3_6_BASE:1.115
	SMP_SYNC_A:1.112
	SMP_SYNC_B:1.112
	OPENBSD_3_5:1.107.0.2
	OPENBSD_3_5_BASE:1.107
	OPENBSD_3_4:1.102.0.2
	OPENBSD_3_4_BASE:1.102
	UBC_SYNC_A:1.102
	OPENBSD_3_3:1.100.0.2
	OPENBSD_3_3_BASE:1.100
	OPENBSD_3_2:1.85.0.2
	OPENBSD_3_2_BASE:1.85
	OPENBSD_3_1:1.70.0.2
	OPENBSD_3_1_BASE:1.70
	UBC_SYNC_B:1.87
	UBC:1.51.0.2
	UBC_BASE:1.51
	OPENBSD_3_0:1.46.0.2
	OPENBSD_3_0_BASE:1.46
	OPENBSD_2_9:1.40.0.2
	OPENBSD_2_9_BASE:1.40
	OPENBSD_2_8:1.35.0.2
	OPENBSD_2_8_BASE:1.35
	OPENBSD_2_7:1.32.0.4
	OPENBSD_2_7_BASE:1.32
	SMP:1.32.0.2
	SMP_BASE:1.32
	kame_19991208:1.28
	OPENBSD_2_6:1.21.0.2
	OPENBSD_2_6_BASE:1.21
	OPENBSD_2_5:1.13.0.2
	OPENBSD_2_5_BASE:1.13
	OPENBSD_2_4:1.2.0.2
	OPENBSD_2_4_BASE:1.2;
locks; strict;
comment	@ * @;


1.172
date	2016.10.19.08.28.20;	author guenther;	state Exp;
branches;
next	1.171;
commitid	VoR9X3uHTxRSYC5r;

1.171
date	2016.09.16.02.35.41;	author dlg;	state Exp;
branches;
next	1.170;
commitid	Fei4687v68qad1tP;

1.170
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.169;
commitid	RlO92XR575sygHqm;

1.169
date	2016.06.07.06.23.19;	author dlg;	state Exp;
branches;
next	1.168;
commitid	N0upL0onl7Raz5yi;

1.168
date	2015.08.30.16.50.25;	author kettenis;	state Exp;
branches;
next	1.167;
commitid	GUY1V8h8BxoRGCW9;

1.167
date	2015.07.13.13.07.39;	author kettenis;	state Exp;
branches;
next	1.166;
commitid	1IFhD4sNyKsPdhSp;

1.166
date	2015.07.13.08.20.41;	author kettenis;	state Exp;
branches;
next	1.165;
commitid	Q09oIjkB1Jlz5W34;

1.165
date	2015.02.11.01.58.57;	author dlg;	state Exp;
branches;
next	1.164;
commitid	jWKj5kJJfjWkYGfv;

1.164
date	2014.12.17.15.01.45;	author deraadt;	state Exp;
branches;
next	1.163;
commitid	C6XE7SFGDfwfHXJP;

1.163
date	2014.11.16.12.30.57;	author deraadt;	state Exp;
branches;
next	1.162;
commitid	yv0ECmCdICvq576h;

1.162
date	2014.05.12.14.35.56;	author kettenis;	state Exp;
branches;
next	1.161;

1.161
date	2014.04.08.09.34.23;	author mpi;	state Exp;
branches;
next	1.160;

1.160
date	2012.06.03.13.28.40;	author jsing;	state Exp;
branches;
next	1.159;

1.159
date	2012.06.01.16.03.59;	author jsing;	state Exp;
branches;
next	1.158;

1.158
date	2011.07.22.19.43.52;	author miod;	state Exp;
branches;
next	1.157;

1.157
date	2011.05.30.22.25.21;	author oga;	state Exp;
branches;
next	1.156;

1.156
date	2011.05.02.19.42.39;	author miod;	state Exp;
branches;
next	1.155;

1.155
date	2010.05.24.15.04.55;	author deraadt;	state Exp;
branches;
next	1.154;

1.154
date	2010.05.22.22.12.42;	author kettenis;	state Exp;
branches;
next	1.153;

1.153
date	2010.05.05.19.34.27;	author kettenis;	state Exp;
branches;
next	1.152;

1.152
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.151;

1.151
date	2010.04.20.23.27.01;	author deraadt;	state Exp;
branches;
next	1.150;

1.150
date	2010.03.30.02.38.03;	author deraadt;	state Exp;
branches;
next	1.149;

1.149
date	2010.03.28.18.00.51;	author kettenis;	state Exp;
branches;
next	1.148;

1.148
date	2010.03.16.17.12.50;	author miod;	state Exp;
branches;
next	1.147;

1.147
date	2010.01.03.19.23.49;	author kettenis;	state Exp;
branches;
next	1.146;

1.146
date	2009.11.11.18.09.55;	author deraadt;	state Exp;
branches;
next	1.145;

1.145
date	2009.08.13.16.31.11;	author deraadt;	state Exp;
branches;
next	1.144;

1.144
date	2009.08.12.17.50.55;	author kettenis;	state Exp;
branches;
next	1.143;

1.143
date	2009.08.09.19.10.10;	author kettenis;	state Exp;
branches;
next	1.142;

1.142
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.141;

1.141
date	2009.07.29.18.31.11;	author kettenis;	state Exp;
branches;
next	1.140;

1.140
date	2009.07.26.15.47.23;	author kettenis;	state Exp;
branches;
next	1.139;

1.139
date	2009.07.25.12.41.46;	author kettenis;	state Exp;
branches;
next	1.138;

1.138
date	2009.07.24.21.57.25;	author deraadt;	state Exp;
branches;
next	1.137;

1.137
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.136;

1.136
date	2009.06.11.20.10.51;	author kettenis;	state Exp;
branches;
next	1.135;

1.135
date	2009.06.02.23.00.18;	author oga;	state Exp;
branches;
next	1.134;

1.134
date	2008.10.01.19.13.01;	author kettenis;	state Exp;
branches;
next	1.133;

1.133
date	2008.09.14.10.09.42;	author kettenis;	state Exp;
branches;
next	1.132;

1.132
date	2008.04.18.06.42.21;	author djm;	state Exp;
branches;
next	1.131;

1.131
date	2007.09.22.09.57.40;	author martin;	state Exp;
branches;
next	1.130;

1.130
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.129;

1.129
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.128;

1.128
date	2005.12.25.21.39.04;	author miod;	state Exp;
branches;
next	1.127;

1.127
date	2005.10.26.18.35.44;	author martin;	state Exp;
branches;
next	1.126;

1.126
date	2005.04.18.12.19.44;	author mickey;	state Exp;
branches;
next	1.125;

1.125
date	2005.04.07.13.16.12;	author mickey;	state Exp;
branches;
next	1.124;

1.124
date	2005.04.07.00.21.51;	author mickey;	state Exp;
branches;
next	1.123;

1.123
date	2005.03.15.05.09.23;	author mickey;	state Exp;
branches;
next	1.122;

1.122
date	2004.11.27.20.05.26;	author mickey;	state Exp;
branches;
next	1.121;

1.121
date	2004.09.19.01.30.11;	author mickey;	state Exp;
branches;
next	1.120;

1.120
date	2004.09.18.06.43.23;	author mickey;	state Exp;
branches;
next	1.119;

1.119
date	2004.09.15.01.00.41;	author mickey;	state Exp;
branches;
next	1.118;

1.118
date	2004.09.14.23.56.55;	author mickey;	state Exp;
branches;
next	1.117;

1.117
date	2004.09.14.23.18.58;	author mickey;	state Exp;
branches;
next	1.116;

1.116
date	2004.09.14.22.23.03;	author mickey;	state Exp;
branches;
next	1.115;

1.115
date	2004.08.05.09.06.24;	author mickey;	state Exp;
branches;
next	1.114;

1.114
date	2004.08.01.07.13.49;	author mickey;	state Exp;
branches;
next	1.113;

1.113
date	2004.06.30.18.11.48;	author mickey;	state Exp;
branches;
next	1.112;

1.112
date	2004.06.09.20.17.23;	author tedu;	state Exp;
branches;
next	1.111;

1.111
date	2004.05.27.21.04.07;	author tedu;	state Exp;
branches;
next	1.110;

1.110
date	2004.04.21.22.17.55;	author mickey;	state Exp;
branches;
next	1.109;

1.109
date	2004.04.21.22.14.34;	author mickey;	state Exp;
branches;
next	1.108;

1.108
date	2004.04.07.18.24.19;	author mickey;	state Exp;
branches;
next	1.107;

1.107
date	2004.01.14.09.12.49;	author mickey;	state Exp;
branches;
next	1.106;

1.106
date	2003.12.20.21.49.06;	author miod;	state Exp;
branches;
next	1.105;

1.105
date	2003.12.01.23.56.30;	author mickey;	state Exp;
branches;
next	1.104;

1.104
date	2003.11.24.19.27.03;	author mickey;	state Exp;
branches;
next	1.103;

1.103
date	2003.10.31.21.24.19;	author mickey;	state Exp;
branches;
next	1.102;

1.102
date	2003.05.07.21.50.43;	author mickey;	state Exp;
branches;
next	1.101;

1.101
date	2003.04.23.20.21.36;	author mickey;	state Exp;
branches;
next	1.100;

1.100
date	2003.02.26.22.32.42;	author mickey;	state Exp;
branches;
next	1.99;

1.99
date	2003.02.18.09.40.43;	author miod;	state Exp;
branches;
next	1.98;

1.98
date	2003.02.05.18.54.22;	author mickey;	state Exp;
branches;
next	1.97;

1.97
date	2003.01.31.23.42.28;	author mickey;	state Exp;
branches;
next	1.96;

1.96
date	2003.01.29.17.06.05;	author mickey;	state Exp;
branches;
next	1.95;

1.95
date	2003.01.22.23.56.33;	author mickey;	state Exp;
branches;
next	1.94;

1.94
date	2003.01.22.21.01.42;	author mickey;	state Exp;
branches;
next	1.93;

1.93
date	2003.01.22.18.16.34;	author mickey;	state Exp;
branches;
next	1.92;

1.92
date	2003.01.22.16.59.45;	author mickey;	state Exp;
branches;
next	1.91;

1.91
date	2002.12.19.00.16.20;	author mickey;	state Exp;
branches;
next	1.90;

1.90
date	2002.11.07.19.22.56;	author mickey;	state Exp;
branches;
next	1.89;

1.89
date	2002.11.07.19.18.18;	author mickey;	state Exp;
branches;
next	1.88;

1.88
date	2002.10.30.23.55.58;	author mickey;	state Exp;
branches;
next	1.87;

1.87
date	2002.10.28.20.49.16;	author mickey;	state Exp;
branches;
next	1.86;

1.86
date	2002.10.17.02.21.08;	author mickey;	state Exp;
branches;
next	1.85;

1.85
date	2002.09.15.09.42.58;	author mickey;	state Exp;
branches;
next	1.84;

1.84
date	2002.09.10.22.37.46;	author mickey;	state Exp;
branches;
next	1.83;

1.83
date	2002.09.10.22.25.46;	author mickey;	state Exp;
branches;
next	1.82;

1.82
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.81;

1.81
date	2002.09.05.18.41.19;	author mickey;	state Exp;
branches;
next	1.80;

1.80
date	2002.07.31.05.03.30;	author mickey;	state Exp;
branches;
next	1.79;

1.79
date	2002.07.23.16.08.57;	author mickey;	state Exp;
branches;
next	1.78;

1.78
date	2002.07.18.04.35.03;	author mickey;	state Exp;
branches;
next	1.77;

1.77
date	2002.07.17.22.08.07;	author mickey;	state Exp;
branches;
next	1.76;

1.76
date	2002.05.20.06.13.00;	author mickey;	state Exp;
branches;
next	1.75;

1.75
date	2002.05.20.03.33.11;	author mickey;	state Exp;
branches;
next	1.74;

1.74
date	2002.05.20.01.24.26;	author mickey;	state Exp;
branches;
next	1.73;

1.73
date	2002.04.30.17.26.52;	author mickey;	state Exp;
branches;
next	1.72;

1.72
date	2002.04.22.00.56.31;	author mickey;	state Exp;
branches;
next	1.71;

1.71
date	2002.04.22.00.19.59;	author mickey;	state Exp;
branches;
next	1.70;

1.70
date	2002.04.01.16.19.59;	author mickey;	state Exp;
branches;
next	1.69;

1.69
date	2002.04.01.16.15.32;	author mickey;	state Exp;
branches;
next	1.68;

1.68
date	2002.03.27.21.39.25;	author mickey;	state Exp;
branches;
next	1.67;

1.67
date	2002.03.20.01.02.18;	author mickey;	state Exp;
branches;
next	1.66;

1.66
date	2002.03.19.23.16.52;	author mickey;	state Exp;
branches;
next	1.65;

1.65
date	2002.03.19.23.08.55;	author mickey;	state Exp;
branches;
next	1.64;

1.64
date	2002.03.19.21.22.13;	author mickey;	state Exp;
branches;
next	1.63;

1.63
date	2002.03.19.00.33.18;	author mickey;	state Exp;
branches;
next	1.62;

1.62
date	2002.03.15.21.44.18;	author mickey;	state Exp;
branches;
next	1.61;

1.61
date	2002.03.14.01.26.31;	author millert;	state Exp;
branches;
next	1.60;

1.60
date	2002.02.21.06.12.30;	author mickey;	state Exp;
branches;
next	1.59;

1.59
date	2002.02.12.16.28.53;	author mickey;	state Exp;
branches;
next	1.58;

1.58
date	2002.02.08.03.49.21;	author mickey;	state Exp;
branches;
next	1.57;

1.57
date	2002.02.07.05.47.03;	author mickey;	state Exp;
branches;
next	1.56;

1.56
date	2002.02.06.19.29.06;	author mickey;	state Exp;
branches;
next	1.55;

1.55
date	2002.02.04.18.13.05;	author mickey;	state Exp;
branches;
next	1.54;

1.54
date	2002.02.03.01.58.13;	author mickey;	state Exp;
branches;
next	1.53;

1.53
date	2002.01.28.06.02.57;	author mickey;	state Exp;
branches;
next	1.52;

1.52
date	2001.12.22.00.20.04;	author mickey;	state Exp;
branches;
next	1.51;

1.51
date	2001.11.28.16.24.26;	author art;	state Exp;
branches
	1.51.2.1;
next	1.50;

1.50
date	2001.11.28.14.20.16;	author art;	state Exp;
branches;
next	1.49;

1.49
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2001.11.28.13.47.38;	author art;	state Exp;
branches;
next	1.47;

1.47
date	2001.11.06.20.57.21;	author mickey;	state Exp;
branches;
next	1.46;

1.46
date	2001.07.25.13.25.31;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2001.06.08.08.08.48;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2001.05.09.15.31.24;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2001.05.05.21.26.36;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2001.04.29.20.58.55;	author mickey;	state Exp;
branches;
next	1.41;

1.41
date	2001.04.29.20.57.25;	author mickey;	state Exp;
branches;
next	1.40;

1.40
date	2001.03.29.00.12.54;	author mickey;	state Exp;
branches;
next	1.39;

1.39
date	2001.02.23.22.54.32;	author mickey;	state Exp;
branches;
next	1.38;

1.38
date	2001.01.30.21.44.16;	author mickey;	state Exp;
branches;
next	1.37;

1.37
date	2000.11.24.20.49.24;	author mickey;	state Exp;
branches;
next	1.36;

1.36
date	2000.11.08.16.44.53;	author mickey;	state Exp;
branches;
next	1.35;

1.35
date	2000.08.15.20.22.10;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2000.08.15.20.04.36;	author mickey;	state Exp;
branches;
next	1.33;

1.33
date	2000.07.02.02.41.57;	author mickey;	state Exp;
branches;
next	1.32;

1.32
date	2000.01.17.06.51.58;	author mickey;	state Exp;
branches
	1.32.2.1;
next	1.31;

1.31
date	2000.01.17.04.49.02;	author mickey;	state Exp;
branches;
next	1.30;

1.30
date	99.12.12.17.45.23;	author mickey;	state Exp;
branches;
next	1.29;

1.29
date	99.12.12.03.16.26;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	99.11.25.18.35.21;	author mickey;	state Exp;
branches;
next	1.27;

1.27
date	99.11.25.17.39.46;	author mickey;	state Exp;
branches;
next	1.26;

1.26
date	99.11.25.17.34.32;	author mickey;	state Exp;
branches;
next	1.25;

1.25
date	99.11.22.05.08.36;	author mickey;	state Exp;
branches;
next	1.24;

1.24
date	99.11.19.18.47.41;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	99.11.14.02.33.44;	author mickey;	state Exp;
branches;
next	1.22;

1.22
date	99.10.27.01.22.53;	author mickey;	state Exp;
branches;
next	1.21;

1.21
date	99.09.18.20.05.54;	author mickey;	state Exp;
branches;
next	1.20;

1.20
date	99.09.03.18.00.47;	author art;	state Exp;
branches;
next	1.19;

1.19
date	99.08.03.15.35.23;	author mickey;	state Exp;
branches;
next	1.18;

1.18
date	99.08.03.00.53.30;	author mickey;	state Exp;
branches;
next	1.17;

1.17
date	99.07.21.07.37.20;	author mickey;	state Exp;
branches;
next	1.16;

1.16
date	99.06.30.18.59.06;	author mickey;	state Exp;
branches;
next	1.15;

1.15
date	99.06.11.15.51.35;	author mickey;	state Exp;
branches;
next	1.14;

1.14
date	99.04.20.20.50.08;	author mickey;	state Exp;
branches;
next	1.13;

1.13
date	99.02.01.20.29.50;	author mickey;	state Exp;
branches;
next	1.12;

1.12
date	99.01.20.19.39.51;	author mickey;	state Exp;
branches;
next	1.11;

1.11
date	99.01.20.19.29.50;	author mickey;	state Exp;
branches;
next	1.10;

1.10
date	99.01.03.18.42.50;	author mickey;	state Exp;
branches;
next	1.9;

1.9
date	99.01.03.04.01.36;	author mickey;	state Exp;
branches;
next	1.8;

1.8
date	98.12.30.02.11.52;	author mickey;	state Exp;
branches;
next	1.7;

1.7
date	98.12.08.19.49.38;	author mickey;	state Exp;
branches;
next	1.6;

1.6
date	98.12.05.15.53.53;	author mickey;	state Exp;
branches;
next	1.5;

1.5
date	98.11.24.04.56.36;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	98.11.11.05.18.12;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	98.10.30.22.16.42;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	98.09.12.03.14.49;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	98.08.20.15.46.49;	author mickey;	state Exp;
branches;
next	;

1.32.2.1
date	2001.04.18.16.06.15;	author niklas;	state Exp;
branches;
next	1.32.2.2;

1.32.2.2
date	2001.07.04.10.16.04;	author niklas;	state Exp;
branches;
next	1.32.2.3;

1.32.2.3
date	2001.10.31.02.52.47;	author nate;	state Exp;
branches;
next	1.32.2.4;

1.32.2.4
date	2001.11.13.21.00.51;	author niklas;	state Exp;
branches;
next	1.32.2.5;

1.32.2.5
date	2001.12.05.00.39.10;	author niklas;	state Exp;
branches;
next	1.32.2.6;

1.32.2.6
date	2002.03.06.00.57.22;	author niklas;	state Exp;
branches;
next	1.32.2.7;

1.32.2.7
date	2002.03.28.10.19.25;	author niklas;	state Exp;
branches;
next	1.32.2.8;

1.32.2.8
date	2003.03.27.23.26.53;	author niklas;	state Exp;
branches;
next	1.32.2.9;

1.32.2.9
date	2003.05.13.19.41.04;	author ho;	state Exp;
branches;
next	1.32.2.10;

1.32.2.10
date	2004.02.19.10.48.40;	author niklas;	state Exp;
branches;
next	1.32.2.11;

1.32.2.11
date	2004.06.05.23.10.48;	author niklas;	state Exp;
branches;
next	1.32.2.12;

1.32.2.12
date	2004.06.10.11.40.23;	author niklas;	state Exp;
branches;
next	;

1.51.2.1
date	2002.01.31.22.55.09;	author niklas;	state Exp;
branches;
next	1.51.2.2;

1.51.2.2
date	2002.06.11.03.35.37;	author art;	state Exp;
branches;
next	1.51.2.3;

1.51.2.3
date	2002.10.29.00.28.02;	author art;	state Exp;
branches;
next	1.51.2.4;

1.51.2.4
date	2003.05.19.21.49.41;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.172
log
@Change pmap_proc_iflush() to take a process instead of a proc
powerpc: rename second argument of pmap_proc_iflush() to match other archs

ok kettenis@@
@
text
@/*	$OpenBSD: pmap.c,v 1.171 2016/09/16 02:35:41 dlg Exp $	*/

/*
 * Copyright (c) 1998-2004 Michael Shalayeff
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR OR HIS RELATIVES BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF MIND, USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
 * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 * THE POSSIBILITY OF SUCH DAMAGE.
 */
/*
 * References:
 * 1. PA7100LC ERS, Hewlett-Packard, March 30 1999, Public version 1.0
 * 2. PA7300LC ERS, Hewlett-Packard, March 18 1996, Version 1.0
 *
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/atomic.h>
#include <sys/proc.h>
#include <sys/user.h>
#include <sys/pool.h>
#include <sys/extent.h>

#include <uvm/uvm_extern.h>

#include <machine/cpufunc.h>
#include <machine/iomod.h>

#include <dev/rndvar.h>

#ifdef PMAPDEBUG
#define	DPRINTF(l,s)	do {		\
	if ((pmapdebug & (l)) == (l))	\
		printf s;		\
} while(0)
#define	PDB_FOLLOW	0x00000001
#define	PDB_INIT	0x00000002
#define	PDB_ENTER	0x00000004
#define	PDB_REMOVE	0x00000008
#define	PDB_CREATE	0x00000010
#define	PDB_PTPAGE	0x00000020
#define	PDB_CACHE	0x00000040
#define	PDB_BITS	0x00000080
#define	PDB_COLLECT	0x00000100
#define	PDB_PROTECT	0x00000200
#define	PDB_EXTRACT	0x00000400
#define	PDB_VP		0x00000800
#define	PDB_PV		0x00001000
#define	PDB_PARANOIA	0x00002000
#define	PDB_WIRING	0x00004000
#define	PDB_PMAP	0x00008000
#define	PDB_STEAL	0x00010000
#define	PDB_PHYS	0x00020000
#define	PDB_POOL	0x00040000
int pmapdebug = 0
/*	| PDB_INIT */
/*	| PDB_FOLLOW */
/*	| PDB_VP */
/*	| PDB_PV */
/*	| PDB_ENTER */
/*	| PDB_REMOVE */
/*	| PDB_STEAL */
/*	| PDB_PROTECT */
/*	| PDB_PHYS */
	;
#else
#define	DPRINTF(l,s)	/* */
#endif

paddr_t physical_steal, physical_end;

int		pmap_hptsize = 16 * PAGE_SIZE;	/* patchable */
vaddr_t		pmap_hpt;

struct pmap	kernel_pmap_store;
int		hppa_sid_max = HPPA_SID_MAX;
struct pool	pmap_pmap_pool;
struct pool	pmap_pv_pool;
int		pmap_pvlowat = 252;
int 		pmap_initialized;

u_int	hppa_prot[8];

#define	pmap_sid(pmap, va) \
	(((va & 0xc0000000) != 0xc0000000)? pmap->pmap_space : HPPA_SID_KERNEL)

#define	pmap_pvh_attrs(a) \
	(((a) & PTE_PROT(TLB_DIRTY)) | ((a) ^ PTE_PROT(TLB_REFTRAP)))

struct vm_page	*pmap_pagealloc(struct uvm_object *obj, voff_t off);
void		 pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte);
#ifdef DDB
void		 pmap_dump_table(pa_space_t space, vaddr_t sva);
void		 pmap_dump_pv(paddr_t pa);
#endif
int		 pmap_check_alias(struct vm_page *pg, vaddr_t va,
		    pt_entry_t pte);

#define	IS_IOPAGE(pa)	((pa) >= HPPA_IOBEGIN)

static inline void
pmap_lock(struct pmap *pmap)
{
	if (pmap != pmap_kernel())
		mtx_enter(&pmap->pm_mtx);
}

static inline void
pmap_unlock(struct pmap *pmap)
{
	if (pmap != pmap_kernel())
		mtx_leave(&pmap->pm_mtx);
}

struct vm_page *
pmap_pagealloc(struct uvm_object *obj, voff_t off)
{
	struct vm_page *pg;

	if ((pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO)) == NULL)
		printf("pmap_pagealloc fail\n");

	return (pg);
}

#ifdef USE_HPT
/*
 * This hash function is the one used by the hardware TLB walker on the 7100LC.
 */
static __inline struct vp_entry *
pmap_hash(struct pmap *pmap, vaddr_t va)
{
	return (struct vp_entry *)(pmap_hpt +
	    (((va >> 8) ^ (pmap->pm_space << 9)) & (pmap_hptsize - 1)));
}

static __inline u_int32_t
pmap_vtag(struct pmap *pmap, vaddr_t va)
{
	return (0x80000000 | (pmap->pm_space & 0xffff) |
	    ((va >> 1) & 0x7fff0000));
}
#endif

static __inline void
pmap_sdir_set(pa_space_t space, volatile u_int32_t *pd)
{
	volatile u_int32_t *vtop;

	mfctl(CR_VTOP, vtop);
#ifdef PMAPDEBUG
	if (!vtop)
		panic("pmap_sdir_set: zero vtop");
#endif
	vtop[space] = (u_int32_t)pd;
}

static __inline u_int32_t *
pmap_sdir_get(pa_space_t space)
{
	u_int32_t *vtop;

	mfctl(CR_VTOP, vtop);
	return ((u_int32_t *)vtop[space]);
}

static __inline volatile pt_entry_t *
pmap_pde_get(volatile u_int32_t *pd, vaddr_t va)
{
	return ((pt_entry_t *)pd[va >> 22]);
}

static __inline void
pmap_pde_set(struct pmap *pm, vaddr_t va, paddr_t ptp)
{
#ifdef PMAPDEBUG
	if (ptp & PGOFSET)
		panic("pmap_pde_set, unaligned ptp 0x%lx", ptp);
#endif
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_set(%p, 0x%lx, 0x%lx)\n", pm, va, ptp));

	pm->pm_pdir[va >> 22] = ptp;
}

static __inline pt_entry_t *
pmap_pde_alloc(struct pmap *pm, vaddr_t va, struct vm_page **pdep)
{
	struct vm_page *pg;
	volatile pt_entry_t *pde;
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_alloc(%p, 0x%lx, %p)\n", pm, va, pdep));

	pmap_unlock(pm);
	pg = pmap_pagealloc(&pm->pm_obj, va);
	pmap_lock(pm);
	if (pg == NULL)
		return (NULL);
	pde = pmap_pde_get(pm->pm_pdir, va);
	if (pde) {
		pmap_unlock(pm);
		uvm_pagefree(pg);
		pmap_lock(pm);
		return (pt_entry_t *)pde;
	}

	pa = VM_PAGE_TO_PHYS(pg);

	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pde_alloc: pde %lx\n", pa));

	atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
	pg->wire_count = 1;		/* no mappings yet */
	pmap_pde_set(pm, va, pa);
	pm->pm_stats.resident_count++;	/* count PTP as resident */
	pm->pm_ptphint = pg;
	if (pdep)
		*pdep = pg;
	return ((pt_entry_t *)pa);
}

static __inline struct vm_page *
pmap_pde_ptp(struct pmap *pm, volatile pt_entry_t *pde)
{
	paddr_t pa = (paddr_t)pde;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp(%p, %p)\n", pm, pde));

	if (pm->pm_ptphint && VM_PAGE_TO_PHYS(pm->pm_ptphint) == pa)
		return (pm->pm_ptphint);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp: lookup 0x%lx\n", pa));

	return (PHYS_TO_VM_PAGE(pa));
}

static __inline void
pmap_pde_release(struct pmap *pmap, vaddr_t va, struct vm_page *ptp)
{
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_PV,
	    ("pmap_pde_release(%p, 0x%lx, %p)\n", pmap, va, ptp));

	if (pmap != pmap_kernel() && --ptp->wire_count <= 1) {
		DPRINTF(PDB_FOLLOW|PDB_PV,
		    ("pmap_pde_release: disposing ptp %p\n", ptp));
		
		pmap_pde_set(pmap, va, 0);
		pmap->pm_stats.resident_count--;
		if (pmap->pm_ptphint == ptp) {
			pmap->pm_ptphint = RBT_ROOT(uvm_objtree,
			    &pmap->pm_obj.memt);
		}
		ptp->wire_count = 0;
#ifdef DIAGNOSTIC
		if (ptp->pg_flags & PG_BUSY)
			panic("pmap_pde_release: busy page table page");
#endif
		pa = VM_PAGE_TO_PHYS(ptp);
		pdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
		pdtlb(HPPA_SID_KERNEL, pa);
		uvm_pagefree(ptp);
	}
}

static __inline pt_entry_t
pmap_pte_get(volatile pt_entry_t *pde, vaddr_t va)
{
	return (pde[(va >> 12) & 0x3ff]);
}

static __inline void
pmap_pte_set(volatile pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
{
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pte_set(%p, 0x%lx, 0x%x)\n",
	    pde, va, pte));

#ifdef PMAPDEBUG
	if (!pde)
		panic("pmap_pte_set: zero pde");

	if ((paddr_t)pde & PGOFSET)
		panic("pmap_pte_set, unaligned pde %p", pde);
#endif

	pde[(va >> 12) & 0x3ff] = pte;
}

void
pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte)
{
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	if (pte & PTE_PROT(TLB_EXECUTE)) {
		ficache(pmap->pm_space, va, PAGE_SIZE);
		pdtlb(pmap->pm_space, va);
		pitlb(pmap->pm_space, va);
	} else
		pdtlb(pmap->pm_space, va);
#ifdef USE_HPT
	if (pmap_hpt) {
		struct vp_entry *hpt;
		hpt = pmap_hash(pmap, va);
		if (hpt->vp_tag == pmap_vtag(pmap, va))
			hpt->vp_tag = 0xffff;
	}
#endif
}

static __inline pt_entry_t
pmap_vp_find(struct pmap *pm, vaddr_t va)
{
	volatile pt_entry_t *pde;

	if (!(pde = pmap_pde_get(pm->pm_pdir, va)))
		return (0);

	return (pmap_pte_get(pde, va));
}

#ifdef DDB
void
pmap_dump_table(pa_space_t space, vaddr_t sva)
{
	pa_space_t sp;

	for (sp = 0; sp <= hppa_sid_max; sp++) {
		volatile pt_entry_t *pde;
		pt_entry_t pte;
		vaddr_t va, pdemask;
		u_int32_t *pd;

		if (((int)space >= 0 && sp != space) ||
		    !(pd = pmap_sdir_get(sp)))
			continue;

		for (pdemask = 1, va = sva ? sva : 0;
		    va < 0xfffff000; va += PAGE_SIZE) {
			if (pdemask != (va & PDE_MASK)) {
				pdemask = va & PDE_MASK;
				if (!(pde = pmap_pde_get(pd, va))) {
					va = pdemask + (~PDE_MASK + 1);
					va -= PAGE_SIZE;
					continue;
				}
				printf("%x:%8p:\n", sp, pde);
			}

			if (!(pte = pmap_pte_get(pde, va)))
				continue;

			printf("0x%08lx-0x%08x:%b\n", va, pte & ~PAGE_MASK,
			    TLB_PROT(pte & PAGE_MASK), TLB_BITS);
		}
	}
}

void
pmap_dump_pv(paddr_t pa)
{
	struct vm_page *pg;
	struct pv_entry *pve;

	pg = PHYS_TO_VM_PAGE(pa);
	if (pg != NULL) {
		for (pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
			printf("%x:%lx\n", pve->pv_pmap->pm_space, pve->pv_va);
	}
}
#endif

int
pmap_check_alias(struct vm_page *pg, vaddr_t va, pt_entry_t pte)
{
	struct pv_entry *pve;
	int ret = 0;

	/* check for non-equ aliased mappings */
	mtx_enter(&pg->mdpage.pvh_mtx);
	for (pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		pte |= pmap_vp_find(pve->pv_pmap, pve->pv_va);
		if ((va & HPPA_PGAOFF) != (pve->pv_va & HPPA_PGAOFF) &&
		    (pte & PTE_PROT(TLB_WRITE))) {
#ifdef PMAPDEBUG
			printf("pmap_check_alias: "
			    "aliased writable mapping 0x%x:0x%lx\n",
			    pve->pv_pmap->pm_space, pve->pv_va);
#endif
			ret++;
		}
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	return (ret);
}

static __inline struct pv_entry *
pmap_pv_alloc(void)
{
	struct pv_entry *pv;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc()\n"));

	pv = pool_get(&pmap_pv_pool, PR_NOWAIT);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc: %p\n", pv));

	return (pv);
}

static __inline void
pmap_pv_free(struct pv_entry *pv)
{
	if (pv->pv_ptp)
		pmap_pde_release(pv->pv_pmap, pv->pv_va, pv->pv_ptp);

	pool_put(&pmap_pv_pool, pv);
}

static __inline void
pmap_pv_enter(struct vm_page *pg, struct pv_entry *pve, struct pmap *pm,
    vaddr_t va, struct vm_page *pdep)
{
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_enter(%p, %p, %p, 0x%lx, %p)\n",
	    pg, pve, pm, va, pdep));
	pve->pv_pmap = pm;
	pve->pv_va = va;
	pve->pv_ptp = pdep;
	mtx_enter(&pg->mdpage.pvh_mtx);
	pve->pv_next = pg->mdpage.pvh_list;
	pg->mdpage.pvh_list = pve;
	mtx_leave(&pg->mdpage.pvh_mtx);
}

static __inline struct pv_entry *
pmap_pv_remove(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry **pve, *pv;

	mtx_enter(&pg->mdpage.pvh_mtx);
	for (pv = *(pve = &pg->mdpage.pvh_list);
	    pv; pv = *(pve = &(*pve)->pv_next))
		if (pv->pv_pmap == pmap && pv->pv_va == va) {
			*pve = pv->pv_next;
			break;
		}
	mtx_leave(&pg->mdpage.pvh_mtx);
	return (pv);
}

void
pmap_bootstrap(vaddr_t vstart)
{
	extern int resvphysmem, etext, __rodata_end, __data_start;
	extern u_int *ie_mem;
	extern paddr_t hppa_vtop;
	vaddr_t va, addr = round_page(vstart), eaddr;
	vsize_t size;
	struct pmap *kpm;
	int npdes, nkpdes;

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_bootstrap(0x%lx)\n", vstart));

	uvm_setpagesize();

	hppa_prot[PROT_NONE]  = TLB_AR_NA;
	hppa_prot[PROT_READ]  = TLB_AR_R;
	hppa_prot[PROT_WRITE] = TLB_AR_RW;
	hppa_prot[PROT_READ | PROT_WRITE] = TLB_AR_RW;
	hppa_prot[PROT_EXEC]  = TLB_AR_RX;
	hppa_prot[PROT_READ | PROT_EXEC] = TLB_AR_RX;
	hppa_prot[PROT_WRITE | PROT_EXEC] = TLB_AR_RWX;
	hppa_prot[PROT_READ | PROT_WRITE | PROT_EXEC] = TLB_AR_RWX;

	/*
	 * Initialize kernel pmap
	 */
	kpm = &kernel_pmap_store;
	bzero(kpm, sizeof(*kpm));
	uvm_objinit(&kpm->pm_obj, NULL, 1);
	kpm->pm_space = HPPA_SID_KERNEL;
	kpm->pm_pid = HPPA_PID_KERNEL;
	kpm->pm_pdir_pg = NULL;
	kpm->pm_pdir = (u_int32_t *)addr;
	bzero((void *)addr, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
	addr += PAGE_SIZE;

	/*
	 * Allocate various tables and structures.
	 */

	mtctl(addr, CR_VTOP);
	hppa_vtop = addr;
	size = round_page((hppa_sid_max + 1) * 4);
	bzero((void *)addr, size);
	fdcache(HPPA_SID_KERNEL, addr, size);
	DPRINTF(PDB_INIT, ("vtop: 0x%lx @@ 0x%lx\n", size, addr));
	addr += size;
	pmap_sdir_set(HPPA_SID_KERNEL, kpm->pm_pdir);

	ie_mem = (u_int *)addr;
	addr += 0x8000;

#ifdef USE_HPT
	if (pmap_hptsize) {
		struct vp_entry *hptp;
		int i, error;

		/* must be aligned to the size XXX */
		if (addr & (pmap_hptsize - 1))
			addr += pmap_hptsize;
		addr &= ~(pmap_hptsize - 1);

		bzero((void *)addr, pmap_hptsize);
		for (hptp = (struct vp_entry *)addr, i = pmap_hptsize / 16; i--;)
			hptp[i].vp_tag = 0xffff;
		pmap_hpt = addr;
		addr += pmap_hptsize;

		DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n",
		    pmap_hptsize, addr));

		if ((error = (cpu_hpt_init)(pmap_hpt, pmap_hptsize)) < 0) {
			printf("WARNING: HPT init error %d -- DISABLED\n",
			    error);
			pmap_hpt = 0;
		} else
			DPRINTF(PDB_INIT,
			    ("HPT: installed for %d entries @@ 0x%x\n",
			    pmap_hptsize / sizeof(struct vp_entry), addr));
	}
#endif

	/* XXX PCXS needs this inserted into an IBTLB */
	/*	and can block-map the whole phys w/ another */

	/*
	 * We use separate mappings for the first 4MB of kernel text
	 * and whetever is left to avoid the mapping to cover kernel
	 * data.
	 */
	for (va = 0; va < (vaddr_t)&etext; va += size) {
		size = (vaddr_t)&etext - va;
		if (size > 4 * 1024 * 1024)
			size = 4 * 1024 * 1024;

		if (btlb_insert(HPPA_SID_KERNEL, va, va, &size,
		    pmap_sid2pid(HPPA_SID_KERNEL) |
		    pmap_prot(pmap_kernel(), PROT_READ | PROT_EXEC)) < 0) {
			printf("WARNING: cannot block map kernel text\n");
			break;
		}
	}

	if (&__rodata_end < &__data_start) {
		physical_steal = (vaddr_t)&__rodata_end;
		physical_end = (vaddr_t)&__data_start;
		DPRINTF(PDB_INIT, ("physpool: 0x%lx @@ 0x%lx\n",
		    physical_end - physical_steal, physical_steal));
	}

	/* kernel virtual is the last gig of the moohicans */
	nkpdes = physmem >> 14;	/* at least 16/gig for kmem */
	if (nkpdes < 4)
		nkpdes = 4;		/* ... but no less than four */
	nkpdes += HPPA_IOLEN / PDE_SIZE; /* ... and io space too */
	npdes = nkpdes + (physmem + atop(PDE_SIZE) - 1) / atop(PDE_SIZE);

	/* map the pdes */
	for (va = 0; npdes--; va += PDE_SIZE, addr += PAGE_SIZE) {

		/* last nkpdes are for the kernel virtual */
		if (npdes == nkpdes - 1)
			va = SYSCALLGATE;
		if (npdes == HPPA_IOLEN / PDE_SIZE - 1)
			va = HPPA_IOBEGIN;
		/* now map the pde for the physmem */
		bzero((void *)addr, PAGE_SIZE);
		fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
		DPRINTF(PDB_INIT|PDB_VP,
		    ("pde premap 0x%lx 0x%lx\n", va, addr));
		pmap_pde_set(kpm, va, addr);
		kpm->pm_stats.resident_count++; /* count PTP as resident */
	}

	resvphysmem = atop(addr);
	eaddr = physmem - atop(round_page(MSGBUFSIZE));
	DPRINTF(PDB_INIT, ("physmem: 0x%x - 0x%lx\n", resvphysmem, eaddr));
	uvm_page_physload(0, eaddr, resvphysmem, eaddr, 0);

	/* TODO optimize/inline the kenter */
	for (va = 0; va < ptoa(physmem); va += PAGE_SIZE) {
		extern struct user *proc0paddr;
		vm_prot_t prot = PROT_READ | PROT_WRITE;

		if (va < (vaddr_t)&etext)
			prot = PROT_READ | PROT_EXEC;
		else if (va < (vaddr_t)&__rodata_end)
			prot = PROT_READ;
		else if (va == (vaddr_t)proc0paddr + USPACE)
			prot = PROT_NONE;

		pmap_kenter_pa(va, va, prot);
	}

	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%lx\n", &etext, va));
}

void
pmap_init(void)
{
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init()\n"));

	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, IPL_NONE, 0,
	    "pmappl", NULL);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, IPL_VM, 0,
	    "pmappv", NULL);
	pool_setlowat(&pmap_pv_pool, pmap_pvlowat);
	pool_sethiwat(&pmap_pv_pool, pmap_pvlowat * 32);

	pmap_initialized = 1;

	/*
	 * map SysCall gateways page once for everybody
	 * NB: we'll have to remap the phys memory
	 *     if we have any at SYSCALLGATE address (;
	 */
	{
		volatile pt_entry_t *pde;

		if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, SYSCALLGATE)) &&
		    !(pde = pmap_pde_alloc(pmap_kernel(), SYSCALLGATE, NULL)))
			panic("pmap_init: cannot allocate pde");

		pmap_pte_set(pde, SYSCALLGATE, (paddr_t)&gateway_page |
		    PTE_PROT(TLB_GATE_PROT));
	}

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init(): done\n"));
}

void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = SYSCALLGATE + PAGE_SIZE;
	*endp = VM_MAX_KERNEL_ADDRESS;
}

struct pmap *
pmap_create(void)
{
	struct pmap *pmap;
	pa_space_t space;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_create()\n"));

	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);

	mtx_init(&pmap->pm_mtx, IPL_VM);

	uvm_objinit(&pmap->pm_obj, NULL, 1);

	for (space = 1 + arc4random_uniform(hppa_sid_max);
	    pmap_sdir_get(space); space = (space + 1) % hppa_sid_max);

	if ((pmap->pm_pdir_pg = pmap_pagealloc(NULL, 0)) == NULL)
		panic("pmap_create: no pages");
	pmap->pm_ptphint = NULL;
	pmap->pm_pdir = (u_int32_t *)VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
	pmap_sdir_set(space, pmap->pm_pdir);

	pmap->pm_space = space;
	pmap->pm_pid = (space + 1) << 1;

	pmap->pm_stats.resident_count = 1;
	pmap->pm_stats.wired_count = 0;

	return (pmap);
}

void
pmap_destroy(struct pmap *pmap)
{
	paddr_t pa;
	int refs;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_destroy(%p)\n", pmap));

	refs = atomic_dec_int_nv(&pmap->pm_obj.uo_refs);
	if (refs > 0)
		return;

	KASSERT(RBT_EMPTY(uvm_objtree, &pmap->pm_obj.memt));

	pmap_sdir_set(pmap->pm_space, 0);

	pa = VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
	pdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, pa);
	uvm_pagefree(pmap->pm_pdir_pg);

	pmap->pm_pdir_pg = NULL;
	pool_put(&pmap_pmap_pool, pmap);
}

/*
 * Add a reference to the specified pmap.
 */
void
pmap_reference(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_reference(%p)\n", pmap));

	atomic_inc_int(&pmap->pm_obj.uo_refs);
}

void
pmap_collect(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_collect(%p)\n", pmap));
	/* nothing yet */
}

int
pmap_enter(struct pmap *pmap, vaddr_t va, paddr_t pa, vm_prot_t prot, int flags)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg, *ptp = NULL;
	struct pv_entry *pve = NULL;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, 0x%lx, 0x%lx, 0x%x, 0x%x)\n",
	    pmap, va, pa, prot, flags));
	pmap_lock(pmap);

	if (!(pde = pmap_pde_get(pmap->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap, va, &ptp))) {
		if (flags & PMAP_CANFAIL) {
			pmap_unlock(pmap);
			return (ENOMEM);
		}
		panic("pmap_enter: cannot allocate pde");
	}

	if (!ptp)
		ptp = pmap_pde_ptp(pmap, pde);

	if ((pte = pmap_pte_get(pde, va))) {
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: remapping 0x%x -> 0x%lx\n", pte, pa));

		pmap_pte_flush(pmap, va, pte);
		if (wired && !(pte & PTE_PROT(TLB_WIRED)))
			pmap->pm_stats.wired_count++;
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)))
			pmap->pm_stats.wired_count--;

		if (PTE_PAGE(pte) == pa) {
			DPRINTF(PDB_FOLLOW|PDB_ENTER,
			    ("pmap_enter: same page\n"));
			goto enter;
		}

		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
		if (pg != NULL) {
			pve = pmap_pv_remove(pg, pmap, va);
			mtx_enter(&pg->mdpage.pvh_mtx);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			mtx_leave(&pg->mdpage.pvh_mtx);
		}
	} else {
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: new mapping 0x%lx -> 0x%lx\n", va, pa));
		pte = PTE_PROT(TLB_REFTRAP);
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
		if (ptp)
			ptp->wire_count++;
	}

	if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa)))) {
		if (!pve && !(pve = pmap_pv_alloc())) {
			if (flags & PMAP_CANFAIL) {
				pmap_unlock(pmap);
				return (ENOMEM);
			}
			panic("pmap_enter: no pv entries available");
		}
		pte |= PTE_PROT(pmap_prot(pmap, prot));
		if (pmap_check_alias(pg, va, pte))
			pmap_page_remove(pg);
		pmap_pv_enter(pg, pve, pmap, va, ptp);
	} else if (pve)
		pmap_pv_free(pve);

enter:
	/* preserve old ref & mod */
	pte = pa | PTE_PROT(pmap_prot(pmap, prot)) |
	    (pte & PTE_PROT(TLB_UNCACHABLE|TLB_DIRTY|TLB_REFTRAP));
	if (IS_IOPAGE(pa))
		pte |= PTE_PROT(TLB_UNCACHABLE);
	if (wired)
		pte |= PTE_PROT(TLB_WIRED);
	pmap_pte_set(pde, va, pte);

	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_enter: leaving\n"));
	pmap_unlock(pmap);

	return (0);
}

void
pmap_remove(struct pmap *pmap, vaddr_t sva, vaddr_t eva)
{
	struct pv_entry *pve;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg, *ptp;
	vaddr_t pdemask;
	int batch;

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_remove(%p, 0x%lx, 0x%lx)\n", pmap, sva, eva));
	pmap_lock(pmap);

	for (batch = 0; sva < eva; sva += PAGE_SIZE) {
		pdemask = sva & PDE_MASK;
		if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
			sva = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
			continue;
		}
		if (pdemask == sva) {
			if (sva + (~PDE_MASK + 1) <= eva)
				batch = 1;
			else
				batch = 0;
		}

		if ((pte = pmap_pte_get(pde, sva))) {

			/* TODO measure here the speed tradeoff
			 * for flushing whole 4M vs per-page
			 * in case of non-complete pde fill
			 */
			pmap_pte_flush(pmap, sva, pte);
			if (pte & PTE_PROT(TLB_WIRED))
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;

			/* iff properly accounted pde will be dropped anyway */
			if (!batch)
				pmap_pte_set(pde, sva, 0);

			if (pmap_initialized &&
			    (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
				mtx_enter(&pg->mdpage.pvh_mtx);
				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
				mtx_leave(&pg->mdpage.pvh_mtx);
				if ((pve = pmap_pv_remove(pg, pmap, sva)))
					pmap_pv_free(pve);
			} else {
				if (IS_IOPAGE(PTE_PAGE(pte))) {
					ptp = pmap_pde_ptp(pmap, pde);
					if (ptp != NULL)
						pmap_pde_release(pmap, sva, ptp);
				}
			}
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_remove: leaving\n"));
	pmap_unlock(pmap);
}

void
pmap_write_protect(struct pmap *pmap, vaddr_t sva, vaddr_t eva, vm_prot_t prot)
{
	struct vm_page *pg;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	u_int tlbprot, pdemask;

	DPRINTF(PDB_FOLLOW|PDB_PMAP,
	    ("pmap_write_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva, prot));
	pmap_lock(pmap);

	sva = trunc_page(sva);
	tlbprot = PTE_PROT(pmap_prot(pmap, prot));

	for (pdemask = 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
				continue;
			}
		}
		if ((pte = pmap_pte_get(pde, sva))) {

			DPRINTF(PDB_PMAP,
			    ("pmap_write_protect: va=0x%lx pte=0x%x\n",
			    sva,  pte));
			/*
			 * Determine if mapping is changing.
			 * If not, nothing to do.
			 */
			if ((pte & PTE_PROT(TLB_AR_MASK)) == tlbprot)
				continue;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			if (pg != NULL) {
				mtx_enter(&pg->mdpage.pvh_mtx);
				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
				mtx_leave(&pg->mdpage.pvh_mtx);
			}

			pmap_pte_flush(pmap, sva, pte);
			pte &= ~PTE_PROT(TLB_AR_MASK);
			pte |= tlbprot;
			pmap_pte_set(pde, sva, pte);
		}
	}

	pmap_unlock(pmap);
}

void
pmap_page_remove(struct vm_page *pg)
{
	struct pv_entry *pve;

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove(%p)\n", pg));

	if (pg->mdpage.pvh_list == NULL)
		return;

	mtx_enter(&pg->mdpage.pvh_mtx);
	while ((pve = pg->mdpage.pvh_list)) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		volatile pt_entry_t *pde;
		pt_entry_t pte;
		u_int attrs;

		pg->mdpage.pvh_list = pve->pv_next;
		pmap_reference(pmap);
		mtx_leave(&pg->mdpage.pvh_mtx);

		pmap_lock(pmap);
		pde = pmap_pde_get(pmap->pm_pdir, va);
		pte = pmap_pte_get(pde, va);
		attrs = pmap_pvh_attrs(pte);

		pmap_pte_flush(pmap, va, pte);
		if (pte & PTE_PROT(TLB_WIRED))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		pmap_pte_set(pde, va, 0);
		pmap_unlock(pmap);

		pmap_destroy(pmap);
		pmap_pv_free(pve);
		mtx_enter(&pg->mdpage.pvh_mtx);
		pg->mdpage.pvh_attrs |= attrs;
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove: leaving\n"));
}

void
pmap_unwire(struct pmap *pmap, vaddr_t	va)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte = 0;

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire(%p, 0x%lx)\n", pmap, va));
	pmap_lock(pmap);

	if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
		pte = pmap_pte_get(pde, va);

		if (pte & PTE_PROT(TLB_WIRED)) {
			pte &= ~PTE_PROT(TLB_WIRED);
			pmap->pm_stats.wired_count--;
			pmap_pte_set(pde, va, pte);
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire: leaving\n"));
	pmap_unlock(pmap);

#ifdef DIAGNOSTIC
	if (!pte)
		panic("pmap_unwire: invalid va 0x%lx", va);
#endif
}

boolean_t
pmap_changebit(struct vm_page *pg, u_int set, u_int clear)
{
	struct pv_entry *pve;
	pt_entry_t res;

	DPRINTF(PDB_FOLLOW|PDB_BITS,
	    ("pmap_changebit(%p, %x, %x)\n", pg, set, clear));

	mtx_enter(&pg->mdpage.pvh_mtx);
	res = pg->mdpage.pvh_attrs = 0;
	for (pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		volatile pt_entry_t *pde;
		pt_entry_t opte, pte;

		if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
			opte = pte = pmap_pte_get(pde, va);
#ifdef PMAPDEBUG
			if (!pte) {
				printf("pmap_changebit: zero pte for 0x%lx\n",
				    va);
				continue;
			}
#endif
			pte &= ~clear;
			pte |= set;
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			res |= pmap_pvh_attrs(opte);

			if (opte != pte) {
				pmap_pte_flush(pmap, va, opte);
				pmap_pte_set(pde, va, pte);
			}
		}
	}
	mtx_leave(&pg->mdpage.pvh_mtx);

	return ((res & (clear | set)) != 0);
}

boolean_t
pmap_testbit(struct vm_page *pg, u_int bit)
{
	struct pv_entry *pve;
	pt_entry_t pte;
	boolean_t ret;

	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bit));

	mtx_enter(&pg->mdpage.pvh_mtx);
	for (pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bit) && pve;
	    pve = pve->pv_next) {
		pte = pmap_vp_find(pve->pv_pmap, pve->pv_va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
	}
	ret = ((pg->mdpage.pvh_attrs & bit) != 0);
	mtx_leave(&pg->mdpage.pvh_mtx);

	return ret;
}

boolean_t
pmap_extract(struct pmap *pmap, vaddr_t va, paddr_t *pap)
{
	pt_entry_t pte;

	DPRINTF(PDB_FOLLOW|PDB_EXTRACT, ("pmap_extract(%p, %lx)\n", pmap, va));

	pte = pmap_vp_find(pmap, va);
	if (pte) {
		if (pap)
			*pap = (pte & ~PGOFSET) | (va & PGOFSET);
		return (TRUE);
	}

	return (FALSE);
}

void
pmap_activate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	struct pcb *pcb = &p->p_addr->u_pcb;

	pcb->pcb_space = pmap->pm_space;
}

void
pmap_deactivate(struct proc *p)
{

}

static __inline void
pmap_flush_page(struct vm_page *pg, int purge)
{
	struct pv_entry *pve;

	/* purge cache for all possible mappings for the pa */
	mtx_enter(&pg->mdpage.pvh_mtx);
	for (pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		if (purge)
			pdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		else
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		ficache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		pdtlb(pve->pv_pmap->pm_space, pve->pv_va);
		pitlb(pve->pv_pmap->pm_space, pve->pv_va);
	}
	mtx_leave(&pg->mdpage.pvh_mtx);
}

void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);

	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%lx)\n", pa));

	pmap_flush_page(pg, 1);
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, pa);
}

void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t spa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dpa = VM_PAGE_TO_PHYS(dstpg);
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%lx, %lx)\n", spa, dpa));

	pmap_flush_page(srcpg, 0);
	pmap_flush_page(dstpg, 1);
	bcopy((void *)spa, (void *)dpa, PAGE_SIZE);
	pdcache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, spa);
	pdtlb(HPPA_SID_KERNEL, dpa);
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	volatile pt_entry_t *pde;
	pt_entry_t pte, opte;

	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%lx, %lx, %x)\n", va, pa, prot));

	if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap_kernel(), va, NULL)))
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%lx", va);
	opte = pmap_pte_get(pde, va);
	pte = pa | PTE_PROT(TLB_WIRED | TLB_REFTRAP |
	    pmap_prot(pmap_kernel(), prot));
	if (IS_IOPAGE(pa))
		pte |= PTE_PROT(TLB_UNCACHABLE);
	if (opte)
		pmap_pte_flush(pmap_kernel(), va, opte);
	pmap_pte_set(pde, va, pte);
	pmap_kernel()->pm_stats.wired_count++;
	pmap_kernel()->pm_stats.resident_count++;

#ifdef PMAPDEBUG
	{
		struct vm_page *pg;

		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
			if (pmap_check_alias(pg, va, pte))
				Debugger();
		}
	}
#endif
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
}

void
pmap_kremove(vaddr_t va, vsize_t size)
{
	struct pv_entry *pve;
	vaddr_t eva, pdemask;
	volatile pt_entry_t *pde;
	pt_entry_t pte;
	struct vm_page *pg;

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_kremove(%lx, %lx)\n", va, size));
#ifdef PMAPDEBUG
	if (va < ptoa(physmem)) {
		printf("pmap_kremove(%lx, %lx): unmapping physmem\n", va, size);
		return;
	}
#endif

	for (pdemask = 1, eva = va + size; va < eva; va += PAGE_SIZE) {
		if (pdemask != (va & PDE_MASK)) {
			pdemask = va & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va))) {
				va = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
				continue;
			}
		}
		if (!(pte = pmap_pte_get(pde, va))) {
#ifdef DEBUG
			printf("pmap_kremove: unmapping unmapped 0x%x\n", va);
#endif
			continue;
		}

		pmap_pte_flush(pmap_kernel(), va, pte);
		pmap_pte_set(pde, va, 0);
		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
			mtx_enter(&pg->mdpage.pvh_mtx);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			mtx_leave(&pg->mdpage.pvh_mtx);
			/* just in case we have enter/kenter mismatch */
			if ((pve = pmap_pv_remove(pg, pmap_kernel(), va)))
				pmap_pv_free(pve);
		}
	}

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_kremove: leaving\n"));
}

void
pmap_proc_iflush(struct process *pr, vaddr_t va, vsize_t len)
{
	pmap_t pmap = vm_map_pmap(&pr->ps_vmspace->vm_map);

	fdcache(pmap->pm_space, va, len);
	sync_caches();
	ficache(pmap->pm_space, va, len);
	sync_caches();
}

struct vm_page *
pmap_unmap_direct(vaddr_t va)
{
	fdcache(HPPA_SID_KERNEL, va, PAGE_SIZE);
	pdtlb(HPPA_SID_KERNEL, va);
	return (PHYS_TO_VM_PAGE(va));
}
@


1.171
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.170 2016/09/15 02:00:17 dlg Exp $	*/
d1253 1
a1253 1
pmap_proc_iflush(struct proc *p, vaddr_t va, vsize_t len)
d1255 1
a1255 1
	pmap_t pmap = vm_map_pmap(&p->p_vmspace->vm_map);
@


1.170
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.169 2016/06/07 06:23:19 dlg Exp $	*/
d272 4
a275 2
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = RB_ROOT(&pmap->pm_obj.memt);
d715 1
a715 1
	KASSERT(RB_EMPTY(&pmap->pm_obj.memt));
@


1.169
log
@consistently set ipls on pmap pools.

this is a step toward making ipls unconditionaly on pools.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.168 2015/08/30 16:50:25 kettenis Exp $	*/
d634 1
a634 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0,
d636 2
a637 4
	pool_setipl(&pmap_pmap_pool, IPL_NONE);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pmappv",
	    NULL);
	pool_setipl(&pmap_pv_pool, IPL_VM);
@


1.168
log
@The pmap_pmap_pool pool will never be used in interrupt context, so pass the
PR_WAITOK flag to pmap_init and pass NULL as the pool allocator.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.167 2015/07/13 13:07:39 kettenis Exp $	*/
d634 1
a634 1
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, PR_WAITOK,
d636 1
@


1.167
log
@First stab at making the hppa mpsafe.  Not quite there yet though.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.166 2015/07/13 08:20:41 kettenis Exp $	*/
d634 2
a635 2
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
@


1.166
log
@Remove DIAGNOSTIC code that cleans up the pmap in pmap_destroy().  Replace it
with a single KASSERT that checks whether the RB tree is empty.  Seems uvm
was fixed some time ago and no longer leaves mappings behind.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.165 2015/02/11 01:58:57 dlg Exp $	*/
d37 1
d115 1
a115 1
int		 pmap_check_alias(struct pv_entry *pve, vaddr_t va,
d120 14
d210 1
d216 4
a219 1
	if ((pg = pmap_pagealloc(&pm->pm_obj, va)) == NULL)
d221 7
d392 1
a392 1
pmap_check_alias(struct pv_entry *pve, vaddr_t va, pt_entry_t pte)
d394 2
a395 1
	int ret;
d398 2
a399 1
	for (ret = 0; pve; pve = pve->pv_next) {
d411 1
d448 1
d451 1
d459 1
d466 1
d638 1
d680 2
d710 1
a710 1
	refs = --pmap->pm_obj.uo_refs;
d735 1
a735 1
	pmap->pm_obj.uo_refs++;
d757 1
d761 2
a762 1
		if (flags & PMAP_CANFAIL)
d764 1
d790 1
d792 1
d807 2
a808 1
			if (flags & PMAP_CANFAIL)
d810 1
d814 1
a814 1
		if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
d831 1
d848 1
d880 1
a880 1

d882 1
d896 1
d909 1
d935 2
a936 1
			if (pg != NULL)
d938 2
d947 2
d954 1
a954 1
	struct pv_entry *pve, *ppve;
d961 2
a962 2
	for (pve = pg->mdpage.pvh_list; pve;
	     pve = (ppve = pve)->pv_next, pmap_pv_free(ppve)) {
d967 5
d973 1
d976 1
a976 1
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d984 6
d991 1
a991 1
	pg->mdpage.pvh_list = NULL;
d1003 1
d1016 1
d1033 1
d1061 1
d1071 1
d1075 1
d1081 2
d1084 1
a1084 1
	return ((pg->mdpage.pvh_attrs & bit) != 0);
d1125 1
d1135 1
d1195 1
a1195 2

			if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
d1239 1
a1239 1

d1241 1
@


1.165
log
@sys/lock.h and machine/lock.h dont provide anything this code uses.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.164 2014/12/17 15:01:45 deraadt Exp $	*/
a668 3
#ifdef DIAGNOSTIC
	struct vm_page *pg;
#endif
d678 1
a678 38
#ifdef DIAGNOSTIC
	while ((pg = RB_ROOT(&pmap->pm_obj.memt))) {
		pt_entry_t *pde, *epde;
		struct vm_page *spg;
		struct pv_entry *pv, *npv;

		KASSERT(pg != pmap->pm_pdir_pg);
		pa = VM_PAGE_TO_PHYS(pg);
#ifdef PMAPDEBUG
		printf("pmap_destroy(%p): stray ptp 0x%lx w/ %u ents:",
		    pmap, pa, pg->wire_count - 1);
#endif

		pde = (pt_entry_t *)pa;
		epde = (pt_entry_t *)(pa + PAGE_SIZE);
		for (; pde < epde; pde++) {
			if (*pde == 0)
				continue;

			spg = PHYS_TO_VM_PAGE(PTE_PAGE(*pde));
			if (spg == NULL)
				continue;
			for (pv = spg->mdpage.pvh_list; pv != NULL; pv = npv) {
				npv = pv->pv_next;
				if (pv->pv_pmap == pmap) {
#ifdef PMAPDEBUG
					printf(" 0x%lx", pv->pv_va);
#endif
					pmap_remove(pmap, pv->pv_va,
					    pv->pv_va + PAGE_SIZE);
				}
			}
		}
#ifdef PMAPDEBUG
		printf("\n");
#endif
	}
#endif
@


1.164
log
@remove simplelocks
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.163 2014/11/16 12:30:57 deraadt Exp $	*/
a36 1
#include <sys/lock.h>
@


1.163
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.162 2014/05/12 14:35:56 kettenis Exp $	*/
a98 1
struct simplelock pvalloc_lock;
a359 1
		simple_lock(&pg->mdpage.pvh_lock);
a361 1
		simple_unlock(&pg->mdpage.pvh_lock);
a394 2
	simple_lock(&pvalloc_lock);

a396 2
	simple_unlock(&pvalloc_lock);

a404 2
	simple_lock(&pvalloc_lock);

a408 2

	simple_unlock(&pvalloc_lock);
a428 1
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
a434 1
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
a601 2
	simple_lock_init(&pvalloc_lock);

a677 1
	simple_lock(&pmap->pm_lock);
a678 2
	simple_unlock(&pmap->pm_lock);

a739 1
	simple_lock(&pmap->pm_lock);
a740 1
	simple_unlock(&pmap->pm_lock);
a762 2
	simple_lock(&pmap->pm_lock);

d765 1
a765 2
		if (flags & PMAP_CANFAIL) {
			simple_unlock(&pmap->pm_lock);
a766 2
		}

a773 1

a790 1
			simple_lock(&pg->mdpage.pvh_lock);
a792 1
			simple_unlock(&pg->mdpage.pvh_lock);
d807 1
a807 3
			if (flags & PMAP_CANFAIL) {
				simple_unlock(&pg->mdpage.pvh_lock);
				simple_unlock(&pmap->pm_lock);
a808 1
			}
a811 1
		simple_lock(&pg->mdpage.pvh_lock);
a814 1
		simple_unlock(&pg->mdpage.pvh_lock);
a827 2
	simple_unlock(&pmap->pm_lock);

a845 2
	simple_lock(&pmap->pm_lock);

a876 1
				simple_lock(&pg->mdpage.pvh_lock);
a879 1
				simple_unlock(&pg->mdpage.pvh_lock);
a889 2
	simple_unlock(&pmap->pm_lock);

a906 2
	simple_lock(&pmap->pm_lock);

d928 1
a928 2
			if (pg != NULL) {
				simple_lock(&pg->mdpage.pvh_lock);
a929 2
				simple_unlock(&pg->mdpage.pvh_lock);
			}
a936 2

	simple_unlock(&pmap->pm_lock);
a948 1
	simple_lock(&pg->mdpage.pvh_lock);
a955 2
		simple_lock(&pmap->pm_lock);

a965 1
		simple_unlock(&pmap->pm_lock);
a967 1
	simple_unlock(&pg->mdpage.pvh_lock);
a969 1

a979 1
	simple_lock(&pmap->pm_lock);
a988 1
	simple_unlock(&pmap->pm_lock);
a1006 1
	simple_lock(&pg->mdpage.pvh_lock);
a1013 1
		simple_lock(&pmap->pm_lock);
a1032 1
		simple_unlock(&pmap->pm_lock);
a1033 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1045 1
	simple_lock(&pg->mdpage.pvh_lock);
a1047 1
		simple_lock(&pve->pv_pmap->pm_lock);
a1048 1
		simple_unlock(&pve->pv_pmap->pm_lock);
a1050 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1061 1
	simple_lock(&pmap->pm_lock);
a1062 2
	simple_unlock(&pmap->pm_lock);

a1092 1
	simple_lock(&pg->mdpage.pvh_lock);
a1101 1
	simple_unlock(&pg->mdpage.pvh_lock);
a1141 2
	simple_lock(&pmap->pm_lock);

a1161 1
			simple_lock(&pg->mdpage.pvh_lock);
a1163 1
			simple_unlock(&pg->mdpage.pvh_lock);
a1166 2
	simple_unlock(&pmap->pm_lock);

a1187 2
	simple_lock(&pmap->pm_lock);

a1206 1
			simple_lock(&pg->mdpage.pvh_lock);
a1210 1
			simple_unlock(&pg->mdpage.pvh_lock);
a1212 2

	simple_unlock(&pmap->pm_lock);
@


1.162
log
@Fix format strings in debug code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.161 2014/04/08 09:34:23 mpi Exp $	*/
d466 8
a473 8
	hppa_prot[UVM_PROT_NONE]  = TLB_AR_NA;
	hppa_prot[UVM_PROT_READ]  = TLB_AR_R;
	hppa_prot[UVM_PROT_WRITE] = TLB_AR_RW;
	hppa_prot[UVM_PROT_RW]    = TLB_AR_RW;
	hppa_prot[UVM_PROT_EXEC]  = TLB_AR_RX;
	hppa_prot[UVM_PROT_RX]    = TLB_AR_RX;
	hppa_prot[UVM_PROT_WX]    = TLB_AR_RWX;
	hppa_prot[UVM_PROT_RWX]   = TLB_AR_RWX;
d550 1
a550 1
		    pmap_prot(pmap_kernel(), UVM_PROT_RX)) < 0) {
d595 1
a595 1
		vm_prot_t prot = UVM_PROT_RW;
d598 1
a598 1
			prot = UVM_PROT_RX;
d600 1
a600 1
			prot = UVM_PROT_READ;
d602 1
a602 1
			prot = UVM_PROT_NONE;
@


1.161
log
@Less <uvm/uvm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.160 2012/06/03 13:28:40 jsing Exp $	*/
d185 1
a185 1
		panic("pmap_pde_set, unaligned ptp 0x%x", ptp);
d188 1
a188 1
	    ("pmap_pde_set(%p, 0x%x, 0x%x)\n", pm, va, ptp));
d200 1
a200 1
	    ("pmap_pde_alloc(%p, 0x%x, %p)\n", pm, va, pdep));
d207 1
a207 1
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pde_alloc: pde %x\n", pa));
d229 1
a229 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp: lookup 0x%x\n", pa));
d240 1
a240 1
	    ("pmap_pde_release(%p, 0x%x, %p)\n", pmap, va, ptp));
d271 1
a271 1
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pte_set(%p, 0x%x, 0x%x)\n",
d381 1
a381 1
			    "aliased writable mapping 0x%x:0x%x\n",
d426 1
a426 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_enter(%p, %p, %p, 0x%x, %p)\n",
d462 1
a462 1
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_bootstrap(0x%x)\n", vstart));
d498 1
a498 1
	DPRINTF(PDB_INIT, ("vtop: 0x%x @@ 0x%x\n", size, addr));
d559 1
a559 1
		DPRINTF(PDB_INIT, ("physpool: 0x%x @@ 0x%x\n",
d581 2
a582 1
		DPRINTF(PDB_INIT|PDB_VP, ("pde premap 0x%x 0x%x\n", va, addr));
d589 1
a589 1
	DPRINTF(PDB_INIT, ("physmem: 0x%x - 0x%x\n", resvphysmem, eaddr));
d607 1
a607 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext, va));
d726 1
a726 1
					printf(" 0x%x", pv->pv_va);
d780 1
a780 1
	    ("pmap_enter(%p, 0x%x, 0x%x, 0x%x, 0x%x)\n",
d801 1
a801 1
		    ("pmap_enter: remapping 0x%x -> 0x%x\n", pte, pa));
d824 1
a824 1
		    ("pmap_enter: new mapping 0x%x -> 0x%x\n", va, pa));
d879 1
a879 1
	    ("pmap_remove(%p, 0x%x, 0x%x)\n", pmap, sva, eva));
d943 1
a943 1
	    ("pmap_write_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot));
d961 1
a961 1
			    ("pmap_write_protect: va=0x%x pte=0x%x\n",
d1032 1
a1032 1
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire(%p, 0x%x)\n", pmap, va));
d1076 1
a1076 1
				printf("pmap_changebit: zero pte for 0x%x\n",
d1124 1
a1124 1
	DPRINTF(PDB_FOLLOW|PDB_EXTRACT, ("pmap_extract(%p, %x)\n", pmap, va));
d1178 1
a1178 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%x)\n", pa));
d1191 1
a1191 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%x, %x)\n", spa, dpa));
d1209 1
a1209 1
	    ("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot));
d1255 1
a1255 1
	    ("pmap_kremove(%x, %x)\n", va, size));
d1258 1
a1258 1
		printf("pmap_kremove(%x, %x): unmapping physmem\n", va, size);
@


1.160
log
@Avoid the unlikely but possible use of an uninitialised variable.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.159 2012/06/01 16:03:59 jsing Exp $	*/
d43 1
a43 1
#include <uvm/uvm.h>
@


1.159
log
@Ansify function definitions, fix some style(9) and whitespace issues.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.157 2011/05/30 22:25:21 oga Exp $	*/
d775 1
a775 1
	struct pv_entry *pve;
a824 1
		pve = NULL;
@


1.158
log
@Fix PMAPDEBUG compile
@
text
@d428 3
a430 3
	pve->pv_pmap	= pm;
	pve->pv_va	= va;
	pve->pv_ptp	= pdep;
d441 1
a441 1
	for(pv = *(pve = &pg->mdpage.pvh_list);
d452 1
a452 2
pmap_bootstrap(vstart)
	vaddr_t vstart;
d553 1
a553 1
	       }
d610 1
a610 1
pmap_init()
d652 1
a652 1
pmap_create()
d682 1
a682 2
pmap_destroy(pmap)
	struct pmap *pmap;
d753 1
a753 2
pmap_reference(pmap)
	struct pmap *pmap;
d770 1
a770 6
pmap_enter(pmap, va, pa, prot, flags)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
d869 1
a869 4
pmap_remove(pmap, sva, eva)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d935 1
a935 5
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
	vm_prot_t prot;
d988 1
a988 2
pmap_page_remove(pg)
	struct vm_page *pg;
d1027 1
a1027 3
pmap_unwire(pmap, va)
	struct pmap *pmap;
	vaddr_t	va;
d1065 1
a1065 1
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
d1107 1
a1107 1
	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bit) && pve;
d1120 1
a1120 4
pmap_extract(pmap, va, pap)
	struct pmap *pmap;
	vaddr_t va;
	paddr_t *pap;
d1161 1
a1161 1
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
d1203 1
a1203 4
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d1246 1
a1246 3
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
@


1.157
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.156 2011/05/02 19:42:39 miod Exp $	*/
d727 1
a727 1
					printf(" 0x%x", haggis->pv_va);
@


1.156
log
@Cope with userland mappings of unmanaged pages (i.e. device memory being
mmap'ed), by properly making these mappings uncached, and correctly accounting
their removal to not get stuck with leftover pte pages.

While there, change a can't happen infinite loop in pmap_destroy() into a
KASSERT.

feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.155 2010/05/24 15:04:55 deraadt Exp $	*/
d590 1
a590 2
	uvm_page_physload(0, eaddr,
	    resvphysmem, eaddr, VM_FREELIST_DEFAULT);
@


1.155
log
@Add missing prototypes
ok jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.154 2010/05/22 22:12:42 kettenis Exp $	*/
d119 2
d360 6
a365 4
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		printf("%x:%lx\n", pve->pv_pmap->pm_space, pve->pv_va);
	simple_unlock(&pg->mdpage.pvh_lock);
d705 2
a706 5
		struct vm_page *sheep;
		struct pv_entry *haggis;

		if (pg == pmap->pm_pdir_pg)
			continue;
d708 2
d711 2
a712 2
		printf("pmap_destroy(%p): stray ptp 0x%lx w/ %d ents:",
		    pmap, VM_PAGE_TO_PHYS(pg), pg->wire_count - 1);
d715 2
a716 2
		pde = (pt_entry_t *)VM_PAGE_TO_PHYS(pg);
		epde = (pt_entry_t *)(VM_PAGE_TO_PHYS(pg) + PAGE_SIZE);
d721 6
a726 3
			sheep = PHYS_TO_VM_PAGE(PTE_PAGE(*pde));
			for (haggis = sheep->mdpage.pvh_list; haggis != NULL; )
				if (haggis->pv_pmap == pmap) {
d730 4
a733 8
					pmap_remove(pmap, haggis->pv_va,
					    haggis->pv_va + PAGE_SIZE);

					/* exploit the sacred knowledge
					   of lambeous ozzmosis */
					haggis = sheep->mdpage.pvh_list;
				} else
					haggis = haggis->pv_next;
d824 6
a829 3
		simple_lock(&pg->mdpage.pvh_lock);
		pve = pmap_pv_remove(pg, pmap, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
a839 1
		simple_lock(&pg->mdpage.pvh_lock);
d852 1
d856 1
a858 1
	simple_unlock(&pg->mdpage.pvh_lock);
d864 2
d886 1
a886 1
	struct vm_page *pg;
d931 6
d987 5
a991 3
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pg->mdpage.pvh_lock);
d1244 1
a1244 1
	if (pa >= HPPA_IOBEGIN)
@


1.154
log
@Remove instruction TLB purges for direct mappings that will never have the
X bit set.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.153 2010/05/05 19:34:27 kettenis Exp $	*/
d109 9
@


1.153
log
@Since we never set the X bit for directly mappings, nothing can be moved into
the instruction cache through them.  So we don't need to flush the
instruction cache and purge instruction TLBs for these mappings.

ok jsing@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.152 2010/04/30 21:56:39 oga Exp $	*/
a246 1
		pitlb(HPPA_SID_KERNEL, pa);
a734 1
	pitlb(HPPA_SID_KERNEL, pa);
@


1.152
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.151 2010/04/20 23:27:01 deraadt Exp $	*/
a1182 1
	ficache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
a1183 1
	pitlb(HPPA_SID_KERNEL, pa);
a1197 2
	ficache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	ficache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
a1199 2
	pitlb(HPPA_SID_KERNEL, spa);
	pitlb(HPPA_SID_KERNEL, dpa);
a1317 1
	ficache(HPPA_SID_KERNEL, va, PAGE_SIZE);
a1318 1
	pitlb(HPPA_SID_KERNEL, va);
@


1.151
log
@cleanup more confusion regarding user.h before proc.h, or missing proc.h
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.150 2010/03/30 02:38:03 deraadt Exp $	*/
d469 1
a469 5
	simple_lock_init(&kpm->pm_lock);
	kpm->pm_obj.pgops = NULL;
	RB_INIT(&kpm->pm_obj.memt);
	kpm->pm_obj.uo_npages = 0;
	kpm->pm_obj.uo_refs = 1;
d651 1
a651 5
	simple_lock_init(&pmap->pm_lock);
	pmap->pm_obj.pgops = NULL;	/* currently not a mappable object */
	RB_INIT(&pmap->pm_obj.memt);
	pmap->pm_obj.uo_npages = 0;
	pmap->pm_obj.uo_refs = 1;
@


1.150
log
@We will never condtionalize ficache on PTE_PROT(TLB_EXECUTE) because it
is risky.  Delete the comment suggesting we might.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.149 2010/03/28 18:00:51 kettenis Exp $	*/
d38 1
a39 1
#include <sys/proc.h>
@


1.149
log
@Fix user-after-free bug in pmap_remove().  Page table pages are freed as soon
as the last page table entry that was actually used is cleared.  So make sure
we check the page table page is still there for every page we remove.
Otherwise we will bring back the tlb entry and cache lines when we touch the
freed page, and we will create an illegal alias (non-equivalent mapping)
as soon as the page gets re-used.

Seems to fix the last remaining issue with fast page recycling (although I
need to do a bit more testing to be sure).

Help from drahn@@, ok jsing@@ and miod@@ (for an earlier version of this diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.148 2010/03/16 17:12:50 miod Exp $	*/
a1173 1
		/* XXX Conditionalize ficache on PTE_PROT(TLB_EXECUTE)? */
@


1.148
log
@PCXS processors (really, all PA-RISC processors with separate I and D tlb
entries) may use the DTLB to resolve an address in ficache(), so make sure
pdtlb() always happens after ficache(), not before.
This problem was exposed with revision 1.145 of pmap.c.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.147 2010/01/03 19:23:49 kettenis Exp $	*/
d889 8
a896 8
	for (batch = 0, pdemask = 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva = pdemask + (~PDE_MASK + 1) - PAGE_SIZE;
				continue;
			}
			if (pdemask == sva && sva + (~PDE_MASK + 1) <= eva)
@


1.147
log
@Implement pmap_proc_iflush() such that the instruction cache is synchronized
with the data cache when ptrace(2) is used to write into a process' address
space.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.146 2009/11/11 18:09:55 deraadt Exp $	*/
a278 1
	pdtlb(pmap->pm_space, va);
d281 1
d283 2
a284 1
	}
a1173 1
		pdtlb(pve->pv_pmap->pm_space, pve->pv_va);
d1176 1
d1192 1
a1193 1
	ficache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
d1209 2
a1212 2
	ficache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	ficache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
d1333 1
a1334 1
	ficache(HPPA_SID_KERNEL, va, PAGE_SIZE);
@


1.146
log
@add a icache flush which appears to make the machines even more stable
(no crashes in nearly forever)
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.145 2009/08/13 16:31:11 deraadt Exp $	*/
d1315 11
@


1.145
log
@In pmap_pte_flush flush the icache before the dcache, as is done elsewhere.
We cannot find gaurantees in the documents that any icache filling from
the cache is terminated by a icache flush or idtlb invalidation, so this is
probably safer, and unlikely to be unsafer.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.144 2009/08/12 17:50:55 kettenis Exp $	*/
d1174 2
@


1.144
log
@Non-equivalent writable aliases (aka illegal aliases) are bad.  Since PA-RISC
doesn't have a usuable uncachable bit, whack all other mappings of a page
if we are about to create a non-equivalent writable alias by entering a
mapping for it.

ok art@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.143 2009/08/09 19:10:10 kettenis Exp $	*/
d278 2
a283 2
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	pdtlb(pmap->pm_space, va);
@


1.143
log
@Unconditionally enable checks for writable non-uequivalent mappings.  They're
evil so we should complain about them until we have a way to fix them.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.142 2009/08/06 15:28:14 oga Exp $	*/
d366 1
d370 1
a419 5
	if (pmap_check_alias(pve, va, 0))
#ifdef PMAPDEBUG
		Debugger()
#endif
		;
d847 3
@


1.142
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.141 2009/07/29 18:31:11 kettenis Exp $	*/
a355 1
#ifdef PMAPDEBUG
a374 1
#endif
d418 1
d420 1
a420 2
	if (pmap_check_alias(pve, va, 0))
		Debugger();
d422 1
@


1.141
log
@Get rid of non-equivalent aliases of the pcb by moving the fpu state out
of the pcb and using the p_addr member of 'struct proc' to calculate the
address of the kernel stack when switching to virtual mode after taking a trap.
Remove the now unecessary cache flushes; they're actually harmful since they
create non-equivalent aliases.  This seems to fix the memory corruption we
have been observing from time to time.

This diff does not rename fpu_curpcb, which is now somewhat incorrectly named.
I hope to change things back again as soon as we are able to map the pcb 1:1.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.140 2009/07/26 15:47:23 kettenis Exp $	*/
d238 1
a238 1
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d474 1
a474 1
	TAILQ_INIT(&kpm->pm_obj.memq);
d660 1
a660 1
	TAILQ_INIT(&pmap->pm_obj.memq);
d702 1
a702 1
	while ((pg = TAILQ_FIRST(&pmap->pm_obj.memq))) {
@


1.140
log
@I'm not as smart as mickey, so replace:

	batch = pdemask == sva && sva + ~PDE_MASK + 1 <= eva;

with something that's a little bit easier to read.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.139 2009/07/25 12:41:46 kettenis Exp $	*/
a1153 2
	pcb->pcb_uva = (vaddr_t)p->p_addr;
	fdcache(HPPA_SID_KERNEL, (vaddr_t)pcb, PAGE_SIZE);
@


1.139
log
@Make sure that if a PDE isn't there, we skip to the start of the address
range covered by the next PDE and not somewhere in the middle.  The old could
have skipped over some valid PTE's causing them to stay behind in the pmap.
Since we would not flush the cache for those pages either this could also
cause memory corruption when dirty cache lines would be written back to
memory at a later stage.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.138 2009/07/24 21:57:25 deraadt Exp $	*/
d896 4
a899 1
			batch = pdemask == sva && sva + ~PDE_MASK + 1 <= eva;
@


1.138
log
@in pmap_kenter_pa(), flush the old pte before installing the new one.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.137 2009/06/16 00:11:29 oga Exp $	*/
d326 2
a327 1
					va += ~PDE_MASK + 1 - PAGE_SIZE;
d893 1
a893 1
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
d955 1
a955 1
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
d1286 1
a1286 1
				va += ~PDE_MASK + 1 - PAGE_SIZE;
@


1.137
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.134 2008/10/01 19:13:01 kettenis Exp $	*/
d1235 2
a1239 2
	if (opte)
		pmap_pte_flush(pmap_kernel(), va, opte);
@


1.136
log
@Correctly flush direct mappings (cache/tlb).  Uncovered by ariane's new
allocator.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.135 2009/06/02 23:00:18 oga Exp $	*/
d238 1
a238 1
			pmap->pm_ptphint = RB_ROOT(&pmap->pm_obj.memt);
d473 1
a473 1
	RB_INIT(&kpm->pm_obj.memt);
d659 1
a659 1
	RB_INIT(&pmap->pm_obj.memt);
d701 1
a701 1
	while ((pg = RB_ROOT(&pmap->pm_obj.memt))) {
@


1.135
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.134 2008/10/01 19:13:01 kettenis Exp $	*/
d226 2
d234 1
d244 4
d688 1
d740 1
d742 5
d748 1
d1312 10
@


1.134
log
@Be more agressive in flusing the cache.  In addition to flushing the
instruction cache, also purge TLB entries.  The PA-RISC 2.0
architecture says that cache lines may be moved in when a translation
exists even if no access is done.  This might have been hurting us
badly since we create illegal aliases in pmap_zero_page() and
pmap_copy_page().

Probably not perfect yet, and perhaps a bit of a sledgehammer, but it
makes PA-RISC 2.0 machines stable again.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.133 2008/09/14 10:09:42 kettenis Exp $	*/
d235 1
a235 1
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d466 1
a466 1
	TAILQ_INIT(&kpm->pm_obj.memq);
d652 1
a652 1
	TAILQ_INIT(&pmap->pm_obj.memq);
d693 1
a693 1
	while ((pg = TAILQ_FIRST(&pmap->pm_obj.memq))) {
@


1.133
log
@Use one block mapping to map kernel text up to 4MB and another one for the
remainder to prevent covering kernel data.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.132 2008/04/18 06:42:21 djm Exp $	*/
d568 1
d1152 1
a1152 1
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
d1157 3
d1173 3
d1190 6
@


1.132
log
@use arc4random_uniform() for random number requests that are not a
power of two.

use arc4random_bytes() when requesting more than a word of PRNG
output.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.131 2007/09/22 09:57:40 martin Exp $	*/
d441 1
a441 1
	vaddr_t va, addr = round_page(vstart), eaddr, t;
d525 18
a542 5
	t = (vaddr_t)&etext;
	if (btlb_insert(HPPA_SID_KERNEL, 0, 0, &t,
	    pmap_sid2pid(HPPA_SID_KERNEL) |
	    pmap_prot(pmap_kernel(), UVM_PROT_RX)) < 0)
		printf("WARNING: cannot block map kernel text\n");
@


1.131
log
@replace even more ctob and btoc with ptoa and atop respectively plus
uvm_extern.h where needed
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.130 2007/04/13 18:57:49 art Exp $	*/
d642 1
a642 1
	for (space = 1 + (arc4random() % hppa_sid_max);
@


1.130
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.129 2007/04/04 17:44:45 art Exp $	*/
d543 1
a543 1
	npdes = nkpdes + (physmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
@


1.129
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.128 2005/12/25 21:39:04 miod Exp $	*/
d198 1
a198 1
	pg->pg_flags &= ~PG_BUSY;		/* never busy */
@


1.128
log
@KERN_RESOURCE_SHORTAGE -> ENOMEM
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.127 2005/10/26 18:35:44 martin Exp $	*/
d198 1
a198 1
	pg->flags &= ~PG_BUSY;		/* never busy */
d238 1
a238 1
		if (ptp->flags & PG_BUSY)
@


1.127
log
@no more hppa_round_page() and hppa_trunc_page() macros

ok mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.126 2005/04/18 12:19:44 mickey Exp $	*/
d769 1
a769 1
			return (KERN_RESOURCE_SHORTAGE);
d817 1
a817 1
				return (KERN_RESOURCE_SHORTAGE);
@


1.126
log
@oops. uvm_page_physload() has different semantics for start and end...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.125 2005/04/07 13:16:12 mickey Exp $	*/
d441 1
a441 1
	vaddr_t va, addr = hppa_round_page(vstart), eaddr, t;
d483 1
a483 1
	size = hppa_round_page((hppa_sid_max + 1) * 4);
d916 1
a916 1
	sva = hppa_trunc_page(sva);
@


1.125
log
@on some machines it's possible to limit the amount of physmem initialised dring post so having msgbuf at the end of memory shall work
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.124 2005/04/07 00:21:51 mickey Exp $	*/
d563 1
a563 1
	uvm_page_physload(0, physmem,
@


1.124
log
@64bit-friendly pdc.h and iomod.h and correspondent changes elsewhere
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.123 2005/03/15 05:09:23 mickey Exp $	*/
d441 1
a441 1
	vaddr_t va, addr = hppa_round_page(vstart), t;
d561 2
a562 1
	DPRINTF(PDB_INIT, ("physmem: 0x%x - 0x%x\n", resvphysmem, physmem));
d564 1
a564 1
	    resvphysmem, physmem, VM_FREELIST_DEFAULT);
@


1.123
log
@hafta flush pcb once active (and a tush tush tush)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.122 2004/11/27 20:05:26 mickey Exp $	*/
d1191 1
a1191 1
	if (pa >= HPPA_IOSPACE)
@


1.122
log
@use a pm_lock define instead of the long fluff
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.121 2004/09/19 01:30:11 mickey Exp $	*/
d1121 1
@


1.121
log
@turn totalphysmem into physmem and name old physmem into smth it really is
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.120 2004/09/18 06:43:23 mickey Exp $	*/
d464 1
a464 1
	simple_lock_init(&kpm->pm_obj.vmobjlock);
d635 1
a635 1
	simple_lock_init(&pmap->pm_obj.vmobjlock);
d670 1
a670 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d672 1
a672 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d732 1
a732 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d734 1
a734 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d762 1
a762 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d767 1
a767 1
			simple_unlock(&pmap->pm_obj.vmobjlock);
d815 1
a815 1
				simple_unlock(&pmap->pm_obj.vmobjlock);
d833 1
a833 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d856 1
a856 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d895 1
a895 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d918 1
a918 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d952 1
a952 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d974 1
a974 1
		simple_lock(&pmap->pm_obj.vmobjlock);
d986 1
a986 1
		simple_unlock(&pmap->pm_obj.vmobjlock);
d1005 1
a1005 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1015 1
a1015 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1042 1
a1042 1
		simple_lock(&pmap->pm_obj.vmobjlock);
d1062 1
a1062 1
		simple_unlock(&pmap->pm_obj.vmobjlock);
d1080 1
a1080 1
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
d1082 1
a1082 1
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
d1100 1
a1100 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1102 1
a1102 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1182 1
a1182 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1211 1
a1211 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1236 1
a1236 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1266 1
a1266 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
@


1.120
log
@map rodata rdonly in case it gets pushed out of the text batc
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.119 2004/09/15 01:00:41 mickey Exp $	*/
d438 2
a439 2
	extern int etext, __rodata_end, __data_start;
	extern u_int totalphysmem, *ie_mem;
d539 1
a539 1
	nkpdes = totalphysmem >> 14;	/* at least 16/gig for kmem */
d543 1
a543 1
	npdes = nkpdes + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
d560 4
a563 4
	physmem = atop(addr);
	DPRINTF(PDB_INIT, ("physmem: 0x%x - 0x%x\n", physmem, totalphysmem));
	uvm_page_physload(0, totalphysmem,
	    physmem, totalphysmem, VM_FREELIST_DEFAULT);
d566 1
a566 1
	for (va = 0; va < ptoa(totalphysmem); va += PAGE_SIZE) {
a1220 3
#ifdef PMAPDEBUG
	extern u_int totalphysmem;
#endif
d1230 1
a1230 1
	if (va < ptoa(totalphysmem)) {
@


1.119
log
@kill old dead comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.118 2004/09/14 23:56:55 mickey Exp $	*/
d572 2
@


1.118
log
@allow kenter_pa to remap ptes and do proper wired/resident accounting there
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.117 2004/09/14 23:18:58 mickey Exp $	*/
d538 1
a538 6
	/*
	 * NOTE: we no longer trash the BTLB w/ unused entries,
	 * lazy map only needed pieces (see bus_mem_add_mapping() for refs).
	 */

	/* kernel virtual is the last gig of the mohicans */
@


1.117
log
@need an include and dprintf on pmap_init() return
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.116 2004/09/14 22:23:03 mickey Exp $	*/
d315 1
a315 1
		    va < VM_MAX_KERNEL_ADDRESS; va += PAGE_SIZE) {
d529 1
a529 2
		panic("pmap_bootstrap: cannot block map kernel text");
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count = atop(t);
d1180 1
a1180 1
	pt_entry_t pte;
d1190 1
a1190 6
#ifdef DIAGNOSTIC
	if ((pte = pmap_pte_get(pde, va)))
		panic("pmap_kenter_pa: 0x%lx is already mapped %p:0x%x",
		    va, pde, pte);
#endif

d1196 4
@


1.116
log
@better nkpdes calculation including pdes for io space now; move uvm_page_physload() to later where pdes are all mapped already
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.115 2004/08/05 09:06:24 mickey Exp $	*/
d46 1
d618 2
@


1.115
log
@keep at least a page in pv_entries pool and uppen the high mark
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.114 2004/08/01 07:13:49 mickey Exp $	*/
d543 2
a544 2
	/* takes about 16 per gig of initial kmem ... */
	nkpdes = totalphysmem >> 14;
d546 2
a547 1
		nkpdes = 4;	/* ... but no less than four */
a548 2
	uvm_page_physload(0, totalphysmem,
	    atop(addr) + npdes, totalphysmem, VM_FREELIST_DEFAULT);
d553 1
a553 1
		/* last four pdes are for the kernel virtual */
d556 2
d566 3
@


1.114
log
@oops. cannot sleep on pool_get
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.113 2004/06/30 18:11:48 mickey Exp $	*/
d97 1
d593 2
@


1.113
log
@fix a panic msg
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.112 2004/06/09 20:17:23 tedu Exp $	*/
d377 1
a377 1
	pv = pool_get(&pmap_pv_pool, 0);
@


1.112
log
@rename POOLPAGE macros to pmap_map_direct
break out uvm_km_page bits for this case, no thread here
lots of testing tech@@, deraadt@@, naddy@@, mickey@@, ...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.111 2004/05/27 21:04:07 tedu Exp $	*/
d237 1
a237 1
			panic("pmap_destroy: busy page table page");
@


1.111
log
@hppa pmap needs to use old pool allocator for now.
i know the fix, but it's not here yet.
ok mickey
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.110 2004/04/21 22:17:55 mickey Exp $	*/
a101 28
/*
 * workaround until the uvm_km_getpage can be used this early.
 */
void *hppa_pool_page_alloc(struct pool *, int);
void hppa_pool_page_free(struct pool *, void *);

void *
hppa_pool_page_alloc(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object,
	    waitok)); 
}

void
hppa_pool_page_free(struct pool *pp, void *v)
{

	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
}

struct pool_allocator hppa_pool_allocator = {
	hppa_pool_page_alloc, hppa_pool_page_free, 0,
};



d591 1
a591 1
	    &hppa_pool_allocator);
@


1.110
log
@fix locking
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.109 2004/04/21 22:14:34 mickey Exp $	*/
d102 28
d619 1
a619 1
	    &pool_allocator_nointr);
@


1.109
log
@put some volatile on volatiles
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.108 2004/04/07 18:24:19 mickey Exp $	*/
a407 1
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
a413 1
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
d626 1
d760 2
a761 1
		if (flags & PMAP_CANFAIL)
d763 1
d802 1
a804 1

d808 1
d817 1
a1192 2
	simple_unlock(&pmap->pm_obj.vmobjlock);

d1206 2
@


1.108
log
@update copyright; miod@@ is fine w/ files where he holds it too
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.107 2004/01/14 09:12:49 mickey Exp $	*/
d140 1
a140 1
pmap_sdir_set(pa_space_t space, u_int32_t *pd)
d142 1
a142 1
	u_int32_t *vtop;
d161 2
a162 2
static __inline pt_entry_t *
pmap_pde_get(u_int32_t *pd, vaddr_t va)
d207 1
a207 1
pmap_pde_ptp(struct pmap *pm, pt_entry_t *pde)
d232 2
d235 4
a239 2
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d244 1
a244 1
pmap_pte_get(pt_entry_t *pde, vaddr_t va)
d250 1
a250 1
pmap_pte_set(pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
d288 1
a288 1
	pt_entry_t *pde;
d303 2
a304 1
		pt_entry_t *pde, pte;
d603 1
a603 1
		pt_entry_t *pde;
d747 2
a748 1
	pt_entry_t *pde, pte;
d838 2
a839 1
	pt_entry_t *pde, pte;
d899 2
a900 1
	pt_entry_t *pde, pte;
d962 2
a963 1
		pt_entry_t *pde, pte;
d991 2
a992 1
	pt_entry_t *pde, pte = 0;
d1030 2
a1031 1
		pt_entry_t *pde, opte, pte;
d1167 2
a1168 1
	pt_entry_t *pde, pte;
d1218 2
a1219 1
	pt_entry_t *pde, pte;
@


1.107
log
@catching a stray sheep destroying a pmap shall result in a proper haggis; this fixes a panic resulted from a diagnostic check und clean whose dirty little causes should really get fixed (eventually); w/ grumpy testing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.106 2003/12/20 21:49:06 miod Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2003 Michael Shalayeff
a14 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Michael Shalayeff.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
d19 8
a26 7
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF MIND,
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
@


1.106
log
@Pass -Wformat, fix a few uninitialized variables as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.105 2003/12/01 23:56:30 mickey Exp $	*/
d673 36
a708 6
		printf("pmap_destroy: unaccounted ptp 0x%lx count %d\n",
		    VM_PAGE_TO_PHYS(pg), pg->wire_count);
		if (pg->flags & PG_BUSY)
			panic("pmap_destroy: busy page table page");
		pg->wire_count = 0;
		uvm_pagefree(pg);
@


1.105
log
@when allocating kpdes -- allocate no less than four
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.104 2003/11/24 19:27:03 mickey Exp $	*/
d311 2
a312 2
		for (pdemask = 1, va = sva? sva: 0; va < VM_MAX_KERNEL_ADDRESS;
		    va += PAGE_SIZE) {
d319 1
a319 1
				printf("%x:0x%08x:\n", sp, pde);
d325 1
a325 1
			printf("0x%08x-0x%08x:%b\n", va, pte & ~PAGE_MASK,
d340 1
a340 1
		printf("%x:%x\n", pve->pv_pmap->pm_space, pve->pv_va);
d673 1
a673 1
		printf("pmap_destroy: unaccounted ptp 0x%x count %d\n",
d877 1
a877 1
	for(pdemask = 1; sva < eva; sva += PAGE_SIZE) {
d976 1
a976 1
		panic("pmap_unwire: invalid va 0x%x", va);
d1139 1
a1139 1
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%x", va);
d1142 1
a1142 1
		panic("pmap_kenter_pa: 0x%x is already mapped %p:0x%x",
@


1.104
log
@experimental support for HVT as a 2nd level tlb
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.103 2003/10/31 21:24:19 mickey Exp $	*/
d543 4
a546 2
	/* takes about 16 per gig of initial kmem */
	nkpdes = (totalphysmem >> 14);
@


1.103
log
@allocate initial pdes for the kmem proportional to the totalphysmem to allow large memory sizes and also more verbose diagnostic message in pmap_destroy()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.102 2003/05/07 21:50:43 mickey Exp $	*/
d94 2
a95 3
#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
int		pmap_hptsize = 256;	/* patchable */
#endif
d124 1
a124 1
#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
d128 9
a136 2
static inline struct hpt_entry *
pmap_hash(pa_space_t sp, vaddr_t va)
d138 2
a139 11
	struct hpt_entry *hpt;
	__asm __volatile (
		"extru	%2, 23, 20, %%r22\n\t"	/* r22 = (va >> 8) */
		"zdep	%1, 22, 16, %%r23\n\t"	/* r23 = (sp << 9) */
		"xor	%%r22,%%r23, %%r23\n\t"	/* r23 ^= r22 */
		"mfctl	%%cr24, %%r22\n\t"	/* r22 = sizeof(HPT)-1 */
		"and	%%r22,%%r23, %%r23\n\t"	/* r23 &= r22 */
		"mfctl	%%cr25, %%r22\n\t"	/* r22 = addr of HPT table */
		"or	%%r23, %%r22, %0"	/* %0 = HPT entry */
		: "=r" (hpt) : "r" (sp), "r" (va) : "r22", "r23");
	return hpt;
d275 8
a441 3
#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	struct vp_entry *hptp;
#endif
d492 12
a503 13
#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	if (pmap_hptsize && (cpu_type == hpcxl || cpu_type == hpcxl2)) {
		int error;

		if (pmap_hptsize > pdc_hwtlb.max_size)
			pmap_hptsize = pdc_hwtlb.max_size;
		else if (pmap_hptsize < pdc_hwtlb.min_size)
			pmap_hptsize = pdc_hwtlb.min_size;

		size = pmap_hptsize * sizeof(*hptp);
		bzero((void *)addr, size);
		/* Allocate the HPT */
		for (hptp = (struct vp_entry *)addr, i = pmap_hptsize; i--;)
d505 2
d508 2
a509 1
		DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n", size, addr));
d511 8
a518 11
		if ((error = (cpu_hpt_init)(addr, size)) < 0) {
			printf("WARNING: HPT init error %d\n", error);
		} else {
			printf("HPT: %d entries @@ 0x%x\n",
			    pmap_hptsize / sizeof(struct vp_entry), addr);
		}

		/* TODO find a way to avoid using cr*, use cpu regs instead */
		mtctl(addr, CR_VTOP);
		mtctl(size - 1, CR_HPTMASK);
		addr += size;	/* should keep the alignment right */
d520 1
a520 1
#endif	/* HP7100LC_CPU | HP7300LC_CPU */
@


1.102
log
@double semicolon in local var decl
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.101 2003/04/23 20:21:36 mickey Exp $	*/
d441 1
a441 1
	int npdes;
d542 3
a544 2
	/* four more for the the kernel virtual */
	npdes = 4 + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
d552 1
a552 1
		if (npdes == 3)
d670 2
a671 2
		printf("pmap_destroy: unaccounted ptp 0x%x\n",
		    VM_PAGE_TO_PHYS(pg));
@


1.101
log
@be extra cautious in managing pv-entries for pages that are not managable
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.100 2003/02/26 22:32:42 mickey Exp $	*/
d1067 1
a1067 1
	struct pcb *pcb = &p->p_addr->u_pcb;;
@


1.100
log
@minor fix in a debug printf
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.99 2003/02/18 09:40:43 miod Exp $	*/
a99 1
boolean_t	pmap_initialized = FALSE;
d103 1
d116 1
a116 5
	struct vm_page *pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO);

	if (!pg) {
		/* wait and pageout */
d118 3
a120 2
		return (NULL);
	}
a261 3
	if (pte && pmap_initialized && pte < physical_end &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page)
		panic("pmap_pte_set: invalid pte 0x%x", pte);
d528 1
a528 2
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
	    physmem = atop(t);
d550 1
a550 1
		/* last pde is for the start of kernel virtual */
d560 2
d590 1
a590 1
	pmap_initialized = TRUE;
d768 1
a768 1
	if (pmap_initialized) {
a775 1
		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa));
d803 1
d837 2
a838 2
			if (pmap_initialized) {
				struct vm_page *pg;
a839 1
				pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
d920 2
a921 1
	for (pve = pg->mdpage.pvh_list; pve; ) {
a938 4

		ppve = pve;
		pve = pve->pv_next;
		pmap_pv_free(ppve);
d1152 3
a1154 2
		if (pmap_initialized) {
			struct vm_page *pg;
a1155 1
			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
d1177 1
d1207 1
a1207 2
		if (pmap_initialized) {
			struct vm_page *pg;
a1208 1
			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
@


1.99
log
@Let kernels compile without DDB, USELEDS, and DIAGNOSTIC.
For future intallation media.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.98 2003/02/05 18:54:22 mickey Exp $	*/
d813 1
a813 1
	    ("pmap_remove(%p, 0x%x, 0x%x\n", pmap, sva, eva));
@


1.98
log
@in clearbits only flush the pte if it has changed. bits/bit in testbit
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.97 2003/01/31 23:42:28 mickey Exp $	*/
d658 1
d660 1
@


1.97
log
@do not merge the rodata sections into the .text . they could be moved above 4m later should there be too much code
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.96 2003/01/29 17:06:05 mickey Exp $	*/
d997 1
a997 1
		pt_entry_t *pde, pte;
d1001 1
a1001 1
			pte = pmap_pte_get(pde, va);
a1008 2
			pmap_pte_flush(pmap, va, pte);
			res |= pmap_pvh_attrs(pte);
d1012 1
d1014 4
a1017 1
			pmap_pte_set(pde, va, pte);
d1023 1
a1023 1
	return ((res & clear) != 0);
d1027 1
a1027 1
pmap_testbit(struct vm_page *pg, u_int bits)
d1032 1
a1032 1
	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bits));
d1035 1
a1035 1
	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bits) && pve;
d1044 1
a1044 1
	return ((pg->mdpage.pvh_attrs & bits) != 0);
@


1.96
log
@ref bit is reverse meaning in the pte, therefore should be
set on initial mapping (both enter and kenter), this seems to
repair numerous userland issues.
do not play DIRTY games either.
repair border condition on the batch unmap (as well).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.95 2003/01/22 23:56:33 mickey Exp $	*/
d438 1
a438 1
	extern char etext, etext1;
d529 1
a529 1
	t = (vaddr_t)&etext1;
d537 3
a539 3
	if (&etext < &etext1) {
		physical_steal = (vaddr_t)&etext;
		physical_end = (vaddr_t)&etext1;
d572 1
a572 1
		if (va < (vaddr_t)&etext1)
d580 1
a580 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext1, va));
@


1.95
log
@make pmapdebug be zero by default
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.94 2003/01/22 21:01:42 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2002 Michael Shalayeff
d761 1
a761 1
		pte = PTE_PROT(0);
d822 1
a822 1
			batch = pdemask == sva && sva + ~PDE_MASK + 1 < eva;
a902 2
			if (!(tlbprot & TLB_WRITE))
				pte &= ~PTE_PROT(TLB_DIRTY);
d1147 2
a1148 1
	pte = pa | PTE_PROT(TLB_WIRED|pmap_prot(pmap_kernel(), prot));
@


1.94
log
@fix wired accounting in pmap_enter()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.93 2003/01/22 18:16:34 mickey Exp $	*/
d78 2
a79 2
	| PDB_INIT
	| PDB_FOLLOW
@


1.93
log
@consistantly use uvm_prot_* vs vm_prot_* evewrhere
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.92 2003/01/22 16:59:45 mickey Exp $	*/
d743 1
a743 1
		if (wired && !(pte & PTE_PROT(TLB_WIRED)) == 0)
d745 1
a745 1
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)) != 0)
@


1.92
log
@use the same style for the pdemask-optimized loops (four of 'em)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.91 2002/12/19 00:16:20 mickey Exp $	*/
d453 8
a460 8
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_NA;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_R;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
@


1.91
log
@debugging check_alias is only needed in PMAPDEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.90 2002/11/07 19:22:56 mickey Exp $	*/
d304 2
a306 2
		pt_entry_t *pde, pte;
		vaddr_t va, pdemask = 1;
d312 1
a312 1
		for (va = sva? sva : 0; va < VM_MAX_KERNEL_ADDRESS;
d807 1
a808 1
	u_int pdemask;
d815 1
a815 1
	for (batch = 0, pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
d877 1
a877 1
	for(pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
d1181 1
a1181 1
	vaddr_t eva = va + size, pdemask;
d1195 1
a1195 1
	for (pdemask = va + 1; va < eva; va += PAGE_SIZE) {
@


1.90
log
@pmap_[de]activate() are not nops; w/ art@@'s help
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.89 2002/11/07 19:18:18 mickey Exp $	*/
d411 1
d414 1
@


1.89
log
@make pte flushing into a separate function,
add a missing pte flush in page_remove(),
removed a few dead debugs and ifdefs.
use local vars inestead of long -> chains./
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.88 2002/10/30 23:55:58 mickey Exp $	*/
d1066 16
@


1.88
log
@as found on a 256m machine of millert@@'s allocate pdes for the whole kernel virtual and do not bother (just four anyway); as a side work do not adjust addr,size since there is no need
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.87 2002/10/28 20:49:16 mickey Exp $	*/
a239 1
#if 1
a243 1
#endif
a267 7
#if 0
	if (pte && !(pte & PTE_PROT(TLB_UNCACHABLE)) &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page) {
		printf("pmap_pte_set: cached pte\n");
		Debugger();
	}
#endif
d275 11
d740 1
a740 6
		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(pmap->pm_space, va, PAGE_SIZE);
		pitlb(pmap->pm_space, va);
		fdcache(pmap->pm_space, va, PAGE_SIZE);
		pdtlb(pmap->pm_space, va);

a819 1
			/* XXX not until ptp acct works
a820 1
			*/
a824 4
			if (pte & PTE_PROT(TLB_WIRED))
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;

d829 4
a832 5
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);
d900 1
a900 6
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);

a916 1
	pt_entry_t *pde, pte;
d925 5
a929 1
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
d931 3
a933 3
		pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va);
		pte = pmap_pte_get(pde, pve->pv_va);
		pmap_pte_set(pde, pve->pv_va, 0);
d935 1
d937 2
a938 2
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;
d940 2
a941 1
		simple_unlock(&pve->pmap->pm_obj.vmobjlock);
a942 1
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d987 1
a987 1
	pt_entry_t *pde, pte, res;
d995 7
a1001 3
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
		if ((pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va))) {
			pte = pmap_pte_get(pde, pve->pv_va);
d1005 1
a1005 1
				    pve->pv_va);
d1009 1
a1009 10
			if (pte & PTE_PROT(TLB_EXECUTE)) {
				ficache(pve->pv_pmap->pm_space,
				    pve->pv_va, PAGE_SIZE);
				pitlb(pve->pv_pmap->pm_space, pve->pv_va);
			}

			/* XXX flush only if there was mod ? */
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
			pdtlb(pve->pv_pmap->pm_space, pve->pv_va);

d1013 1
a1013 1
			pg->mdpage.pvh_attrs = pmap_pvh_attrs(pte);
d1015 1
a1015 1
			pmap_pte_set(pde, pve->pv_va, pte);
d1017 1
a1017 1
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
d1192 1
a1192 6
		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(HPPA_SID_KERNEL, va, PAGE_SIZE);
		pitlb(HPPA_SID_KERNEL, va);
		fdcache(HPPA_SID_KERNEL, va, PAGE_SIZE);
		pdtlb(HPPA_SID_KERNEL, va);

@


1.87
log
@do not use asm for accessing the page tables since they are mapped now.
fix ptp accounting and move diagnostic check in pmap_destroy()
into a DIAGNOSTIC and it has not caught a one problem so far.
when random-allocating the space ids use linear rehashing instead
of a full new random which produces a better cache locality.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.86 2002/10/17 02:21:08 mickey Exp $	*/
d437 1
a437 1
	vaddr_t va, endaddr, addr = hppa_round_page(vstart), t;
d519 1
a519 1
		addr += size;
d524 1
d545 2
a546 4
	/* one for the start of the kernel virtual */
	npdes = 1 + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
	addr = round_page(addr);
	size = npdes * PAGE_SIZE;
d548 1
a548 1
	    atop(addr + size), totalphysmem, VM_FREELIST_DEFAULT);
d554 1
a554 1
		if (!npdes)
d576 1
a576 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext1, endaddr));
@


1.86
log
@convert to use vm_page_md instead of pmap_physseg, make code smaller and simpler, indeed; after art's suggestion and by looking into his diffs oneyed
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.85 2002/09/15 09:42:58 mickey Exp $	*/
d150 1
a150 1
pmap_sdir_set(pa_space_t space, paddr_t pa)
d152 1
a152 1
	paddr_t vtop;
d159 1
a159 2
	asm("stws	%0, 0(%1)\n\tsync"
	    :: "r" (pa), "r" (vtop + (space << 2)));
d162 1
a162 1
static __inline paddr_t
d165 1
a165 1
	paddr_t vtop, pa;
d168 1
a168 4
	asm("ldwx,s	%2(%1), %0\n\tsync"
	    : "=&r" (pa) : "r" (vtop), "r" (space));

	return (pa);
d172 1
a172 1
pmap_pde_get(paddr_t pa, vaddr_t va)
d174 1
a174 6
	pt_entry_t *pde;

	asm("ldwx,s	%2(%1), %0\n\tsync"
	    : "=&r" (pde) : "r" (pa), "r" (va >> 22));

	return (pde);
d187 1
a187 2
	asm("stws	%0, 0(%1)\n\tsync"
	    :: "r" (ptp), "r" ((paddr_t)pm->pm_pdir + ((va >> 20) & 0xffc)));
d237 1
a237 2
	ptp->wire_count--;
	if (ptp->wire_count <= 1 && pmap != pmap_kernel()) {
d240 1
a240 4

		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
#if 0
d246 2
d254 1
a254 6
	pt_entry_t pte;

	asm("ldwx,s	%2(%1),%0"
	    : "=&r" (pte) : "r" (pde),  "r" ((va >> 12) & 0x3ff));

	return (pte);
d280 2
a281 2
	asm("stws	%0, 0(%1)"
	    :: "r" (pte), "r" ((paddr_t)pde + ((va >> 10) & 0xffc)));
d302 1
a302 1
		paddr_t pa;
d307 1
a307 1
		    !(pa = pmap_sdir_get(sp)))
d314 1
a314 1
				if (!(pde = pmap_pde_get(pa, va))) {
d355 1
a355 1
			printf("pmap_pv_enter: "
d471 1
a471 1
	kpm->pm_pdir = addr;
d633 2
a634 3
	do
		space = 1 + (arc4random() % hppa_sid_max);
	while (pmap_sdir_get(space));
d639 1
a639 1
	pmap->pm_pdir = VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
a666 1
	TAILQ_FOREACH(pg, &pmap->pm_obj.memq, listq) {
d668 3
a672 1
#endif
d676 2
a677 1

d679 1
a679 2
	pmap->pm_pdir_pg = NULL;	/* XXX cache it? */
	pmap_sdir_set(pmap->pm_space, 0);
@


1.85
log
@better alias checking, verified w/ the regress
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.84 2002/09/10 22:37:46 mickey Exp $	*/
d350 1
a350 1
	struct pv_head *pvh;
a351 5
	int bank, off;

	bank = vm_physseg_find(atop(pa), &off);
	if (bank != -1)
		return;
d353 3
a355 3
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	simple_lock(&pvh->pvh_lock);
	for(pve = pvh->pvh_list; pve; pve = pve->pv_next)
d357 1
a357 1
	simple_unlock(&pvh->pvh_lock);
d415 1
a415 1
pmap_pv_enter(struct pv_head *pvh, struct pv_entry *pve, struct pmap *pm,
d419 1
a419 1
	    pvh, pve, pm, va, pdep));
d423 3
a425 3
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;
	pvh->pvh_list = pve;
d428 1
a428 1
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
d432 1
a432 1
pmap_pv_remove(struct pv_head *pvh, struct pmap *pmap, vaddr_t va)
d436 3
a438 2
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	for(pv = *(pve = &pvh->pvh_list); pv; pv = *(pve = &(*pve)->pv_next))
d443 1
a443 1
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
a560 6
	addr = hppa_round_page(addr);
	size = hppa_round_page(sizeof(struct pv_head) * totalphysmem);
	bzero ((caddr_t)addr, size);

	DPRINTF(PDB_INIT, ("pmseg.pvhead: 0x%x @@ 0x%x\n", size, addr));

d563 2
a564 1
	endaddr = addr + size + npdes * PAGE_SIZE;
d566 1
a566 3
	    atop(endaddr), totalphysmem, VM_FREELIST_DEFAULT);
	vm_physmem[0].pmseg.pvhead = (struct pv_head *)addr;
	addr += size;
d730 1
a730 2
	struct vm_page *ptp = NULL;
	struct pv_head *pvh;
a731 1
	int bank, off;
d773 4
a776 9
		bank = vm_physseg_find(atop(PTE_PAGE(pte)), &off);
		if (bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_pv_remove(pvh, pmap, va);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pvh->pvh_lock);
		} else
			pve = NULL;
d789 2
a790 2
	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
d798 4
a801 7
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
		pmap_pv_enter(pvh, pve, pmap, va, ptp);
	} else {
		pvh = NULL;
		if (pve)
			pmap_pv_free(pve);
	}
a823 1
	struct pv_head *pvh;
d826 1
a826 1
	int bank, off, batch;
d866 7
a872 6
			bank = vm_physseg_find(atop(pte), &off);
			if (pmap_initialized && bank != -1) {
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				simple_lock(&pvh->pvh_lock);
				pvh->pvh_attrs |= pmap_pvh_attrs(pte);
				if ((pve = pmap_pv_remove(pvh, pmap, sva)))
d874 1
a874 1
				simple_unlock(&pvh->pvh_lock);
d891 1
a893 2
	struct pv_head *pvh;
	int bank, off;
d923 4
a926 9
			bank = vm_physseg_find(atop(pte), &off);
			if (bank == -1) {
				printf("pmap_write_protect: unmanaged page?\n");
				return;
			}
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pvh->pvh_lock);
a948 1
	struct pv_head *pvh;
a950 1
	int bank, off;
d954 1
a954 8
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
		return;
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL)
d957 2
a958 3
	simple_lock(&pvh->pvh_lock);

	for (pve = pvh->pvh_list; pve; ) {
d971 1
a971 1
		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
d976 2
a977 2
	pvh->pvh_list = NULL;
	simple_unlock(&pvh->pvh_lock);
a1014 1
	struct pv_head *pvh;
a1016 1
	int bank, off;
d1021 3
a1023 10
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_changebit: unmanaged page?\n");
		return(FALSE);
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	simple_lock(&pvh->pvh_lock);
	res = pvh->pvh_attrs = 0;
	for(pve = pvh->pvh_list; pve; pve = pve->pv_next) {
d1047 1
a1047 1
			pvh->pvh_attrs = pmap_pvh_attrs(pte);
d1053 1
a1053 1
	simple_unlock(&pvh->pvh_lock);
a1060 1
	struct pv_head *pvh;
a1062 1
	int bank, off;
d1066 2
a1067 9
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_testbit: unmanaged page?\n");
		return(FALSE);
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	simple_lock(&pvh->pvh_lock);
	for(pve = pvh->pvh_list; !(pvh->pvh_attrs & bits) && pve;
d1072 1
a1072 1
		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
d1074 1
a1074 1
	simple_unlock(&pvh->pvh_lock);
d1076 1
a1076 1
	return ((pvh->pvh_attrs & bits) != 0);
d1103 1
a1103 1
pmap_flush_page(paddr_t pa, int purge)
a1104 1
	struct pv_head *pvh;
a1105 1
	int bank, off;
d1107 3
a1109 9
	bank = vm_physseg_find(atop(pa), &off);
#ifdef DIAGNOSTIC
	if (bank == -1)
		panic("pmap_flush_page: unmanaged page 0x%x", pa);
#endif
	/* purge all possible mappings for the pa */
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	simple_lock(&pvh->pvh_lock);
	for(pve = pvh->pvh_list; pve; pve = pve->pv_next)
d1114 1
a1114 1
	simple_unlock(&pvh->pvh_lock);
d1124 1
a1124 1
	pmap_flush_page(pa, 1);
d1136 2
a1137 2
	pmap_flush_page(spa, 0);
	pmap_flush_page(dpa, 1);
d1174 6
a1179 7
		struct pv_head *pvh;
		int bank, off;
		bank = vm_physseg_find(atop(pte), &off);
		if (pmap_initialized && bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			if (pmap_check_alias(pvh->pvh_list, va, pte))
d1181 1
a1181 1
			simple_unlock(&pvh->pvh_lock);
a1196 1
	struct pv_head *pvh;
a1198 1
	int bank, off;
d1233 6
a1238 5
		bank = vm_physseg_find(atop(pte), &off);
		if (pmap_initialized && bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
d1240 1
a1240 1
			if ((pve = pmap_pv_remove(pvh, pmap_kernel(), va)))
d1242 1
a1242 1
			simple_unlock(&pvh->pvh_lock);
@


1.84
log
@randomize space ids, plus we are not gonna run out now (;
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.83 2002/09/10 22:25:46 mickey Exp $	*/
d368 1
a368 1
pmap_check_alias(struct pv_entry *pve, vaddr_t va)
d370 1
a370 2
	pt_entry_t pte;
	int ret = 0;
d373 2
a374 2
	for (pte = 0; pve; pve = pve->pv_next) {
		pt_entry_t pte1 = pmap_vp_find(pve->pv_pmap, pve->pv_va);
d376 1
a376 1
		    pte && (pte1 & PTE_PROT(TLB_WRITE))) {
d378 1
a378 1
			    "aliased writable mapping %d:0x%x",
a381 1
		pte |= pte1;
d431 1
a431 1
	if (pmap_check_alias(pve, va))
d1243 1
a1243 1
			if (pmap_check_alias(pvh->pvh_list, va))
@


1.83
log
@comment out a bit of unused code now
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.82 2002/09/10 18:29:43 art Exp $	*/
d51 2
d99 1
a99 1
int		pmap_sid_counter, hppa_sid_max = HPPA_SID_MAX;
d663 3
a665 5
	if (pmap_sid_counter >= hppa_sid_max) {
		/* collect some */
		panic("pmap_create: outer space");
	} else
		space = ++pmap_sid_counter;
@


1.82
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.81 2002/09/05 18:41:19 mickey Exp $	*/
d858 1
a858 1
	for (pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
d865 1
d867 1
@


1.81
log
@move the kernel virtual away from the physical addresses
and equivalently map the whole physical.
this allows a lot of simplification in how kernel
deals w/ page tables and physical pages.
avoid series of bugs related to that.
check for aliased mappings (none found so far),
and properly flush/purge pages on zero/copy.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.80 2002/07/31 05:03:30 mickey Exp $	*/
d1181 1
a1181 2
pmap_zero_page(pa)
	paddr_t pa;
d1183 2
d1193 1
a1193 3
pmap_copy_page(spa, dpa)
	paddr_t spa;
	paddr_t dpa;
d1195 2
@


1.80
log
@init pm_ptphint, hint from art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.79 2002/07/23 16:08:57 mickey Exp $	*/
d26 2
a27 2
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * MOND, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
d76 2
a77 2
/*	| PDB_INIT */
/*	| PDB_FOLLOW */
a89 1
vaddr_t	virtual_avail;
a90 1
vaddr_t pmap_pv_page, pmap_pv_page_end;
a102 7
void    *pmap_pv_page_alloc(struct pool *, int);
void    pmap_pv_page_free(struct pool *, void *);

struct pool_allocator pmap_allocator_pv = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0
};

d111 15
d130 1
a130 1
static __inline struct hpt_entry *
d157 1
a157 1
	asm("stwas	%0, 0(%1)\n\tsync"
d167 1
a167 1
	asm("ldwax,s	%2(%1), %0\n\tsync"
d178 1
a178 1
	asm("ldwax,s	%2(%1), %0\n\tsync"
d194 1
a194 1
	asm("stwas	%0, 0(%1)\n\tsync"
d207 1
a207 26
	/* special hacking for pre-mapping the kernel */
	if (!pmap_initialized) {
		register u_int32_t sm;

		if (physical_steal >= physical_end)
			panic("pmap_pde_alloc: out of steallage");

		pa = physical_steal;
		physical_steal += PAGE_SIZE;
		rsm(0, sm);
		if (sm & PSW_D)
			pmap_zero_page(pa);
		else
			bzero((void *)pa, PAGE_SIZE);
		pmap_pde_set(pm, va, pa);
		pm->pm_stats.resident_count++;	/* count PTP as resident */

		DPRINTF(PDB_VP, ("pmap_pde_alloc: pde %x\n", pa));
		return ((pt_entry_t *)pa);
	}

	va &= PDE_MASK;
	pg = uvm_pagealloc(&pm->pm_obj, va, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (pg == NULL) {
		/* try to steal somewhere */
a208 1
	}
d250 3
a254 2
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d257 1
d266 1
a266 1
	asm("ldwax,s	%2(%1),%0"
d284 2
a285 2
		panic("pmap_pte_set: invalid pte");

d291 1
a291 1

d295 1
a295 1
	asm("stwas	%0, 0(%1)"
d305 1
a305 1
		return (NULL);
d310 1
d319 1
a319 1
		vaddr_t va, pdemask = virtual_avail + 1;
d325 1
a325 1
		for (va = sva? sva : virtual_avail; va < VM_MAX_KERNEL_ADDRESS;
d345 43
a424 1

d431 2
d441 1
d447 1
d458 1
a458 1
	vaddr_t va, addr = hppa_round_page(vstart), t;
d464 1
d544 1
a544 1
	/* XXX PCXS needs two separate inserts in separate btlbs */
a565 3
	pmap_pv_page = addr;
	pmap_pv_page_end =
	addr += (totalphysmem / (16 * 1024)) * PAGE_SIZE;
a567 1
	virtual_avail = addr + size;
d569 1
a569 1
	DPRINTF(PDB_INIT, ("pmseg.pvent: 0x%x @@ 0x%x\n", size, addr));
d571 3
a573 1
	/* XXX we might need to split this for isa */
d575 1
a575 2
		atop(virtual_avail), totalphysmem, VM_FREELIST_DEFAULT);
	/* we have only one initial phys memory segment */
d577 4
d582 12
a593 6
	addr += size;
	/* map .text to avoid reloading the btlb on heavy faults */
	for (va = 0; va < (vaddr_t)&etext1; va += PAGE_SIZE)
		pmap_kenter_pa(va, va, UVM_PROT_RX);
	/* now we know how much to map */
	for (; va < addr; va += PAGE_SIZE) {
d595 1
d597 6
a602 3
		pmap_kenter_pa(va, va,
		    (va == (vaddr_t)proc0paddr + USPACE)?
		    UVM_PROT_NONE : UVM_PROT_RW);
d605 1
a605 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext1, addr));
d613 2
d618 1
a618 3
	    &pmap_allocator_pv);

	simple_lock_init(&pvalloc_lock);
a625 2
	 *
	 * no spls since no interrupts.
d642 1
a642 1
	*startp = virtual_avail;
d667 1
a667 3
	pmap->pm_pdir_pg = uvm_pagealloc(NULL, 0, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (!pmap->pm_pdir_pg)
d679 1
a679 1
	return(pmap);
d800 1
a800 1
		pte = PTE_PROT(TLB_REFTRAP);
d828 1
a828 1
	pte = pa | PTE_PROT(TLB_UNCACHABLE|pmap_prot(pmap, prot)) |
d947 1
a947 1
				printf("pmap_page_remove: unmanaged page?\n");
d955 2
a956 1
			ficache(pmap->pm_space, sva, PAGE_SIZE);
d1013 1
d1062 1
a1062 1
		printf("pmap_testbits: unmanaged page?\n");
d1067 1
a1068 2

	simple_lock(&pvh->pvh_lock);
d1073 12
a1084 3
			res |= pmap_pvh_attrs(pte);
			pte &= ~clear;
			pte |= set;
a1085 2
			if (pte & PTE_PROT(TLB_EXECUTE))
				pitlb(pve->pv_pmap->pm_space, pve->pv_va);
d1090 5
a1098 1
	pvh->pvh_attrs = res;
d1116 1
a1116 1
		printf("pmap_testbits: unmanaged page?\n");
d1120 1
a1121 1
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
d1157 23
a1183 4
	extern int dcache_line_mask;
	register paddr_t pe = pa + PAGE_SIZE;
	int s;

d1186 3
a1188 28
	/*
	 * do not allow any ints to happen, since cache is in
	 * inconsistant state during the loop.
	 *
	 * do not rsm(PSW_I) since that will lose ints completely,
	 * instead, keep 'em pending (or verify by the book).
	 */
	s = splhigh();

	while (pa < pe) {
		__asm volatile(			/* can use ,bc */
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)"
		    : "+r" (pa) :: "memory");

		if (!(pa & dcache_line_mask))
			__asm volatile("rsm	%1, %%r0\n\t"
				       "nop ! nop ! nop\n\t"
				       "fdc	%2(%0)\n\t"
				       "nop ! nop ! nop\n\t"
				       "ssm	%1, %%r0"
			    :: "r" (pa), "i" (PSW_D), "r" (-4) : "memory");
	}

	sync_caches();
	splx(s);
a1195 4
	extern int dcache_line_mask;
	register paddr_t spe = spa + PAGE_SIZE;
	int s;

d1198 5
a1202 29
	s = splhigh();
	/* XXX flush cache for the sva (from spa) ??? */

	while (spa < spe) {
		__asm volatile(			/* can use ,bc */
		    "ldwas,ma	4(%0),%%r22\n\t"
		    "ldwas,ma	4(%0),%%r21\n\t"
		    "stwas,ma	%%r22,4(%1)\n\t"
		    "stwas,ma	%%r21,4(%1)\n\t"
		    "ldwas,ma	4(%0),%%r22\n\t"
		    "ldwas,ma	4(%0),%%r21\n\t"
		    "stwas,ma	%%r22,4(%1)\n\t"
		    "stwas,ma	%%r21,4(%1)\n\t"
		    : "+r" (spa), "+r" (dpa) :: "r22", "r21", "memory");

		if (!(spa & dcache_line_mask))
			__asm volatile(
			    "rsm	%2, %%r0\n\t"
			    "nop ! nop ! nop\n\t"
			    "pdc	%3(%0)\n\t"
			    "fdc	%3(%1)\n\t"
			    "nop ! nop\n\t"
			    "ssm	%2, %%r0"
			    :: "r" (spa), "r" (dpa), "i" (PSW_D), "r" (-4)
			    : "memory");
	}

	sync_caches();
	splx(s);
d1220 1
a1220 1
		panic("pmap_kenter_pa: cannot allocate pde");
d1227 2
a1228 2
	pte = pa | PTE_PROT(TLB_WIRED|TLB_DIRTY|pmap_prot(pmap_kernel(), prot));
	/* if (pa >= HPPA_IOSPACE) */
d1234 14
d1256 3
d1267 6
a1312 29
}

void *
pmap_pv_page_alloc(struct pool *pp, int flags)
{
	DPRINTF(PDB_FOLLOW|PDB_POOL,
	    ("pmap_pv_page_alloc(%p, %x)\n", pp, flags));

	if (pmap_pv_page < pmap_pv_page_end) {
		void *v = (void *)pmap_pv_page;
		pmap_pv_page += PAGE_SIZE;
		return (v);
	}

	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    (flags & PR_WAITOK) ? TRUE : FALSE));
}

void
pmap_pv_page_free(struct pool *pp, void *v)
{
	vaddr_t va = (vaddr_t)v;

	DPRINTF(PDB_FOLLOW|PDB_POOL, ("pmap_pv_page_free(%p, %p)\n", pp, v));

	if (va < virtual_avail)
		panic("pmap_pv_page_alloc: freeing the last page");
	else
		uvm_km_free_poolpage1(kernel_map, va);
@


1.79
log
@print page protection w/ bits in dump_table
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.78 2002/07/18 04:35:03 mickey Exp $	*/
a620 2
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;
a627 2
	pmap->pm_space = space;
	pmap->pm_pid = (space + 1) << 1;
d632 1
d634 1
d636 5
a640 1
	pmap_sdir_set(space, pmap->pm_pdir);
@


1.78
log
@make reserved pages for the pv_pool variable on physmem (should pool get preload in the future this is to be changed immidiately). map the page above the kernel stack unmapped
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.77 2002/07/17 22:08:07 mickey Exp $	*/
d356 2
a357 1
			printf("0x%08x-0x%08x\n", va, pte);
@


1.77
log
@be consistant w/ NBPG vs PAGE_SIZE
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.76 2002/05/20 06:13:00 mickey Exp $	*/
d92 1
a92 1
vaddr_t pmap_pv_page;
d203 2
d210 5
a214 2
		bzero((void *)pa, PAGE_SIZE);
		fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
d300 1
a300 1
	if (pte && pte < physical_end &&
d536 2
a537 1
	addr += PAGE_SIZE;
d551 3
d555 7
a561 2
	for (va = (vaddr_t)&etext1; va < addr; va += PAGE_SIZE)
		pmap_kenter_pa(va, va, UVM_PROT_RW);
d563 1
a563 2
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n",
	    &etext1, addr));
d1280 1
a1280 1
	if (pmap_pv_page) {
d1282 1
a1282 1
		pmap_pv_page = 0;
@


1.76
log
@make sure runing addr gets properly aligned after 0-level page table allocation
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.75 2002/05/20 03:33:11 mickey Exp $	*/
d719 1
a719 1
			ficache(pmap->pm_space, va, NBPG);
d721 1
a721 1
		fdcache(pmap->pm_space, va, NBPG);
d1238 1
a1238 1
			ficache(HPPA_SID_KERNEL, va, NBPG);
d1240 1
a1240 1
		fdcache(HPPA_SID_KERNEL, va, NBPG);
@


1.75
log
@gateway page could be cachable, it's not written into.
whaen changing protection on the page reset
the dirty bit along w/ the write, save it to the pv-header.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.74 2002/05/20 01:24:26 mickey Exp $	*/
d467 5
a471 5
	bzero((void *)addr, (hppa_sid_max + 1) * 4);
	fdcache(HPPA_SID_KERNEL, addr, (hppa_sid_max + 1) * 4);
	DPRINTF(PDB_INIT, ("vtop: 0x%x @@ 0x%x\n",
	    (hppa_sid_max + 1) * 4, addr));
	addr += (hppa_sid_max + 1) * 4;
@


1.74
log
@do not free the kernel page dirs, they are hard to account for sometimes; also some dprintfs here and there
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.73 2002/04/30 17:26:52 mickey Exp $	*/
d299 2
a300 1
	if (pte && !(pte & PTE_PROT(TLB_UNCACHABLE))) {
d582 1
a582 1
		    PTE_PROT(TLB_UNCACHABLE|TLB_GATE_PROT));
d861 2
d892 10
d907 2
@


1.73
log
@uninitialized variable usage was here
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.72 2002/04/22 00:56:31 mickey Exp $	*/
d76 1
a76 1
	| PDB_INIT
d185 3
d257 3
d261 4
a264 1
	if (ptp->wire_count <= 1) {
d782 1
a782 1
	DPRINTF(PDB_FOLLOW, ("pmap_enter: leaving\n"));
@


1.72
log
@first we flush, then we advance
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.71 2002/04/22 00:19:59 mickey Exp $	*/
a623 1
	pa_space_t space;
d638 1
a638 1
			panic("pmap_release: busy page table page");
d646 1
a646 1
	pmap_sdir_set(space, 0);
@


1.71
log
@no longer batc the data, only the code.
this removes a constraint and an ld.script dependancy
for the .data and the dynamically allocated kernel
structures existed w/ batcing. also makes pmap_steal_memory
obsolete and replaced w/ the pmap_virtual_space.
implement batching for pmap_remove to avoid
zeroing out the pte entries if pde will be removed anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.70 2002/04/01 16:19:59 mickey Exp $	*/
a446 1
	addr += PAGE_SIZE;
d449 1
@


1.70
log
@better support for machine checks tocs and power fails
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.69 2002/04/01 16:15:32 mickey Exp $	*/
d76 1
a79 1
/*	| PDB_INIT */
d90 1
a90 1
vaddr_t	virtual_steal, virtual_avail;
d92 1
d151 2
a152 1
	asm("stwas	%0, 0(%1)":: "r" (pa), "r" (vtop + (space << 2)));
d161 2
a162 1
	asm("ldwax,s	%2(%1), %0": "=&r" (pa) : "r" (vtop), "r" (space));
d172 2
a173 1
	asm("ldwax,s	%2(%1), %0": "=&r" (pde) : "r" (pa), "r" (va >> 22));
d181 5
a185 1
	asm("stwas	%0, 0(%1)"
d198 16
d270 2
a271 2
	asm("ldwax,s	%2(%1),%0" : "=&r" (pte)
	    : "r" (pde),  "r" ((va >> 12) & 0x3ff));
d285 2
a286 1
	if (pte && pte < virtual_steal &&
d290 2
a291 1
	if (pte && !(pte & PTE_PROT(TLB_UNCACHABLE)))
d293 4
d413 1
a413 1
	vaddr_t addr = hppa_round_page(vstart), t;
a418 1
	int i;
d459 2
a460 1
	printf("vtop: 0x%x @@ 0x%x\n", (hppa_sid_max + 1) * 4, addr);
a497 3
	/* map the kernel space, which will give us virtual_avail */
	vstart = hppa_round_page(addr + (totalphysmem - (atop(addr))) *
	    (16 + sizeof(struct pv_head) + sizeof(struct vm_page)));
d502 1
a502 1
	    pmap_prot(pmap_kernel(), VM_PROT_READ|VM_PROT_EXECUTE)) < 0)
a503 7
	t = vstart - (vaddr_t)&etext1;
	if (btlb_insert(HPPA_SID_KERNEL, (vaddr_t)&etext1, (vaddr_t)&etext1, &t,
	    pmap_sid2pid(HPPA_SID_KERNEL) | TLB_UNCACHABLE |
	    pmap_prot(pmap_kernel(), VM_PROT_ALL)) < 0)
		panic("pmap_bootstrap: cannot block map kernel");
	vstart = (vaddr_t)&etext1 + t;
	virtual_avail = vstart;
d505 1
a505 1
	    physmem = atop(vstart);
d520 2
d524 1
d529 2
a530 4
	virtual_steal = addr + size;
	i = atop(virtual_avail - virtual_steal);
	uvm_page_physload(0, totalphysmem + i,
		atop(virtual_avail), totalphysmem + i, VM_FREELIST_DEFAULT);
d534 4
a537 41
	DPRINTF(PDB_INIT, ("stealspool: 0x%x @@ 0x%x\n",
	    virtual_avail - virtual_steal, virtual_steal));
}

/*
 * pmap_steal_memory(size, startp, endp)
 *	steals memory block of size `size' from directly mapped
 *	segment (mapped behind the scenes).
 *	directly mapped cannot be grown dynamically once allocated.
 */
vaddr_t
pmap_steal_memory(size, startp, endp)
	vsize_t size;
	vaddr_t *startp;
	vaddr_t *endp;
{
	vaddr_t va;

	DPRINTF(PDB_FOLLOW|PDB_STEAL,
	    ("pmap_steal_memory(%x, %x, %x)\n", size, startp, endp));

	if (startp)
		*startp = virtual_avail;
	if (endp)
		*endp = VM_MAX_KERNEL_ADDRESS;

	size = hppa_round_page(size);
	if (size <= virtual_avail - virtual_steal) {

		DPRINTF(PDB_STEAL,
		    ("pmap_steal_memory: steal %d bytes (%x+%x,%x)\n",
		    size, virtual_steal, size, virtual_avail));

		va = virtual_steal;
		virtual_steal += size;

		/* make seg0 smaller (reduce fake top border) */
		vm_physmem[0].end -= atop(size);
		vm_physmem[0].avail_end -= atop(size);
	} else
		va = NULL;
d539 2
a540 1
	return va;
a551 3
	/* deplete the steal area */
	pool_prime(&pmap_pv_pool, (virtual_avail - virtual_steal) / PAGE_SIZE *
	    pmap_pv_pool.pr_itemsperpage);
d576 7
d788 1
a788 1
	int bank, off;
d803 1
d822 3
a824 1
			pmap_pte_set(pde, sva, 0);
d881 1
a881 2
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
a1240 2
	vaddr_t va;

d1244 4
a1247 5
	/*
		TODO
	if (list not empty) {
		get from the list;
		return (va);
a1248 7
	*/

	if ((va = pmap_steal_memory(PAGE_SIZE, NULL, NULL)))
		return (void *)va;

	DPRINTF(PDB_FOLLOW|PDB_POOL,
	    ("pmap_pv_page_alloc: uvm_km_alloc_poolpage1\n"));
d1261 3
a1263 3
	if (va < virtual_avail) {
		/* TODO save on list somehow */
	} else
@


1.69
log
@be careful to preserve page attrs on removing the mapping
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.68 2002/03/27 21:39:25 mickey Exp $	*/
d382 1
d427 1
@


1.68
log
@must be at least 7 insns between rsm/ssm; minot cleanups and debugging printfs
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.67 2002/03/20 01:02:18 mickey Exp $	*/
d820 5
a824 1
			if (pte & PTE_PROT(pte))
d835 3
a837 3
				pve = pmap_pv_remove(pvh, pmap, sva);
				if (pve) {
					pvh->pvh_attrs |= pmap_pvh_attrs(pte);
d839 1
a839 1
				}
d1195 3
d1199 1
a1199 1
	vaddr_t eva = va + size, pdemask;
d1228 10
@


1.67
log
@a few good typos
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.66 2002/03/19 23:16:52 mickey Exp $	*/
d45 1
a45 1
#include <sys/malloc.h>
a48 5
#include <machine/reg.h>
#include <machine/psl.h>
#include <machine/cpu.h>
#include <machine/pmap.h>
#include <machine/pte.h>
a49 2
#include <machine/pdc.h>
#include <machine/iomod.h>
d202 1
a202 1
	pg->flags &= ~PG_BUSY;	/* never busy */
d284 1
a284 1
pmap_dump_table(pa_space_t space)
d297 1
a297 1
		for (va = virtual_avail; va < VM_MAX_KERNEL_ADDRESS;
d488 2
d510 3
d874 2
a875 1
			    ("pmap_write_protect: pte=0x%x\n", pte));
d1093 4
a1096 4
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)"
d1101 1
d1103 1
d1124 1
a1124 1
	/* XXX flush cache for the spa ??? */
d1128 8
a1135 8
		    "ldwas,ma 4(%0),%%r22\n\t"
		    "ldwas,ma 4(%0),%%r21\n\t"
		    "stwas,ma %%r22,4(%1)\n\t"
		    "stwas,ma %%r21,4(%1)\n\t"
		    "ldwas,ma 4(%0),%%r22\n\t"
		    "ldwas,ma 4(%0),%%r21\n\t"
		    "stwas,ma %%r22,4(%1)\n\t"
		    "stwas,ma %%r21,4(%1)\n\t"
d1141 1
d1144 1
a1235 3
	if ((va = pmap_steal_memory(PAGE_SIZE, NULL, NULL)))
		return (void *)va;

d1243 3
@


1.66
log
@locking in the kenter/kremove and uncache io space (in the future)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.65 2002/03/19 23:08:55 mickey Exp $	*/
d492 1
a492 1
	if (etext < etext1) {
d709 1
a709 1
		panic("pmap_kenter: cannot allocate pde");
d1175 1
a1175 1
		pte |= TLB_UNCACHABLE;
@


1.65
log
@track the gap in text
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.64 2002/03/19 21:22:13 mickey Exp $	*/
d1162 2
d1173 3
a1175 1
	pte = pa | PTE_PROT(TLB_UNCACHABLE|TLB_WIRED|TLB_DIRTY|pmap_prot(pmap_kernel(), prot));
d1178 2
d1194 2
d1219 2
@


1.64
log
@empty is uncached by definition
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.63 2002/03/19 00:33:18 mickey Exp $	*/
d98 1
d387 1
a387 1
	extern char etext;
d477 1
a477 1
	t = (vaddr_t)&etext;
d482 2
a483 2
	t = vstart - (vaddr_t)&etext;
	if (btlb_insert(HPPA_SID_KERNEL, (vaddr_t)&etext, (vaddr_t)&etext, &t,
d487 1
a487 1
	vstart = (vaddr_t)&etext + t;
d491 5
@


1.63
log
@do things uncached for now
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.62 2002/03/15 21:44:18 mickey Exp $	*/
d270 3
a273 2
	if (!(pte & PTE_PROT(TLB_UNCACHABLE)))
		Debugger();
@


1.62
log
@rewrite a pmap to use multilevel page tables.
lower 12 bits contain the perms, no unused bits left,
but a couple for off-tlb use (as the ref implemented now).
do not use the hvt, which might get some use later
if proven to speed thigs up, tlb handlers would po
another dozen of insns though, but if that's worth its...
move on the data seg and map kernel text rdonly (idea form fredette),
since all of the page0 mods done before that we are all fine
except for some viper fluff, but later w/ that.
this also picks up a bit more of ddb magic for bpt and ss.
tlb handlers can use a little bit more of attention,
but things, visually, seem to be much faster already, --
sorry, no benchmarks for now.

* effort sponsored in part by the `henry st. old ale house'
* and mr.pete and mr.lee in particular in thier generous entrirety.
* the proj took a little more that 72man*h as it was expected,
* but within murhy's law estimations.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.61 2002/03/14 01:26:31 millert Exp $	*/
d271 2
d560 1
a560 1
	/* depleet the steal area */
d583 1
a583 1
		    PTE_PROT(TLB_GATE_PROT));
d770 1
a770 1
	pte = pa | PTE_PROT(pmap_prot(pmap, prot)) |
d868 2
d997 2
a998 1
			pitlb(pve->pv_pmap->pm_space, pve->pv_va);
@


1.61
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.60 2002/02/21 06:12:30 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2001 Michael Shalayeff
d27 1
a27 1
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
a32 92
 * Copyright 1996 1995 by Open Software Foundation, Inc.
 *              All Rights Reserved
 *
 * Permission to use, copy, modify, and distribute this software and
 * its documentation for any purpose and without fee is hereby granted,
 * provided that the above copyright notice appears in all copies and
 * that both the copyright notice and this permission notice appear in
 * supporting documentation.
 *
 * OSF DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE
 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
 * FOR A PARTICULAR PURPOSE.
 *
 * IN NO EVENT SHALL OSF BE LIABLE FOR ANY SPECIAL, INDIRECT, OR
 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
 * LOSS OF USE, DATA OR PROFITS, WHETHER IN ACTION OF CONTRACT,
 * NEGLIGENCE, OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION
 * WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Mach Operating System
 * Copyright (c) 1990,1991,1992,1993,1994 The University of Utah and
 * the Computer Systems Laboratory (CSL).
 * Copyright (c) 1991,1987 Carnegie Mellon University.
 * All rights reserved.
 *
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation,
 * and that all advertising materials mentioning features or use of
 * this software display the following acknowledgement: ``This product
 * includes software developed by the Computer Systems Laboratory at
 * the University of Utah.''
 *
 * CARNEGIE MELLON, THE UNIVERSITY OF UTAH AND CSL ALLOW FREE USE OF
 * THIS SOFTWARE IN ITS "AS IS" CONDITION, AND DISCLAIM ANY LIABILITY
 * OF ANY KIND FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF
 * THIS SOFTWARE.
 *
 * CSL requests users of this software to return to csl-dist@@cs.utah.edu any
 * improvements that they make and grant CSL redistribution rights.
 *
 * Carnegie Mellon requests users of this software to return to
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 * any improvements or extensions that they make and grant Carnegie Mellon
 * the rights to redistribute these changes.
 *
 * 	Utah $Hdr: pmap.c 1.49 94/12/15$
 *	Author: Mike Hibler, Bob Wheeler, University of Utah CSL, 10/90
 */
/*
 *	Manages physical address maps for hppa.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information to
 *	when physical maps must be made correct.
 *
 */
/*
 * CAVEATS:
 *
 *	PAGE_SIZE must equal NBPG
 *	Needs more work for MP support
 *	page maps are stored as linear linked lists, some
 *		improvement may be achieved should we use smth else
 *	protection id (pid) allocation should be done in a pid_t fashion
 *		(maybe just use the pid itself)
 *	some ppl say, block tlb entries should be maintained somewhere in uvm
 *		and be ready for reloads in the fault handler.
 *
d44 1
d73 2
a74 2
#define	PDB_PDRTAB	0x00000400
#define	PDB_VA		0x00000800
d78 1
a78 1
#define	PDB_PVDUMP	0x00008000
d81 1
d84 1
a84 1
/*	| PDB_VA */
d97 5
a101 1
vaddr_t	virtual_steal, virtual_avail, virtual_end;
d104 1
a104 1
pmap_t		kernel_pmap;
d106 10
d117 1
a117 16
TAILQ_HEAD(, pmap)	pmap_freelist;	/* list of free pmaps */
u_int pmap_nfree;
struct simplelock pmap_freelock;	/* and lock */

struct simplelock pmap_lock;	/* XXX this is all broken */
struct simplelock sid_pid_lock;	/* pids */

u_int	pages_per_vm_page;
u_int	sid_counter;

TAILQ_HEAD(, pv_page) pv_page_freelist;
u_int pv_nfree;

#ifdef PMAPDEBUG
void pmap_hptdump(int sp);
#endif
a118 3
u_int	kern_prot[8], user_prot[8];

void pmap_pinit(pmap_t);
d122 4
d144 1
a145 4
/*
 * pmap_enter_va(space, va, pv)
 *	insert mapping entry into va->pa translation hash table.
 */
d147 1
a147 1
pmap_enter_va(struct pv_entry *pv)
d149 6
a154 12
	struct hpt_entry *hpt = pmap_hash(pv->pv_space, pv->pv_va);
#if defined(PMAPDEBUG) || defined(DIAGNOSTIC)
	struct pv_entry *pvp = hpt->hpt_entry;
#endif
	DPRINTF(PDB_FOLLOW | PDB_VA,
	    ("pmap_enter_va(%x,%x,%p): hpt=%p, pvp=%p\n",
	    pv->pv_space, pv->pv_va, pv, hpt, pvp));
#ifdef DIAGNOSTIC
	while(pvp && (pvp->pv_va != pv->pv_va || pvp->pv_space != pv->pv_space))
		pvp = pvp->pv_hash;
	if (pvp)
		panic("pmap_enter_va: pv_entry is already in hpt_table");
d156 1
a156 4
	/* we assume that normally there are no duplicate entries
	   would be inserted (use DIAGNOSTIC should you want a proof) */
	pv->pv_hash = hpt->hpt_entry;
	hpt->hpt_entry = pv;
d159 2
a160 6
/*
 * pmap_find_va(space, va)
 *	returns address of the pv_entry correspondent to sp:va
 */
static __inline struct pv_entry *
pmap_find_va(pa_space_t space, vaddr_t va)
d162 1
a162 1
	struct pv_entry *pvp = pmap_hash(space, va)->hpt_entry;
d164 2
a165 1
	DPRINTF(PDB_FOLLOW | PDB_VA, ("pmap_find_va(%x,%x)\n", space, va));
d167 9
a175 2
	while(pvp && (pvp->pv_va != va || pvp->pv_space != space))
		pvp = pvp->pv_hash;
d177 1
a177 1
	return pvp;
a179 5
/*
 * Clear the HPT table entry for the corresponding space/offset to reflect
 * the fact that we have possibly changed the mapping, and need to pick
 * up new values from the mapping structure on the next access.
 */
d181 1
a181 1
pmap_clear_va(pa_space_t space, vaddr_t va)
d183 2
a184 4
	struct hpt_entry *hpt = pmap_hash(space, va);

	hpt->hpt_valid = 0;
	hpt->hpt_space = -1;
d187 2
a188 6
/*
 * pmap_remove_va(pv)
 *	removes pv_entry from the va->pa translation hash table
 */
static __inline void
pmap_remove_va(struct pv_entry *pv)
d190 2
a191 2
	struct hpt_entry *hpt = pmap_hash(pv->pv_space, pv->pv_va);
	struct pv_entry **pvp = (struct pv_entry **)&hpt->hpt_entry;
d193 2
a194 2
	DPRINTF(PDB_FOLLOW | PDB_VA,
	    ("pmap_remove_va(%p), hpt=%p, pvp=%p\n", pv, hpt, pvp));
d196 6
a201 14
	while(*pvp && *pvp != pv)
		pvp = &(*pvp)->pv_hash;
	if (*pvp) {
		*pvp = (*pvp)->pv_hash;
		pv->pv_hash = NULL;
		if (hptbtop(pv->pv_va) == hpt->hpt_vpn &&
		    pv->pv_space == hpt->hpt_space) {
			hpt->hpt_space = -1;
			hpt->hpt_valid = 0;
		}
	} else {
#ifdef DIAGNOSTIC
		printf("pmap_remove_va: entry not found\n");
#endif
a202 1
}
d204 3
a206 10
/*
 * pmap_insert_pvp(pvp, flag)
 *	loads the passed page into pv_entries free list.
 *	flag specifies how the page was allocated where possible
 *	choices are (0)static, (1)malloc; (probably bogus, but see free_pv)
 */
static __inline void
pmap_insert_pvp(struct pv_page *pvp, u_int flag)
{
	struct pv_entry *pv;
d208 8
a215 7
	bzero(pvp, sizeof(*pvp));
	for (pv = &pvp->pvp_pv[0]; pv < &pvp->pvp_pv[NPVPPG - 1]; pv++)
		pv->pv_next = pv + 1;
	pvp->pvp_flag = flag;
	pvp->pvp_freelist = &pvp->pvp_pv[0];
	pv_nfree += pvp->pvp_nfree = NPVPPG;
	TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_list);
d218 2
a219 8
/*
 * pmap_alloc_pv()
 *	allocates the pv_entry from the pv_entries free list.
 *	once we've ran out of preallocated pv_entries, nothing
 *	can be done, since tlb fault handlers work in phys mode.
 */
static __inline struct pv_entry *
pmap_alloc_pv(void)
d221 1
a221 2
	struct pv_page *pvp;
	struct pv_entry *pv;
d223 1
a223 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_alloc_pv()\n"));
d225 2
a226 11
	if (pv_nfree == 0) {
#if notyet
		MALLOC(pvp, struct pv_page *, NBPG, M_VMPVENT, M_WAITOK);

		if (!pvp)
			panic("pmap_alloc_pv: alloc failed");
		pmap_insert_pvp(pvp, 0);
#else
		panic("out of pv_entries");
#endif
	}
d228 1
a228 13
	--pv_nfree;
	pvp = TAILQ_FIRST(&pv_page_freelist);
	if (--pvp->pvp_nfree == 0)
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_list);
	pv = pvp->pvp_freelist;
#ifdef DIAGNOSTIC
	if (pv == 0)
		panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
	pvp->pvp_freelist = pv->pv_next;
	pv->pv_next = NULL;
	pv->pv_hash = NULL;
	pv->pv_pmap = NULL;
d230 1
a230 1
	return pv;
a232 6
/*
 * pmap_free_pv(pv)
 *	return pv_entry back into free list.
 *	once full page of entries has been freed and that page
 *	was allocated dynamically, free the page.
 */
d234 1
a234 1
pmap_free_pv(struct pv_entry *pv)
d236 8
a243 24
	struct pv_page *pvp;

	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_free_pv(%p)\n", pv));

	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_nfree) {
	case 1:
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_list);
	default:
		pv->pv_next = pvp->pvp_freelist;
		pvp->pvp_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		if (!pvp->pvp_flag) {
#ifdef notyet
			pv_nfree -= NPVPPG - 1;
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_list);
			FREE((vaddr_t) pvp, M_VMPVENT);
#else
			panic("pmap_free_pv: mallocated pv page");
#endif
		}
		break;
d247 2
a248 8
/*
 * pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv)
 *	insert specified mapping into pa->va translation list,
 *	where pv specifies the list head (for particular pa)
 */
static __inline struct pv_entry *
pmap_enter_pv(pmap_t pmap, vaddr_t va, u_int tlbprot, u_int tlbpage,
    struct pv_entry *pv)
d250 1
a250 1
	struct pv_entry *npv, *hpv;
d252 2
a253 2
	if (!pmap_initialized)
		return NULL;
d255 2
a256 4
#ifdef DEBUG
	if (pv == NULL)
		printf("pmap_enter_pv: zero pv\n");
#endif
d258 5
a262 2
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_enter_pv: pv %p: %lx/%p/%p\n",
	    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
a263 12
	if (pv->pv_pmap == NULL) {
		/*
		 * No entries yet, use header as the first entry
		 */
		DPRINTF(PDB_ENTER, ("pmap_enter_pv: no entries yet\n"));
		hpv = npv = NULL;
	} else {
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		DPRINTF(PDB_ENTER, ("pmap_enter_pv: adding to the list\n"));
d265 5
a269 7
		for (npv = pv; npv; npv = npv->pv_next)
			if (pmap == npv->pv_pmap && va == npv->pv_va) {
				printf("pmap_enter_pv: %p already in pv_tab",
				    npv);
				pmap_enter_va(npv); /* HACK UGLY HACK HACK */
				return (npv);
			}
d271 11
a281 13
		hpv = pv;
		npv = pv->pv_next;
		pv = pmap_alloc_pv();
	}
	pv->pv_va = va;
	pv->pv_pmap = pmap;
	pv->pv_space = pmap->pmap_space;
	pv->pv_tlbprot = tlbprot;
	pv->pv_tlbpage = tlbpage;
	pv->pv_next = npv;
	if (hpv)
		hpv->pv_next = pv;
	pmap_enter_va(pv);
d283 1
a283 1
	return pv;
d286 2
a287 7
/*
 * pmap_remove_pv(ppv, pv)
 *	remove mapping for specified va and pmap, from
 *	pa->va translation list, having pv as a list head.
 */
static __inline void
pmap_remove_pv(struct pv_entry *ppv, struct pv_entry *pv)
d289 1
d291 4
a294 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_remove_pv(%p,%p)\n", ppv, pv));
d296 3
a298 5
	/*
	 * Clear it from cache and TLB
	 */
	ficache(ppv->pv_space, ppv->pv_va, PAGE_SIZE);
	pitlb(ppv->pv_space, ppv->pv_va);
d300 10
a309 2
	fdcache(ppv->pv_space, ppv->pv_va, PAGE_SIZE);
	pdtlb(ppv->pv_space, ppv->pv_va);
d311 2
a312 19
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (ppv == pv) {
		ppv = pv->pv_next;
		pmap_remove_va(pv);
		if (ppv) {
			ppv->pv_tlbprot |= pv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);
			*pv = *ppv;
			pmap_free_pv(ppv);
		} else
			pv->pv_pmap = NULL;
	} else {
		for (; pv && pv->pv_next != ppv; pv = pv->pv_next)
			;
d314 1
a314 10
		if (pv) {
			pv->pv_tlbprot |= ppv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);
			pv->pv_next = ppv->pv_next;
			pmap_remove_va(ppv);
			pmap_free_pv(ppv);
		} else {
#ifdef DEBUG
			panic("pmap_remove_pv: npv == NULL\n");
#endif
a318 4
/*
 * pmap_find_pv(pa)
 *	returns head of the pa->va translation list for specified pa.
 */
d320 1
a320 1
pmap_find_pv(paddr_t pa)
d322 11
a332 1
	int bank, off;
d334 1
a334 5
	if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
		DPRINTF(PDB_PV, ("pmap_find_pv(%x):  %d:%d\n", pa, bank, off));
		return &vm_physmem[bank].pmseg.pvent[off];
	} else
		return NULL;
a336 4
/*
 * Flush caches and TLB entries refering to physical page pa.  If cmp is
 * non-zero, we do not affect the cache or TLB entires for that mapping.
 */
d338 1
a338 1
pmap_clear_pv(paddr_t pa, struct pv_entry *cpv)
d340 6
a345 1
	struct pv_entry *pv;
d347 2
a348 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_clear_pv(%x,%p)\n", pa, cpv));
d350 6
a355 2
	if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap)
		return;
d357 8
a364 10
	for (; pv; pv = pv->pv_next) {
		if (pv == cpv)
			continue;
		DPRINTF(PDB_PV,
		    ("pmap_clear_pv: %x:%x\n", pv->pv_space, pv->pv_va));
		/*
		 * have to clear the icache first since fic uses the dtlb.
		 */
		ficache(pv->pv_space, pv->pv_va, NBPG);
		pitlb(pv->pv_space, pv->pv_va);
d366 4
a369 2
		fdcache(pv->pv_space, pv->pv_va, NBPG);
		pdtlb(pv->pv_space, pv->pv_va);
d371 6
a376 2
		pmap_clear_va(pv->pv_space, pv->pv_va);
	}
a378 9
/*
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, and allocate the system page table.
 *	Called with mapping OFF.
 *
 *	Parameters:
 *	vstart	PA of first available physical page
 *	vend	PA of last available physical page
 */
d380 6
a385 7
pmap_bootstrap(vstart, vend)
	vaddr_t *vstart;
	vaddr_t *vend;
{
	extern int maxproc; /* used to estimate pv_entries pool size */
	extern u_int totalphysmem;
	vaddr_t addr;
d387 4
a390 2
	struct pv_page *pvp;
	struct hpt_entry *hptp;
d393 1
a393 1
	DPRINTF(PDB_FOLLOW, ("pmap_bootstrap(%p, %p)\n", vstart, vend));
d397 8
a404 22
	pages_per_vm_page = PAGE_SIZE / NBPG;
	/* XXX for now */
	if (pages_per_vm_page != 1)
		panic("HPPA page != VM page");

	kern_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_NA;
	kern_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_KR;
	kern_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_KRW;
	kern_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_KRW;
	kern_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_KRX;
	kern_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_KRX;
	kern_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_KRWX;
	kern_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_KRWX;

	user_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_NA;
	user_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_UR;
	user_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_URW;
	user_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_URW;
	user_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_URX;
	user_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_URX;
	user_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_URWX;
	user_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_URWX;
d409 14
a422 10
	kernel_pmap = &kernel_pmap_store;
#if	NCPUS > 1
	lock_init(&pmap_lock, FALSE, ETAP_VM_PMAP_SYS, ETAP_VM_PMAP_SYS_I);
#endif	/* NCPUS > 1 */
	simple_lock_init(&kernel_pmap->pmap_lock);
	simple_lock_init(&pmap_freelock);
	simple_lock_init(&sid_pid_lock);

	kernel_pmap->pmap_refcnt = 1;
	kernel_pmap->pmap_space = HPPA_SID_KERNEL;
a426 24
	addr = hppa_round_page(*vstart);
	virtual_end = *vend;
	pvp = (struct pv_page *)addr;

	mfctl(CR_HPTMASK, size);
	addr = (addr + size) & ~(size);

	DPRINTF(PDB_INIT, ("pmap_bootstrap: allocating %d pv_pages\n",
	    (struct pv_page *)addr - pvp));

	TAILQ_INIT(&pv_page_freelist);
	for (; pvp + 1 <= (struct pv_page *)addr; pvp++)
		pmap_insert_pvp(pvp, 1);

	/* Allocate the HPT */
	for (hptp = (struct hpt_entry *)addr;
	     ((u_int)hptp - addr) <= size; hptp++) {
		hptp->hpt_valid   = 0;
		hptp->hpt_vpn     = 0;
		hptp->hpt_space   = -1;
		hptp->hpt_tlbpage = 0;
		hptp->hpt_tlbprot = 0;
		hptp->hpt_entry   = NULL;
	}
d428 26
a453 1
	DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n", size + 1, addr));
d455 6
a460 4
	/* load cr25 with the address of the HPT table
	   NB: It sez CR_VTOP, but we (and the TLB handlers) know better ... */
	mtctl(addr, CR_VTOP);
	addr += size + 1;
d462 6
a467 5
	/*
	 * we know that btlb_insert() will round it up to the next
	 * power of two at least anyway
	 */
	for (physmem = 1; physmem < btoc(addr); physmem *= 2);
d470 2
a471 3
	*vstart = hppa_round_page(addr + (totalphysmem - physmem) *
	    (sizeof(struct pv_entry) * maxproc / 8 +
	    sizeof(struct vm_page)));
d473 2
a474 1
	if (btlb_insert(HPPA_SID_KERNEL, 0, 0, vstart,
d476 6
a481 1
	    pmap_prot(kernel_pmap, VM_PROT_ALL)) < 0)
d483 4
a486 1
	virtual_avail = *vstart;
d493 2
a494 1
	size = hppa_round_page(sizeof(struct pv_entry) * totalphysmem);
d497 1
a497 1
	DPRINTF(PDB_INIT, ("pv_array: 0x%x @@ 0x%x\n", size, addr));
d499 1
d505 1
a505 12
	vm_physmem[0].pmseg.pvent = (struct pv_entry *)addr;
	/* mtctl(addr, CR_PSEG); */

	/* here will be a hole due to the kernel memory alignment
	   and we use it for pmap_steal_memory */
}

void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = virtual_avail;
	*vendp = virtual_end;
d522 1
a522 1
	DPRINTF(PDB_FOLLOW,
d528 1
a528 1
		*endp = virtual_end;
a548 6
/*
 * Finishes the initialization of the pmap module.
 * This procedure is called from vm_mem_init() in vm/vm_init.c
 * to initialize any remaining data structures that the pmap module
 * needs to map virtual memory (VM is already ON).
 */
d552 1
a552 1
	struct pv_page *pvp;
d554 7
a560 5
#ifdef PMAPDEBUG
	int opmapdebug = pmapdebug;
	DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));
	pmapdebug = 0;
#endif
d562 1
a562 15
	/* allocate the rest of the steal area for pv_pages */
#ifdef PMAPDEBUG
	printf("pmap_init: %d pv_pages @@ %x allocated\n",
	    (virtual_avail - virtual_steal) / sizeof(struct pv_page),
	    virtual_steal);
#endif
	while ((pvp = (struct pv_page *)
	    pmap_steal_memory(sizeof(*pvp), NULL, NULL)))
		pmap_insert_pvp(pvp, 1);

#ifdef PMAPDEBUG
	pmapdebug = opmapdebug /* | PDB_VA | PDB_PV */;
#endif
	TAILQ_INIT(&pmap_freelist);
	sid_counter = HPPA_SID_KERNEL + 1;
d573 2
a574 17
	pmap_enter_pv(pmap_kernel(), SYSCALLGATE, TLB_GATE_PROT,
	    tlbbtop((paddr_t)&gateway_page),
	    pmap_find_pv((paddr_t)&gateway_page));
}

/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
void
pmap_pinit(pmap)
	pmap_t pmap;
{
	pa_space_t sid;
	int s;

	DPRINTF(PDB_FOLLOW, ("pmap_pinit(%p)\n", pmap));
d576 3
a578 1
	if (!(sid = pmap->pmap_space)) {
d580 2
a581 16
		/*
		 * Allocate space and protection IDs for the pmap.
		 * If all are allocated, there is nothing we can do.
		 */
		s = splimp();
		if (sid_counter < HPPA_SID_MAX) {
			sid = sid_counter;
			sid_counter++;
		} else
			sid = 0;
		splx(s);

		if (sid == 0)
			panic("no more space ids\n");

		simple_lock_init(&pmap->pmap_lock);
a582 7

	s = splimp();
	pmap->pmap_space = sid;
	pmap->pmap_refcnt = 1;
	pmap->pmap_stats.resident_count = 0;
	pmap->pmap_stats.wired_count = 0;
	splx(s);
d585 1
a585 7
/*
 * pmap_create()
 *
 * Create and return a physical map.
 * the map is an actual physical map, and may be referenced by the hardware.
 */
pmap_t
d588 5
a592 2
	register pmap_t pmap;
	int s;
d594 13
a606 1
	DPRINTF(PDB_FOLLOW, ("pmap_create()\n"));
d608 7
a614 16
	/*
	 * If there is a pmap in the pmap free list, reuse it.
	 */
	s = splimp();
	if (pmap_nfree) {
		pmap = pmap_freelist.tqh_first;
		TAILQ_REMOVE(&pmap_freelist, pmap, pmap_list);
		pmap_nfree--;
		splx(s);
	} else {
		splx(s);
		MALLOC(pmap, struct pmap *, sizeof(*pmap), M_VMMAP, M_NOWAIT);
		if (pmap == NULL)
			return NULL;
		bzero(pmap, sizeof(*pmap));
	}
d616 1
a616 1
	pmap_pinit(pmap);
a620 6
/*
 * pmap_destroy(pmap)
 *	Gives up a reference to the specified pmap.  When the reference count
 *	reaches zero the pmap structure is added to the pmap free list.
 *	Should only be called if the map contains no valid mappings.
 */
d623 1
a623 1
	pmap_t pmap;
d625 5
a629 2
	int ref_count;
	int s;
d631 3
a633 1
	DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
d635 1
a635 1
	if (!pmap)
d638 8
a645 1
	s = splimp();
d647 5
a651 1
	ref_count = --pmap->pmap_refcnt;
a652 16
	if (ref_count < 0)
		panic("pmap_destroy(): ref_count < 0");
	if (!ref_count) {
		assert(pmap->pmap_stats.resident_count == 0);

		/*
		 * Add the pmap to the pmap free list
		 * We cannot free() disposed pmaps because of
		 * PID shortage of 2^16
		 * (do some random pid allocation later)
		 */
		TAILQ_INSERT_HEAD(&pmap_freelist, pmap, pmap_list);
		pmap_nfree++;
	}
	splx(s);
}
d654 1
a654 5
 * pmap_enter(pmap, va, pa, prot, flags)
 *	Create a translation for the virtual address (va) to the physical
 *	address (pa) in the pmap with the protection requested. If the
 *	translation is wired then we can not allow a page fault to occur
 *	for this mapping.
d656 18
d676 1
a676 1
	pmap_t pmap;
d682 5
a686 4
	register struct pv_entry *pv, *ppv;
	u_int tlbpage, tlbprot;
	pa_space_t space;
	boolean_t waswired;
a687 1
	int s;
d689 18
a706 9
	pa = hppa_trunc_page(pa);
	va = hppa_trunc_page(va);
	space = pmap_sid(pmap, va);
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW &&
	    (!pmap_initialized || pmapdebug & PDB_ENTER))
		printf("pmap_enter(%p, %x, %x, %x, %swired)\n", pmap, va, pa,
		    prot, wired? "" : "un");
#endif
d708 2
a709 1
	s = splimp();	/* are we already high enough? XXX */
d711 5
a715 2
	if (!(pv = pmap_find_pv(pa)))
		panic("pmap_enter: pmap_find_pv failed");
d717 4
a720 2
	tlbpage = tlbbtop(pa);
	tlbprot = TLB_UNCACHEABLE | pmap_prot(pmap, prot) | pmap_sid2pid(space);
d722 5
a726 8
	if (!(ppv = pmap_find_va(space, va))) {
		/*
		 * Mapping for this virtual address doesn't exist.
		 * Enter a new mapping.
		 */
		DPRINTF(PDB_ENTER, ("pmap_enter: new mapping\n"));
		pv = pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv);
		pmap->pmap_stats.resident_count++;
d728 9
d738 19
a756 16

		/* see if we are remapping the page to another PA */
		if (ppv->pv_tlbpage != tlbpage) {
			DPRINTF(PDB_ENTER, ("pmap_enter: moving pa %x -> %x\n",
			    ppv->pv_tlbpage, tlbpage));
			/* update tlbprot to avoid extra subsequent fault */
			pmap_remove_pv(ppv, pmap_find_pv(tlbptob(ppv->pv_tlbpage)));
			pv = pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv);
		} else {
			/* We are just changing the protection.  */
			DPRINTF(PDB_ENTER, ("pmap_enter: changing %b->%b\n",
			    ppv->pv_tlbprot, TLB_BITS, tlbprot, TLB_BITS));
			pv = ppv;
			ppv->pv_tlbprot = (tlbprot & ~TLB_PID_MASK) |
			    (ppv->pv_tlbprot & ~(TLB_AR_MASK|TLB_PID_MASK));
			pmap_clear_pv(pa, NULL);
d758 15
a772 1
	}
d774 1
a774 12
	/*
	 * Add in software bits and adjust statistics
	 */
	waswired = pv->pv_tlbprot & TLB_WIRED;
	if (wired && !waswired) {
		pv->pv_tlbprot |= TLB_WIRED;
		pmap->pmap_stats.wired_count++;
	} else if (!wired && waswired) {
		pv->pv_tlbprot &= ~TLB_WIRED;
		pmap->pmap_stats.wired_count--;
	}
	splx(s);
d776 1
a776 2
	simple_unlock(&pmap->pmap_lock);
	DPRINTF(PDB_ENTER, ("pmap_enter: leaving\n"));
a780 7
/*
 * pmap_remove(pmap, sva, eva)
 *	unmaps all virtual addresses v in the virtual address
 *	range determined by [sva, eva) and pmap.
 *	sva and eva must be on machine independent page boundaries and
 *	sva must be less than or equal to eva.
 */
d783 3
a785 3
	register pmap_t pmap;
	register vaddr_t sva;
	register vaddr_t eva;
d787 5
a791 3
	register struct pv_entry *pv;
	register pa_space_t space;
	int s;
d793 2
a794 1
	DPRINTF(PDB_FOLLOW, ("pmap_remove(%p, %x, %x)\n", pmap, sva, eva));
d796 1
a796 2
	if(!pmap)
		return;
d798 8
a805 1
	s = splimp();
d807 1
a807 2
	sva = hppa_trunc_page(sva);
	space = pmap_sid(pmap, sva);
d809 21
a829 11
	while (pmap->pmap_stats.resident_count && ((sva < eva))) {
		pv = pmap_find_va(space, sva);

		DPRINTF(PDB_REMOVE, ("pmap_remove: removing %p for 0x%x:0x%x\n",
		    pv, space, sva));
		if (pv) {
			pmap->pmap_stats.resident_count--;
			if (pv->pv_tlbprot & TLB_WIRED)
				pmap->pmap_stats.wired_count--;
			pmap_remove_pv(pv,
			    pmap_find_pv(tlbptob(pv->pv_tlbpage)));
a830 1
		sva += PAGE_SIZE;
d833 3
a835 1
	splx(s);
a837 5
/*
 *	pmap_page_protect(pa, prot)
 *
 *	Lower the permission for all mappings to a given page.
 */
d839 4
a842 2
pmap_page_protect(pg, prot)
	struct vm_page *pg;
d845 8
a852 5
	register struct pv_entry *pv;
	register pmap_t pmap;
	register u_int tlbprot;
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s;
d854 1
a854 2
	DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_page_protect(%x, %x)\n", pa, prot));
d856 7
a862 9
	switch (prot) {
	case VM_PROT_ALL:
		return;
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		s = splimp();
		if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap) {
			splx(s);
			break;
d864 1
a865 1
		for ( ; pv; pv = pv->pv_next) {
d867 2
a868 2
			 * Compare new protection with old to see if
			 * anything needs to be changed.
d870 2
a871 1
			tlbprot = pmap_prot(pv->pv_pmap, prot);
d873 9
a881 29
			if ((pv->pv_tlbprot & TLB_AR_MASK) != tlbprot) {
				pv->pv_tlbprot &= ~TLB_AR_MASK;
				pv->pv_tlbprot |= tlbprot;

				/*
				 * Purge the current TLB entry (if any)
				 * to force a fault and reload with the
				 * new protection.
				 */
				ficache(pv->pv_space, pv->pv_va, NBPG);
				pitlb(pv->pv_space, pv->pv_va);
				fdcache(pv->pv_space, pv->pv_va, NBPG);
				pdtlb(pv->pv_space, pv->pv_va);
				pmap_clear_va(pv->pv_space, pv->pv_va);
			}
		}
		splx(s);
		break;
	default:
		s = splimp();
		while ((pv = pmap_find_pv(pa)) && pv->pv_pmap) {

			DPRINTF(PDB_PROTECT, ("pv={%p,%x:%x,%b,%x}->%p\n",
			    pv->pv_pmap, pv->pv_space, pv->pv_va,
			    pv->pv_tlbprot, TLB_BITS,
			    tlbptob(pv->pv_tlbpage), pv->pv_hash));
			pmap = pv->pv_pmap;
			pmap_remove_pv(pv, pv);
			pmap->pmap_stats.resident_count--;
a882 2
		splx(s);
		break;
d884 2
a887 7
/*
 * pmap_protect(pmap, s, e, prot)
 *	changes the protection on all virtual addresses v in the
 *	virtual address range determined by [s, e) and pmap to prot.
 *	s and e must be on machine independent page boundaries and
 *	s must be less than or equal to e.
 */
d889 2
a890 5
pmap_protect(pmap, sva, eva, prot)
	pmap_t pmap;
	vaddr_t sva;
	vaddr_t eva;
	vm_prot_t prot;
d892 4
a895 3
	register struct pv_entry *pv;
	u_int tlbprot;
	pa_space_t space;
d897 1
a897 2
	DPRINTF(PDB_FOLLOW,
	    ("pmap_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot));
d899 3
a901 1
	if (!pmap)
d903 1
d905 2
a906 5
	if (prot == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}
	if (prot & VM_PROT_WRITE)
d909 8
a916 3
	sva = hppa_trunc_page(sva);
	space = pmap_sid(pmap, sva);
	tlbprot = pmap_prot(pmap, prot);
d918 3
a920 8
	for(; sva < eva; sva += PAGE_SIZE) {
		if((pv = pmap_find_va(space, sva))) {
			/*
			 * Determine if mapping is changing.
			 * If not, nothing to do.
			 */
			if ((pv->pv_tlbprot & TLB_AR_MASK) == tlbprot)
				continue;
d922 1
a922 2
			pv->pv_tlbprot &= ~TLB_AR_MASK;
			pv->pv_tlbprot |= tlbprot;
d924 4
a927 10
			/*
			 * Purge the current TLB entry (if any) to force
			 * a fault and reload with the new protection.
			 */
			ficache(space, sva, NBPG);
			pitlb(space, sva);
			fdcache(space, sva, NBPG);
			pdtlb(space, sva);
			pmap_clear_va(space, sva);
		}
d929 4
a934 10
/*
 *	Routine:	pmap_unwire
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 *
 * Change the wiring for a given virtual page. This routine currently is
 * only used to unwire pages and hence the mapping entry will exist.
 */
d937 1
a937 1
	pmap_t	pmap;
d940 64
a1003 2
	struct pv_entry *pv;
	int s;
d1005 2
a1006 2
	va = hppa_trunc_page(va);
	DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %x)\n", pmap, va));
d1008 7
a1014 2
	if (!pmap)
		return;
d1016 1
a1016 1
	simple_lock(&pmap->pmap_lock);
d1018 14
a1031 7
	s = splimp();
	if ((pv = pmap_find_va(pmap_sid(pmap, va), va)) == NULL)
		panic("pmap_unwire: can't find mapping entry");

	if (pv->pv_tlbprot & TLB_WIRED) {
		pv->pv_tlbprot &= ~TLB_WIRED;
		pmap->pmap_stats.wired_count--;
d1033 3
a1035 2
	splx(s);
	simple_unlock(&pmap->pmap_lock);
a1037 7
/*
 * pmap_extract(pmap, va, pap)
 *	fills in the physical address corrsponding to the
 *	virtual address specified by pmap and va into the
 *	storage pointed to by pap and returns TRUE if the
 *	virtual address is mapped. returns FALSE in not mapped.
 */
d1040 1
a1040 1
	pmap_t pmap;
d1044 3
a1046 2
	struct pv_entry *pv;
	int s;
d1048 3
a1050 1
	DPRINTF(PDB_FOLLOW, ("pmap_extract(%p, %x)\n", pmap, va));
d1052 3
a1054 5
	s = splimp();
	if (!(pv = pmap_find_va(pmap_sid(pmap, va), hppa_trunc_page(va))))
		return (FALSE);
	else {
		*pap = tlbptob(pv->pv_tlbpage) + (va & PGOFSET);
d1057 2
a1058 1
	splx(s);
a1060 5
/*
 * pmap_zero_page(pa)
 *
 * Zeros the specified page.
 */
d1063 1
a1063 1
	register paddr_t pa;
a1078 1
	pmap_clear_pv(pa, NULL);
d1081 1
a1081 1
		__asm volatile(
a1098 8
/*
 * pmap_copy_page(src, dst)
 *
 * pmap_copy_page copies the src page to the destination page. If a mapping
 * can be found for the source, we use that virtual address. Otherwise, a
 * slower physical page copy must be done. The destination is always a
 * physical address sivnce there is usually no mapping for it.
 */
d1111 1
a1111 2
	pmap_clear_pv(spa, NULL);
	pmap_clear_pv(dpa, NULL);
d1114 1
a1114 1
		__asm volatile(
d1139 5
a1143 10
/*
 * pmap_clear_modify(pa)
 *	clears the hardware modified ("dirty") bit for one
 *	machine independant page starting at the given
 *	physical address.  phys must be aligned on a machine
 *	independant page boundary.
 */
boolean_t
pmap_clear_modify(pg)
	struct vm_page *pg;
d1145 1
a1145 16
	register struct pv_entry *pv;
	register paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s, ret;

	DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%x)\n", pa));

	s = splimp();
	for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next)
		if (pv->pv_tlbprot & TLB_DIRTY) {
			pitlb(pv->pv_space, pv->pv_va);
			pdtlb(pv->pv_space, pv->pv_va);
			pv->pv_tlbprot &= ~(TLB_DIRTY);
			pmap_clear_va(pv->pv_space, pv->pv_va);
			ret = TRUE;
		}
	splx(s);
d1147 2
a1148 2
	return (ret);
}
d1150 8
a1157 12
/*
 * pmap_is_modified(pa)
 *	returns TRUE if the given physical page has been modified
 *	since the last call to pmap_clear_modify().
 */
boolean_t
pmap_is_modified(pg)
	struct vm_page *pg;
{
	register struct pv_entry *pv;
	register paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s, f = 0;
d1159 2
a1160 1
	DPRINTF(PDB_FOLLOW, ("pmap_is_modified(%x)\n", pa));
d1162 1
a1162 6
	s = splhigh();
	for (pv = pmap_find_pv(pa); pv && pv->pv_pmap && !f; pv = pv->pv_next)
		f |= pv->pv_tlbprot & TLB_DIRTY;
	splx(s);

	return f? TRUE : FALSE;
a1164 56
/*
 * pmap_clear_reference(pa)
 *	clears the hardware referenced bit in the given machine
 *	independant physical page.
 *
 *	Currently, we treat a TLB miss as a reference; i.e. to clear
 *	the reference bit we flush all mappings for pa from the TLBs.
 */
boolean_t
pmap_clear_reference(pg)
	struct vm_page *pg;
{
	register struct pv_entry *pv;
	register paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s, ret;

	DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%x)\n", pa));

	s = splimp();
	for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next)
		if (pv->pv_tlbprot & TLB_REF) {
			pitlb(pv->pv_space, pv->pv_va);
			pdtlb(pv->pv_space, pv->pv_va);
			pv->pv_tlbprot &= ~(TLB_REF);
			pmap_clear_va(pv->pv_space, pv->pv_va);
			ret = TRUE;
		}
	splx(s);

	return (ret);
}

/*
 * pmap_is_referenced(pa)
 *	returns TRUE if the given physical page has been referenced
 *	since the last call to pmap_clear_reference().
 */
boolean_t
pmap_is_referenced(pg)
	struct vm_page *pg;
{
	register struct pv_entry *pv;
	register paddr_t pa = VM_PAGE_TO_PHYS(pg);
	int s, f;

	DPRINTF(PDB_FOLLOW, ("pmap_is_referenced(%x)\n", pa));

	s = splhigh();
	for (pv = pmap_find_pv(pa); pv && pv->pv_pmap && !f; pv = pv->pv_next)
		f |= pv->pv_tlbprot & TLB_REF;
	splx(s);

	return f? TRUE : FALSE;
}

#ifdef notused
d1166 1
a1166 1
pmap_changebit(va, set, reset)
d1168 1
a1168 1
	u_int set, reset;
d1170 2
a1171 2
	register struct pv_entry *pv;
	int s;
d1173 2
a1174 1
	DPRINTF(PDB_FOLLOW, ("pmap_changebit(%x, %x, %x)\n", va, set, reset));
d1176 14
a1189 9
	s = splimp();
	if (!(pv = pmap_find_va(HPPA_SID_KERNEL, va))) {
		splx(s);
		return;
	}

	pv->pv_tlbprot |= set;
	pv->pv_tlbprot &= ~reset;
	splx(s);
d1191 5
a1195 2
	ficache(HPPA_SID_KERNEL, va, NBPG);
	pitlb(HPPA_SID_KERNEL, va);
d1197 2
a1198 2
	fdcache(HPPA_SID_KERNEL, va, NBPG);
	pdtlb(HPPA_SID_KERNEL, va);
d1200 1
a1200 1
	pmap_clear_va(HPPA_SID_KERNEL, va);
a1201 1
#endif
d1203 3
a1205 2
void
pmap_kenter_pa(va, pa, prot)
a1206 4
	paddr_t pa;
	vm_prot_t prot;
{
	register struct pv_entry *pv;
d1208 5
a1212 2
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot));
d1214 5
a1218 20
	va = hppa_trunc_page(va);
	pv = pmap_find_va(HPPA_SID_KERNEL, va);
	if (pv && (pa & HPPA_IOSPACE) == HPPA_IOSPACE)
		/* if already mapped i/o space, nothing to do */
		;
	else {
		if (pv)
			panic("pmap_kenter_pa: mapped already %x", va);
		else
			pmap_kernel()->pmap_stats.resident_count++;

		pv = pmap_alloc_pv();
		pv->pv_va = va;
		pv->pv_pmap = pmap_kernel();
		pv->pv_space = HPPA_SID_KERNEL;
		pv->pv_tlbpage = tlbbtop(pa);
		pv->pv_tlbprot = TLB_WIRED | TLB_DIRTY | TLB_REF |
		    HPPA_PID_KERNEL | pmap_prot(pmap_kernel(), prot) |
		    ((pa & HPPA_IOSPACE) == HPPA_IOSPACE? TLB_UNCACHEABLE : 0);
		pmap_enter_va(pv);
d1220 1
d1222 5
a1226 1
	DPRINTF(PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
d1230 1
a1230 3
pmap_kremove(va, size)
	vaddr_t va;
	vsize_t size;
d1232 1
a1232 1
	register struct pv_entry *pv;
d1234 1
a1234 14
	for (va = hppa_trunc_page(va); size > 0;
	    size -= PAGE_SIZE, va += PAGE_SIZE) {
		pv = pmap_find_va(HPPA_SID_KERNEL, va);
		if (pv) {
			ficache(pv->pv_space, pv->pv_va, NBPG);
			pitlb(pv->pv_space, pv->pv_va);
			fdcache(pv->pv_space, pv->pv_va, NBPG);
			pdtlb(pv->pv_space, pv->pv_va);
			pmap_remove_va(pv);
		} else
			DPRINTF(PDB_REMOVE,
			    ("pmap_kremove: no pv for 0x%x\n", va));
	}
}
d1236 4
a1239 36
#if defined(PMAPDEBUG) && defined(DDB)
#include <ddb/db_output.h>
/*
 * prints whole va->pa (aka HPT or HVT)
 */
void
pmap_hptdump(sp)
	int sp;
{
	register struct hpt_entry *hpt, *ehpt;
	register struct pv_entry *pv;
	register int hpthf;

	mfctl(CR_HPTMASK, ehpt);
	mfctl(CR_VTOP, hpt);
	ehpt = (struct hpt_entry *)((int)hpt + (int)ehpt + 1);
	db_printf("HPT dump %p-%p:\n", hpt, ehpt);
	for (hpthf = 0; hpt < ehpt; hpt++, hpthf = 0)
		for (pv = hpt->hpt_entry; pv; pv = pv->pv_hash)
			if (sp < 0 || sp == pv->pv_space) {
				if (!hpthf) {
					db_printf(
					    "hpt@@%p: %x{%sv=%x:%x},%b,%x\n",
					    hpt, *(u_int *)hpt,
					    (hpt->hpt_valid?"ok,":""),
					    hpt->hpt_space, hpt->hpt_vpn << 9,
					    hpt->hpt_tlbprot, TLB_BITS,
					    tlbptob(hpt->hpt_tlbpage));

					hpthf++;
				}
				db_printf("    pv={%p,%x:%x,%b,%x}->%p\n",
				    pv->pv_pmap, pv->pv_space, pv->pv_va,
				    pv->pv_tlbprot, TLB_BITS,
				    tlbptob(pv->pv_tlbpage), pv->pv_hash);
			}
a1240 1
#endif
@


1.60
log
@pmap.pmap_pd is redundant -- no more
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.59 2002/02/12 16:28:53 mickey Exp $	*/
d207 1
a207 1
void pmap_hptdump __P((int sp));
d212 1
a212 1
void pmap_pinit __P((pmap_t));
@


1.59
log
@no need to zero out the page offset in pmap_hash, it'd already been done
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.58 2002/02/08 03:49:21 mickey Exp $	*/
d201 1
a201 1
u_int	pid_counter;
a644 1
	kernel_pmap->pmap_pid = HPPA_PID_KERNEL;
d692 3
a694 3
	if (btlb_insert(kernel_pmap->pmap_space, 0, 0, vstart,
			kernel_pmap->pmap_pid |
			pmap_prot(kernel_pmap, VM_PROT_ALL)) < 0)
d799 1
a799 1
	pid_counter = HPPA_PID_KERNEL + 2;
d823 1
a823 1
	register u_int pid;
d826 1
a826 1
	DPRINTF(PDB_FOLLOW, ("pmap_pinit(%p), pid=%x\n", pmap, pmap->pmap_pid));
d828 1
a828 1
	if (!(pid = pmap->pmap_pid)) {
d835 3
a837 3
		if (pid_counter < HPPA_MAX_PID) {
			pid = pid_counter;
			pid_counter += 2;
d839 1
a839 1
			pid = 0;
d842 2
a843 2
		if (pid == 0)
			panic ("no more pmap ids\n");
d849 1
a849 2
	pmap->pmap_pid = pid;
	pmap->pmap_space = (pid >> 1) - 1;
d968 1
a968 1
	tlbprot = TLB_UNCACHEABLE | pmap_prot(pmap, prot) | pmap->pmap_pid;
@


1.58
log
@do not set any bits in pmap_enter, let the handlers take care of it;
only need clear_va if changing the prot, since otherwise we are clear.
flush the cache and tlb for the page being removed in kremove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.57 2002/02/07 05:47:03 mickey Exp $	*/
a225 1
		"dep	%%r0, 31, 4, %%r22\n\t"	/* r22 &= ~0xf */
@


1.57
log
@fix pid calculation and pmap_extract; from fredette@@
replase disabling ints w/ splhigh()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.56 2002/02/06 19:29:06 mickey Exp $	*/
d971 1
a971 4
	tlbprot = TLB_UNCACHEABLE | TLB_REF |
	    pmap_prot(pmap, prot) | pmap->pmap_pid;
	if (flags & VM_PROT_WRITE)
		tlbprot |= TLB_DIRTY;
d998 1
a999 3

		/* Flush the current TLB entry to force a fault and reload */
		pmap_clear_pv(pa, NULL);
d1514 3
a1516 5
		if (pv) {
			if (pv->pv_tlbprot & TLB_WIRED)
				pmap_kernel()->pmap_stats.wired_count--;
			pmap_remove_pv(pv, pmap_find_pv(pa));
		} else
d1524 2
a1525 2
		pv->pv_tlbprot = HPPA_PID_KERNEL | TLB_WIRED | 
		    pmap_prot(pmap_kernel(), prot) |
d1540 1
a1540 1
	for (va = hppa_trunc_page(va); size;
d1543 5
a1547 1
		if (pv)
d1549 1
a1549 1
		else
@


1.56
log
@better flushing/purging of removed/changed mappings, hack around the pv/vp desync
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.55 2002/02/04 18:13:05 mickey Exp $	*/
d830 1
a830 1
	if (!pmap->pmap_pid) {
d852 1
a852 1
	pmap->pmap_space = (pmap->pmap_pid >> 1) - 1;
d956 2
a969 2
	va = hppa_trunc_page(va);
	space = pmap_sid(pmap, va);
d971 4
a974 1
	tlbprot = TLB_UNCACHEABLE | pmap_prot(pmap, prot) | pmap->pmap_pid;
d1219 1
d1229 1
d1237 1
d1255 1
a1256 1
	va = hppa_trunc_page(va);
d1259 2
a1260 1
	if (!(pv = pmap_find_va(pmap_sid(pmap, va), va & ~PGOFSET)))
d1266 1
d1280 1
a1280 1
	u_int32_t psw;
d1284 8
a1291 1
	rsm(PSW_I, psw);
d1310 1
a1310 1
	ssm(PSW_I, psw);
d1328 1
a1328 1
	u_int32_t psw;
d1332 1
a1332 1
	rsm(PSW_I, psw);
d1359 1
a1359 1
	ssm(PSW_I, psw);
@


1.55
log
@make hptdump take a space arg and print only entried for that space, all on -1
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2002/01/28 06:02:57 mickey Exp $	*/
d380 2
d458 6
a463 2
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				panic("pmap_enter_pv: already in pv_tab");
d512 2
a513 2
			/*npv->pv_tlbprot |= pv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);*/
d523 2
a1108 2
				pmap_clear_va(pv->pv_space, pv->pv_va);

d1114 3
d1118 1
a1118 1
				pitlb(pv->pv_space, pv->pv_va);
a1186 2
			pmap_clear_va(space, pv->pv_va);

d1191 5
a1195 2
			pitlb(space, pv->pv_va);
			pdtlb(space, pv->pv_va);
d1516 1
a1516 1
		pv->pv_tlbprot = HPPA_PID_KERNEL |
a1517 2
		    TLB_WIRED | /* TLB_REF | TLB_DIRTY | */
		    TLB_UNCACHEABLE |
@


1.54
log
@map user space uncached, later to be dealt with.
fix {is,clear}_{refe,mod} properly scan the lists, finally.
new, rewritten code is to make those things better.
make page{zero,copy} better.
do debug printfs through a DPRINTF macro.
@
text
@d207 1
a207 1
void pmap_hptdump __P((void));
d1541 2
a1542 1
pmap_hptdump()
d1546 1
d1552 14
a1565 8
	for (; hpt < ehpt; hpt++)
		if (hpt->hpt_valid || hpt->hpt_entry) {
			db_printf("hpt@@%p: %x{%sv=%x:%x},%b,%x\n",
			    hpt, *(int *)hpt, (hpt->hpt_valid?"ok,":""),
			    hpt->hpt_space, hpt->hpt_vpn << 9,
			    hpt->hpt_tlbprot, TLB_BITS,
			    tlbptob(hpt->hpt_tlbpage));
			for (pv = hpt->hpt_entry; pv; pv = pv->pv_hash)
d1570 1
a1570 1
		}
@


1.53
log
@malloc/free is not really used for pvs allocs
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.52 2001/12/22 00:20:04 mickey Exp $	*/
a123 5
 *	usage of __inline grows the code size by 100%, but hopefully
 *		makes it faster as well, since the functions are actually
 *		very small.
 *		retail:  8.1k -> 15.1K
 *		debug:  12.2k -> 22.1K
a133 1
#include <sys/malloc.h>
d136 1
a148 3
#define static	/**/
#define	__inline /* */

d150 4
d171 1
d181 1
d183 2
d247 3
a249 5
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_VA)) == (PDB_FOLLOW | PDB_VA))
		printf("pmap_enter_va(%x,%x,%p): hpt=%p, pvp=%p\n",
		    pv->pv_space, pv->pv_va, pv, hpt, pvp);
#endif
d271 1
a271 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_VA)) == (PDB_FOLLOW | PDB_VA))
		printf("pmap_find_va(%x,%x)\n", space, va);
#endif
d303 2
a304 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_VA)) == (PDB_FOLLOW | PDB_VA))
		printf("pmap_remove_va(%p), hpt=%p, pvp=%p\n", pv, hpt, pvp);
#endif
d355 1
a355 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_PV)) == (PDB_FOLLOW | PDB_PV))
		printf("pmap_alloc_pv()\n");
#endif
d358 1
a358 1
#if 0
d363 1
a363 1
		pmap_insert_pvp(pvp, 1);
d370 1
a370 1
	pvp = pv_page_freelist.tqh_first;
d395 1
a395 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_PV)) == (PDB_FOLLOW | PDB_PV))
		printf("pmap_free_pv(%p)\n", pv);
#endif
d400 1
a400 1
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_list);
d439 2
a440 5
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_PV)) == (PDB_FOLLOW | PDB_PV))
		printf("pmap_enter_pv: pv %p: %lx/%p/%p\n",
		    pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
#endif
d446 1
a446 4
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter_pv: no entries yet\n");
#endif
d453 1
a454 3
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter_pv: adding to the list\n");

d485 1
a485 4
#ifdef PMAPDEBUG
	if ((pmapdebug & (PDB_FOLLOW | PDB_PV)) == (PDB_FOLLOW | PDB_PV))
		printf("pmap_remove_pv(%p,%p)\n", ppv, pv);
#endif
d538 1
a538 4
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_PV)
			printf("pmap_find_pv(%x):  %d:%d\n", pa, bank, off);
#endif
a551 1
	int s;
d553 1
a553 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_PV)
		printf("pmap_clear_pv(%x,%p)\n", pa, cpv);
#endif
a557 1
	s = splimp();
d561 2
a562 4
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_PV)
			printf("pmap_clear_pv: %x:%x\n", pv->pv_space, pv->pv_va);
#endif
a573 1
	splx(s);
d597 2
a598 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_bootstrap(%p, %p)\n", vstart, vend);
#endif
d649 4
a652 5
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_bootstrap: allocating %d pv_pages\n",
		    (struct pv_page *)addr - pvp);
#endif
d667 3
a669 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_INIT)
		printf("hpt_table: 0x%x @@ %p\n", size + 1, addr);
#endif
d683 2
a684 2
				  (sizeof(struct pv_entry) * maxproc / 8 +
				   sizeof(struct vm_page)));
d699 2
a700 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_INIT)
		printf("pv_array: 0x%x @@ 0x%x\n", size, addr);
#endif
d734 4
a737 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_steal_memory(%x, %x, %x)\n", size, startp, endp);
#endif
d745 5
a749 5
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_STEAL)
			printf("pmap_steal_memory: steal %d bytes (%x+%x,%x)\n",
			    size, virtual_steal, size, virtual_avail);
#endif
d775 1
a775 2
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_init()\n");
d820 1
a820 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pinit(%p), pid=%x\n", pmap, pmap->pmap_pid);
#endif
d863 1
a863 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_create()\n");
#endif
d900 1
a900 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
#endif
d918 1
d955 2
a959 2
	s = splimp();	/* are we already high enough? XXX */

d963 1
a963 4
	tlbprot = TLB_UNCACHEABLE | TLB_REF | pmap_prot(pmap, prot) | pmap->pmap_pid;

	if (flags & VM_PROT_WRITE)
		tlbprot |= TLB_DIRTY;
d970 1
a970 4
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter: new mapping\n");
#endif
d978 2
a979 5
#ifdef PMAPDEBUG
			if (pmapdebug & PDB_ENTER)
				printf("pmap_enter: moving pa %x -> %x\n",
					ppv->pv_tlbpage, tlbpage);
#endif
d985 2
a986 6
#ifdef PMAPDEBUG
			if (pmapdebug & PDB_ENTER)
				printf("pmap_enter: changing %b->%b\n",
				    ppv->pv_tlbprot, TLB_BITS,
				    tlbprot, TLB_BITS);
#endif
d1010 1
a1010 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_enter: leaving\n");
#endif
d1032 2
a1033 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_remove(%p, %x, %x)\n", pmap, sva, eva);
#endif
d1044 3
a1046 5
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_REMOVE)
			printf ("pmap_remove: removing %p for 0x%x:0x%x\n",
				pv, space, sva);
#endif
d1076 2
a1077 4
#ifdef PMAPDEBUG
	if (pmapdebug & (PDB_FOLLOW & PDB_PROTECT))
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
#endif
d1117 5
a1121 7
#ifdef PMAPDEBUG
			if (pmapdebug & PDB_PROTECT)
				printf("pv={%p,%x:%x,%b,%x}->%p\n",
				    pv->pv_pmap, pv->pv_space, pv->pv_va,
				    pv->pv_tlbprot, TLB_BITS,
				    tlbptob(pv->pv_tlbpage), pv->pv_hash);
#endif
d1149 2
a1150 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot);
#endif
d1208 1
a1208 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_unwire(%p, %x)\n", pmap, va);
#endif
d1241 1
a1241 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %x)\n", pmap, va);
#endif
d1262 1
d1264 1
a1264 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%x)\n", pa);
#endif
d1266 1
d1270 5
a1274 1
		__asm volatile("stwas,ma %%r0,4(%0)\n\t"
d1278 3
a1280 3
			__asm volatile("rsm %1, %%r0\n\t"
				       "fdc %2(%0)\n\t"
				       "ssm %1, %%r0"
d1285 1
d1303 1
a1303 1
	int s;
d1305 1
a1305 4
#ifdef PMAPDEBUG
	if (1 && pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%x, %x)\n", spa, dpa);
#endif
d1307 1
a1310 2
	s = splimp(); /* XXX are we already that high? */

d1312 10
a1321 3
		__asm volatile("ldwas,ma 4(%0),%%r23\n\t"
			       "stwas,ma %%r23,4(%1)\n\t"
		    : "+r" (spa), "+r" (dpa) :: "r23", "memory");
d1324 5
a1328 4
			__asm volatile("rsm %2, %%r0\n\t"
				       "pdc %3(%0)\n\t"
				       "fdc %3(%1)\n\t"
				       "ssm %2, %%r0"
d1334 1
a1334 1
	splx(s);
d1350 1
a1350 1
	int s;
d1352 1
a1352 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%x)\n", pa);
#endif
d1355 8
a1362 9
	if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap) {
		splx(s);
		return FALSE;
	}

	pitlb(pv->pv_space, pv->pv_va);
	pdtlb(pv->pv_space, pv->pv_va);
	pv->pv_tlbprot &= ~(TLB_DIRTY);
	pmap_clear_va(pv->pv_space, pv->pv_va);
d1365 1
a1365 1
	return TRUE;
d1379 3
a1381 2
	boolean_t ret;
	int s;
a1382 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_is_modified(%x)\n", pa);
#endif
d1384 2
a1385 2
	ret = (pv = pmap_find_pv(pa)) && pv->pv_pmap &&
	    pv->pv_tlbprot & TLB_DIRTY;
d1388 1
a1388 1
	return ret;
d1405 1
a1405 1
	int s;
d1407 1
a1407 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%x)\n", pa);
#endif
d1410 8
a1417 9
	if (!(pv = pmap_find_pv(pa))) {
		splx(s);
		return FALSE;
	}

	pitlb(pv->pv_space, pv->pv_va);
	pdtlb(pv->pv_space, pv->pv_va);
	pv->pv_tlbprot &= ~(TLB_REF);
	pmap_clear_va(pv->pv_space, pv->pv_va);
d1420 1
a1420 1
	return TRUE;
d1434 1
a1434 2
	boolean_t ret;
	int s;
d1436 1
a1436 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_is_referenced(%x)\n", pa);
#endif
d1439 2
a1440 2
	ret = (pv = pmap_find_pv(pa)) && pv->pv_pmap &&
	    pv->pv_tlbprot & TLB_REF;
d1443 1
a1443 1
	return ret;
d1455 1
a1455 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_changebit(%x, %x, %x)\n", va, set, reset);
#endif
d1484 3
a1486 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_ENTER)
		printf("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot);
#endif
d1506 1
a1506 1
		pv->pv_tlbprot = TLB_UNCACHEABLE |
d1508 2
a1509 1
		    HPPA_PID_KERNEL | TLB_WIRED | TLB_REF |
d1514 1
a1514 4
#ifdef PMAPDEBUG
	if (pmapdebug & PDB_ENTER)
		printf("pmap_kenter_pa: leaving\n");
#endif
d1529 3
a1531 4
#ifdef PMAPDEBUG
		else if (pmapdebug & PDB_REMOVE)
			printf("pmap_kremove: no pv for 0x%x\n", va);
#endif
@


1.52
log
@fix kenter/kremove, some spl police
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51 2001/11/28 16:24:26 art Exp $	*/
d421 1
d425 3
@


1.51
log
@more typedef zapping vm_page_t -> struct vm_page *
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2001/11/28 14:20:16 art Exp $	*/
a246 1
	int s;
a251 1
	s = splimp();
a261 1
	splx(s);
a438 1
	int s;
a446 1
	s = splimp();
a489 2
	splx(s);

a500 1
	int s;
a507 6
	 * Remove from the PV table (raise IPL since we
	 * may be called at interrupt time).
	 */
	s = splimp();

	/*
a545 1
	splx(s);
d839 2
d1004 2
a1057 1
	s = splhigh();	/* are we already high enough? XXX */
a1571 1
	int s;
a1576 1
	s = splimp();
d1583 3
a1585 4
		if (!pv || !(pv->pv_tlbprot & TLB_WIRED))
			pmap_kernel()->pmap_stats.wired_count++;

		if (pv)
d1587 1
a1587 1
		else
d1595 3
a1597 2
		pv->pv_tlbprot = TLB_UNCACHEABLE | TLB_WIRED | TLB_REF | TLB_DIRTY |
		    pmap_prot(pmap_kernel(), prot) | HPPA_PID_KERNEL |
a1601 1
	splx(s);
d1613 12
a1624 1
	pmap_remove(kernel_pmap, va, va + size);
@


1.51.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.53 2002/01/28 06:02:57 mickey Exp $	*/
d247 1
d253 1
d264 1
a423 1
#ifdef notyet
a426 3
#else
			panic("pmap_free_pv: mallocated pv page");
#endif
d442 1
d451 1
d495 2
d508 1
d516 6
d560 1
a853 2
	 *
	 * no spls since no interrupts.
a1016 2
	s = splimp();	/* are we already high enough? XXX */

d1069 1
d1584 1
d1590 1
d1597 4
a1600 3
		if (pv) {
			if (pv->pv_tlbprot & TLB_WIRED)
				pmap_kernel()->pmap_stats.wired_count--;
d1602 1
a1602 1
		} else
d1610 2
a1611 3
		pv->pv_tlbprot = TLB_UNCACHEABLE |
		    pmap_prot(pmap_kernel(), prot) |
		    HPPA_PID_KERNEL | TLB_WIRED | TLB_REF |
d1616 1
d1628 1
a1628 12
	register struct pv_entry *pv;

	for (va = hppa_trunc_page(va); size;
	    size -= PAGE_SIZE, va += PAGE_SIZE) {
		pv = pmap_find_va(HPPA_SID_KERNEL, va);
		if (pv)
			pmap_remove_va(pv);
#ifdef PMAPDEBUG
		else if (pmapdebug & PDB_REMOVE)
			printf("pmap_kremove: no pv for 0x%x\n", va);
#endif
	}
@


1.51.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51.2.1 2002/01/31 22:55:09 niklas Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2002 Michael Shalayeff
d27 1
a27 1
 * MOND, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
d33 97
d139 1
a141 2
#include <sys/pool.h>
#include <sys/extent.h>
d145 5
d151 5
a157 4
#define	DPRINTF(l,s)	do {		\
	if ((pmapdebug & (l)) == (l))	\
		printf s;		\
} while(0)
d168 2
a169 2
#define	PDB_EXTRACT	0x00000400
#define	PDB_VP		0x00000800
d173 1
a173 1
#define	PDB_PMAP	0x00008000
a174 2
#define	PDB_PHYS	0x00020000
#define	PDB_POOL	0x00040000
a175 1
/*	| PDB_INIT */
d177 1
a177 1
/*	| PDB_VP */
d179 1
a183 1
/*	| PDB_PHYS */
a184 2
#else
#define	DPRINTF(l,s)	/* */
d187 1
a187 7
vaddr_t	virtual_avail;
paddr_t physical_steal, physical_end;
vaddr_t pmap_pv_page;

#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
int		pmap_hptsize = 256;	/* patchable */
#endif
d190 1
a190 1
int		pmap_sid_counter, hppa_sid_max = HPPA_SID_MAX;
a191 10
struct pool	pmap_pmap_pool;
struct pool	pmap_pv_pool;
struct simplelock pvalloc_lock;

void    *pmap_pv_page_alloc(struct pool *, int);
void    pmap_pv_page_free(struct pool *, void *);

struct pool_allocator pmap_allocator_pv = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0
};
d193 18
a210 1
u_int	hppa_prot[8];
d212 1
a215 4
#define	pmap_pvh_attrs(a) \
	(((a) & PTE_PROT(TLB_DIRTY)) | ((a) ^ PTE_PROT(TLB_REFTRAP)))

#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
d226 1
a234 1
#endif
d236 4
d241 1
a241 1
pmap_sdir_set(pa_space_t space, paddr_t pa)
d243 4
a246 3
	paddr_t vtop;

	mfctl(CR_VTOP, vtop);
d248 3
a250 2
	if (!vtop)
		panic("pmap_sdir_set: zero vtop");
d252 10
a261 2
	asm("stwas	%0, 0(%1)\n\tsync"
	    :: "r" (pa), "r" (vtop + (space << 2)));
d264 6
a269 2
static __inline paddr_t
pmap_sdir_get(pa_space_t space)
d271 6
a276 1
	paddr_t vtop, pa;
d278 2
a279 3
	mfctl(CR_VTOP, vtop);
	asm("ldwax,s	%2(%1), %0\n\tsync"
	    : "=&r" (pa) : "r" (vtop), "r" (space));
d281 1
a281 1
	return (pa);
d284 7
a290 2
static __inline pt_entry_t *
pmap_pde_get(paddr_t pa, vaddr_t va)
d292 1
a292 4
	pt_entry_t *pde;

	asm("ldwax,s	%2(%1), %0\n\tsync"
	    : "=&r" (pde) : "r" (pa), "r" (va >> 22));
d294 2
a295 1
	return (pde);
d298 4
d303 1
a303 1
pmap_pde_set(struct pmap *pm, vaddr_t va, paddr_t ptp)
d305 3
d309 2
a310 2
	if (ptp & PGOFSET)
		panic("pmap_pde_set, unaligned ptp 0x%x", ptp);
a311 2
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_set(%p, 0x%x, 0x%x)\n", pm, va, ptp));
d313 15
a327 2
	asm("stwas	%0, 0(%1)\n\tsync"
	    :: "r" (ptp), "r" ((paddr_t)pm->pm_pdir + ((va >> 20) & 0xffc)));
d330 8
a337 2
static __inline pt_entry_t *
pmap_pde_alloc(struct pmap *pm, vaddr_t va, struct vm_page **pdep)
d339 1
a339 31
	struct vm_page *pg;
	paddr_t pa;

	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_alloc(%p, 0x%x, %p)\n", pm, va, pdep));

	/* special hacking for pre-mapping the kernel */
	if (!pmap_initialized) {
		if (physical_steal >= physical_end)
			panic("pmap_pde_alloc: out of steallage");

		pa = physical_steal;
		physical_steal += PAGE_SIZE;
		bzero((void *)pa, PAGE_SIZE);
		fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
		pmap_pde_set(pm, va, pa);
		pm->pm_stats.resident_count++;	/* count PTP as resident */

		DPRINTF(PDB_VP, ("pmap_pde_alloc: pde %x\n", pa));
		return ((pt_entry_t *)pa);
	}

	va &= PDE_MASK;
	pg = uvm_pagealloc(&pm->pm_obj, va, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (pg == NULL) {
		/* try to steal somewhere */
		return (NULL);
	}

	pa = VM_PAGE_TO_PHYS(pg);
d341 7
a347 10
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pde_alloc: pde %x\n", pa));

	pg->flags &= ~PG_BUSY;		/* never busy */
	pg->wire_count = 1;		/* no mappings yet */
	pmap_pde_set(pm, va, pa);
	pm->pm_stats.resident_count++;	/* count PTP as resident */
	pm->pm_ptphint = pg;
	if (pdep)
		*pdep = pg;
	return ((pt_entry_t *)pa);
d350 8
a357 2
static __inline struct vm_page *
pmap_pde_ptp(struct pmap *pm, pt_entry_t *pde)
d359 2
a360 1
	paddr_t pa = (paddr_t)pde;
d362 4
a365 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp(%p, %p)\n", pm, pde));
d367 11
a377 2
	if (pm->pm_ptphint && VM_PAGE_TO_PHYS(pm->pm_ptphint) == pa)
		return (pm->pm_ptphint);
d379 11
a389 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp: lookup 0x%x\n", pa));
d391 1
a391 1
	return (PHYS_TO_VM_PAGE(pa));
d394 6
d401 1
a401 1
pmap_pde_release(struct pmap *pmap, vaddr_t va, struct vm_page *ptp)
d403 1
a403 2
	DPRINTF(PDB_FOLLOW|PDB_PV,
	    ("pmap_pde_release(%p, 0x%x, %p)\n", pmap, va, ptp));
d405 4
a408 4
	ptp->wire_count--;
	if (ptp->wire_count <= 1 && pmap != pmap_kernel()) {
		DPRINTF(PDB_FOLLOW|PDB_PV,
		    ("pmap_pde_release: disposing ptp %p\n", ptp));
d410 20
a429 6
		pmap_pde_set(pmap, va, 0);
		pmap->pm_stats.resident_count--;
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
		ptp->wire_count = 0;
		uvm_pagefree(ptp);
d433 8
a440 2
static __inline pt_entry_t
pmap_pte_get(pt_entry_t *pde, vaddr_t va)
d442 1
a442 1
	pt_entry_t pte;
d444 2
a445 2
	asm("ldwax,s	%2(%1),%0"
	    : "=&r" (pte) : "r" (pde),  "r" ((va >> 12) & 0x3ff));
d447 4
a450 2
	return (pte);
}
d452 5
a456 5
static __inline void
pmap_pte_set(pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
{
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pte_set(%p, 0x%x, 0x%x)\n",
	    pde, va, pte));
d458 14
d473 2
a474 12
	if (!pde)
		panic("pmap_pte_set: zero pde");

	if (pte && pte < physical_end &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page)
		panic("pmap_pte_set: invalid pte");

	if (pte && !(pte & PTE_PROT(TLB_UNCACHABLE)) &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page) {
		printf("pmap_pte_set: cached pte\n");
		Debugger();
	}
d476 3
a478 2
	if ((paddr_t)pde & PGOFSET)
		panic("pmap_pte_set, unaligned pde %p", pde);
d480 13
a492 8
	asm("stwas	%0, 0(%1)"
	    :: "r" (pte), "r" ((paddr_t)pde + ((va >> 10) & 0xffc)));
}

static __inline pt_entry_t
pmap_vp_find(struct pmap *pm, vaddr_t va)
{
	pt_entry_t *pde;
d494 1
a494 4
	if (!(pde = pmap_pde_get(pm->pm_pdir, va)))
		return (NULL);

	return (pmap_pte_get(pde, va));
d497 7
a503 2
void
pmap_dump_table(pa_space_t space, vaddr_t sva)
a504 1
	pa_space_t sp;
d506 4
a509 4
	for (sp = 0; sp <= hppa_sid_max; sp++) {
		paddr_t pa;
		pt_entry_t *pde, pte;
		vaddr_t va, pdemask = virtual_avail + 1;
d511 5
a515 3
		if (((int)space >= 0 && sp != space) ||
		    !(pa = pmap_sdir_get(sp)))
			continue;
d517 2
a518 10
		for (va = sva? sva : virtual_avail; va < VM_MAX_KERNEL_ADDRESS;
		    va += PAGE_SIZE) {
			if (pdemask != (va & PDE_MASK)) {
				pdemask = va & PDE_MASK;
				if (!(pde = pmap_pde_get(pa, va))) {
					va += ~PDE_MASK + 1 - PAGE_SIZE;
					continue;
				}
				printf("%x:0x%08x:\n", sp, pde);
			}
d520 19
a538 2
			if (!(pte = pmap_pte_get(pde, va)))
				continue;
d540 8
a547 1
			printf("0x%08x-0x%08x\n", va, pte);
d552 4
d557 1
a557 1
pmap_pv_alloc(void)
d559 1
a559 1
	struct pv_entry *pv;
d561 8
a568 11
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc()\n"));

	simple_lock(&pvalloc_lock);

	pv = pool_get(&pmap_pv_pool, 0);

	simple_unlock(&pvalloc_lock);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc: %p\n", pv));

	return (pv);
d571 4
d576 1
a576 1
pmap_pv_free(struct pv_entry *pv)
d578 2
a579 1
	simple_lock(&pvalloc_lock);
d581 4
a584 2
	if (pv->pv_ptp)
		pmap_pde_release(pv->pv_pmap, pv->pv_va, pv->pv_ptp);
d586 2
a587 1
	pool_put(&pmap_pv_pool, pv);
d589 13
a601 2
	simple_unlock(&pvalloc_lock);
}
d603 2
a604 6
static __inline void
pmap_pv_enter(struct pv_head *pvh, struct pv_entry *pve, struct pmap *pm,
    vaddr_t va, struct vm_page *pdep)
{
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_enter(%p, %p, %p, 0x%x, %p)\n",
	    pvh, pve, pm, va, pdep));
d606 3
a608 20
	pve->pv_pmap	= pm;
	pve->pv_va	= va;
	pve->pv_ptp	= pdep;
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;
	pvh->pvh_list = pve;
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
}

static __inline struct pv_entry *
pmap_pv_remove(struct pv_head *pvh, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry **pve, *pv;

	for(pv = *(pve = &pvh->pvh_list); pv; pv = *(pve = &(*pve)->pv_next))
		if (pv->pv_pmap == pmap && pv->pv_va == va) {
			*pve = pv->pv_next;
			break;
		}
	return (pv);
d611 9
d621 7
a627 7
pmap_bootstrap(vstart)
	vaddr_t vstart;
{
	extern char etext, etext1;
	extern u_int totalphysmem, *ie_mem;
	extern paddr_t hppa_vtop;
	vaddr_t va, addr = hppa_round_page(vstart), t;
d629 6
a634 2
#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	struct vp_entry *hptp;
a635 3
	struct pmap *kpm;

	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_bootstrap(0x%x)\n", vstart));
d639 22
a660 8
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_NA;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_R;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
d665 11
a675 14
	kpm = &kernel_pmap_store;
	bzero(kpm, sizeof(*kpm));
	simple_lock_init(&kpm->pm_obj.vmobjlock);
	kpm->pm_obj.pgops = NULL;
	TAILQ_INIT(&kpm->pm_obj.memq);
	kpm->pm_obj.uo_npages = 0;
	kpm->pm_obj.uo_refs = 1;
	kpm->pm_space = HPPA_SID_KERNEL;
	kpm->pm_pid = HPPA_PID_KERNEL;
	kpm->pm_pdir_pg = NULL;
	kpm->pm_pdir = addr;
	bzero((void *)addr, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
	addr += PAGE_SIZE;
d680 3
d684 27
d712 1
a712 25
	hppa_vtop = addr;
	size = hppa_round_page((hppa_sid_max + 1) * 4);
	bzero((void *)addr, size);
	fdcache(HPPA_SID_KERNEL, addr, size);
	DPRINTF(PDB_INIT, ("vtop: 0x%x @@ 0x%x\n", size, addr));
	addr += size;
	pmap_sdir_set(HPPA_SID_KERNEL, kpm->pm_pdir);

	ie_mem = (u_int *)addr;
	addr += 0x8000;

#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	if (pmap_hptsize && (cpu_type == hpcxl || cpu_type == hpcxl2)) {
		int error;

		if (pmap_hptsize > pdc_hwtlb.max_size)
			pmap_hptsize = pdc_hwtlb.max_size;
		else if (pmap_hptsize < pdc_hwtlb.min_size)
			pmap_hptsize = pdc_hwtlb.min_size;

		size = pmap_hptsize * sizeof(*hptp);
		bzero((void *)addr, size);
		/* Allocate the HPT */
		for (hptp = (struct vp_entry *)addr, i = pmap_hptsize; i--;)
			hptp[i].vp_tag = 0xffff;
d714 5
a718 15
		DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n", size, addr));

		if ((error = (cpu_hpt_init)(addr, size)) < 0) {
			printf("WARNING: HPT init error %d\n", error);
		} else {
			printf("HPT: %d entries @@ 0x%x\n",
			    pmap_hptsize / sizeof(struct vp_entry), addr);
		}

		/* TODO find a way to avoid using cr*, use cpu regs instead */
		mtctl(addr, CR_VTOP);
		mtctl(size - 1, CR_HPTMASK);
		addr += size;
	}
#endif	/* HP7100LC_CPU | HP7300LC_CPU */
d720 4
d725 5
a729 14
	t = (vaddr_t)&etext1;
	if (btlb_insert(HPPA_SID_KERNEL, 0, 0, &t,
	    pmap_sid2pid(HPPA_SID_KERNEL) |
	    pmap_prot(pmap_kernel(), UVM_PROT_RX)) < 0)
		panic("pmap_bootstrap: cannot block map kernel text");
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
	    physmem = atop(t);

	if (&etext < &etext1) {
		physical_steal = (vaddr_t)&etext;
		physical_end = (vaddr_t)&etext1;
		DPRINTF(PDB_INIT, ("physpool: 0x%x @@ 0x%x\n",
		    physical_end - physical_steal, physical_steal));
	}
d736 1
a736 4
	addr = hppa_round_page(addr);
	pmap_pv_page = addr;
	addr += PAGE_SIZE;
	size = hppa_round_page(sizeof(struct pv_head) * totalphysmem);
d738 23
a760 1
	virtual_avail = addr + size;
d762 21
a782 1
	DPRINTF(PDB_INIT, ("pmseg.pvent: 0x%x @@ 0x%x\n", size, addr));
d784 9
a792 5
	/* XXX we might need to split this for isa */
	uvm_page_physload(0, totalphysmem,
		atop(virtual_avail), totalphysmem, VM_FREELIST_DEFAULT);
	/* we have only one initial phys memory segment */
	vm_physmem[0].pmseg.pvhead = (struct pv_head *)addr;
d794 5
a798 4
	addr += size;
	/* now we know how much to map */
	for (va = (vaddr_t)&etext1; va < addr; va += PAGE_SIZE)
		pmap_kenter_pa(va, va, UVM_PROT_RW);
d800 1
a800 2
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n",
	    &etext1, addr));
d803 6
d812 8
a819 1
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init()\n"));
d821 9
a829 4
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pmappv",
	    &pmap_allocator_pv);
d831 5
a835 1
	simple_lock_init(&pvalloc_lock);
d846 3
a848 10
	{
		pt_entry_t *pde;

		if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, SYSCALLGATE)) &&
		    !(pde = pmap_pde_alloc(pmap_kernel(), SYSCALLGATE, NULL)))
			panic("pmap_init: cannot allocate pde");

		pmap_pte_set(pde, SYSCALLGATE, (paddr_t)&gateway_page |
		    PTE_PROT(TLB_GATE_PROT));
	}
d851 4
d856 2
a857 1
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
d859 2
a860 3
	*startp = virtual_avail;
	*endp = VM_MAX_KERNEL_ADDRESS;
}
d862 4
a865 5
struct pmap *
pmap_create()
{
	struct pmap *pmap;
	pa_space_t space;
d867 1
a867 2
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_create()\n"));
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d869 11
a879 13
	simple_lock_init(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.pgops = NULL;	/* currently not a mappable object */
	TAILQ_INIT(&pmap->pm_obj.memq);
	pmap->pm_obj.uo_npages = 0;
	pmap->pm_obj.uo_refs = 1;
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;

	if (pmap_sid_counter >= hppa_sid_max) {
		/* collect some */
		panic("pmap_create: outer space");
	} else
		space = ++pmap_sid_counter;
d881 2
a882 7
	pmap->pm_space = space;
	pmap->pm_pid = (space + 1) << 1;
	pmap->pm_pdir_pg = uvm_pagealloc(NULL, 0, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (!pmap->pm_pdir_pg)
		panic("pmap_create: no pages");
	pmap->pm_pdir = VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
d884 2
a885 1
	pmap_sdir_set(space, pmap->pm_pdir);
d887 7
a893 1
	return(pmap);
d896 8
a903 3
void
pmap_destroy(pmap)
	struct pmap *pmap;
d905 2
a906 2
	struct vm_page *pg;
	int refs;
d908 4
a911 1
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_destroy(%p)\n", pmap));
d913 16
a928 3
	simple_lock(&pmap->pm_obj.vmobjlock);
	refs = --pmap->pm_obj.uo_refs;
	simple_unlock(&pmap->pm_obj.vmobjlock);
d930 1
a930 2
	if (refs > 0)
		return;
d932 1
a932 13
	TAILQ_FOREACH(pg, &pmap->pm_obj.memq, listq) {
#ifdef DIAGNOSTIC
		if (pg->flags & PG_BUSY)
			panic("pmap_destroy: busy page table page");
#endif
		pg->wire_count = 0;
		uvm_pagefree(pg);
	}

	uvm_pagefree(pmap->pm_pdir_pg);
	pmap->pm_pdir_pg = NULL;	/* XXX cache it? */
	pmap_sdir_set(pmap->pm_space, 0);
	pool_put(&pmap_pmap_pool, pmap);
d936 4
a939 1
 * Add a reference to the specified pmap.
d942 2
a943 2
pmap_reference(pmap)
	struct pmap *pmap;
d945 2
a946 1
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_reference(%p)\n", pmap));
d948 4
a951 4
	simple_lock(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.uo_refs++;
	simple_unlock(&pmap->pm_obj.vmobjlock);
}
d953 21
a973 5
void
pmap_collect(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_collect(%p)\n", pmap));
	/* nothing yet */
d975 7
a981 1

d984 1
a984 1
	struct pmap *pmap;
d990 4
a993 5
	pt_entry_t *pde, pte;
	struct vm_page *ptp = NULL;
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off;
d995 1
d997 7
a1003 3
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, 0x%x, 0x%x, 0x%x, 0x%x)\n",
	    pmap, va, pa, prot, flags));
d1005 2
a1006 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1008 1
a1008 4
	if (!(pde = pmap_pde_get(pmap->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap, va, &ptp))) {
		if (flags & PMAP_CANFAIL)
			return (KERN_RESOURCE_SHORTAGE);
d1010 19
a1028 2
		panic("pmap_enter: cannot allocate pde");
	}
d1030 1
a1030 2
	if (!ptp)
		ptp = pmap_pde_ptp(pmap, pde);
d1032 21
a1052 20
	if ((pte = pmap_pte_get(pde, va))) {

		DPRINTF(PDB_ENTER,
		    ("pmap_enter: remapping 0x%x -> 0x%x\n", pte, pa));

		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(pmap->pm_space, va, NBPG);
		pitlb(pmap->pm_space, va);
		fdcache(pmap->pm_space, va, NBPG);
		pdtlb(pmap->pm_space, va);

		if (wired && !(pte & PTE_PROT(TLB_WIRED)) == 0)
			pmap->pm_stats.wired_count++;
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)) != 0)
			pmap->pm_stats.wired_count--;

		if (PTE_PAGE(pte) == pa) {
			DPRINTF(PDB_FOLLOW|PDB_ENTER,
			    ("pmap_enter: same page\n"));
			goto enter;
d1055 3
a1057 45
		bank = vm_physseg_find(atop(PTE_PAGE(pte)), &off);
		if (bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_pv_remove(pvh, pmap, va);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pvh->pvh_lock);
		} else
			pve = NULL;
	} else {
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: new mapping 0x%x -> 0x%x\n", va, pa));
		pte = PTE_PROT(TLB_REFTRAP);
		pve = NULL;
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
		if (ptp)
			ptp->wire_count++;
	}

	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		if (!pve && !(pve = pmap_pv_alloc())) {
			if (flags & PMAP_CANFAIL) {
				simple_unlock(&pmap->pm_obj.vmobjlock);
				return (KERN_RESOURCE_SHORTAGE);
			}
			panic("pmap_enter: no pv entries available");
		}
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
		pmap_pv_enter(pvh, pve, pmap, va, ptp);
	} else {
		pvh = NULL;
		if (pve)
			pmap_pv_free(pve);
	}

enter:
	/* preserve old ref & mod */
	pte = pa | PTE_PROT(TLB_UNCACHABLE|pmap_prot(pmap, prot)) |
	    (pte & PTE_PROT(TLB_UNCACHABLE|TLB_DIRTY|TLB_REFTRAP));
	if (wired)
		pte |= PTE_PROT(TLB_WIRED);
	pmap_pte_set(pde, va, pte);
d1059 12
a1070 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1072 5
a1076 1
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_enter: leaving\n"));
d1081 7
d1090 3
a1092 3
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d1094 12
a1105 20
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *pde, pte;
	int bank, off, batch;
	u_int pdemask;

	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_remove(%p, 0x%x, 0x%x\n", pmap, sva, eva));

	simple_lock(&pmap->pm_obj.vmobjlock);

	for (pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
			batch = pdemask == sva && sva + ~PDE_MASK + 1 < eva;
		}
d1107 2
a1108 1
		if ((pte = pmap_pte_get(pde, sva))) {
d1110 13
a1122 27
			if (pte & PTE_PROT(TLB_WIRED))
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;

			/* TODO measure here the speed tradeoff
			 * for flushing whole 4M vs per-page
			 * in case of non-complete pde fill
			 */
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);

			/* iff properly accounted pde will be dropped anyway */
			if (!batch)
				pmap_pte_set(pde, sva, 0);

			bank = vm_physseg_find(atop(pte), &off);
			if (pmap_initialized && bank != -1) {
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				simple_lock(&pvh->pvh_lock);
				pvh->pvh_attrs |= pmap_pvh_attrs(pte);
				if ((pve = pmap_pv_remove(pvh, pmap, sva)))
					pmap_pv_free(pve);
				simple_unlock(&pvh->pvh_lock);
			}
d1124 1
d1127 1
a1127 3
	simple_unlock(&pmap->pm_obj.vmobjlock);

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_remove: leaving\n"));
d1130 5
d1136 2
a1137 4
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d1140 5
a1144 4
	pt_entry_t *pde, pte;
	u_int tlbprot, pdemask;
	struct pv_head *pvh;
	int bank, off;
d1146 4
a1149 2
	DPRINTF(PDB_FOLLOW|PDB_PMAP,
	    ("pmap_write_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot));
d1151 9
a1159 12
	sva = hppa_trunc_page(sva);
	tlbprot = PTE_PROT(pmap_prot(pmap, prot));

	simple_lock(&pmap->pm_obj.vmobjlock);

	for(pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
a1160 1
		if ((pte = pmap_pte_get(pde, sva))) {
d1162 1
a1162 3
			DPRINTF(PDB_PMAP,
			    ("pmap_write_protect: va=0x%x pte=0x%x\n",
			    sva,  pte));
d1164 2
a1165 2
			 * Determine if mapping is changing.
			 * If not, nothing to do.
d1167 1
a1167 2
			if ((pte & PTE_PROT(TLB_AR_MASK)) == tlbprot)
				continue;
d1169 13
a1181 4
			bank = vm_physseg_find(atop(pte), &off);
			if (bank == -1) {
				printf("pmap_page_remove: unmanaged page?\n");
				return;
a1182 15
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pvh->pvh_lock);

			ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);

			if (!(tlbprot & TLB_WRITE))
				pte &= ~PTE_PROT(TLB_DIRTY);
			pte &= ~PTE_PROT(TLB_AR_MASK);
			pte |= tlbprot;
			pmap_pte_set(pde, sva, pte);
d1184 18
a1202 2

	simple_unlock(&pmap->pm_obj.vmobjlock);
d1205 7
d1213 5
a1217 2
pmap_page_remove(pg)
	struct vm_page *pg;
d1219 8
a1226 4
	struct pv_head *pvh;
	struct pv_entry *pve, *ppve;
	pt_entry_t *pde, pte;
	int bank, off;
d1228 2
a1229 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove(%p)\n", pg));
d1231 2
a1232 3
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
d1235 1
a1235 3

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL)
d1238 3
a1240 1
	simple_lock(&pvh->pvh_lock);
d1242 8
a1249 2
	for (pve = pvh->pvh_list; pve; ) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
d1251 2
a1252 3
		pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va);
		pte = pmap_pte_get(pde, pve->pv_va);
		pmap_pte_set(pde, pve->pv_va, 0);
d1254 1
a1254 3
		if (pte & PTE_PROT(TLB_WIRED))
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;
d1256 7
a1262 6
		simple_unlock(&pve->pmap->pm_obj.vmobjlock);

		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
		ppve = pve;
		pve = pve->pv_next;
		pmap_pv_free(ppve);
a1263 4
	simple_unlock(&pvh->pvh_lock);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove: leaving\n"));

d1266 10
d1278 1
a1278 1
	struct pmap *pmap;
d1281 1
a1281 1
	pt_entry_t *pde, pte = 0;
d1283 4
a1286 19
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire(%p, 0x%x)\n", pmap, va));

	simple_lock(&pmap->pm_obj.vmobjlock);
	if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
		pte = pmap_pte_get(pde, va);

		if (pte & PTE_PROT(TLB_WIRED)) {
			pte &= ~PTE_PROT(TLB_WIRED);
			pmap->pm_stats.wired_count--;
			pmap_pte_set(pde, va, pte);
		}
	}
	simple_unlock(&pmap->pm_obj.vmobjlock);

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire: leaving\n"));

#ifdef DIAGNOSTIC
	if (!pte)
		panic("pmap_unwire: invalid va 0x%x", va);
a1287 1
}
d1289 2
a1290 7
boolean_t
pmap_changebit(struct vm_page *pg, u_int set, u_int clear)
{
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *pde, pte, res;
	int bank, off;
d1292 1
a1292 2
	DPRINTF(PDB_FOLLOW|PDB_BITS,
	    ("pmap_changebit(%p, %x, %x)\n", pg, set, clear));
d1294 2
a1295 23
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_testbits: unmanaged page?\n");
		return(FALSE);
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	res = pvh->pvh_attrs = 0;

	simple_lock(&pvh->pvh_lock);
	for(pve = pvh->pvh_list; pve; pve = pve->pv_next) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
		if ((pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va))) {
			pte = pmap_pte_get(pde, pve->pv_va);
			res |= pmap_pvh_attrs(pte);
			pte &= ~clear;
			pte |= set;

			if (pte & PTE_PROT(TLB_EXECUTE))
				pitlb(pve->pv_pmap->pm_space, pve->pv_va);
			/* XXX flush only if there was mod ? */
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
			pdtlb(pve->pv_pmap->pm_space, pve->pv_va);
d1297 3
a1299 3
			pmap_pte_set(pde, pve->pv_va, pte);
		}
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
d1301 1
a1301 34
	pvh->pvh_attrs = res;
	simple_unlock(&pvh->pvh_lock);

	return ((res & clear) != 0);
}

boolean_t
pmap_testbit(struct vm_page *pg, u_int bits)
{
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t pte;
	int bank, off;

	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bits));

	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_testbits: unmanaged page?\n");
		return(FALSE);
	}

	simple_lock(&pvh->pvh_lock);
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	for(pve = pvh->pvh_list; !(pvh->pvh_attrs & bits) && pve;
	    pve = pve->pv_next) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
		pte = pmap_vp_find(pve->pv_pmap, pve->pv_va);
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
	}
	simple_unlock(&pvh->pvh_lock);

	return ((pvh->pvh_attrs & bits) != 0);
d1304 7
d1313 1
a1313 1
	struct pmap *pmap;
d1317 1
a1317 1
	pt_entry_t pte;
d1319 5
a1323 1
	DPRINTF(PDB_FOLLOW|PDB_EXTRACT, ("pmap_extract(%p, %x)\n", pmap, va));
d1325 4
a1328 7
	simple_lock(&pmap->pm_obj.vmobjlock);
	pte = pmap_vp_find(pmap, va);
	simple_unlock(&pmap->pm_obj.vmobjlock);

	if (pte) {
		if (pap)
			*pap = (pte & ~PGOFSET) | (va & PGOFSET);
a1330 2

	return (FALSE);
d1333 5
d1340 1
a1340 1
	paddr_t pa;
a1343 1
	int s;
d1345 4
a1348 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%x)\n", pa));
d1350 1
a1350 8
	/*
	 * do not allow any ints to happen, since cache is in
	 * inconsistant state during the loop.
	 *
	 * do not rsm(PSW_I) since that will lose ints completely,
	 * instead, keep 'em pending (or verify by the book).
	 */
	s = splhigh();
d1353 1
a1353 5
		__asm volatile(			/* can use ,bc */
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)\n\t"
		    "stwas,ma	%%r0,4(%0)"
d1357 3
a1359 5
			__asm volatile("rsm	%1, %%r0\n\t"
				       "nop ! nop ! nop\n\t"
				       "fdc	%2(%0)\n\t"
				       "nop ! nop ! nop\n\t"
				       "ssm	%1, %%r0"
a1363 1
	splx(s);
d1366 8
d1383 7
a1389 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%x, %x)\n", spa, dpa));
d1391 1
a1391 2
	s = splhigh();
	/* XXX flush cache for the sva (from spa) ??? */
d1394 3
a1396 10
		__asm volatile(			/* can use ,bc */
		    "ldwas,ma	4(%0),%%r22\n\t"
		    "ldwas,ma	4(%0),%%r21\n\t"
		    "stwas,ma	%%r22,4(%1)\n\t"
		    "stwas,ma	%%r21,4(%1)\n\t"
		    "ldwas,ma	4(%0),%%r22\n\t"
		    "ldwas,ma	4(%0),%%r21\n\t"
		    "stwas,ma	%%r22,4(%1)\n\t"
		    "stwas,ma	%%r21,4(%1)\n\t"
		    : "+r" (spa), "+r" (dpa) :: "r22", "r21", "memory");
d1399 4
a1402 7
			__asm volatile(
			    "rsm	%2, %%r0\n\t"
			    "nop ! nop ! nop\n\t"
			    "pdc	%3(%0)\n\t"
			    "fdc	%3(%1)\n\t"
			    "nop ! nop\n\t"
			    "ssm	%2, %%r0"
d1411 43
a1453 5
void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d1455 13
a1467 1
	pt_entry_t *pde, pte;
d1469 2
a1470 2
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot));
d1472 15
a1486 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1488 3
a1490 7
	if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap_kernel(), va, NULL)))
		panic("pmap_kenter_pa: cannot allocate pde");
#ifdef DIAGNOSTIC
	if ((pte = pmap_pte_get(pde, va)))
		panic("pmap_kenter_pa: 0x%x is already mapped %p:0x%x",
		    va, pde, pte);
d1493 5
a1497 4
	pte = pa | PTE_PROT(TLB_WIRED|TLB_DIRTY|pmap_prot(pmap_kernel(), prot));
	/* if (pa >= HPPA_IOSPACE) */
		pte |= PTE_PROT(TLB_UNCACHABLE);
	pmap_pte_set(pde, va, pte);
d1499 5
a1503 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1505 1
a1505 1
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
d1508 28
d1537 1
a1537 1
pmap_kremove(va, size)
d1539 1
a1539 1
	vsize_t size;
d1541 2
a1542 5
	struct pv_entry *pve;
	struct pv_head *pvh;
	vaddr_t eva = va + size, pdemask;
	pt_entry_t *pde, pte;
	int bank, off;
d1544 4
a1547 2
	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_kremove(%x, %x)\n", va, size));
d1549 5
a1553 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1555 3
a1557 14
	for (pdemask = va + 1; va < eva; va += PAGE_SIZE) {
		if (pdemask != (va & PDE_MASK)) {
			pdemask = va & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va))) {
				va += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
		}
		if (!(pte = pmap_pte_get(pde, va))) {
#ifdef DEBUG
			printf("pmap_kremove: unmapping unmapped 0x%x\n", va);
#endif
			continue;
		}
d1559 2
a1560 18
		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(HPPA_SID_KERNEL, va, NBPG);
		pitlb(HPPA_SID_KERNEL, va);
		fdcache(HPPA_SID_KERNEL, va, NBPG);
		pdtlb(HPPA_SID_KERNEL, va);

		pmap_pte_set(pde, va, 0);
		bank = vm_physseg_find(atop(pte), &off);
		if (pmap_initialized && bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			/* just in case we have enter/kenter mismatch */
			if ((pve = pmap_pv_remove(pvh, pmap_kernel(), va)))
				pmap_pv_free(pve);
			simple_unlock(&pvh->pvh_lock);
		}
	}
d1562 2
a1563 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1565 1
a1565 1
	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_kremove: leaving\n"));
d1567 1
d1569 5
a1573 2
void *
pmap_pv_page_alloc(struct pool *pp, int flags)
d1575 18
a1592 2
	DPRINTF(PDB_FOLLOW|PDB_POOL,
	    ("pmap_pv_page_alloc(%p, %x)\n", pp, flags));
d1594 10
a1603 4
	if (pmap_pv_page) {
		void *v = (void *)pmap_pv_page;
		pmap_pv_page = 0;
		return (v);
d1606 4
a1609 2
	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    (flags & PR_WAITOK) ? TRUE : FALSE));
d1613 3
a1615 1
pmap_pv_page_free(struct pool *pp, void *v)
d1617 13
a1629 1
	vaddr_t va = (vaddr_t)v;
d1631 10
a1640 1
	DPRINTF(PDB_FOLLOW|PDB_POOL, ("pmap_pv_page_free(%p, %p)\n", pp, v));
d1642 17
a1658 4
	if (va < virtual_avail)
		panic("pmap_pv_page_alloc: freeing the last page");
	else
		uvm_km_free_poolpage1(kernel_map, va);
d1660 1
@


1.51.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.51.2.2 2002/06/11 03:35:37 art Exp $	*/
d26 2
a27 2
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF MIND,
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
a50 2
#include <dev/rndvar.h>

d76 2
a77 2
	| PDB_INIT
	| PDB_FOLLOW
d90 1
d92 1
d99 1
a99 1
int		hppa_sid_max = HPPA_SID_MAX;
d105 7
a119 15
struct vm_page *
pmap_pagealloc(struct uvm_object *obj, voff_t off)
{
	struct vm_page *pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO);

	if (!pg) {
		/* wait and pageout */

		return (NULL);
	}

	return (pg);
}

d124 1
a124 1
static inline struct hpt_entry *
d142 1
a142 1
pmap_sdir_set(pa_space_t space, u_int32_t *pd)
d144 1
a144 1
	u_int32_t *vtop;
d151 2
a152 1
	vtop[space] = (u_int32_t)pd;
d155 1
a155 1
static __inline u_int32_t *
d158 1
a158 1
	u_int32_t *vtop;
d161 4
a164 1
	return ((u_int32_t *)vtop[space]);
d168 1
a168 1
pmap_pde_get(u_int32_t *pd, vaddr_t va)
d170 6
a175 1
	return ((pt_entry_t *)pd[va >> 22]);
d188 2
a189 1
	pm->pm_pdir[va >> 22] = ptp;
d201 21
a221 1
	if ((pg = pmap_pagealloc(&pm->pm_obj, va)) == NULL)
d223 1
d260 2
a261 1
	if (pmap != pmap_kernel() && --ptp->wire_count <= 1) {
d264 1
a264 1
#if 1
d267 2
a270 3
#endif
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d277 6
a282 1
	return (pde[(va >> 12) & 0x3ff]);
d295 1
a295 1
	if (pte && pmap_initialized && pte < physical_end &&
d297 2
a298 2
		panic("pmap_pte_set: invalid pte 0x%x", pte);
#if 0
d304 1
a304 1
#endif
d308 2
a309 2

	pde[(va >> 12) & 0x3ff] = pte;
d318 1
a318 1
		return (0);
a322 1
#ifdef DDB
d329 1
a329 1
		u_int32_t *pd;
d331 1
a331 1
		vaddr_t va, pdemask = 1;
d334 1
a334 1
		    !(pd = pmap_sdir_get(sp)))
d337 1
a337 1
		for (va = sva? sva : 0; va < VM_MAX_KERNEL_ADDRESS;
d341 1
a341 1
				if (!(pde = pmap_pde_get(pd, va))) {
d351 1
a351 35
			printf("0x%08x-0x%08x:%b\n", va, pte & ~PAGE_MASK,
			    TLB_PROT(pte & PAGE_MASK), TLB_BITS);
		}
	}
}

void
pmap_dump_pv(paddr_t pa)
{
	struct vm_page *pg;
	struct pv_entry *pve;

	pg = PHYS_TO_VM_PAGE(pa);
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		printf("%x:%x\n", pve->pv_pmap->pm_space, pve->pv_va);
	simple_unlock(&pg->mdpage.pvh_lock);
}
#endif

#ifdef PMAPDEBUG
int
pmap_check_alias(struct pv_entry *pve, vaddr_t va, pt_entry_t pte)
{
	int ret;

	/* check for non-equ aliased mappings */
	for (ret = 0; pve; pve = pve->pv_next) {
		pte |= pmap_vp_find(pve->pv_pmap, pve->pv_va);
		if ((va & HPPA_PGAOFF) != (pve->pv_va & HPPA_PGAOFF) &&
		    (pte & PTE_PROT(TLB_WRITE))) {
			printf("pmap_check_alias: "
			    "aliased writable mapping 0x%x:0x%x\n",
			    pve->pv_pmap->pm_space, pve->pv_va);
			ret++;
a353 2

	return (ret);
a354 1
#endif
d388 1
a388 1
pmap_pv_enter(struct vm_page *pg, struct pv_entry *pve, struct pmap *pm,
d392 2
a393 1
	    pg, pve, pm, va, pdep));
d397 4
a400 6
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
	pve->pv_next = pg->mdpage.pvh_list;
	pg->mdpage.pvh_list = pve;
	if (pmap_check_alias(pve, va, 0))
		Debugger();
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
d404 1
a404 1
pmap_pv_remove(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
d408 1
a408 3
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
	for(pv = *(pve = &pg->mdpage.pvh_list);
	    pv; pv = *(pve = &(*pve)->pv_next))
a412 1
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
d423 1
a423 1
	vaddr_t va, endaddr, addr = hppa_round_page(vstart), t;
a428 1
	int npdes;
d456 1
a456 1
	kpm->pm_pdir = (u_int32_t *)addr;
d508 1
a508 1
	/* XXX PCXS needs this inserted into an IBTLB */
d529 6
a534 6
	/* one for the start of the kernel virtual */
	npdes = 1 + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
	addr = round_page(addr);
	size = npdes * PAGE_SIZE;
	uvm_page_physload(0, totalphysmem,
	    atop(addr + size), totalphysmem, VM_FREELIST_DEFAULT);
d536 1
a536 2
	/* map the pdes */
	for (va = 0; npdes--; va += PDE_SIZE, addr += PAGE_SIZE) {
d538 5
a542 9
		/* last pde is for the start of kernel virtual */
		if (!npdes)
			va = SYSCALLGATE;
		/* now map the pde for the physmem */
		bzero((void *)addr, PAGE_SIZE);
		DPRINTF(PDB_INIT|PDB_VP, ("pde premap 0x%x 0x%x\n", va, addr));
		pmap_pde_set(kpm, va, addr);
		kpm->pm_stats.resident_count++; /* count PTP as resident */
	}
d544 4
a547 12
	/* TODO optimize/inline the kenter */
	for (va = 0; va < ptoa(totalphysmem); va += PAGE_SIZE) {
		extern struct user *proc0paddr;
		vm_prot_t prot = UVM_PROT_RW;

		if (va < (vaddr_t)&etext1)
			prot = UVM_PROT_RX;
		else if (va == (vaddr_t)proc0paddr + USPACE)
			prot = UVM_PROT_NONE;

		pmap_kenter_pa(va, va, prot);
	}
d549 2
a550 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext1, endaddr));
a557 2
	simple_lock_init(&pvalloc_lock);

d561 3
a563 1
	    &pool_allocator_nointr);
d571 2
d589 1
a589 1
	*startp = SYSCALLGATE + PAGE_SIZE;
d607 2
d610 5
a614 8
	for (space = 1 + (arc4random() % hppa_sid_max);
	    pmap_sdir_get(space); space = (space + 1) % hppa_sid_max);

	if ((pmap->pm_pdir_pg = pmap_pagealloc(NULL, 0)) == NULL)
		panic("pmap_create: no pages");
	pmap->pm_ptphint = NULL;
	pmap->pm_pdir = (u_int32_t *)VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
	pmap_sdir_set(space, pmap->pm_pdir);
d618 5
d624 1
a624 2
	pmap->pm_stats.resident_count = 1;
	pmap->pm_stats.wired_count = 0;
d626 1
a626 1
	return (pmap);
d645 1
a646 3
	while ((pg = TAILQ_FIRST(&pmap->pm_obj.memq))) {
		printf("pmap_destroy: unaccounted ptp 0x%x\n",
		    VM_PAGE_TO_PHYS(pg));
d649 1
d653 3
a655 1
#endif
a656 2
	uvm_pagefree(pmap->pm_pdir_pg);
	pmap->pm_pdir_pg = NULL;
d690 2
a691 1
	struct vm_page *pg, *ptp = NULL;
d693 1
d719 1
a719 1
			ficache(pmap->pm_space, va, PAGE_SIZE);
d721 1
a721 1
		fdcache(pmap->pm_space, va, PAGE_SIZE);
d735 9
a743 4
		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
		simple_lock(&pg->mdpage.pvh_lock);
		pve = pmap_pv_remove(pg, pmap, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d747 1
a747 1
		pte = PTE_PROT(0);
d756 2
a757 2

	if (pmap_initialized) {
d765 7
a771 4
		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa));
		pmap_pv_enter(pg, pve, pmap, va, ptp);
	} else if (pve)
		pmap_pv_free(pve);
d775 1
a775 1
	pte = pa | PTE_PROT(pmap_prot(pmap, prot)) |
d794 1
d797 1
a797 1
	int batch;
d805 1
a805 1
	for (batch = 0, pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
a811 1
			/* XXX not until ptp acct works
a812 1
			*/
d835 6
a840 7
			if (pmap_initialized) {
				struct vm_page *pg;

				pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
				simple_lock(&pg->mdpage.pvh_lock);
				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
				if ((pve = pmap_pv_remove(pg, pmap, sva)))
d842 1
a842 1
				simple_unlock(&pg->mdpage.pvh_lock);
a858 1
	struct vm_page *pg;
d861 2
d892 9
a900 4
			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pg->mdpage.pvh_lock);
d902 1
a902 2
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
d922 1
d925 1
d929 3
a931 1
	if (pg->mdpage.pvh_list == NULL)
d933 1
d935 7
a941 2
	simple_lock(&pg->mdpage.pvh_lock);
	for (pve = pg->mdpage.pvh_list; pve; ) {
d954 1
a954 1
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d959 1
a959 2
	pg->mdpage.pvh_list = NULL;
	simple_unlock(&pg->mdpage.pvh_lock);
d997 1
d1000 1
d1005 11
a1015 3
	simple_lock(&pg->mdpage.pvh_lock);
	res = pg->mdpage.pvh_attrs = 0;
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
d1019 5
a1023 10
#ifdef PMAPDEBUG
			if (!pte) {
				printf("pmap_changebit: zero pte for 0x%x\n",
				    pve->pv_va);
				continue;
			}
#endif
			if (pte & PTE_PROT(TLB_EXECUTE)) {
				ficache(pve->pv_pmap->pm_space,
				    pve->pv_va, PAGE_SIZE);
a1024 2
			}

a1028 5
			res |= pmap_pvh_attrs(pte);
			pte &= ~clear;
			pte |= set;
			pg->mdpage.pvh_attrs = pmap_pvh_attrs(pte);

d1033 2
a1034 1
	simple_unlock(&pg->mdpage.pvh_lock);
d1042 1
d1045 1
d1049 9
a1057 2
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bits) && pve;
d1062 1
a1062 1
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d1064 1
a1064 1
	simple_unlock(&pg->mdpage.pvh_lock);
d1066 1
a1066 1
	return ((pg->mdpage.pvh_attrs & bits) != 0);
d1092 3
a1094 2
static __inline void
pmap_flush_page(struct vm_page *pg, int purge)
d1096 3
a1098 1
	struct pv_entry *pve;
d1100 1
a1100 9
	/* purge cache for all possible mappings for the pa */
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		if (purge)
			pdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		else
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
	simple_unlock(&pg->mdpage.pvh_lock);
}
d1102 8
a1109 4
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1111 16
a1126 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%x)\n", pa));
d1128 2
a1129 3
	pmap_flush_page(pg, 1);
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
d1133 8
a1140 4
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t spa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dpa = VM_PAGE_TO_PHYS(dstpg);
d1143 29
a1171 5
	pmap_flush_page(srcpg, 0);
	pmap_flush_page(dstpg, 1);
	bcopy((void *)spa, (void *)dpa, PAGE_SIZE);
	pdcache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
d1189 1
a1189 1
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%x", va);
d1196 2
a1197 2
	pte = pa | PTE_PROT(TLB_WIRED|pmap_prot(pmap_kernel(), prot));
	if (pa >= HPPA_IOSPACE)
a1202 13
#ifdef PMAPDEBUG
	{
		if (pmap_initialized) {
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
				Debugger();
			simple_unlock(&pg->mdpage.pvh_lock);
		}
	}
#endif
a1210 3
#ifdef PMAPDEBUG
	extern u_int totalphysmem;
#endif
d1212 1
d1215 1
a1218 6
#ifdef PMAPDEBUG
	if (va < ptoa(totalphysmem)) {
		printf("pmap_kremove(%x, %x): unmapping physmem\n", va, size);
		return;
	}
#endif
d1238 1
a1238 1
			ficache(HPPA_SID_KERNEL, va, PAGE_SIZE);
d1240 1
a1240 1
		fdcache(HPPA_SID_KERNEL, va, PAGE_SIZE);
d1244 5
a1248 6
		if (pmap_initialized) {
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d1250 1
a1250 1
			if ((pve = pmap_pv_remove(pg, pmap_kernel(), va)))
d1252 1
a1252 1
			simple_unlock(&pg->mdpage.pvh_lock);
d1259 29
@


1.51.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d4 1
a4 1
 * Copyright (c) 1998-2003 Michael Shalayeff
d78 2
a79 2
/*	| PDB_INIT */
/*	| PDB_FOLLOW */
d100 1
a103 1
int 		pmap_initialized;
d116 2
a117 1
	struct vm_page *pg;
d119 5
a123 3
	if ((pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO)) == NULL)
		printf("pmap_pagealloc fail\n");
d240 1
d245 1
d267 10
a283 11
void
pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte)
{
	if (pte & PTE_PROT(TLB_EXECUTE)) {
		ficache(pmap->pm_space, va, PAGE_SIZE);
		pitlb(pmap->pm_space, va);
	}
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	pdtlb(pmap->pm_space, va);
}

d302 1
d304 1
a304 2
		vaddr_t va, pdemask;
		u_int32_t *pd;
d310 1
a310 1
		for (pdemask = 1, va = sva? sva: 0; va < VM_MAX_KERNEL_ADDRESS;
a408 1
#ifdef PMAPDEBUG
a410 1
#endif
d434 1
a434 1
	extern int etext, __rodata_end, __data_start;
d437 1
a437 1
	vaddr_t va, addr = hppa_round_page(vstart), t;
d449 8
a456 8
	hppa_prot[UVM_PROT_NONE]  = TLB_AR_NA;
	hppa_prot[UVM_PROT_READ]  = TLB_AR_R;
	hppa_prot[UVM_PROT_WRITE] = TLB_AR_RW;
	hppa_prot[UVM_PROT_RW]    = TLB_AR_RW;
	hppa_prot[UVM_PROT_EXEC]  = TLB_AR_RX;
	hppa_prot[UVM_PROT_RX]    = TLB_AR_RX;
	hppa_prot[UVM_PROT_WX]    = TLB_AR_RWX;
	hppa_prot[UVM_PROT_RWX]   = TLB_AR_RWX;
d519 1
a519 1
		addr += size;	/* should keep the alignment right */
d524 1
a524 2
	/*	and can block-map the whole phys w/ another */
	t = (vaddr_t)&etext;
d529 2
a530 1
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count = atop(t);
d532 3
a534 3
	if (&__rodata_end < &__data_start) {
		physical_steal = (vaddr_t)&__rodata_end;
		physical_end = (vaddr_t)&__data_start;
d544 4
a547 2
	/* four more for the the kernel virtual */
	npdes = 4 + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
d549 1
a549 1
	    atop(addr) + npdes, totalphysmem, VM_FREELIST_DEFAULT);
d554 2
a555 2
		/* last four pdes are for the kernel virtual */
		if (npdes == 3)
a563 2
	physmem = atop(addr);

d569 1
a569 1
		if (va < (vaddr_t)&etext)
d577 1
a577 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext, va));
d592 1
a592 1
	pmap_initialized = 1;
a654 1
#ifdef DIAGNOSTIC
a655 1
#endif
d739 7
a745 2
		pmap_pte_flush(pmap, va, pte);
		if (wired && !(pte & PTE_PROT(TLB_WIRED)))
d747 1
a747 1
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)))
d763 1
a763 1
		pte = PTE_PROT(TLB_REFTRAP);
d773 1
a773 1
	if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa)))) {
d781 1
a808 2
	struct vm_page *pg;
	vaddr_t pdemask;
d810 1
d813 1
a813 1
	    ("pmap_remove(%p, 0x%x, 0x%x)\n", pmap, sva, eva));
d817 1
a817 1
	for (batch = 0, pdemask = 1; sva < eva; sva += PAGE_SIZE) {
d824 3
a826 1
			batch = pdemask == sva && sva + ~PDE_MASK + 1 <= eva;
d831 4
d839 5
a843 4
			pmap_pte_flush(pmap, sva, pte);
			if (pte & PTE_PROT(TLB_WIRED))
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;
d849 2
a850 2
			if (pmap_initialized &&
			    (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d852 1
d886 1
a886 1
	for(pdemask = 1; sva < eva; sva += PAGE_SIZE) {
d911 8
a918 1
			pmap_pte_flush(pmap, sva, pte);
d933 1
d941 2
a942 5
	for (pve = pg->mdpage.pvh_list; pve;
	     pve = (ppve = pve)->pv_next, pmap_pv_free(ppve)) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		pt_entry_t *pde, pte;
d944 3
a946 1
		simple_lock(&pmap->pm_obj.vmobjlock);
d948 3
a950 3
		pde = pmap_pde_get(pmap->pm_pdir, va);
		pte = pmap_pte_get(pde, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d952 1
a952 4
		pmap_pte_flush(pmap, va, pte);
		if (pte & PTE_PROT(TLB_WIRED))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;
d954 4
a957 2
		pmap_pte_set(pde, va, 0);
		simple_unlock(&pmap->pm_obj.vmobjlock);
d999 1
a999 1
	pt_entry_t res;
d1007 3
a1009 7
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		pt_entry_t *pde, opte, pte;

		simple_lock(&pmap->pm_obj.vmobjlock);
		if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
			opte = pte = pmap_pte_get(pde, va);
d1013 1
a1013 1
				    va);
d1017 11
d1030 1
a1030 2
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			res |= pmap_pvh_attrs(opte);
d1032 1
a1032 4
			if (opte != pte) {
				pmap_pte_flush(pmap, va, opte);
				pmap_pte_set(pde, va, pte);
			}
d1034 1
a1034 1
		simple_unlock(&pmap->pm_obj.vmobjlock);
d1038 1
a1038 1
	return ((res & (clear | set)) != 0);
d1042 1
a1042 1
pmap_testbit(struct vm_page *pg, u_int bit)
d1047 1
a1047 1
	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bit));
d1050 1
a1050 1
	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bit) && pve;
d1059 1
a1059 1
	return ((pg->mdpage.pvh_attrs & bit) != 0);
a1084 16
void
pmap_activate(struct proc *p)
{
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	struct pcb *pcb = &p->p_addr->u_pcb;

	pcb->pcb_space = pmap->pm_space;
	pcb->pcb_uva = (vaddr_t)p->p_addr;
}

void
pmap_deactivate(struct proc *p)
{

}

d1148 1
a1148 2
	pte = pa | PTE_PROT(TLB_WIRED | TLB_REFTRAP |
	    pmap_prot(pmap_kernel(), prot));
d1157 2
a1158 3
		struct vm_page *pg;

		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d1160 1
d1180 1
a1180 1
	vaddr_t eva, pdemask;
a1181 1
	struct vm_page *pg;
d1194 1
a1194 1
	for (pdemask = 1, eva = va + size; va < eva; va += PAGE_SIZE) {
d1209 6
a1214 1
		pmap_pte_flush(pmap_kernel(), va, pte);
d1216 2
a1217 1
		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d1219 1
@


1.50
log
@make pmap_virtual_space madatory in all pmaps.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2001/11/28 14:13:06 art Exp $	*/
d1145 1
a1145 1
	vm_page_t pg;
d1428 1
a1428 1
	vm_page_t pg;
d1461 1
a1461 1
	vm_page_t pg;
d1490 1
a1490 1
	vm_page_t pg;
d1523 1
a1523 1
	vm_page_t pg;
@


1.49
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2001/11/28 13:47:38 art Exp $	*/
d764 7
@


1.48
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.47 2001/11/06 20:57:21 mickey Exp $	*/
a1613 13
}

void
pmap_kenter_pgs(va, pgs, npgs)
	vaddr_t va;
	vm_page_t *pgs;
	int npgs;
{
	int i;

	va = hppa_trunc_page(va);
	for (i = 0; i < npgs; i++)
		pmap_kenter_pa(va + i*NBPG, VM_PAGE_TO_PHYS(pgs[i]), VM_PROT_ALL);
@


1.47
log
@pmap_pinit needs proto now, also disable cache for mapped pages
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/07/25 13:25:31 art Exp $	*/
d1079 1
a1079 1
	return (KERN_SUCCESS);
@


1.46
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2001/06/08 08:08:48 art Exp $	*/
d212 1
d1013 1
a1013 1
	tlbprot = TLB_REF | pmap_prot(pmap, prot) | pmap->pmap_pid;
d1603 1
a1603 1
		pv->pv_tlbprot = TLB_WIRED | TLB_REF | TLB_DIRTY |
@


1.45
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44 2001/05/09 15:31:24 art Exp $	*/
d977 1
a977 1
 * pmap_enter(pmap, va, pa, prot, wired, access_type)
d983 2
a984 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d988 2
a989 2
	vm_prot_t prot, access_type;
	boolean_t wired;
d995 1
d1014 1
a1014 1
	if (access_type & VM_PROT_WRITE)
d1077 2
@


1.44
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2001/05/05 21:26:36 art Exp $	*/
d1302 5
a1306 4
 * pmap_extract(pmap, va)
 *	returns the physical address corrsponding to the
 *	virtual address specified by pmap and va if the
 *	virtual address is mapped and 0 if it is not.
d1308 2
a1309 2
paddr_t
pmap_extract(pmap, va)
d1312 1
d1314 1
a1314 1
	register struct pv_entry *pv;
d1323 5
a1327 3
		return(0);
	else
		return tlbptob(pv->pv_tlbpage) + (va & PGOFSET);
@


1.43
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2001/04/29 20:58:55 mickey Exp $	*/
d1264 1
a1264 1
 *	Routine:	pmap_change_wiring
d1274 2
a1275 2
pmap_change_wiring(pmap, va, wired)
	register pmap_t	pmap;
a1276 1
	boolean_t	wired;
d1278 1
a1278 2
	register struct pv_entry *pv;
	boolean_t waswired;
d1283 1
a1283 2
		printf("pmap_change_wiring(%p, %x, %swire)\n",
		    pmap, va, wired? "": "un");
d1292 1
a1292 1
		panic("pmap_change_wiring: can't find mapping entry");
d1294 1
a1294 5
	waswired = pv->pv_tlbprot & TLB_WIRED;
	if (wired && !waswired) {
		pv->pv_tlbprot |= TLB_WIRED;
		pmap->pmap_stats.wired_count++;
	} else if (!wired && waswired) {
@


1.42
log
@pseg not yet
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2001/04/29 20:57:25 mickey Exp $	*/
d412 1
a412 1
	pvp = (struct pv_page *) trunc_page(pv);
@


1.41
log
@fix pmap_enter to compile
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/03/29 00:12:54 mickey Exp $	*/
d759 1
a759 1
	mtctl(addr, CR_PSEG);
@


1.40
log
@various fixes, new _pmap_enter for pmap new, space police
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2001/02/23 22:54:32 mickey Exp $	*/
d983 2
a984 2
int
_pmap_enter(pmap, va, pa, prot, flags)
d988 2
a989 2
	vm_prot_t prot;
	int flags;
a993 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1005 1
a1005 3
	if (!(pv = pmap_find_pv(pa))) {
		if (flags & PMAP_CANFAIL)
			return (KERN_RESOURCE_SHORTAGE);
a1006 1
	}
d1013 3
a1075 1
	return (KERN_SUCCESS);
@


1.39
log
@fix damn pcxl damn function
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2001/01/30 21:44:16 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2000 Michael Shalayeff
d111 1
a111 1
 *	
d154 3
d183 1
d240 1
a240 1
pmap_enter_va(pa_space_t space, vaddr_t va, struct pv_entry *pv)
d242 1
a242 1
	struct hpt_entry *hpt = pmap_hash(space, va);
d244 1
a244 1
	struct pv_entry *pvp =	hpt->hpt_entry;
d246 1
d250 1
a250 1
		    space, va, pv, hpt, pvp);
d252 1
d254 1
a254 1
	while(pvp && (pvp->pv_va != va || pvp->pv_space != space))
d263 1
d273 1
a273 1
	struct pv_entry *pvp =	pmap_hash(space, va)->hpt_entry;
a314 2
	pmap_clear_va(pv->pv_space, pv->pv_va);

d391 1
d439 1
a439 1
{	
d492 1
a492 1
	pmap_enter_va(pv->pv_space, va, pv);
d500 1
a500 1
 * pmap_remove_pv(pmap, va, pv)
d505 1
a505 1
pmap_remove_pv(pmap_t pmap, vaddr_t va, struct pv_entry *pv)
a506 1
	struct pv_entry *npv;
d511 1
a511 1
		printf("pmap_remove_pv(%p,%x,%p)\n", pmap, va, pv);
d523 2
a524 2
	ficache(pv->pv_space, pv->pv_va, PAGE_SIZE);
	pitlb(pv->pv_space, pv->pv_va);
d526 2
a527 2
	fdcache(pv->pv_space, pv->pv_va, PAGE_SIZE);
	pdtlb(pv->pv_space, pv->pv_va);
d535 2
a536 2
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		npv = pv->pv_next;
d538 5
a542 3
		if (npv) {
			*pv = *npv;
			pmap_free_pv(npv);
d546 7
a552 8
		for (npv = pv->pv_next; npv; pv = npv, npv = npv->pv_next) {
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
		}
		if (npv) {
			pv->pv_next = npv->pv_next;
			pmap_remove_va(pv);
			pmap_free_pv(npv);
d554 2
a555 3
#ifdef PMAPDEBUG
			if (pmapdebug & PDB_PV)
				printf("pmap_remove_pv: npv == NULL\n");
d746 1
a746 2
	size = hppa_round_page(sizeof(struct pv_entry) *
	    (totalphysmem - atop(virtual_avail)));
d755 1
a755 1
	uvm_page_physload(atop(virtual_avail), totalphysmem + i,
d759 1
a759 1
	addr += size;
d845 1
a845 1
	 *     if we have one at SYSCALLGATE address (;
d848 2
a849 1
	    tlbbtop((paddr_t)&gateway_page), pmap_alloc_pv());
d861 1
d874 1
a874 1
		simple_lock(&sid_pid_lock);
d880 1
a880 1
		simple_unlock(&sid_pid_lock);
d888 1
a888 1
	simple_lock(&pmap->pmap_lock);
d894 1
a894 1
	simple_unlock(&pmap->pmap_lock);
d907 1
d917 1
a917 1
	simple_lock(&pmap_freelock);
d922 1
a922 1
		simple_unlock(&pmap_freelock);
d924 1
d947 1
d957 1
a957 1
	simple_lock(&pmap->pmap_lock);
a965 2
		simple_unlock(&pmap->pmap_lock);

a970 1
		simple_lock(&pmap_freelock);
a972 1
		simple_unlock(&pmap_freelock);
d974 1
a975 1

d983 2
a984 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d989 1
a989 2
	boolean_t wired;
	vm_prot_t access_type;
d992 1
a992 1
	u_int tlbpage = tlbbtop(pa), tlbprot;
d994 1
d998 1
d1006 5
a1010 4
	simple_lock(&pmap->pmap_lock);

	if (!pmap || !(pv = pmap_find_pv(pa)))
		return;
d1012 1
d1014 1
a1014 2
	pmap_clear_va(space, va);

a1016 4
	/* this saves on extra dirty fault? */
	if (access_type & VM_PROT_WRITE)
		tlbprot |= TLB_DIRTY;

d1039 1
a1039 1
			pmap_remove_pv(pmap, va, ppv);
d1077 1
d1095 1
d1104 1
a1104 1
	simple_lock(&pmap->pmap_lock);
d1106 1
d1120 2
a1121 1
			pmap_remove_pv(pmap, sva, pv);
d1126 1
a1126 1
	simple_unlock(&pmap->pmap_lock);
d1146 1
a1146 1
	if (pmapdebug & PDB_FOLLOW)
d1188 7
d1196 1
a1196 2
			simple_lock(&pmap->pmap_lock);
			pmap_remove_pv(pmap, pv->pv_va, pv);
a1197 1
			simple_unlock(&pmap->pmap_lock);
d1237 1
a1237 2
	simple_lock(&pmap->pmap_lock);

d1249 1
a1249 1
			
d1252 1
a1252 1
			
d1254 1
a1254 1
			
a1262 1
	simple_unlock(&pmap->pmap_lock);
d1284 1
d1323 1
d1439 5
a1443 4
	for (; pv; pv = pv->pv_next) {
		pitlb(pv->pv_space, pv->pv_va);
		pdtlb(pv->pv_space, pv->pv_va);
		pv->pv_tlbprot &= ~(TLB_DIRTY);
a1444 3
		pmap_clear_va(pv->pv_space, pv->pv_va);
	}
	splx(s);
d1459 1
d1467 2
a1468 7
	if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap) {
		splx(s);
		return FALSE;
	}

	for (; pv && !(pv->pv_tlbprot & TLB_DIRTY);)
		pv = pv->pv_next;
d1471 1
a1471 1
	return pv != NULL;
d1501 5
a1505 4
	for (; pv; pv = pv->pv_next) {
		pitlb(pv->pv_space, pv->pv_va);
		pdtlb(pv->pv_space, pv->pv_va);
		pv->pv_tlbprot &= ~(TLB_REF);
a1506 3
		pmap_clear_va(pv->pv_space, pv->pv_va);
	}
	splx(s);
d1521 1
d1530 2
a1531 7
	if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap) {
		splx(s);
		return FALSE;
	}

	for (; pv && !(pv->pv_tlbprot & TLB_REF);)
		pv = pv->pv_next;
d1534 1
a1534 1
	return pv != NULL;
d1537 1
a1537 1
#if 0
d1539 2
a1540 2
pmap_changebit(pg, set, reset)
	vm_page_t pg;
a1543 1
	register paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1548 1
a1548 1
		printf("pmap_changebit(%p[%x], %x, %x)\n", pg, pa, set, reset);
d1552 1
a1552 1
	if (!(pv = pmap_find_pv(pa)) || !pv->pv_pmap) {
d1557 2
a1558 6
	while (pv) {
		pv->pv_tlbprot |= set;
		pv->pv_tlbprot &= ~reset;
		pv = pv->pv_next;
	}
	pmap_clear_pv(pa, NULL);
d1560 8
d1578 1
a1582 1
	simple_lock(&pmap_kernel()->pmap_lock);
d1584 3
a1586 1
	pv = pmap_find_pv(pa);
d1595 1
a1595 1
			pmap_remove_pv(pmap_kernel(), va, pv);
d1602 1
a1602 1
		pv->pv_space = pmap_kernel()->pmap_space;
d1607 1
a1607 1
		pmap_enter_va(pv->pv_space, va, pv);
d1610 1
a1610 2
	simple_unlock(&pmap_kernel()->pmap_lock);

d1625 1
d1655 5
a1659 4
			db_printf("hpt@@%p: %x{%sv=%x:%x}, prot=%b, pa=%x\n",
				  hpt, *(int *)hpt, (hpt->hpt_valid?"ok,":""),
				  hpt->hpt_space, hpt->hpt_vpn << 9,
				  hpt->hpt_tlbprot, TLB_BITS, hpt->hpt_tlbpage);
d1662 3
a1664 3
					  pv->pv_pmap, pv->pv_space, pv->pv_va,
					  pv->pv_tlbprot, TLB_BITS,
					  pv->pv_tlbpage, pv->pv_hash);
@


1.38
log
@forgot to remove changebit; from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2000/11/24 20:49:24 mickey Exp $	*/
d220 1
a220 1
		"zdep	%1, 26, 16, %%r23\n\t"	/* r23 = (sp << 5) */
@


1.37
log
@raise dirty in kenter_pa too
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2000/11/08 16:44:53 mickey Exp $	*/
d1537 1
d1566 1
@


1.36
log
@betterer pmap_kenter_pa() mostly from Havard Eidnes <he@@runit.sintef.no>
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2000/08/15 20:22:10 mickey Exp $	*/
d1597 1
a1597 1
		pv->pv_tlbprot = TLB_WIRED | TLB_REF |
@


1.35
log
@note about pcxs and kernel base here
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2000/08/15 20:04:36 mickey Exp $	*/
d573 1
a573 1
		panic("pmap_find_pv: mapping unmappable");
d1000 3
a1002 1
	if (!pmap)
a1004 2
	simple_lock(&pmap->pmap_lock);

a1006 1
	pv = pmap_find_pv(pa);
d1010 1
a1010 1
	/* this saves on extra dirty fault */
d1111 3
a1114 1
			pmap->pmap_stats.resident_count--;
d1572 1
d1577 1
d1579 12
a1590 1
	simple_lock(&pmap_kernel()->pmap_lock);
d1592 10
a1601 4
	pmap_enter_pv(pmap_kernel(), va, pmap_prot(pmap_kernel(), prot),
	    tlbbtop(pa), pmap_find_pv(pa));
	pmap_kernel()->pmap_stats.resident_count++;
	pmap_kernel()->pmap_stats.wired_count++;
@


1.34
log
@equiv_end no longer used w/ new kvtop
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2000/07/02 02:41:57 mickey Exp $	*/
d729 1
@


1.33
log
@new kvtop through lpa; also put some spu/sfu bits in cpu.h
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2000/01/17 06:51:58 mickey Exp $	*/
a183 2

long equiv_end = 0;
@


1.32
log
@spaces everywhere
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2000/01/17 04:49:02 mickey Exp $	*/
a1609 12
}

int
kvtop(va)
	caddr_t va;
{
	if ((vaddr_t)va < virtual_avail)
		return (int)va;
	else if ((vaddr_t)va >= HPPA_IOBEGIN)
		return (int)va;
	else
		return (int)pmap_extract(pmap_kernel(), (vaddr_t)va);
@


1.32.2.1
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/03/29 00:12:54 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998-2001 Michael Shalayeff
d111 1
a111 1
 *
a153 3
#define static	/**/
#define	__inline /* */

a179 1
/*	| PDB_PROTECT */
d185 2
d222 1
a222 1
		"zdep	%1, 22, 16, %%r23\n\t"	/* r23 = (sp << 9) */
d238 1
a238 1
pmap_enter_va(struct pv_entry *pv)
d240 1
a240 1
	struct hpt_entry *hpt = pmap_hash(pv->pv_space, pv->pv_va);
d242 1
a242 1
	struct pv_entry *pvp = hpt->hpt_entry;
a243 1
	int s;
d247 1
a247 1
		    pv->pv_space, pv->pv_va, pv, hpt, pvp);
a248 1
	s = splimp();
d250 1
a250 1
	while(pvp && (pvp->pv_va != pv->pv_va || pvp->pv_space != pv->pv_space))
a258 1
	splx(s);
d268 1
a268 1
	struct pv_entry *pvp = pmap_hash(space, va)->hpt_entry;
d310 2
a387 1
	pv->pv_next = NULL;
d435 1
a435 1
{
d488 1
a488 1
	pmap_enter_va(pv);
d496 1
a496 1
 * pmap_remove_pv(ppv, pv)
d501 1
a501 1
pmap_remove_pv(struct pv_entry *ppv, struct pv_entry *pv)
d503 1
d508 1
a508 1
		printf("pmap_remove_pv(%p,%p)\n", ppv, pv);
d520 2
a521 2
	ficache(ppv->pv_space, ppv->pv_va, PAGE_SIZE);
	pitlb(ppv->pv_space, ppv->pv_va);
d523 2
a524 2
	fdcache(ppv->pv_space, ppv->pv_va, PAGE_SIZE);
	pdtlb(ppv->pv_space, ppv->pv_va);
d532 2
a533 2
	if (ppv == pv) {
		ppv = pv->pv_next;
d535 3
a537 5
		if (ppv) {
			/*npv->pv_tlbprot |= pv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);*/
			*pv = *ppv;
			pmap_free_pv(ppv);
d541 8
a548 7
		for (; pv && pv->pv_next != ppv; pv = pv->pv_next)
			;

		if (pv) {
			pv->pv_next = ppv->pv_next;
			pmap_remove_va(ppv);
			pmap_free_pv(ppv);
d550 3
a552 2
#ifdef DEBUG
			panic("pmap_remove_pv: npv == NULL\n");
d575 1
a575 1
		return NULL;
a730 1
	/* XXX PCXS needs two separate inserts in separate btlbs */
d742 2
a743 1
	size = hppa_round_page(sizeof(struct pv_entry) * totalphysmem);
d752 1
a752 1
	uvm_page_physload(0, totalphysmem + i,
d756 1
a756 1
	mtctl(addr, CR_PSEG);
d842 1
a842 1
	 *     if we have any at SYSCALLGATE address (;
d845 1
a845 2
	    tlbbtop((paddr_t)&gateway_page),
	    pmap_find_pv((paddr_t)&gateway_page));
a856 1
	int s;
d869 1
a869 1
		s = splimp();
d875 1
a875 1
		splx(s);
d883 1
a883 1
	s = splimp();
d889 1
a889 1
	splx(s);
a901 1
	int s;
d911 1
a911 1
	s = splimp();
d916 1
a916 1
		splx(s);
a917 1
		splx(s);
a939 1
	int s;
d949 1
a949 1
	s = splimp();
d958 2
d965 1
d968 1
a969 1
	splx(s);
d971 1
d979 2
a980 2
int
_pmap_enter(pmap, va, pa, prot, flags)
d985 2
a986 1
	int flags;
d989 1
a989 1
	u_int tlbpage, tlbprot;
a990 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
a993 1
	pa = hppa_trunc_page(pa);
d1001 4
a1004 5
	if (!(pv = pmap_find_pv(pa))) {
		if (flags & PMAP_CANFAIL)
			return (KERN_RESOURCE_SHORTAGE);
		panic("pmap_enter: pmap_find_pv failed");
	}
a1005 1
	va = hppa_trunc_page(va);
d1007 3
a1009 1
	tlbpage = tlbbtop(pa);
d1012 4
d1038 1
a1038 1
			pmap_remove_pv(ppv, pmap_find_pv(tlbptob(ppv->pv_tlbpage)));
a1075 1
	return (KERN_SUCCESS);
a1092 1
	int s;
d1101 1
a1101 1
	s = splimp();
a1102 1
	sva = hppa_trunc_page(sva);
d1113 1
a1114 4
			if (pv->pv_tlbprot & TLB_WIRED)
				pmap->pmap_stats.wired_count--;
			pmap_remove_pv(pv,
			    pmap_find_pv(tlbptob(pv->pv_tlbpage)));
d1119 1
a1119 1
	splx(s);
d1139 1
a1139 1
	if (pmapdebug & (PDB_FOLLOW & PDB_PROTECT))
a1180 7
#ifdef PMAPDEBUG
			if (pmapdebug & PDB_PROTECT)
				printf("pv={%p,%x:%x,%b,%x}->%p\n",
				    pv->pv_pmap, pv->pv_space, pv->pv_va,
				    pv->pv_tlbprot, TLB_BITS,
				    tlbptob(pv->pv_tlbpage), pv->pv_hash);
#endif
d1182 2
a1183 1
			pmap_remove_pv(pv, pv);
d1185 1
d1225 2
a1226 1
	sva = hppa_trunc_page(sva);
d1238 1
a1238 1

d1241 1
a1241 1

d1243 1
a1243 1

d1252 1
a1273 1
	va = hppa_trunc_page(va);
a1311 1
	va = hppa_trunc_page(va);
d1427 7
a1433 4
	pitlb(pv->pv_space, pv->pv_va);
	pdtlb(pv->pv_space, pv->pv_va);
	pv->pv_tlbprot &= ~(TLB_DIRTY);
	pmap_clear_va(pv->pv_space, pv->pv_va);
a1434 1

a1448 1
	boolean_t ret;
d1456 7
a1462 2
	ret = (pv = pmap_find_pv(pa)) && pv->pv_pmap &&
	    pv->pv_tlbprot & TLB_DIRTY;
d1465 1
a1465 1
	return ret;
d1495 7
a1501 4
	pitlb(pv->pv_space, pv->pv_va);
	pdtlb(pv->pv_space, pv->pv_va);
	pv->pv_tlbprot &= ~(TLB_REF);
	pmap_clear_va(pv->pv_space, pv->pv_va);
a1502 1

a1516 1
	boolean_t ret;
d1525 7
a1531 2
	ret = (pv = pmap_find_pv(pa)) && pv->pv_pmap &&
	    pv->pv_tlbprot & TLB_REF;
d1534 1
a1534 1
	return ret;
a1536 1
#ifdef notused
d1538 2
a1539 2
pmap_changebit(va, set, reset)
	vaddr_t va;
d1543 1
d1548 1
a1548 1
		printf("pmap_changebit(%x, %x, %x)\n", va, set, reset);
d1552 1
a1552 1
	if (!(pv = pmap_find_va(HPPA_SID_KERNEL, va))) {
d1557 6
a1562 2
	pv->pv_tlbprot |= set;
	pv->pv_tlbprot &= ~reset;
a1563 8

	ficache(HPPA_SID_KERNEL, va, NBPG);
	pitlb(HPPA_SID_KERNEL, va);

	fdcache(HPPA_SID_KERNEL, va, NBPG);
	pdtlb(HPPA_SID_KERNEL, va);

	pmap_clear_va(HPPA_SID_KERNEL, va);
a1564 1
#endif
a1571 2
	register struct pv_entry *pv;
	int s;
d1577 6
a1582 14
	s = splimp();
	va = hppa_trunc_page(va);
	pv = pmap_find_va(HPPA_SID_KERNEL, va);
	if (pv && (pa & HPPA_IOSPACE) == HPPA_IOSPACE)
		/* if already mapped i/o space, nothing to do */
		;
	else {
		if (!pv || !(pv->pv_tlbprot & TLB_WIRED))
			pmap_kernel()->pmap_stats.wired_count++;

		if (pv)
			pmap_remove_pv(pv, pmap_find_pv(pa));
		else
			pmap_kernel()->pmap_stats.resident_count++;
d1584 1
a1584 10
		pv = pmap_alloc_pv();
		pv->pv_va = va;
		pv->pv_pmap = pmap_kernel();
		pv->pv_space = HPPA_SID_KERNEL;
		pv->pv_tlbpage = tlbbtop(pa);
		pv->pv_tlbprot = TLB_WIRED | TLB_REF | TLB_DIRTY |
		    pmap_prot(pmap_kernel(), prot) | HPPA_PID_KERNEL |
		    ((pa & HPPA_IOSPACE) == HPPA_IOSPACE? TLB_UNCACHEABLE : 0);
		pmap_enter_va(pv);
	}
a1585 1
	splx(s);
a1599 1
	va = hppa_trunc_page(va);
d1612 12
d1641 4
a1644 5
			db_printf("hpt@@%p: %x{%sv=%x:%x},%b,%x\n",
			    hpt, *(int *)hpt, (hpt->hpt_valid?"ok,":""),
			    hpt->hpt_space, hpt->hpt_vpn << 9,
			    hpt->hpt_tlbprot, TLB_BITS,
			    tlbptob(hpt->hpt_tlbpage));
d1647 3
a1649 3
				    pv->pv_pmap, pv->pv_space, pv->pv_va,
				    pv->pv_tlbprot, TLB_BITS,
				    tlbptob(pv->pv_tlbpage), pv->pv_hash);
@


1.32.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32.2.1 2001/04/18 16:06:15 niklas Exp $	*/
d412 1
a412 1
	pvp = (struct pv_page *) trunc_page((vaddr_t)pv);
d759 1
a759 1
	/* mtctl(addr, CR_PSEG); */
d983 2
a984 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d988 2
a989 2
	vm_prot_t prot, access_type;
	boolean_t wired;
d994 1
d1006 3
a1008 1
	if (!(pv = pmap_find_pv(pa)))
d1010 1
a1016 3
	if (access_type & VM_PROT_WRITE)
		tlbprot |= TLB_DIRTY;

d1077 1
d1266 1
a1266 1
 *	Routine:	pmap_unwire
d1276 2
a1277 2
pmap_unwire(pmap, va)
	pmap_t	pmap;
d1279 1
d1281 2
a1282 1
	struct pv_entry *pv;
d1287 2
a1288 1
		printf("pmap_unwire(%p, %x)\n", pmap, va);
d1297 1
a1297 1
		panic("pmap_unwire: can't find mapping entry");
d1299 5
a1303 1
	if (pv->pv_tlbprot & TLB_WIRED) {
d1311 4
a1314 5
 * pmap_extract(pmap, va, pap)
 *	fills in the physical address corrsponding to the
 *	virtual address specified by pmap and va into the
 *	storage pointed to by pap and returns TRUE if the
 *	virtual address is mapped. returns FALSE in not mapped.
d1316 2
a1317 2
boolean_t
pmap_extract(pmap, va, pap)
a1319 1
	paddr_t *pap;
d1321 1
a1321 1
	struct pv_entry *pv;
d1330 3
a1332 5
		return (FALSE);
	else {
		*pap = tlbptob(pv->pv_tlbpage) + (va & PGOFSET);
		return (TRUE);
	}
@


1.32.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32.2.2 2001/07/04 10:16:04 niklas Exp $	*/
d977 1
a977 1
 * pmap_enter(pmap, va, pa, prot, flags)
d983 2
a984 2
int
pmap_enter(pmap, va, pa, prot, flags)
d988 2
a989 2
	vm_prot_t prot;
	int flags;
a994 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1013 1
a1013 1
	if (flags & VM_PROT_WRITE)
a1075 2

	return (KERN_SUCCESS);
@


1.32.2.4
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a211 1
void pmap_pinit __P((pmap_t));
d1012 1
a1012 1
	tlbprot = TLB_UNCACHEABLE | TLB_REF | pmap_prot(pmap, prot) | pmap->pmap_pid;
d1602 1
a1602 1
		pv->pv_tlbprot = TLB_UNCACHEABLE | TLB_WIRED | TLB_REF | TLB_DIRTY |
@


1.32.2.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32.2.4 2001/11/13 21:00:51 niklas Exp $	*/
a765 7
void
pmap_virtual_space(vaddr_t *vstartp, vaddr_t *vendp)
{
	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

d1079 1
a1079 1
	return (0);
d1138 1
a1138 1
	struct vm_page *pg;
d1421 1
a1421 1
	struct vm_page *pg;
d1454 1
a1454 1
	struct vm_page *pg;
d1483 1
a1483 1
	struct vm_page *pg;
d1516 1
a1516 1
	struct vm_page *pg;
d1614 13
@


1.32.2.6
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d124 5
d139 1
a141 1
#include <sys/malloc.h>
d154 3
a157 4
#define	DPRINTF(l,s)	do {		\
	if ((pmapdebug & (l)) == (l))	\
		printf s;		\
} while(0)
a174 1
#define	PDB_PHYS	0x00020000
a183 1
/*	| PDB_PHYS */
a184 2
#else
#define	DPRINTF(l,s)	/* */
d201 1
a201 1
u_int	sid_counter;
d207 1
a207 1
void pmap_hptdump __P((int sp));
d226 1
d247 7
a253 3
	DPRINTF(PDB_FOLLOW | PDB_VA,
	    ("pmap_enter_va(%x,%x,%p): hpt=%p, pvp=%p\n",
	    pv->pv_space, pv->pv_va, pv, hpt, pvp));
d264 1
d276 4
a279 1
	DPRINTF(PDB_FOLLOW | PDB_VA, ("pmap_find_va(%x,%x)\n", space, va));
d311 4
a314 2
	DPRINTF(PDB_FOLLOW | PDB_VA,
	    ("pmap_remove_va(%p), hpt=%p, pvp=%p\n", pv, hpt, pvp));
d365 4
a368 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_alloc_pv()\n"));
d371 1
a371 1
#if notyet
d376 1
a376 1
		pmap_insert_pvp(pvp, 0);
d383 1
a383 1
	pvp = TAILQ_FIRST(&pv_page_freelist);
a392 2
	pv->pv_hash = NULL;
	pv->pv_pmap = NULL;
d408 4
a411 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_free_pv(%p)\n", pv));
d416 1
a416 1
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_list);
a423 1
#ifdef notyet
a426 3
#else
			panic("pmap_free_pv: mallocated pv page");
#endif
d442 1
d451 1
d453 5
a457 2
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_enter_pv: pv %p: %lx/%p/%p\n",
	    pv, pv->pv_va, pv->pv_pmap, pv->pv_next));
d463 4
a466 1
		DPRINTF(PDB_ENTER, ("pmap_enter_pv: no entries yet\n"));
a472 1
		DPRINTF(PDB_ENTER, ("pmap_enter_pv: adding to the list\n"));
d474 3
d478 2
a479 6
			if (pmap == npv->pv_pmap && va == npv->pv_va) {
				printf("pmap_enter_pv: %p already in pv_tab",
				    npv);
				pmap_enter_va(npv); /* HACK UGLY HACK HACK */
				return (npv);
			}
d495 2
d508 6
d515 5
a519 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_remove_pv(%p,%p)\n", ppv, pv));
d540 2
a541 2
			ppv->pv_tlbprot |= pv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);
a550 2
			pv->pv_tlbprot |= ppv->pv_tlbprot &
			    (TLB_DIRTY | TLB_REF);
d560 1
d573 4
a576 1
		DPRINTF(PDB_PV, ("pmap_find_pv(%x):  %d:%d\n", pa, bank, off));
d590 1
d592 4
a595 1
	DPRINTF(PDB_FOLLOW | PDB_PV, ("pmap_clear_pv(%x,%p)\n", pa, cpv));
d600 1
d604 4
a607 2
		DPRINTF(PDB_PV,
		    ("pmap_clear_pv: %x:%x\n", pv->pv_space, pv->pv_va));
d619 1
d643 4
a646 2

	DPRINTF(PDB_FOLLOW, ("pmap_bootstrap(%p, %p)\n", vstart, vend));
d686 1
d697 5
a701 4

	DPRINTF(PDB_INIT, ("pmap_bootstrap: allocating %d pv_pages\n",
	    (struct pv_page *)addr - pvp));

d716 4
a719 3

	DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n", size + 1, addr));

d733 2
a734 2
	    (sizeof(struct pv_entry) * maxproc / 8 +
	    sizeof(struct vm_page)));
d736 3
a738 3
	if (btlb_insert(HPPA_SID_KERNEL, 0, 0, vstart,
	    pmap_sid2pid(HPPA_SID_KERNEL) |
	    pmap_prot(kernel_pmap, VM_PROT_ALL)) < 0)
d749 4
a752 2

	DPRINTF(PDB_INIT, ("pv_array: 0x%x @@ 0x%x\n", size, addr));
d786 4
a789 4

	DPRINTF(PDB_FOLLOW,
	    ("pmap_steal_memory(%x, %x, %x)\n", size, startp, endp));

d797 5
a801 5

		DPRINTF(PDB_STEAL,
		    ("pmap_steal_memory: steal %d bytes (%x+%x,%x)\n",
		    size, virtual_steal, size, virtual_avail));

d827 2
a828 1
	DPRINTF(PDB_FOLLOW, ("pmap_init()\n"));
d846 1
a846 1
	sid_counter = HPPA_SID_KERNEL + 1;
a853 2
	 *
	 * no spls since no interrupts.
d868 1
a868 1
	pa_space_t sid;
d871 4
a874 1
	DPRINTF(PDB_FOLLOW, ("pmap_pinit(%p)\n", pmap));
d876 1
a876 1
	if (!(sid = pmap->pmap_space)) {
d883 3
a885 3
		if (sid_counter < HPPA_SID_MAX) {
			sid = sid_counter;
			sid_counter++;
d887 1
a887 1
			sid = 0;
d890 2
a891 2
		if (sid == 0)
			panic("no more space ids\n");
d897 2
a898 1
	pmap->pmap_space = sid;
d917 4
a920 1
	DPRINTF(PDB_FOLLOW, ("pmap_create()\n"));
d957 4
a960 1
	DPRINTF(PDB_FOLLOW, ("pmap_destroy(%p)\n", pmap));
a977 1
		 * (do some random pid allocation later)
a1006 2
	va = hppa_trunc_page(va);
	space = pmap_sid(pmap, va);
a1013 2
	s = splimp();	/* are we already high enough? XXX */

d1017 2
d1020 4
a1023 1
	tlbprot = TLB_UNCACHEABLE | pmap_prot(pmap, prot) | pmap_sid2pid(space);
d1030 4
a1033 1
		DPRINTF(PDB_ENTER, ("pmap_enter: new mapping\n"));
d1041 5
a1045 2
			DPRINTF(PDB_ENTER, ("pmap_enter: moving pa %x -> %x\n",
			    ppv->pv_tlbpage, tlbpage));
d1051 6
a1056 2
			DPRINTF(PDB_ENTER, ("pmap_enter: changing %b->%b\n",
			    ppv->pv_tlbprot, TLB_BITS, tlbprot, TLB_BITS));
a1059 1
			pmap_clear_pv(pa, NULL);
d1061 3
d1069 1
d1081 4
a1084 1
	DPRINTF(PDB_ENTER, ("pmap_enter: leaving\n"));
d1106 4
a1109 2
	DPRINTF(PDB_FOLLOW, ("pmap_remove(%p, %x, %x)\n", pmap, sva, eva));

d1120 5
a1124 3

		DPRINTF(PDB_REMOVE, ("pmap_remove: removing %p for 0x%x:0x%x\n",
		    pv, space, sva));
d1154 4
a1157 2
	DPRINTF(PDB_FOLLOW|PDB_PROTECT,
	    ("pmap_page_protect(%x, %x)\n", pa, prot));
d1181 2
d1188 1
a1188 1
				ficache(pv->pv_space, pv->pv_va, NBPG);
a1189 3
				fdcache(pv->pv_space, pv->pv_va, NBPG);
				pdtlb(pv->pv_space, pv->pv_va);
				pmap_clear_va(pv->pv_space, pv->pv_va);
d1197 7
a1203 5

			DPRINTF(PDB_PROTECT, ("pv={%p,%x:%x,%b,%x}->%p\n",
			    pv->pv_pmap, pv->pv_space, pv->pv_va,
			    pv->pv_tlbprot, TLB_BITS,
			    tlbptob(pv->pv_tlbpage), pv->pv_hash));
d1231 4
a1234 2
	DPRINTF(PDB_FOLLOW,
	    ("pmap_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot));
d1262 2
d1268 2
a1269 5
			ficache(space, sva, NBPG);
			pitlb(space, sva);
			fdcache(space, sva, NBPG);
			pdtlb(space, sva);
			pmap_clear_va(space, sva);
a1289 1
	int s;
d1292 4
a1295 1
	DPRINTF(PDB_FOLLOW, ("pmap_unwire(%p, %x)\n", pmap, va));
a1301 1
	s = splimp();
a1308 1
	splx(s);
a1325 1
	int s;
d1327 5
a1331 1
	DPRINTF(PDB_FOLLOW, ("pmap_extract(%p, %x)\n", pmap, va));
d1333 1
a1333 2
	s = splimp();
	if (!(pv = pmap_find_va(pmap_sid(pmap, va), hppa_trunc_page(va))))
a1338 1
	splx(s);
a1351 1
	int s;
d1353 4
a1356 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%x)\n", pa));
a1357 8
	/*
	 * do not allow any ints to happen, since cache is in
	 * inconsistant state during the loop.
	 *
	 * do not rsm(PSW_I) since that will lose ints completely,
	 * instead, keep 'em pending (or verify by the book).
	 */
	s = splhigh();
d1361 1
a1361 5
		__asm volatile(
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)\n\t"
		    "stwas,ma %%r0,4(%0)"
d1365 3
a1367 3
			__asm volatile("rsm	%1, %%r0\n\t"
				       "fdc	%2(%0)\n\t"
				       "ssm	%1, %%r0"
a1371 1
	splx(s);
d1391 4
a1394 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%x, %x)\n", spa, dpa));
a1395 1
	s = splhigh();
d1399 2
d1402 3
a1404 10
		__asm volatile(
		    "ldwas,ma 4(%0),%%r22\n\t"
		    "ldwas,ma 4(%0),%%r21\n\t"
		    "stwas,ma %%r22,4(%1)\n\t"
		    "stwas,ma %%r21,4(%1)\n\t"
		    "ldwas,ma 4(%0),%%r22\n\t"
		    "ldwas,ma 4(%0),%%r21\n\t"
		    "stwas,ma %%r22,4(%1)\n\t"
		    "stwas,ma %%r21,4(%1)\n\t"
		    : "+r" (spa), "+r" (dpa) :: "r22", "r21", "memory");
d1407 4
a1410 5
			__asm volatile(
			    "rsm	%2, %%r0\n\t"
			    "pdc	%3(%0)\n\t"
			    "fdc	%3(%1)\n\t"
			    "ssm	%2, %%r0"
d1432 1
a1432 1
	int s, ret;
d1434 4
a1437 1
	DPRINTF(PDB_FOLLOW, ("pmap_clear_modify(%x)\n", pa));
d1440 9
a1448 8
	for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next)
		if (pv->pv_tlbprot & TLB_DIRTY) {
			pitlb(pv->pv_space, pv->pv_va);
			pdtlb(pv->pv_space, pv->pv_va);
			pv->pv_tlbprot &= ~(TLB_DIRTY);
			pmap_clear_va(pv->pv_space, pv->pv_va);
			ret = TRUE;
		}
d1451 1
a1451 1
	return (ret);
d1465 2
a1466 3
	int s, f = 0;

	DPRINTF(PDB_FOLLOW, ("pmap_is_modified(%x)\n", pa));
d1468 4
d1473 2
a1474 2
	for (pv = pmap_find_pv(pa); pv && pv->pv_pmap && !f; pv = pv->pv_next)
		f |= pv->pv_tlbprot & TLB_DIRTY;
d1477 1
a1477 1
	return f? TRUE : FALSE;
d1494 1
a1494 1
	int s, ret;
d1496 4
a1499 1
	DPRINTF(PDB_FOLLOW, ("pmap_clear_reference(%x)\n", pa));
d1502 9
a1510 8
	for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next)
		if (pv->pv_tlbprot & TLB_REF) {
			pitlb(pv->pv_space, pv->pv_va);
			pdtlb(pv->pv_space, pv->pv_va);
			pv->pv_tlbprot &= ~(TLB_REF);
			pmap_clear_va(pv->pv_space, pv->pv_va);
			ret = TRUE;
		}
d1513 1
a1513 1
	return (ret);
d1527 2
a1528 1
	int s, f;
d1530 4
a1533 1
	DPRINTF(PDB_FOLLOW, ("pmap_is_referenced(%x)\n", pa));
d1536 2
a1537 2
	for (pv = pmap_find_pv(pa); pv && pv->pv_pmap && !f; pv = pv->pv_next)
		f |= pv->pv_tlbprot & TLB_REF;
d1540 1
a1540 1
	return f? TRUE : FALSE;
d1552 4
a1555 1
	DPRINTF(PDB_FOLLOW, ("pmap_changebit(%x, %x, %x)\n", va, set, reset));
d1584 5
d1590 1
a1590 3
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot));

d1597 3
d1601 1
a1601 1
			panic("pmap_kenter_pa: mapped already %x", va);
d1610 2
a1611 2
		pv->pv_tlbprot = TLB_WIRED | TLB_DIRTY | TLB_REF |
		    HPPA_PID_KERNEL | pmap_prot(pmap_kernel(), prot) |
d1616 5
a1620 1
	DPRINTF(PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
d1628 1
a1628 15
	register struct pv_entry *pv;

	for (va = hppa_trunc_page(va); size > 0;
	    size -= PAGE_SIZE, va += PAGE_SIZE) {
		pv = pmap_find_va(HPPA_SID_KERNEL, va);
		if (pv) {
			ficache(pv->pv_space, pv->pv_va, NBPG);
			pitlb(pv->pv_space, pv->pv_va);
			fdcache(pv->pv_space, pv->pv_va, NBPG);
			pdtlb(pv->pv_space, pv->pv_va);
			pmap_remove_va(pv);
		} else
			DPRINTF(PDB_REMOVE,
			    ("pmap_kremove: no pv for 0x%x\n", va));
	}
d1637 1
a1637 2
pmap_hptdump(sp)
	int sp;
a1640 1
	register int hpthf;
d1646 8
a1653 14
	for (hpthf = 0; hpt < ehpt; hpt++, hpthf = 0)
		for (pv = hpt->hpt_entry; pv; pv = pv->pv_hash)
			if (sp < 0 || sp == pv->pv_space) {
				if (!hpthf) {
					db_printf(
					    "hpt@@%p: %x{%sv=%x:%x},%b,%x\n",
					    hpt, *(u_int *)hpt,
					    (hpt->hpt_valid?"ok,":""),
					    hpt->hpt_space, hpt->hpt_vpn << 9,
					    hpt->hpt_tlbprot, TLB_BITS,
					    tlbptob(hpt->hpt_tlbpage));

					hpthf++;
				}
d1658 1
a1658 1
			}
@


1.32.2.7
log
@Merge in -current from about a week ago
@
text
@d4 1
a4 1
 * Copyright (c) 1998-2002 Michael Shalayeff
d27 1
a27 1
 * MOND, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
d33 92
a135 1
#include <sys/pool.h>
d164 2
a165 2
#define	PDB_EXTRACT	0x00000400
#define	PDB_VP		0x00000800
d169 1
a169 1
#define	PDB_PMAP	0x00008000
a171 1
#define	PDB_POOL	0x00040000
d174 1
a174 1
/*	| PDB_VP */
d187 1
a187 6
vaddr_t	virtual_steal, virtual_avail;
paddr_t physical_steal, physical_end;

#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
int		pmap_hptsize = 256;	/* patchable */
#endif
d190 1
a190 1
int		pmap_sid_counter, hppa_sid_max = HPPA_SID_MAX;
a191 10
struct pool	pmap_pmap_pool;
struct pool	pmap_pv_pool;
struct simplelock pvalloc_lock;

void    *pmap_pv_page_alloc(struct pool *, int);
void    pmap_pv_page_free(struct pool *, void *);

struct pool_allocator pmap_allocator_pv = {
	pmap_pv_page_alloc, pmap_pv_page_free, 0
};
d193 16
a208 1
u_int	hppa_prot[8];
d210 3
a215 4
#define	pmap_pvh_attrs(a) \
	(((a) & PTE_PROT(TLB_DIRTY)) | ((a) ^ PTE_PROT(TLB_REFTRAP)))

#if defined(HP7100LC_CPU) || defined(HP7300LC_CPU)
a233 1
#endif
d235 4
d240 1
a240 1
pmap_sdir_set(pa_space_t space, paddr_t pa)
d242 12
a253 6
	paddr_t vtop;

	mfctl(CR_VTOP, vtop);
#ifdef PMAPDEBUG
	if (!vtop)
		panic("pmap_sdir_set: zero vtop");
d255 4
a258 1
	asm("stwas	%0, 0(%1)":: "r" (pa), "r" (vtop + (space << 2)));
d261 6
a266 2
static __inline paddr_t
pmap_sdir_get(pa_space_t space)
d268 1
a268 1
	paddr_t vtop, pa;
d270 1
a270 2
	mfctl(CR_VTOP, vtop);
	asm("ldwax,s	%2(%1), %0": "=&r" (pa) : "r" (vtop), "r" (space));
d272 4
a275 1
	return (pa);
d278 7
a284 2
static __inline pt_entry_t *
pmap_pde_get(paddr_t pa, vaddr_t va)
d286 1
a286 1
	pt_entry_t *pde;
d288 2
a289 3
	asm("ldwax,s	%2(%1), %0": "=&r" (pde) : "r" (pa), "r" (va >> 22));

	return (pde);
d292 4
d297 1
a297 1
pmap_pde_set(struct pmap *pm, vaddr_t va, paddr_t ptp)
d299 2
a300 3
	asm("stwas	%0, 0(%1)"
	    :: "r" (ptp), "r" ((paddr_t)pm->pm_pdir + ((va >> 20) & 0xffc)));
}
d302 2
a303 5
static __inline pt_entry_t *
pmap_pde_alloc(struct pmap *pm, vaddr_t va, struct vm_page **pdep)
{
	struct vm_page *pg;
	paddr_t pa;
d305 14
a318 9
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_alloc(%p, 0x%x, %p)\n", pm, va, pdep));

	va &= PDE_MASK;
	pg = uvm_pagealloc(&pm->pm_obj, va, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (pg == NULL) {
		/* try to steal somewhere */
		return (NULL);
d320 1
d322 10
a331 1
	pa = VM_PAGE_TO_PHYS(pg);
d333 7
a339 10
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pde_alloc: pde %x\n", pa));

	pg->flags &= ~PG_BUSY;	/* never busy */
	pg->wire_count = 1;		/* no mappings yet */
	pmap_pde_set(pm, va, pa);
	pm->pm_stats.resident_count++;	/* count PTP as resident */
	pm->pm_ptphint = pg;
	if (pdep)
		*pdep = pg;
	return ((pt_entry_t *)pa);
d342 8
a349 2
static __inline struct vm_page *
pmap_pde_ptp(struct pmap *pm, pt_entry_t *pde)
d351 2
a352 1
	paddr_t pa = (paddr_t)pde;
d354 1
a354 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp(%p, %p)\n", pm, pde));
d356 11
a366 2
	if (pm->pm_ptphint && VM_PAGE_TO_PHYS(pm->pm_ptphint) == pa)
		return (pm->pm_ptphint);
d368 13
a380 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pde_ptp: lookup 0x%x\n", pa));
d382 1
a382 1
	return (PHYS_TO_VM_PAGE(pa));
d385 6
d392 1
a392 1
pmap_pde_release(struct pmap *pmap, vaddr_t va, struct vm_page *ptp)
d394 24
a417 8
	ptp->wire_count--;
	if (ptp->wire_count <= 1) {
		pmap_pde_set(pmap, va, 0);
		pmap->pm_stats.resident_count--;
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
		ptp->wire_count = 0;
		uvm_pagefree(ptp);
d421 8
a428 2
static __inline pt_entry_t
pmap_pte_get(pt_entry_t *pde, vaddr_t va)
d430 1
a430 1
	pt_entry_t pte;
d432 2
a433 2
	asm("ldwax,s	%2(%1),%0" : "=&r" (pte)
	    : "r" (pde),  "r" ((va >> 12) & 0x3ff));
d435 4
a438 2
	return (pte);
}
d440 2
a441 5
static __inline void
pmap_pte_set(pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
{
	DPRINTF(PDB_FOLLOW|PDB_VP, ("pmap_pte_set(%p, 0x%x, 0x%x)\n",
	    pde, va, pte));
d443 12
d456 7
a462 8
	if (!pde)
		panic("pmap_pte_set: zero pde");
	if (pte && pte < virtual_steal &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page)
		panic("pmap_pte_set: invalid pte");

	if (pte && !(pte & PTE_PROT(TLB_UNCACHABLE)))
		Debugger();
d464 13
a476 8
	asm("stwas	%0, 0(%1)"
	    :: "r" (pte), "r" ((paddr_t)pde + ((va >> 10) & 0xffc)));
}

static __inline pt_entry_t
pmap_vp_find(struct pmap *pm, vaddr_t va)
{
	pt_entry_t *pde;
d478 1
a478 4
	if (!(pde = pmap_pde_get(pm->pm_pdir, va)))
		return (NULL);

	return (pmap_pte_get(pde, va));
d481 7
a487 2
void
pmap_dump_table(pa_space_t space)
a488 1
	pa_space_t sp;
d490 1
a490 4
	for (sp = 0; sp <= hppa_sid_max; sp++) {
		paddr_t pa;
		pt_entry_t *pde, pte;
		vaddr_t va, pdemask = virtual_avail + 1;
d492 5
a496 3
		if (((int)space >= 0 && sp != space) ||
		    !(pa = pmap_sdir_get(sp)))
			continue;
d498 2
a499 10
		for (va = virtual_avail; va < VM_MAX_KERNEL_ADDRESS;
		    va += PAGE_SIZE) {
			if (pdemask != (va & PDE_MASK)) {
				pdemask = va & PDE_MASK;
				if (!(pde = pmap_pde_get(pa, va))) {
					va += ~PDE_MASK + 1 - PAGE_SIZE;
					continue;
				}
				printf("%x:0x%08x:\n", sp, pde);
			}
d501 19
a519 2
			if (!(pte = pmap_pte_get(pde, va)))
				continue;
d521 10
a530 1
			printf("0x%08x-0x%08x\n", va, pte);
d535 4
d540 1
a540 1
pmap_pv_alloc(void)
d542 1
a542 1
	struct pv_entry *pv;
d544 5
a548 11
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc()\n"));

	simple_lock(&pvalloc_lock);

	pv = pool_get(&pmap_pv_pool, 0);

	simple_unlock(&pvalloc_lock);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_alloc: %p\n", pv));

	return (pv);
d551 4
d556 1
a556 1
pmap_pv_free(struct pv_entry *pv)
d558 1
a558 1
	simple_lock(&pvalloc_lock);
d560 1
a560 2
	if (pv->pv_ptp)
		pmap_pde_release(pv->pv_pmap, pv->pv_va, pv->pv_ptp);
d562 2
a563 1
	pool_put(&pmap_pv_pool, pv);
d565 10
a574 2
	simple_unlock(&pvalloc_lock);
}
d576 2
a577 6
static __inline void
pmap_pv_enter(struct pv_head *pvh, struct pv_entry *pve, struct pmap *pm,
    vaddr_t va, struct vm_page *pdep)
{
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_pv_enter(%p, %p, %p, 0x%x, %p)\n",
	    pvh, pve, pm, va, pdep));
d579 2
a580 20
	pve->pv_pmap	= pm;
	pve->pv_va	= va;
	pve->pv_ptp	= pdep;
	simple_lock(&pvh->pvh_lock);		/* lock pv_head */
	pve->pv_next = pvh->pvh_list;
	pvh->pvh_list = pve;
	simple_unlock(&pvh->pvh_lock);		/* unlock, done! */
}

static __inline struct pv_entry *
pmap_pv_remove(struct pv_head *pvh, struct pmap *pmap, vaddr_t va)
{
	struct pv_entry **pve, *pv;

	for(pv = *(pve = &pvh->pvh_list); pv; pv = *(pve = &(*pve)->pv_next))
		if (pv->pv_pmap == pmap && pv->pv_va == va) {
			*pve = pv->pv_next;
			break;
		}
	return (pv);
d583 9
d593 7
a599 6
pmap_bootstrap(vstart)
	vaddr_t vstart;
{
	extern char etext, etext1;
	extern u_int totalphysmem, *ie_mem;
	vaddr_t addr = hppa_round_page(vstart), t;
d601 2
a602 4
#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	struct vp_entry *hptp;
#endif
	struct pmap *kpm;
d605 1
a605 1
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_bootstrap(0x%x)\n", vstart));
d609 22
a630 8
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_NA;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE]    =TLB_AR_R;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE]    =TLB_AR_RW;
	hppa_prot[VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE] =TLB_AR_RX;
	hppa_prot[VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
	hppa_prot[VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE] =TLB_AR_RWX;
d635 10
a644 14
	kpm = &kernel_pmap_store;
	bzero(kpm, sizeof(*kpm));
	simple_lock_init(&kpm->pm_obj.vmobjlock);
	kpm->pm_obj.pgops = NULL;
	TAILQ_INIT(&kpm->pm_obj.memq);
	kpm->pm_obj.uo_npages = 0;
	kpm->pm_obj.uo_refs = 1;
	kpm->pm_space = HPPA_SID_KERNEL;
	kpm->pm_pid = HPPA_PID_KERNEL;
	kpm->pm_pdir_pg = NULL;
	kpm->pm_pdir = addr;
	addr += PAGE_SIZE;
	bzero((void *)addr, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, addr, PAGE_SIZE);
d649 24
d674 4
d679 1
a679 23
	bzero((void *)addr, (hppa_sid_max + 1) * 4);
	fdcache(HPPA_SID_KERNEL, addr, (hppa_sid_max + 1) * 4);
	printf("vtop: 0x%x @@ 0x%x\n", (hppa_sid_max + 1) * 4, addr);
	addr += (hppa_sid_max + 1) * 4;
	pmap_sdir_set(HPPA_SID_KERNEL, kpm->pm_pdir);

	ie_mem = (u_int *)addr;
	addr += 0x8000;

#if 0 && (defined(HP7100LC_CPU) || defined(HP7300LC_CPU))
	if (pmap_hptsize && (cpu_type == hpcxl || cpu_type == hpcxl2)) {
		int error;

		if (pmap_hptsize > pdc_hwtlb.max_size)
			pmap_hptsize = pdc_hwtlb.max_size;
		else if (pmap_hptsize < pdc_hwtlb.min_size)
			pmap_hptsize = pdc_hwtlb.min_size;

		size = pmap_hptsize * sizeof(*hptp);
		bzero((void *)addr, size);
		/* Allocate the HPT */
		for (hptp = (struct vp_entry *)addr, i = pmap_hptsize; i--;)
			hptp[i].vp_tag = 0xffff;
d681 5
a685 15
		DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n", size, addr));

		if ((error = (cpu_hpt_init)(addr, size)) < 0) {
			printf("WARNING: HPT init error %d\n", error);
		} else {
			printf("HPT: %d entries @@ 0x%x\n",
			    pmap_hptsize / sizeof(struct vp_entry), addr);
		}

		/* TODO find a way to avoid using cr*, use cpu regs instead */
		mtctl(addr, CR_VTOP);
		mtctl(size - 1, CR_HPTMASK);
		addr += size;
	}
#endif	/* HP7100LC_CPU | HP7300LC_CPU */
d688 3
a690 2
	vstart = hppa_round_page(addr + (totalphysmem - (atop(addr))) *
	    (16 + sizeof(struct pv_head) + sizeof(struct vm_page)));
d692 1
a692 2
	t = (vaddr_t)&etext1;
	if (btlb_insert(HPPA_SID_KERNEL, 0, 0, &t,
d694 1
a694 6
	    pmap_prot(pmap_kernel(), VM_PROT_READ|VM_PROT_EXECUTE)) < 0)
		panic("pmap_bootstrap: cannot block map kernel text");
	t = vstart - (vaddr_t)&etext1;
	if (btlb_insert(HPPA_SID_KERNEL, (vaddr_t)&etext1, (vaddr_t)&etext1, &t,
	    pmap_sid2pid(HPPA_SID_KERNEL) | TLB_UNCACHABLE |
	    pmap_prot(pmap_kernel(), VM_PROT_ALL)) < 0)
d696 1
a696 9
	vstart = (vaddr_t)&etext1 + t;
	virtual_avail = vstart;
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count =
	    physmem = atop(vstart);

	if (&etext < &etext1) {
		physical_steal = (vaddr_t)&etext;
		physical_end = (vaddr_t)&etext1;
	}
d703 1
a703 2
	addr = hppa_round_page(addr);
	size = hppa_round_page(sizeof(struct pv_head) * totalphysmem);
d706 1
a706 1
	DPRINTF(PDB_INIT, ("pmseg.pvent: 0x%x @@ 0x%x\n", size, addr));
a707 1
	/* XXX we might need to split this for isa */
d713 12
a724 1
	vm_physmem[0].pmseg.pvhead = (struct pv_head *)addr;
d741 1
a741 1
	DPRINTF(PDB_FOLLOW|PDB_STEAL,
d747 1
a747 1
		*endp = VM_MAX_KERNEL_ADDRESS;
d768 6
d777 1
a777 1
	DPRINTF(PDB_FOLLOW|PDB_INIT, ("pmap_init()\n"));
d779 5
a783 7
	pool_init(&pmap_pmap_pool, sizeof(struct pmap), 0, 0, 0, "pmappl",
	    &pool_allocator_nointr);
	pool_init(&pmap_pv_pool, sizeof(struct pv_entry), 0, 0, 0, "pmappv",
	    &pmap_allocator_pv);
	/* deplete the steal area */
	pool_prime(&pmap_pv_pool, (virtual_avail - virtual_steal) / PAGE_SIZE *
	    pmap_pv_pool.pr_itemsperpage);
d785 15
a799 1
	simple_lock_init(&pvalloc_lock);
d810 15
a824 2
	{
		pt_entry_t *pde;
d826 1
a826 3
		if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, SYSCALLGATE)) &&
		    !(pde = pmap_pde_alloc(pmap_kernel(), SYSCALLGATE, NULL)))
			panic("pmap_init: cannot allocate pde");
d828 18
a845 2
		pmap_pte_set(pde, SYSCALLGATE, (paddr_t)&gateway_page |
		    PTE_PROT(TLB_UNCACHABLE|TLB_GATE_PROT));
d847 7
d856 7
a862 1
struct pmap *
d865 2
a866 2
	struct pmap *pmap;
	pa_space_t space;
d868 1
a868 2
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_create()\n"));
	pmap = pool_get(&pmap_pmap_pool, PR_WAITOK);
d870 16
a885 21
	simple_lock_init(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.pgops = NULL;	/* currently not a mappable object */
	TAILQ_INIT(&pmap->pm_obj.memq);
	pmap->pm_obj.uo_npages = 0;
	pmap->pm_obj.uo_refs = 1;
	pmap->pm_stats.wired_count = 0;
	pmap->pm_stats.resident_count = 1;

	if (pmap_sid_counter >= hppa_sid_max) {
		/* collect some */
		panic("pmap_create: outer space");
	} else
		space = ++pmap_sid_counter;

	pmap->pm_space = space;
	pmap->pm_pid = (space + 1) << 1;
	pmap->pm_pdir_pg = uvm_pagealloc(NULL, 0, NULL,
	    UVM_PGA_USERESERVE|UVM_PGA_ZERO);
	if (!pmap->pm_pdir_pg)
		panic("pmap_create: no pages");
	pmap->pm_pdir = VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
d887 1
a887 1
	pmap_sdir_set(space, pmap->pm_pdir);
d892 6
d900 1
a900 1
	struct pmap *pmap;
d902 4
a905 3
	struct vm_page *pg;
	pa_space_t space;
	int refs;
d907 2
a908 1
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_destroy(%p)\n", pmap));
d910 1
a910 3
	simple_lock(&pmap->pm_obj.vmobjlock);
	refs = --pmap->pm_obj.uo_refs;
	simple_unlock(&pmap->pm_obj.vmobjlock);
d912 1
a912 2
	if (refs > 0)
		return;
d914 13
a926 7
	TAILQ_FOREACH(pg, &pmap->pm_obj.memq, listq) {
#ifdef DIAGNOSTIC
		if (pg->flags & PG_BUSY)
			panic("pmap_release: busy page table page");
#endif
		pg->wire_count = 0;
		uvm_pagefree(pg);
d928 1
a928 5

	uvm_pagefree(pmap->pm_pdir_pg);
	pmap->pm_pdir_pg = NULL;	/* XXX cache it? */
	pmap_sdir_set(space, 0);
	pool_put(&pmap_pmap_pool, pmap);
a929 1

d931 5
a935 1
 * Add a reference to the specified pmap.
a936 18
void
pmap_reference(pmap)
	struct pmap *pmap;
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_reference(%p)\n", pmap));

	simple_lock(&pmap->pm_obj.vmobjlock);
	pmap->pm_obj.uo_refs++;
	simple_unlock(&pmap->pm_obj.vmobjlock);
}

void
pmap_collect(struct pmap *pmap)
{
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_collect(%p)\n", pmap));
	/* nothing yet */
}

d939 1
a939 1
	struct pmap *pmap;
d945 4
a948 5
	pt_entry_t *pde, pte;
	struct vm_page *ptp = NULL;
	struct pv_head *pvh;
	struct pv_entry *pve;
	int bank, off;
d950 1
d952 9
a960 5
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_enter(%p, 0x%x, 0x%x, 0x%x, 0x%x)\n",
	    pmap, va, pa, prot, flags));

	simple_lock(&pmap->pm_obj.vmobjlock);
d962 1
a962 4
	if (!(pde = pmap_pde_get(pmap->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap, va, &ptp))) {
		if (flags & PMAP_CANFAIL)
			return (KERN_RESOURCE_SHORTAGE);
d964 2
a965 2
		panic("pmap_enter: cannot allocate pde");
	}
d967 2
a968 2
	if (!ptp)
		ptp = pmap_pde_ptp(pmap, pde);
d970 8
a977 1
	if ((pte = pmap_pte_get(pde, va))) {
d979 1
a979 2
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: remapping 0x%x -> 0x%x\n", pte, pa));
d981 15
a995 15
		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(pmap->pm_space, va, NBPG);
		pitlb(pmap->pm_space, va);
		fdcache(pmap->pm_space, va, NBPG);
		pdtlb(pmap->pm_space, va);

		if (wired && !(pte & PTE_PROT(TLB_WIRED)) == 0)
			pmap->pm_stats.wired_count++;
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)) != 0)
			pmap->pm_stats.wired_count--;

		if (PTE_PAGE(pte) == pa) {
			DPRINTF(PDB_FOLLOW|PDB_ENTER,
			    ("pmap_enter: same page\n"));
			goto enter;
d997 1
d999 12
a1010 47
		bank = vm_physseg_find(atop(PTE_PAGE(pte)), &off);
		if (bank != -1) {
			pvh = &vm_physmem[bank].pmseg.pvhead[off];
			simple_lock(&pvh->pvh_lock);
			pve = pmap_pv_remove(pvh, pmap, va);
			pvh->pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pvh->pvh_lock);
		} else
			pve = NULL;
	} else {
		DPRINTF(PDB_ENTER,
		    ("pmap_enter: new mapping 0x%x -> 0x%x\n", va, pa));
		pte = PTE_PROT(TLB_REFTRAP);
		pve = NULL;
		pmap->pm_stats.resident_count++;
		if (wired)
			pmap->pm_stats.wired_count++;
		if (ptp)
			ptp->wire_count++;
	}

	bank = vm_physseg_find(atop(pa), &off);
	if (pmap_initialized && bank != -1) {
		if (!pve && !(pve = pmap_pv_alloc())) {
			if (flags & PMAP_CANFAIL) {
				simple_unlock(&pmap->pm_obj.vmobjlock);
				return (KERN_RESOURCE_SHORTAGE);
			}
			panic("pmap_enter: no pv entries available");
		}
		pvh = &vm_physmem[bank].pmseg.pvhead[off];
		pmap_pv_enter(pvh, pve, pmap, va, ptp);
	} else {
		pvh = NULL;
		if (pve)
			pmap_pv_free(pve);
	}

enter:
	/* preserve old ref & mod */
	pte = pa | PTE_PROT(TLB_UNCACHABLE|pmap_prot(pmap, prot)) |
	    (pte & PTE_PROT(TLB_UNCACHABLE|TLB_DIRTY|TLB_REFTRAP));
	if (wired)
		pte |= PTE_PROT(TLB_WIRED);
	pmap_pte_set(pde, va, pte);

	simple_unlock(&pmap->pm_obj.vmobjlock);
d1012 2
a1013 1
	DPRINTF(PDB_FOLLOW, ("pmap_enter: leaving\n"));
d1018 7
d1027 3
a1029 3
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d1031 5
a1035 5
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *pde, pte;
	int bank, off;
	u_int pdemask;
d1037 2
a1038 2
	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_remove(%p, 0x%x, 0x%x\n", pmap, sva, eva));
d1040 1
a1040 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1042 2
a1043 8
	for (pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
		}
d1045 2
a1046 1
		if ((pte = pmap_pte_get(pde, sva))) {
d1048 8
a1055 21
			if (pte & PTE_PROT(TLB_WIRED))
				pmap->pm_stats.wired_count--;
			pmap->pm_stats.resident_count--;

			if (pte & PTE_PROT(pte))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);

			pmap_pte_set(pde, sva, 0);

			bank = vm_physseg_find(atop(pte), &off);
			if (pmap_initialized && bank != -1) {
				pvh = &vm_physmem[bank].pmseg.pvhead[off];
				pve = pmap_pv_remove(pvh, pmap, sva);
				if (pve) {
					pvh->pvh_attrs |= pmap_pvh_attrs(pte);
					pmap_pv_free(pve);
				}
			}
d1057 1
d1060 1
a1060 3
	simple_unlock(&pmap->pm_obj.vmobjlock);

	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_remove: leaving\n"));
d1063 5
d1069 2
a1070 4
pmap_write_protect(pmap, sva, eva, prot)
	struct pmap *pmap;
	vaddr_t sva;
	vaddr_t eva;
d1073 5
a1077 2
	pt_entry_t *pde, pte;
	u_int tlbprot, pdemask;
d1079 2
a1080 2
	DPRINTF(PDB_FOLLOW|PDB_PMAP,
	    ("pmap_write_protect(%p, %x, %x, %x)\n", pmap, sva, eva, prot));
d1082 9
a1090 12
	sva = hppa_trunc_page(sva);
	tlbprot = PTE_PROT(pmap_prot(pmap, prot));

	simple_lock(&pmap->pm_obj.vmobjlock);

	for(pdemask = sva + 1; sva < eva; sva += PAGE_SIZE) {
		if (pdemask != (sva & PDE_MASK)) {
			pdemask = sva & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap->pm_pdir, sva))) {
				sva += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
a1091 1
		if ((pte = pmap_pte_get(pde, sva))) {
d1093 1
a1093 2
			DPRINTF(PDB_PMAP,
			    ("pmap_write_protect: pte=0x%x\n", pte));
d1095 2
a1096 2
			 * Determine if mapping is changing.
			 * If not, nothing to do.
d1098 1
a1098 2
			if ((pte & PTE_PROT(TLB_AR_MASK)) == tlbprot)
				continue;
d1100 29
a1128 9
			if (pte & PTE_PROT(TLB_EXECUTE))
				ficache(pmap->pm_space, sva, PAGE_SIZE);
			pitlb(pmap->pm_space, sva);
			fdcache(pmap->pm_space, sva, PAGE_SIZE);
			pdtlb(pmap->pm_space, sva);

			pte &= ~PTE_PROT(TLB_AR_MASK);
			pte |= tlbprot;
			pmap_pte_set(pde, sva, pte);
d1130 2
a1132 2

	simple_unlock(&pmap->pm_obj.vmobjlock);
d1135 7
d1143 5
a1147 2
pmap_page_remove(pg)
	struct vm_page *pg;
d1149 3
a1151 4
	struct pv_head *pvh;
	struct pv_entry *pve, *ppve;
	pt_entry_t *pde, pte;
	int bank, off;
d1153 2
a1154 1
	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove(%p)\n", pg));
d1156 5
a1160 3
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_page_remove: unmanaged page?\n");
d1163 1
a1163 3

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	if (pvh->pvh_list == NULL)
d1166 3
a1168 1
	simple_lock(&pvh->pvh_lock);
d1170 8
a1177 2
	for (pve = pvh->pvh_list; pve; ) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
d1179 2
a1180 3
		pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va);
		pte = pmap_pte_get(pde, pve->pv_va);
		pmap_pte_set(pde, pve->pv_va, 0);
d1182 10
a1191 10
		if (pte & PTE_PROT(TLB_WIRED))
			pve->pv_pmap->pm_stats.wired_count--;
		pve->pv_pmap->pm_stats.resident_count--;

		simple_unlock(&pve->pmap->pm_obj.vmobjlock);

		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
		ppve = pve;
		pve = pve->pv_next;
		pmap_pv_free(ppve);
a1192 4
	simple_unlock(&pvh->pvh_lock);

	DPRINTF(PDB_FOLLOW|PDB_PV, ("pmap_page_remove: leaving\n"));

d1195 10
d1207 1
a1207 1
	struct pmap *pmap;
d1210 2
a1211 1
	pt_entry_t *pde, pte = 0;
d1213 2
a1214 1
	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire(%p, 0x%x)\n", pmap, va));
d1216 2
a1217 30
	simple_lock(&pmap->pm_obj.vmobjlock);
	if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
		pte = pmap_pte_get(pde, va);

		if (pte & PTE_PROT(TLB_WIRED)) {
			pte &= ~PTE_PROT(TLB_WIRED);
			pmap->pm_stats.wired_count--;
			pmap_pte_set(pde, va, pte);
		}
	}
	simple_unlock(&pmap->pm_obj.vmobjlock);

	DPRINTF(PDB_FOLLOW|PDB_PMAP, ("pmap_unwire: leaving\n"));

#ifdef DIAGNOSTIC
	if (!pte)
		panic("pmap_unwire: invalid va 0x%x", va);
#endif
}

boolean_t
pmap_changebit(struct vm_page *pg, u_int set, u_int clear)
{
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t *pde, pte, res;
	int bank, off;

	DPRINTF(PDB_FOLLOW|PDB_BITS,
	    ("pmap_changebit(%p, %x, %x)\n", pg, set, clear));
d1219 1
a1219 23
	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_testbits: unmanaged page?\n");
		return(FALSE);
	}

	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	res = pvh->pvh_attrs = 0;

	simple_lock(&pvh->pvh_lock);
	for(pve = pvh->pvh_list; pve; pve = pve->pv_next) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
		if ((pde = pmap_pde_get(pve->pv_pmap->pm_pdir, pve->pv_va))) {
			pte = pmap_pte_get(pde, pve->pv_va);
			res |= pmap_pvh_attrs(pte);
			pte &= ~clear;
			pte |= set;

			if (pte & PTE_PROT(TLB_EXECUTE))
				pitlb(pve->pv_pmap->pm_space, pve->pv_va);
			/* XXX flush only if there was mod ? */
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
			pdtlb(pve->pv_pmap->pm_space, pve->pv_va);
d1221 7
a1227 3
			pmap_pte_set(pde, pve->pv_va, pte);
		}
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
d1229 2
a1230 34
	pvh->pvh_attrs = res;
	simple_unlock(&pvh->pvh_lock);

	return ((res & clear) != 0);
}

boolean_t
pmap_testbit(struct vm_page *pg, u_int bits)
{
	struct pv_head *pvh;
	struct pv_entry *pve;
	pt_entry_t pte;
	int bank, off;

	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bits));

	bank = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), &off);
	if (bank == -1) {
		printf("pmap_testbits: unmanaged page?\n");
		return(FALSE);
	}

	simple_lock(&pvh->pvh_lock);
	pvh = &vm_physmem[bank].pmseg.pvhead[off];
	for(pve = pvh->pvh_list; !(pvh->pvh_attrs & bits) && pve;
	    pve = pve->pv_next) {
		simple_lock(&pve->pv_pmap->pm_obj.vmobjlock);
		pte = pmap_vp_find(pve->pv_pmap, pve->pv_va);
		simple_unlock(&pve->pv_pmap->pm_obj.vmobjlock);
		pvh->pvh_attrs |= pmap_pvh_attrs(pte);
	}
	simple_unlock(&pvh->pvh_lock);

	return ((pvh->pvh_attrs & bits) != 0);
d1233 7
d1242 1
a1242 1
	struct pmap *pmap;
d1246 2
a1247 3
	pt_entry_t pte;

	DPRINTF(PDB_FOLLOW|PDB_EXTRACT, ("pmap_extract(%p, %x)\n", pmap, va));
d1249 1
a1249 3
	simple_lock(&pmap->pm_obj.vmobjlock);
	pte = pmap_vp_find(pmap, va);
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1251 5
a1255 3
	if (pte) {
		if (pap)
			*pap = (pte & ~PGOFSET) | (va & PGOFSET);
d1258 1
a1258 2

	return (FALSE);
d1261 5
d1268 1
a1268 1
	paddr_t pa;
d1284 1
d1287 1
a1287 1
		__asm volatile(			/* can use ,bc */
d1305 8
d1325 2
a1326 1
	/* XXX flush cache for the spa ??? */
d1329 1
a1329 1
		__asm volatile(			/* can use ,bc */
d1354 39
a1392 5
void
pmap_kenter_pa(va, pa, prot)
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
d1394 5
a1398 1
	pt_entry_t *pde, pte;
d1400 36
a1435 2
	DPRINTF(PDB_FOLLOW|PDB_ENTER,
	    ("pmap_kenter_pa(%x, %x, %x)\n", va, pa, prot));
d1437 2
a1438 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1440 12
a1451 8
	if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va)) &&
	    !(pde = pmap_pde_alloc(pmap_kernel(), va, NULL)))
		panic("pmap_kenter_pa: cannot allocate pde");
#ifdef DIAGNOSTIC
	if ((pte = pmap_pte_get(pde, va)))
		panic("pmap_kenter_pa: 0x%x is already mapped %p:0x%x",
		    va, pde, pte);
#endif
d1453 1
a1453 4
	pte = pa | PTE_PROT(TLB_WIRED|TLB_DIRTY|pmap_prot(pmap_kernel(), prot));
	/* if (pa >= HPPA_IOSPACE) */
		pte |= PTE_PROT(TLB_UNCACHABLE);
	pmap_pte_set(pde, va, pte);
d1455 4
a1458 1
	simple_unlock(&pmap->pm_obj.vmobjlock);
d1460 1
a1460 1
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_kenter_pa: leaving\n"));
d1463 1
d1465 1
a1465 1
pmap_kremove(va, size)
d1467 1
a1467 1
	vsize_t size;
d1469 2
a1470 2
	pt_entry_t *pde, pte;
	vaddr_t eva = va + size, pdemask;
d1472 1
a1472 2
	DPRINTF(PDB_FOLLOW|PDB_REMOVE,
	    ("pmap_kremove(%x, %x)\n", va, size));
d1474 5
a1478 1
	simple_lock(&pmap->pm_obj.vmobjlock);
d1480 3
a1482 14
	for (pdemask = va + 1; va < eva; va += PAGE_SIZE) {
		if (pdemask != (va & PDE_MASK)) {
			pdemask = va & PDE_MASK;
			if (!(pde = pmap_pde_get(pmap_kernel()->pm_pdir, va))) {
				va += ~PDE_MASK + 1 - PAGE_SIZE;
				continue;
			}
		}
		if (!(pte = pmap_pte_get(pde, va))) {
#ifdef DEBUG
			printf("pmap_kremove: unmapping unmapped 0x%x\n", va);
#endif
			continue;
		}
d1484 2
a1485 5
		if (pte & PTE_PROT(TLB_EXECUTE))
			ficache(HPPA_SID_KERNEL, va, NBPG);
		pitlb(HPPA_SID_KERNEL, va);
		fdcache(HPPA_SID_KERNEL, va, NBPG);
		pdtlb(HPPA_SID_KERNEL, va);
d1487 2
a1488 4
		pmap_pte_set(pde, va, 0);
	}

	simple_unlock(&pmap->pm_obj.vmobjlock);
d1490 1
a1490 1
	DPRINTF(PDB_FOLLOW|PDB_REMOVE, ("pmap_kremove: leaving\n"));
d1492 1
d1494 5
a1498 2
void *
pmap_pv_page_alloc(struct pool *pp, int flags)
d1500 1
a1500 1
	vaddr_t va;
d1502 2
a1503 2
	DPRINTF(PDB_FOLLOW|PDB_POOL,
	    ("pmap_pv_page_alloc(%p, %x)\n", pp, flags));
d1505 21
a1525 2
	if ((va = pmap_steal_memory(PAGE_SIZE, NULL, NULL)))
		return (void *)va;
d1527 2
a1528 7
	/*
		TODO
	if (list not empty) {
		get from the list;
		return (va);
	}
	*/
d1530 6
a1535 2
	DPRINTF(PDB_FOLLOW|PDB_POOL,
	    ("pmap_pv_page_alloc: uvm_km_alloc_poolpage1\n"));
d1537 13
a1549 2
	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    (flags & PR_WAITOK) ? TRUE : FALSE));
d1552 5
d1558 2
a1559 1
pmap_pv_page_free(struct pool *pp, void *v)
d1561 19
a1579 1
	vaddr_t va = (vaddr_t)v;
d1581 7
a1587 6
	DPRINTF(PDB_FOLLOW|PDB_POOL, ("pmap_pv_page_free(%p, %p)\n", pp, v));

	if (va < virtual_avail) {
		/* TODO save on list somehow */
	} else
		uvm_km_free_poolpage1(kernel_map, va);
d1589 1
@


1.32.2.8
log
@Sync the SMP branch with 3.3
@
text
@d4 1
a4 1
 * Copyright (c) 1998-2003 Michael Shalayeff
d26 2
a27 2
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF MIND,
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
d45 1
a45 1
#include <sys/extent.h>
d49 5
d55 2
a56 2

#include <dev/rndvar.h>
a82 1
/*	| PDB_INIT */
d86 1
d97 1
d105 1
a105 1
int		hppa_sid_max = HPPA_SID_MAX;
d111 7
a125 15
struct vm_page *
pmap_pagealloc(struct uvm_object *obj, voff_t off)
{
	struct vm_page *pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO);

	if (!pg) {
		/* wait and pageout */

		return (NULL);
	}

	return (pg);
}

d130 1
a130 1
static inline struct hpt_entry *
d148 1
a148 1
pmap_sdir_set(pa_space_t space, u_int32_t *pd)
d150 1
a150 1
	u_int32_t *vtop;
d157 1
a157 1
	vtop[space] = (u_int32_t)pd;
d160 1
a160 1
static __inline u_int32_t *
d163 1
a163 1
	u_int32_t *vtop;
d166 3
a168 1
	return ((u_int32_t *)vtop[space]);
d172 1
a172 1
pmap_pde_get(u_int32_t *pd, vaddr_t va)
d174 5
a178 1
	return ((pt_entry_t *)pd[va >> 22]);
d184 2
a185 8
#ifdef PMAPDEBUG
	if (ptp & PGOFSET)
		panic("pmap_pde_set, unaligned ptp 0x%x", ptp);
#endif
	DPRINTF(PDB_FOLLOW|PDB_VP,
	    ("pmap_pde_set(%p, 0x%x, 0x%x)\n", pm, va, ptp));

	pm->pm_pdir[va >> 22] = ptp;
d197 5
a201 1
	if ((pg = pmap_pagealloc(&pm->pm_obj, va)) == NULL)
d203 1
d209 1
a209 1
	pg->flags &= ~PG_BUSY;		/* never busy */
d237 2
a238 6
	DPRINTF(PDB_FOLLOW|PDB_PV,
	    ("pmap_pde_release(%p, 0x%x, %p)\n", pmap, va, ptp));

	if (pmap != pmap_kernel() && --ptp->wire_count <= 1) {
		DPRINTF(PDB_FOLLOW|PDB_PV,
		    ("pmap_pde_release: disposing ptp %p\n", ptp));
d241 2
a244 2
		if (pmap->pm_ptphint == ptp)
			pmap->pm_ptphint = TAILQ_FIRST(&pmap->pm_obj.memq);
d251 6
a256 1
	return (pde[(va >> 12) & 0x3ff]);
d268 3
d272 2
a273 5
	if (pte && pmap_initialized && pte < physical_end &&
	    hppa_trunc_page(pte) != (paddr_t)&gateway_page)
		panic("pmap_pte_set: invalid pte 0x%x", pte);
	if ((paddr_t)pde & PGOFSET)
		panic("pmap_pte_set, unaligned pde %p", pde);
d275 2
a276 13

	pde[(va >> 12) & 0x3ff] = pte;
}

void
pmap_pte_flush(struct pmap *pmap, vaddr_t va, pt_entry_t pte)
{
	if (pte & PTE_PROT(TLB_EXECUTE)) {
		ficache(pmap->pm_space, va, PAGE_SIZE);
		pitlb(pmap->pm_space, va);
	}
	fdcache(pmap->pm_space, va, PAGE_SIZE);
	pdtlb(pmap->pm_space, va);
d285 1
a285 1
		return (0);
a289 1
#ifdef DDB
d291 1
a291 1
pmap_dump_table(pa_space_t space, vaddr_t sva)
d296 1
d298 1
a298 2
		vaddr_t va, pdemask;
		u_int32_t *pd;
d301 1
a301 1
		    !(pd = pmap_sdir_get(sp)))
d304 1
a304 1
		for (pdemask = 1, va = sva? sva: 0; va < VM_MAX_KERNEL_ADDRESS;
d308 1
a308 1
				if (!(pde = pmap_pde_get(pd, va))) {
d318 1
a318 2
			printf("0x%08x-0x%08x:%b\n", va, pte & ~PAGE_MASK,
			    TLB_PROT(pte & PAGE_MASK), TLB_BITS);
a322 36
void
pmap_dump_pv(paddr_t pa)
{
	struct vm_page *pg;
	struct pv_entry *pve;

	pg = PHYS_TO_VM_PAGE(pa);
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		printf("%x:%x\n", pve->pv_pmap->pm_space, pve->pv_va);
	simple_unlock(&pg->mdpage.pvh_lock);
}
#endif

#ifdef PMAPDEBUG
int
pmap_check_alias(struct pv_entry *pve, vaddr_t va, pt_entry_t pte)
{
	int ret;

	/* check for non-equ aliased mappings */
	for (ret = 0; pve; pve = pve->pv_next) {
		pte |= pmap_vp_find(pve->pv_pmap, pve->pv_va);
		if ((va & HPPA_PGAOFF) != (pve->pv_va & HPPA_PGAOFF) &&
		    (pte & PTE_PROT(TLB_WRITE))) {
			printf("pmap_check_alias: "
			    "aliased writable mapping 0x%x:0x%x\n",
			    pve->pv_pmap->pm_space, pve->pv_va);
			ret++;
		}
	}

	return (ret);
}
#endif

d355 1
a355 1
pmap_pv_enter(struct vm_page *pg, struct pv_entry *pve, struct pmap *pm,
d359 2
a360 1
	    pg, pve, pm, va, pdep));
d364 4
a367 8
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
	pve->pv_next = pg->mdpage.pvh_list;
	pg->mdpage.pvh_list = pve;
#ifdef PMAPDEBUG
	if (pmap_check_alias(pve, va, 0))
		Debugger();
#endif
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
d371 1
a371 1
pmap_pv_remove(struct vm_page *pg, struct pmap *pmap, vaddr_t va)
d375 1
a375 3
	simple_lock(&pg->mdpage.pvh_lock);	/* lock pv_head */
	for(pv = *(pve = &pg->mdpage.pvh_list);
	    pv; pv = *(pve = &(*pve)->pv_next))
a379 1
	simple_unlock(&pg->mdpage.pvh_lock);	/* unlock, done! */
d387 1
a387 1
	extern int etext, __rodata_end, __data_start;
d389 1
a389 2
	extern paddr_t hppa_vtop;
	vaddr_t va, addr = hppa_round_page(vstart), t;
d395 1
a395 1
	int npdes;
d401 8
a408 8
	hppa_prot[UVM_PROT_NONE]  = TLB_AR_NA;
	hppa_prot[UVM_PROT_READ]  = TLB_AR_R;
	hppa_prot[UVM_PROT_WRITE] = TLB_AR_RW;
	hppa_prot[UVM_PROT_RW]    = TLB_AR_RW;
	hppa_prot[UVM_PROT_EXEC]  = TLB_AR_RX;
	hppa_prot[UVM_PROT_RX]    = TLB_AR_RX;
	hppa_prot[UVM_PROT_WX]    = TLB_AR_RWX;
	hppa_prot[UVM_PROT_RWX]   = TLB_AR_RWX;
d423 2
a424 1
	kpm->pm_pdir = (u_int32_t *)addr;
a426 1
	addr += PAGE_SIZE;
d433 4
a436 6
	hppa_vtop = addr;
	size = hppa_round_page((hppa_sid_max + 1) * 4);
	bzero((void *)addr, size);
	fdcache(HPPA_SID_KERNEL, addr, size);
	DPRINTF(PDB_INIT, ("vtop: 0x%x @@ 0x%x\n", size, addr));
	addr += size;
d469 1
a469 1
		addr += size;	/* should keep the alignment right */
d473 5
a477 3
	/* XXX PCXS needs this inserted into an IBTLB */
	/*	and can block-map the whole phys w/ another */
	t = (vaddr_t)&etext;
d480 1
a480 1
	    pmap_prot(pmap_kernel(), UVM_PROT_RX)) < 0)
d482 7
d490 1
a490 1
	    physmem = atop(t);
d492 3
a494 5
	if (&__rodata_end < &__data_start) {
		physical_steal = (vaddr_t)&__rodata_end;
		physical_end = (vaddr_t)&__data_start;
		DPRINTF(PDB_INIT, ("physpool: 0x%x @@ 0x%x\n",
		    physical_end - physical_steal, physical_steal));
d502 31
a532 27
	/* four more for the the kernel virtual */
	npdes = 4 + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
	uvm_page_physload(0, totalphysmem,
	    atop(addr) + npdes, totalphysmem, VM_FREELIST_DEFAULT);

	/* map the pdes */
	for (va = 0; npdes--; va += PDE_SIZE, addr += PAGE_SIZE) {

		/* last pde is for the start of kernel virtual */
		if (npdes == 3)
			va = SYSCALLGATE;
		/* now map the pde for the physmem */
		bzero((void *)addr, PAGE_SIZE);
		DPRINTF(PDB_INIT|PDB_VP, ("pde premap 0x%x 0x%x\n", va, addr));
		pmap_pde_set(kpm, va, addr);
		kpm->pm_stats.resident_count++; /* count PTP as resident */
	}

	/* TODO optimize/inline the kenter */
	for (va = 0; va < ptoa(totalphysmem); va += PAGE_SIZE) {
		extern struct user *proc0paddr;
		vm_prot_t prot = UVM_PROT_RW;

		if (va < (vaddr_t)&etext)
			prot = UVM_PROT_RX;
		else if (va == (vaddr_t)proc0paddr + USPACE)
			prot = UVM_PROT_NONE;
d534 20
a553 2
		pmap_kenter_pa(va, va, prot);
	}
d555 1
a555 1
	DPRINTF(PDB_INIT, ("bootstrap: mapped %p - 0x%x\n", &etext, va));
a562 2
	simple_lock_init(&pvalloc_lock);

d566 6
a571 1
	    &pool_allocator_nointr);
d579 2
d590 1
a590 1
		    PTE_PROT(TLB_GATE_PROT));
a593 7
void
pmap_virtual_space(vaddr_t *startp, vaddr_t *endp)
{
	*startp = SYSCALLGATE + PAGE_SIZE;
	*endp = VM_MAX_KERNEL_ADDRESS;
}

d608 2
d611 5
a615 8
	for (space = 1 + (arc4random() % hppa_sid_max);
	    pmap_sdir_get(space); space = (space + 1) % hppa_sid_max);

	if ((pmap->pm_pdir_pg = pmap_pagealloc(NULL, 0)) == NULL)
		panic("pmap_create: no pages");
	pmap->pm_ptphint = NULL;
	pmap->pm_pdir = (u_int32_t *)VM_PAGE_TO_PHYS(pmap->pm_pdir_pg);
	pmap_sdir_set(space, pmap->pm_pdir);
d619 5
d625 1
a625 2
	pmap->pm_stats.resident_count = 1;
	pmap->pm_stats.wired_count = 0;
d627 1
a627 1
	return (pmap);
a633 1
#ifdef DIAGNOSTIC
d635 1
a635 1
#endif
d647 1
a648 3
	while ((pg = TAILQ_FIRST(&pmap->pm_obj.memq))) {
		printf("pmap_destroy: unaccounted ptp 0x%x\n",
		    VM_PAGE_TO_PHYS(pg));
d650 2
a651 1
			panic("pmap_destroy: busy page table page");
d655 1
a655 2
#endif
	pmap_sdir_set(pmap->pm_space, 0);
d657 2
a658 1
	pmap->pm_pdir_pg = NULL;
d692 2
a693 1
	struct vm_page *pg, *ptp = NULL;
d695 1
d720 7
a726 2
		pmap_pte_flush(pmap, va, pte);
		if (wired && !(pte & PTE_PROT(TLB_WIRED)))
d728 1
a728 1
		else if (!wired && (pte & PTE_PROT(TLB_WIRED)))
d737 9
a745 4
		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
		simple_lock(&pg->mdpage.pvh_lock);
		pve = pmap_pv_remove(pg, pmap, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d758 2
a759 2

	if (pmap_initialized) {
d767 7
a773 4
		pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa));
		pmap_pv_enter(pg, pve, pmap, va, ptp);
	} else if (pve)
		pmap_pv_free(pve);
d777 1
a777 1
	pte = pa | PTE_PROT(pmap_prot(pmap, prot)) |
d785 1
a785 1
	DPRINTF(PDB_FOLLOW|PDB_ENTER, ("pmap_enter: leaving\n"));
d796 1
d799 2
a800 2
	vaddr_t pdemask;
	int batch;
d803 1
a803 1
	    ("pmap_remove(%p, 0x%x, 0x%x)\n", pmap, sva, eva));
d807 1
a807 1
	for (batch = 0, pdemask = 1; sva < eva; sva += PAGE_SIZE) {
a813 1
			batch = pdemask == sva && sva + ~PDE_MASK + 1 <= eva;
a817 5
			/* TODO measure here the speed tradeoff
			 * for flushing whole 4M vs per-page
			 * in case of non-complete pde fill
			 */
			pmap_pte_flush(pmap, sva, pte);
d822 14
a835 11
			/* iff properly accounted pde will be dropped anyway */
			if (!batch)
				pmap_pte_set(pde, sva, 0);

			if (pmap_initialized) {
				struct vm_page *pg;

				pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
				simple_lock(&pg->mdpage.pvh_lock);
				pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
				if ((pve = pmap_pv_remove(pg, pmap, sva)))
d837 1
a837 1
				simple_unlock(&pg->mdpage.pvh_lock);
a853 1
	struct vm_page *pg;
d865 1
a865 1
	for(pdemask = 1; sva < eva; sva += PAGE_SIZE) {
d876 1
a876 2
			    ("pmap_write_protect: va=0x%x pte=0x%x\n",
			    sva,  pte));
d884 5
a888 4
			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			simple_unlock(&pg->mdpage.pvh_lock);
a889 1
			pmap_pte_flush(pmap, sva, pte);
d903 1
d905 2
d910 3
a912 1
	if (pg->mdpage.pvh_list == NULL)
d914 1
d916 3
a918 5
	simple_lock(&pg->mdpage.pvh_lock);
	for (pve = pg->mdpage.pvh_list; pve; ) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		pt_entry_t *pde, pte;
d920 1
a920 1
		simple_lock(&pmap->pm_obj.vmobjlock);
d922 6
a927 3
		pde = pmap_pde_get(pmap->pm_pdir, va);
		pte = pmap_pte_get(pde, va);
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
a928 1
		pmap_pte_flush(pmap, va, pte);
d930 2
a931 2
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;
d933 1
a933 2
		pmap_pte_set(pde, va, 0);
		simple_unlock(&pmap->pm_obj.vmobjlock);
d935 1
d940 1
a940 2
	pg->mdpage.pvh_list = NULL;
	simple_unlock(&pg->mdpage.pvh_lock);
d978 1
d980 2
a981 1
	pt_entry_t res;
d986 15
a1000 17
	simple_lock(&pg->mdpage.pvh_lock);
	res = pg->mdpage.pvh_attrs = 0;
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next) {
		struct pmap *pmap = pve->pv_pmap;
		vaddr_t va = pve->pv_va;
		pt_entry_t *pde, opte, pte;

		simple_lock(&pmap->pm_obj.vmobjlock);
		if ((pde = pmap_pde_get(pmap->pm_pdir, va))) {
			opte = pte = pmap_pte_get(pde, va);
#ifdef PMAPDEBUG
			if (!pte) {
				printf("pmap_changebit: zero pte for 0x%x\n",
				    va);
				continue;
			}
#endif
a1002 2
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			res |= pmap_pvh_attrs(opte);
d1004 7
a1010 4
			if (opte != pte) {
				pmap_pte_flush(pmap, va, opte);
				pmap_pte_set(pde, va, pte);
			}
d1012 1
a1012 1
		simple_unlock(&pmap->pm_obj.vmobjlock);
d1014 2
a1015 1
	simple_unlock(&pg->mdpage.pvh_lock);
d1017 1
a1017 1
	return ((res & (clear | set)) != 0);
d1021 1
a1021 1
pmap_testbit(struct vm_page *pg, u_int bit)
d1023 1
d1026 1
d1028 1
a1028 1
	DPRINTF(PDB_FOLLOW|PDB_BITS, ("pmap_testbit(%p, %x)\n", pg, bit));
d1030 9
a1038 2
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; !(pg->mdpage.pvh_attrs & bit) && pve;
d1043 1
a1043 1
		pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
d1045 1
a1045 1
	simple_unlock(&pg->mdpage.pvh_lock);
d1047 1
a1047 1
	return ((pg->mdpage.pvh_attrs & bit) != 0);
d1074 2
a1075 1
pmap_activate(struct proc *p)
d1077 3
a1079 2
	struct pmap *pmap = p->p_vmspace->vm_map.pmap;
	struct pcb *pcb = &p->p_addr->u_pcb;;
d1081 1
a1081 3
	pcb->pcb_space = pmap->pm_space;
	pcb->pcb_uva = (vaddr_t)p->p_addr;
}
d1083 8
a1090 3
void
pmap_deactivate(struct proc *p)
{
d1092 14
a1105 1
}
d1107 2
a1108 13
static __inline void
pmap_flush_page(struct vm_page *pg, int purge)
{
	struct pv_entry *pve;

	/* purge cache for all possible mappings for the pa */
	simple_lock(&pg->mdpage.pvh_lock);
	for(pve = pg->mdpage.pvh_list; pve; pve = pve->pv_next)
		if (purge)
			pdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
		else
			fdcache(pve->pv_pmap->pm_space, pve->pv_va, PAGE_SIZE);
	simple_unlock(&pg->mdpage.pvh_lock);
d1112 7
a1118 3
pmap_zero_page(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1120 1
a1120 1
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_zero_page(%x)\n", pa));
d1122 2
a1123 4
	pmap_flush_page(pg, 1);
	bzero((void *)pa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, pa, PAGE_SIZE);
}
d1125 21
a1145 6
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t spa = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dpa = VM_PAGE_TO_PHYS(dstpg);
	DPRINTF(PDB_FOLLOW|PDB_PHYS, ("pmap_copy_page(%x, %x)\n", spa, dpa));
d1147 2
a1148 5
	pmap_flush_page(srcpg, 0);
	pmap_flush_page(dstpg, 1);
	bcopy((void *)spa, (void *)dpa, PAGE_SIZE);
	pdcache(HPPA_SID_KERNEL, spa, PAGE_SIZE);
	fdcache(HPPA_SID_KERNEL, dpa, PAGE_SIZE);
d1166 1
a1166 1
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%x", va);
d1173 2
a1174 3
	pte = pa | PTE_PROT(TLB_WIRED | TLB_REFTRAP |
	    pmap_prot(pmap_kernel(), prot));
	if (pa >= HPPA_IOSPACE)
a1179 13
#ifdef PMAPDEBUG
	{
		if (pmap_initialized) {
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			if (pmap_check_alias(pg->mdpage.pvh_list, va, pte))
				Debugger();
			simple_unlock(&pg->mdpage.pvh_lock);
		}
	}
#endif
a1187 5
#ifdef PMAPDEBUG
	extern u_int totalphysmem;
#endif
	struct pv_entry *pve;
	vaddr_t eva, pdemask;
d1189 1
a1192 6
#ifdef PMAPDEBUG
	if (va < ptoa(totalphysmem)) {
		printf("pmap_kremove(%x, %x): unmapping physmem\n", va, size);
		return;
	}
#endif
d1196 1
a1196 1
	for (pdemask = 1, eva = va + size; va < eva; va += PAGE_SIZE) {
d1211 6
a1216 1
		pmap_pte_flush(pmap_kernel(), va, pte);
a1217 11
		if (pmap_initialized) {
			struct vm_page *pg;

			pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte));
			simple_lock(&pg->mdpage.pvh_lock);
			pg->mdpage.pvh_attrs |= pmap_pvh_attrs(pte);
			/* just in case we have enter/kenter mismatch */
			if ((pve = pmap_pv_remove(pg, pmap_kernel(), va)))
				pmap_pv_free(pve);
			simple_unlock(&pg->mdpage.pvh_lock);
		}
d1223 39
@


1.32.2.9
log
@Sync the SMP branch to -current.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32.2.8 2003/03/27 23:26:53 niklas Exp $	*/
d100 1
a103 1
int 		pmap_initialized;
d116 5
a120 1
	struct vm_page *pg;
d122 2
a123 3
	if ((pg = uvm_pagealloc(obj, off, NULL,
	    UVM_PGA_USERESERVE | UVM_PGA_ZERO)) == NULL)
		printf("pmap_pagealloc fail\n");
d265 3
d534 2
a535 1
	kpm->pm_stats.wired_count = kpm->pm_stats.resident_count = atop(t);
d557 1
a557 1
		/* last four pdes are for the kernel virtual */
a566 2
	physmem = atop(addr);

d595 1
a595 1
	pmap_initialized = 1;
d773 1
a773 1
	if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pa)))) {
d781 1
a808 1
	struct vm_page *pg;
d842 2
a843 2
			if (pmap_initialized &&
			    (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d845 1
d926 1
a926 2
	for (pve = pg->mdpage.pvh_list; pve;
	     pve = (ppve = pve)->pv_next, pmap_pv_free(ppve)) {
d944 4
d1076 1
a1076 1
	struct pcb *pcb = &p->p_addr->u_pcb;
d1161 2
a1162 3
		struct vm_page *pg;

		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d1164 1
a1185 1
	struct vm_page *pg;
d1215 2
a1216 1
		if (pmap_initialized && (pg = PHYS_TO_VM_PAGE(PTE_PAGE(pte)))) {
d1218 1
@


1.32.2.10
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d94 3
a96 2
int		pmap_hptsize = 16 * PAGE_SIZE;	/* patchable */
vaddr_t		pmap_hpt;
d125 1
a125 1
#ifdef USE_HPT
d129 2
a130 9
static __inline struct vp_entry *
pmap_hash(struct pmap *pmap, vaddr_t va)
{
	return (struct vp_entry *)(pmap_hpt +
	    (((va >> 8) ^ (pmap->pm_space << 9)) & (pmap_hptsize - 1)));
}

static __inline u_int32_t
pmap_vtag(struct pmap *pmap, vaddr_t va)
d132 11
a142 2
	return (0x80000000 | (pmap->pm_space & 0xffff) |
	    ((va >> 1) & 0x7fff0000));
a277 8
#ifdef USE_HPT
	if (pmap_hpt) {
		struct vp_entry *hpt;
		hpt = pmap_hash(pmap, va);
		if (hpt->vp_tag == pmap_vtag(pmap, va))
			hpt->vp_tag = 0xffff;
	}
#endif
d306 2
a307 2
		for (pdemask = 1, va = sva ? sva : 0;
		    va < VM_MAX_KERNEL_ADDRESS; va += PAGE_SIZE) {
d314 1
a314 1
				printf("%x:%8p:\n", sp, pde);
d320 1
a320 1
			printf("0x%08lx-0x%08x:%b\n", va, pte & ~PAGE_MASK,
d335 1
a335 1
		printf("%x:%lx\n", pve->pv_pmap->pm_space, pve->pv_va);
d437 3
d441 1
a441 1
	int npdes, nkpdes;
d490 14
a503 9
#ifdef USE_HPT
	if (pmap_hptsize) {
		struct vp_entry *hptp;
		int i, error;

		/* must be aligned to the size XXX */
		if (addr & (pmap_hptsize - 1))
			addr += pmap_hptsize;
		addr &= ~(pmap_hptsize - 1);
d505 1
a505 5
		bzero((void *)addr, pmap_hptsize);
		for (hptp = (struct vp_entry *)addr, i = pmap_hptsize / 16; i--;)
			hptp[i].vp_tag = 0xffff;
		pmap_hpt = addr;
		addr += pmap_hptsize;
d507 6
a512 2
		DPRINTF(PDB_INIT, ("hpt_table: 0x%x @@ %p\n",
		    pmap_hptsize, addr));
d514 4
a517 8
		if ((error = (cpu_hpt_init)(pmap_hpt, pmap_hptsize)) < 0) {
			printf("WARNING: HPT init error %d -- DISABLED\n",
			    error);
			pmap_hpt = 0;
		} else
			DPRINTF(PDB_INIT,
			    ("HPT: installed for %d entries @@ 0x%x\n",
			    pmap_hptsize / sizeof(struct vp_entry), addr));
d519 1
a519 1
#endif
d542 2
a543 5
	/* takes about 16 per gig of initial kmem ... */
	nkpdes = totalphysmem >> 14;
	if (nkpdes < 4)
		nkpdes = 4;	/* ... but no less than four */
	npdes = nkpdes + (totalphysmem + btoc(PDE_SIZE) - 1) / btoc(PDE_SIZE);
d551 1
a551 1
		if (npdes == nkpdes - 1)
d669 6
a674 36
		pt_entry_t *pde, *epde;
		struct vm_page *sheep;
		struct pv_entry *haggis;

		if (pg == pmap->pm_pdir_pg)
			continue;

#ifdef PMAPDEBUG
		printf("pmap_destroy(%p): stray ptp 0x%lx w/ %d ents:",
		    pmap, VM_PAGE_TO_PHYS(pg), pg->wire_count - 1);
#endif

		pde = (pt_entry_t *)VM_PAGE_TO_PHYS(pg);
		epde = (pt_entry_t *)(VM_PAGE_TO_PHYS(pg) + PAGE_SIZE);
		for (; pde < epde; pde++) {
			if (*pde == 0)
				continue;

			sheep = PHYS_TO_VM_PAGE(PTE_PAGE(*pde));
			for (haggis = sheep->mdpage.pvh_list; haggis != NULL; )
				if (haggis->pv_pmap == pmap) {
#ifdef PMAPDEBUG
					printf(" 0x%x", haggis->pv_va);
#endif
					pmap_remove(pmap, haggis->pv_va,
					    haggis->pv_va + PAGE_SIZE);

					/* exploit the sacred knowledge
					   of lambeous ozzmosis */
					haggis = sheep->mdpage.pvh_list;
				} else
					haggis = haggis->pv_next;
		}
#ifdef PMAPDEBUG
		printf("\n");
#endif
d873 1
a873 1
	for (pdemask = 1; sva < eva; sva += PAGE_SIZE) {
d972 1
a972 1
		panic("pmap_unwire: invalid va 0x%lx", va);
d1135 1
a1135 1
		panic("pmap_kenter_pa: cannot allocate pde for va=0x%lx", va);
d1138 1
a1138 1
		panic("pmap_kenter_pa: 0x%lx is already mapped %p:0x%x",
@


1.32.2.11
log
@Merge with the trunk
@
text
@d4 1
a4 1
 * Copyright (c) 1998-2004 Michael Shalayeff
d15 5
d24 7
a30 8
 * IN NO EVENT SHALL THE AUTHOR OR HIS RELATIVES BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF MIND, USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
 * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 * THE POSSIBILITY OF SUCH DAMAGE.
a105 28
/*
 * workaround until the uvm_km_getpage can be used this early.
 */
void *hppa_pool_page_alloc(struct pool *, int);
void hppa_pool_page_free(struct pool *, void *);

void *
hppa_pool_page_alloc(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object,
	    waitok)); 
}

void
hppa_pool_page_free(struct pool *pp, void *v)
{

	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
}

struct pool_allocator hppa_pool_allocator = {
	hppa_pool_page_alloc, hppa_pool_page_free, 0,
};



d144 1
a144 1
pmap_sdir_set(pa_space_t space, volatile u_int32_t *pd)
d146 1
a146 1
	volatile u_int32_t *vtop;
d165 2
a166 2
static __inline volatile pt_entry_t *
pmap_pde_get(volatile u_int32_t *pd, vaddr_t va)
d211 1
a211 1
pmap_pde_ptp(struct pmap *pm, volatile pt_entry_t *pde)
d236 2
a239 6
		ptp->wire_count = 0;
#ifdef DIAGNOSTIC
		if (ptp->flags & PG_BUSY)
			panic("pmap_destroy: busy page table page");
#endif
		uvm_pagefree(ptp);
d244 1
a244 1
pmap_pte_get(volatile pt_entry_t *pde, vaddr_t va)
d250 1
a250 1
pmap_pte_set(volatile pt_entry_t *pde, vaddr_t va, pt_entry_t pte)
d288 1
a288 1
	volatile pt_entry_t *pde;
d303 1
a303 2
		volatile pt_entry_t *pde;
		pt_entry_t pte;
d407 1
d414 1
d592 1
a592 1
	    &hppa_pool_allocator);
d602 1
a602 1
		volatile pt_entry_t *pde;
a626 1

d746 1
a746 2
	volatile pt_entry_t *pde;
	pt_entry_t pte;
d759 1
a759 2
		if (flags & PMAP_CANFAIL) {
			simple_unlock(&pmap->pm_obj.vmobjlock);
a760 1
		}
a798 1
		simple_lock(&pg->mdpage.pvh_lock);
d801 1
a804 1
				simple_unlock(&pg->mdpage.pvh_lock);
a812 1
	simple_unlock(&pg->mdpage.pvh_lock);
d836 1
a836 2
	volatile pt_entry_t *pde;
	pt_entry_t pte;
d896 1
a896 2
	volatile pt_entry_t *pde;
	pt_entry_t pte;
d958 1
a958 2
		volatile pt_entry_t *pde;
		pt_entry_t pte;
d986 1
a986 2
	volatile pt_entry_t *pde;
	pt_entry_t pte = 0;
d1024 1
a1024 2
		volatile pt_entry_t *pde;
		pt_entry_t opte, pte;
d1160 1
a1160 2
	volatile pt_entry_t *pde;
	pt_entry_t pte;
d1182 2
a1196 2
	simple_unlock(&pmap->pm_obj.vmobjlock);

d1210 1
a1210 2
	volatile pt_entry_t *pde;
	pt_entry_t pte;
@


1.32.2.12
log
@sync with head, make i386 __HAVE_CPUINFO
@
text
@d102 28
d619 1
a619 1
	    NULL);
@


1.31
log
@let pmap_enter_pv return a created pv, which simplifies futher pv operation
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 1999/12/12 17:45:23 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998,1999 Michael Shalayeff
d33 18
a50 18
 * Copyright 1996 1995 by Open Software Foundation, Inc.   
 *              All Rights Reserved 
 *  
 * Permission to use, copy, modify, and distribute this software and 
 * its documentation for any purpose and without fee is hereby granted, 
 * provided that the above copyright notice appears in all copies and 
 * that both the copyright notice and this permission notice appear in 
 * supporting documentation. 
 *  
 * OSF DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE 
 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS 
 * FOR A PARTICULAR PURPOSE. 
 *  
 * IN NO EVENT SHALL OSF BE LIABLE FOR ANY SPECIAL, INDIRECT, OR 
 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM 
 * LOSS OF USE, DATA OR PROFITS, WHETHER IN ACTION OF CONTRACT, 
 * NEGLIGENCE, OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION 
 * WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. 
d247 1
a247 1
		       space, va, pv, hpt, pvp);
d451 1
a451 1
		       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
d676 1
a676 1
       	simple_lock_init(&kernel_pmap->pmap_lock);
d696 1
a696 1
		       (struct pv_page *)addr - pvp);
d743 1
a743 1
			       (totalphysmem - atop(virtual_avail)));
d768 1
a768 1
vaddr_t 
d789 1
a789 1
			       size, virtual_steal, size, virtual_avail);
d839 1
a839 1
        /*
d908 2
a909 2
	/* 
	 * If there is a pmap in the pmap free list, reuse it. 
d929 1
a929 1
/* 
d960 1
a960 1
		/* 
d998 1
a998 1
		       prot, wired? "" : "un");
d1001 2
a1002 2
        if (!pmap)
                return;
d1194 1
a1194 1
 *	changes the protection on all virtual addresses v in the 
d1262 1
a1262 1
 * Change the wiring for a given virtual page. This routine currently is 
d1277 1
a1277 1
		       pmap, va, wired? "": "un");
d1301 1
a1301 1
 *	returns the physical address corrsponding to the 
d1326 1
a1326 1
 * Zeros the specified page. 
d1440 1
a1440 1
 *	returns TRUE if the given physical page has been modified 
d1471 1
a1471 1
 *	independant physical page.  
d1508 1
a1508 1
 *	returns TRUE if the given physical page has been referenced 
d1580 1
a1580 1
		      tlbbtop(pa), pmap_find_pv(pa));
@


1.30
log
@hide debug printf behind #ifdef PMAPDEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 1999/12/12 03:16:26 mickey Exp $	*/
d432 1
a432 1
static __inline void
d440 1
a440 1
		return;
d491 2
d1025 1
a1025 1
		pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv);
d1027 3
a1029 2
	}
	else {
d1039 1
a1039 1
			pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv);
d1048 3
a1051 1
		pv = ppv;
a1055 5

	/*
	 * Determine the protection information for this mapping.
	 */
	/* XXX tlbprot |= (pv->pv_tlbprot & ~(TLB_AR_MASK|TLB_PID_MASK)); */
@


1.29
log
@do homebrew pmap_changebit pmap_new way
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 1999/11/25 18:35:21 mickey Exp $	*/
d1546 4
a1549 1
printf("pmap_changebit(%p[%x], %x, %x)\n", pg, pa, set, reset);
@


1.28
log
@clean some debug stuff
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 1999/11/25 17:39:46 mickey Exp $	*/
d1538 2
a1539 2
pmap_changebit(pa, set, reset)
	paddr_t pa;
d1543 1
d1545 2
@


1.27
log
@fix check for a pv list emptieness returned by pmap_find_pv
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 1999/11/25 17:34:32 mickey Exp $	*/
d599 2
a600 1
		printf("pmap_clear_pv: %x:%x\n", pv->pv_space, pv->pv_va);
a1332 1
	/* register int psw; */
a1341 1
	/* rsm(PSW_I,psw); */
d1344 1
a1344 1
			       : "=r" (pa) : "0" (pa) : "memory");
a1353 1
	/* mtsm(psw); */
d1374 1
a1374 1
	/*if (1 && pmapdebug & PDB_FOLLOW)*/
@


1.26
log
@remove dead pmap_collect_pv()
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.25 1999/11/22 05:08:36 mickey Exp $	*/
d591 1
a591 1
	if (!(pv = pmap_find_pv(pa)))
d598 3
d1148 6
a1153 1
		for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next) {
d1424 1
a1424 1
	if (!(pv = pmap_find_pv(pa))) {
d1458 6
a1463 1
	for (pv = pmap_find_pv(pa); pv && !(pv->pv_tlbprot & TLB_DIRTY);)
d1527 6
a1532 1
	for (pv = pmap_find_pv(pa); pv && !(pv->pv_tlbprot & TLB_REF);)
d1547 5
a1551 1
	pv = pmap_find_pv(pa);
a1552 1
	s = splimp();
@


1.25
log
@better pmap_enter() towards correctness.
fix querying routines towards realty.
also, some comments on routines.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 1999/11/19 18:47:41 mickey Exp $	*/
a425 60

#if 0
/* probably not needed anymore */
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
		if (ph->pv_pmap == 0)
			continue;
		s = splimp();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *) trunc_page(pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		FREE((vaddr_t) pvp, M_VMPVENT);
	}
}
#endif
@


1.24
log
@pmap_map no more
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 1999/11/14 02:33:44 mickey Exp $	*/
d121 9
a129 2
 */
/*
a203 15
/* pv_entry operation */
static __inline void pmap_insert_pvp __P((struct pv_page *, u_int));
static __inline struct pv_entry *pmap_alloc_pv __P((void));
static __inline void pmap_free_pv __P((struct pv_entry *));
static __inline struct pv_entry *pmap_find_pv __P((vaddr_t));
static __inline void pmap_enter_pv __P((pmap_t, vaddr_t, u_int, u_int,
					struct pv_entry *));
static __inline void pmap_remove_pv __P((pmap_t, vaddr_t, struct pv_entry *));
static __inline void pmap_clear_pv __P((vaddr_t, struct pv_entry *));

/* hpt_table operation */
static __inline void pmap_enter_va __P((pa_space_t, paddr_t,struct pv_entry *));
static __inline struct pv_entry *pmap_find_va __P((pa_space_t, paddr_t));
static __inline void pmap_remove_va __P((struct pv_entry *));
static __inline void pmap_clear_va __P((pa_space_t, paddr_t));
d216 1
a216 1
static __inline__ struct hpt_entry *
d219 1
a219 1
	register struct hpt_entry *hpt;
d233 4
d238 1
a238 4
pmap_enter_va(space, va, pv)
	pa_space_t space;
	vaddr_t va;
	struct pv_entry *pv;
d240 1
a240 1
	register struct hpt_entry *hpt = pmap_hash(space, va);
d242 1
a242 1
	register struct pv_entry *pvp =	hpt->hpt_entry;
d245 1
a245 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_VA)
d261 4
d266 1
a266 3
pmap_find_va(space, va)
	pa_space_t space;
	vaddr_t va;
d268 1
a268 1
	register struct pv_entry *pvp =	pmap_hash(space, va)->hpt_entry;
d271 1
a271 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_VA)
d281 18
d300 1
a300 2
pmap_remove_va(pv)
	struct pv_entry *pv;
d302 2
a303 2
	register struct hpt_entry *hpt = pmap_hash(pv->pv_space, pv->pv_va);
	register struct pv_entry **pvp = (struct pv_entry **)&hpt->hpt_entry;
d306 1
a306 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_VA)
d310 2
d330 4
a333 3
 * Clear the HPT table entry for the corresponding space/offset to reflect
 * the fact that we have possibly changed the mapping, and need to pick
 * up new values from the mapping structure on the next access.
d336 1
a336 3
pmap_clear_va(space, va)
	pa_space_t space;
	vaddr_t va;
d338 1
a338 12
	register struct hpt_entry *hpt = pmap_hash(space, va);

	hpt->hpt_valid = 0;
	hpt->hpt_space = -1;
}

static __inline void
pmap_insert_pvp(pvp, flag)
	register struct pv_page *pvp;
	u_int flag;
{
	register struct pv_entry *pv;
d349 6
d356 1
a356 1
pmap_alloc_pv()
d362 1
a362 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_PV)
d367 1
d372 4
a375 1
		pmap_insert_pvp(pvp, 0);
d392 6
d399 1
a399 2
pmap_free_pv(pv)
	struct pv_entry *pv;
d401 1
a401 1
	register struct pv_page *pvp;
d404 1
a404 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_PV)
d428 1
d487 5
d493 2
a494 5
pmap_enter_pv(pmap, va, tlbprot, tlbpage, pv)
	register pmap_t pmap;
	vaddr_t va;
	u_int tlbprot, tlbpage;
	register struct pv_entry *pv;
d496 1
a496 1
	register struct pv_entry *npv, *hpv;
d502 4
d509 1
a509 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_PV)
d553 5
d559 1
a559 4
pmap_remove_pv(pmap, va, pv)
	register pmap_t pmap;
	vaddr_t va;
	struct pv_entry *pv;
d561 1
a561 1
	register struct pv_entry *npv;
d565 1
a565 1
	if (pmapdebug & PDB_FOLLOW && pmapdebug & PDB_PV)
a583 2
	pmap_clear_va(pv->pv_space, pv->pv_va);

d607 5
d618 19
d641 1
a641 3
pmap_clear_pv(pa, cpv)
	paddr_t pa;
	struct pv_entry *cpv;
d643 1
a643 1
	register struct pv_entry *pv;
a671 16
static __inline struct pv_entry *
pmap_find_pv(pa)
	paddr_t pa;
{
	register int bank;
	int off;
	if ((bank = vm_physseg_find(atop(pa), &off)) != -1) {
#ifdef PMAPDEBUG
		if (pmapdebug & PDB_PV)
			printf("pmap_find_pv(%x):  %d:%d\n", pa, bank, off);
#endif
		return &vm_physmem[bank].pmseg.pvent[off];
	} else
		panic("pmap_find_pv: mapping unmappable");
}

d816 6
d864 1
a864 1
pmap_init(void)
d866 1
a866 1
	register struct pv_page *pvp;
d1046 1
d1064 6
a1069 1
	tlbprot = pmap_prot(pmap, prot) | pmap->pmap_pid;
d1090 1
a1090 1
			tlbprot = ppv->pv_tlbprot;
d1097 3
a1099 1
				printf("pmap_enter: changing protection\n");
d1103 2
a1104 4
		/*
		 * Flush the current TLB entry to force
		 * a fault and reload.
		 */
d1111 1
a1111 1
	tlbprot |= (pv->pv_tlbprot & ~(TLB_AR_MASK|TLB_PID_MASK));
d1116 2
a1117 1
	waswired = tlbprot & TLB_WIRED;
d1119 1
a1119 1
		tlbprot |= TLB_WIRED;
d1122 1
a1122 1
		tlbprot &= ~TLB_WIRED;
d1125 1
a1125 1
	pv->pv_tlbprot = tlbprot;
a1423 1
	register int psw;
d1425 1
d1428 1
a1428 1
	if (pmapdebug & PDB_FOLLOW)
d1435 1
a1435 1
	rsm(PSW_I,psw);
d1440 1
a1440 1
			       : "=r" (spa), "=r" (dpa):: "r23", "memory");
d1447 2
a1448 3
				       : "=r" (spa), "=r" (dpa)
				       : "i" (PSW_D), "r" (-4)
				       : "memory");
d1452 1
a1452 1
	mtsm(psw);
d1468 1
d1475 3
a1477 4
	if ((pv = pmap_find_pv(pa))) {
		pv->pv_tlbprot &= ~TLB_DIRTY;
		return TRUE;
	} else
d1479 11
d1503 1
d1509 6
a1514 2
	pv = pmap_find_pv(pa);
	return pv != NULL && (pv->pv_tlbprot & TLB_DIRTY);
d1661 3
@


1.23
log
@fix pmap_create() so it works for user mappings.
fix syscall page mapping.
small fixes to debug printfs.
some knf in rare places.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 1999/10/27 01:22:53 mickey Exp $	*/
a1133 31

#if FORCE_MAP_KERNEL
/*
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 */
vaddr_t
pmap_map(va, spa, epa, prot, wired)
	vaddr_t va;
	paddr_t spa;
	paddr_t epa;
	vm_prot_t prot;
	int wired;
{

#ifdef PMAPDEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%x, %x, %x, %x)\n", va, spa, epa, prot);
#endif

	while (spa < epa) {
		pmap_enter(pmap_kernel(), va, spa, prot, wired, 0);
		va += NBPG;
		spa += NBPG;
	}
	return va;
}
#endif /* FORCE_MAP_KERNEL */
@


1.22
log
@make kvtop() work on page unaligned addresses
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 1999/09/18 20:05:54 mickey Exp $	*/
d842 2
a843 2
	       (virtual_avail - virtual_steal) / sizeof(struct pv_page),
	       virtual_steal);
d845 2
a846 2
	while ((pvp = (struct pv_page *)pmap_steal_memory(
			sizeof(*pvp), NULL, NULL)))
a851 1

d855 2
d863 1
a863 4
	tlbbtop((paddr_t)&gateway_page),
	pmap_find_va(HPPA_SID_KERNEL, SYSCALLGATE));

	pmap_initialized = TRUE;
d878 1
a878 1
		printf("pmap_pinit(%p)\n", pmap);
d896 1
a896 1
			printf("Warning no more pmap ids\n");
a897 1
		pmap->pmap_pid = pid;
d902 2
a903 1
	pmap->pmap_space = pmap->pmap_pid >> 1;
d911 1
a911 1
 * pmap_create(size)
d914 1
a914 6
 *
 * If the size specified for the map is zero, the map is an actual physical
 * map, and may be referenced by the hardware.
 *
 * If the size specified is non-zero, the map will be used in software 
 * only, and is bounded by that size.
d1063 1
a1063 2
		pdtlb(space, va);
		pitlb(space, va);
d1372 1
a1372 1
	register int psw;
d1382 1
a1382 1
	rsm(PSW_I,psw);
d1385 1
a1385 1
			       : "=r" (pa):: "memory");
d1391 1
a1391 2
				       : "=r" (pa): "i" (PSW_D), "r" (-4)
				       : "memory");
d1395 1
a1395 1
	mtsm(psw);
d1646 1
a1646 1
			db_printf("hpt@@%p: %x{%sv=%x:%x}, prot=%x, pa=%x\n",
d1649 1
a1649 1
				  hpt->hpt_tlbprot, hpt->hpt_tlbpage);
d1651 1
a1651 1
				db_printf("    pv={%p,%x:%x,%x,%x}->%p\n",
d1653 2
a1654 2
					  pv->pv_tlbprot, pv->pv_tlbpage,
					  pv->pv_hash);
@


1.21
log
@PMAP_NEW, also kill last vm_{size,offset}_t's
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 1999/09/03 18:00:47 art Exp $	*/
d1364 1
a1364 1
	if (!(pv = pmap_find_va(pmap_sid(pmap, va), va)))
@


1.20
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 1999/08/03 15:35:23 mickey Exp $	*/
d659 1
a659 1
	vm_size_t size;
d788 1
a788 1
	vm_size_t size;
d924 1
a924 2
pmap_create(size)
	vm_size_t size;
d930 1
a930 1
		printf("pmap_create(%d)\n", size);
a932 6
	/*
	 * A software use-only map doesn't even need a pmap structure.
	 */
	if (size) 
		return(NULL);

d1180 2
a1181 2
pmap_page_protect(pa, prot)
	paddr_t pa;
d1187 1
d1275 2
a1276 1
	
a1282 1
			tlbprot = pmap_prot(pmap, prot);
d1295 1
a1296 1
			pitlb(space, pv->pv_va);
d1460 3
a1462 3
void
pmap_clear_modify(pa)
	paddr_t pa;
d1465 1
d1472 1
a1472 1
	if ((pv = pmap_find_pv(pa)))
d1474 3
d1485 2
a1486 2
pmap_is_modified(pa)
	paddr_t pa;
d1488 8
a1495 1
	register struct pv_entry *pv = pmap_find_pv(pa);
d1507 3
a1509 3
void
pmap_clear_reference(pa)
	paddr_t pa;
d1512 1
d1521 6
a1526 1
	for (pv = pmap_find_pv(pa); pv; pv = pv->pv_next) {
a1532 1
	pv->pv_tlbprot &= ~TLB_REF;
d1534 1
d1543 2
a1544 2
pmap_is_referenced(pa)
	paddr_t pa;
d1547 1
d1581 46
@


1.19
log
@flush caches and tlb on mapping removal
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 1999/08/03 00:53:30 mickey Exp $	*/
d1005 1
a1005 1
 * pmap_enter(pmap, va, pa, prot, wired)
d1012 1
a1012 1
pmap_enter(pmap, va, pa, prot, wired)
d1018 1
d1173 1
a1173 1
		pmap_enter(pmap_kernel(), va, spa, prot, wired);
@


1.18
log
@use ddb's printf in pmap_hptdump
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 1999/07/21 07:37:20 mickey Exp $	*/
d548 11
@


1.17
log
@rewrite the tlb miss handler and tlb dirty handler.
optimize for HVT usage (when compiled for proper cpu type).
drop hpt_* global variables, use info from control registers instead.
there are still ways to improve the tlb handlers tho.
machdep also prints cache and tlb coherence states (not related to the above).
You eat greenish muffin. --More-- You write real code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 1999/06/30 18:59:06 mickey Exp $	*/
d166 1
a166 1
	| PDB_FOLLOW
d879 1
a879 1
		if (pid_counter <= HPPA_MAX_PID) {
d887 1
a887 1
			printf("Warning no more pmap id\n");
a888 4
		/* 
		 * Initialize the sids and pid. This is a user pmap, so 
		 * sr4, sr5, and sr6 are identical. sr7 is always KERNEL_SID.
		 */
d1571 2
a1572 1
#ifdef PMAPDEBUG
d1582 1
a1582 1
	printf("HPT dump %p-%p:\n", hpt, ehpt);
d1585 4
a1588 4
			printf("hpt@@%p: %x{%svalid,v=%x:%x}, prot=%x, pa=%x\n",
			       hpt, *(u_int *)hpt, (hpt->hpt_valid?"":"in"),
			       hpt->hpt_space, hpt->hpt_vpn,
			       hpt->hpt_tlbprot, hpt->hpt_tlbpage);
d1590 4
a1593 4
				printf("    pv={%p,%x:%x,%x,%x}->%p\n",
				       pv->pv_pmap, pv->pv_space, pv->pv_va,
				       pv->pv_tlbprot, pv->pv_tlbpage,
				       pv->pv_hash);
@


1.16
log
@do not allocate pv_entries for the block-mapped area
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 1999/06/11 15:51:35 mickey Exp $	*/
d4 1
a4 1
 * Copyright (c) 1998 Michael Shalayeff
d122 6
d169 1
a169 1
/* 	| PDB_INIT */
a183 7
/*
 * Hashed (Hardware) Page Table, for use as a software cache, or as a
 * hardware accessible cache on machines with hardware TLB walkers.
 */
struct hpt_entry *hpt_table;
u_int hpt_hashsize;

d221 20
d247 1
a247 1
	register struct hpt_entry *hpt = &hpt_table[pmap_hash(space, va)];
d273 1
a273 2
	register struct pv_entry *pvp =
		hpt_table[pmap_hash(space, va)].hpt_entry;
d290 1
a290 2
	register struct hpt_entry *hpt =
		&hpt_table[pmap_hash(pv->pv_space, pv->pv_va)];
d325 1
a325 1
	register int hash = pmap_hash(space, va);
d327 2
a328 2
	hpt_table[hash].hpt_valid = 0;
	hpt_table[hash].hpt_space = -1;
d650 1
d700 1
a700 1
	addr = *vstart;
d702 1
a702 1
	pvp = (struct pv_page *)cache_align(addr);
d704 2
a705 3
	size = sizeof(struct hpt_entry) * hpt_hashsize;
	addr = (addr + size-1) & ~(size-1); /* !!! hpt_hashsize is 2^n */
	TAILQ_INIT(&pv_page_freelist);
d711 1
d716 8
a723 8
	hpt_table = (struct hpt_entry *) addr;
	for (i = 0; i < hpt_hashsize; i++) {
		hpt_table[i].hpt_valid   = 0;
		hpt_table[i].hpt_vpn     = 0;
		hpt_table[i].hpt_space   = -1;
		hpt_table[i].hpt_tlbpage = 0;
		hpt_table[i].hpt_tlbprot = 0;
		hpt_table[i].hpt_entry   = NULL;
d727 1
a727 1
		printf("hpt_table: 0x%x @@ %p\n", size, addr);
a728 1
	addr += size;
d731 2
a732 1
	mtctl(hpt_table, CR_VTOP);
d1579 1
a1579 1
	register struct hpt_entry *hpt;
d1582 5
a1586 2
	printf("HPT dump\n");
	for (hpt = hpt_table; hpt < &hpt_table[hpt_hashsize]; hpt++)
d1593 1
a1593 1
				printf("    pv={%p,%x:%x,%x,%x}\n",
d1595 2
a1596 1
				       pv->pv_tlbprot, pv->pv_tlbpage);
@


1.15
log
@fix pmap_remove_va() pasto, so it actually remove mappings (:
map syscall gateway page in pmap_init()
insert some debug printfs
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.14 1999/04/20 20:50:08 mickey Exp $	*/
d716 6
d723 1
a723 1
	*vstart = hppa_round_page(addr + totalphysmem *
@


1.14
log
@no longer blockmap the whole io space
fix bootstrapping
uvm
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1999/02/01 20:29:50 mickey Exp $	*/
d161 3
a163 1
/*	| PDB_INIT */
d165 1
d274 1
a274 1
	register struct pv_entry **pvp = hpt->hpt_entry;
d278 1
a278 1
		printf("pmap_remove_va(%p)\n", pv);
a633 1

a683 1

d821 9
d1102 7
a1108 1
		if ((pv = pmap_find_va(space, sva))) {
@


1.13
log
@fix several typos
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/01/20 19:39:51 mickey Exp $	*/
d118 3
d130 1
a130 4
#include <vm/vm.h>
#include <vm/vm_kern.h>
#include <vm/vm_page.h>
#include <vm/pmap.h>
d159 3
a161 3
int pmapdebug =
	  PDB_FOLLOW
	| PDB_INIT
d163 1
a163 1
	| PDB_STEAL
d167 1
a167 1
vm_offset_t	virtual_steal, virtual_avail, virtual_end;
d199 2
a200 2
static __inline struct pv_entry *pmap_find_pv __P((vm_offset_t));
static __inline void pmap_enter_pv __P((pmap_t, vm_offset_t, u_int, u_int,
d202 2
a203 3
static __inline void pmap_remove_pv __P((pmap_t, vm_offset_t,
					 struct pv_entry *));
static __inline void pmap_clear_pv __P((vm_offset_t, struct pv_entry *));
d206 2
a207 3
static __inline void pmap_enter_va __P((pa_space_t, vm_offset_t,
					struct pv_entry *));
static __inline struct pv_entry *pmap_find_va __P((pa_space_t, vm_offset_t));
d209 1
a209 1
static __inline void pmap_clear_va __P((pa_space_t, vm_offset_t));
a212 4
#if FORCE_MAP_KERNEL
vm_offset_t pmap_map __P((vm_offset_t va, vm_offset_t spa, vm_offset_t epa,
	vm_prot_t prot, int wired));
#endif
d222 1
a222 1
	vm_offset_t va;
d249 1
a249 1
	vm_offset_t va;
d303 1
a303 1
	vm_offset_t va;
d384 1
a384 1
			FREE((vm_offset_t) pvp, M_VMPVENT);
d444 1
a444 1
		FREE((vm_offset_t) pvp, M_VMPVENT);
d452 1
a452 1
	vm_offset_t va;
d512 1
a512 1
	vm_offset_t va;
d563 1
a563 1
	vm_offset_t pa;
d597 1
a597 1
	vm_offset_t pa;
d622 2
a623 2
	vm_offset_t *vstart;
	vm_offset_t *vend;
d625 1
d627 1
a627 1
	vm_offset_t addr, p;
d637 1
a637 1
	vm_set_page_size();
d686 1
a686 1
	addr = (addr + size-1) & ~(size-1);
a705 1
	addr = (vm_offset_t)&hpt_table[hpt_hashsize];
d708 1
a708 2
		printf("hpt_table: 0x%x @@ %p\n", size, hpt_table);
	
d710 1
d715 15
a729 1
	addr = hppa_round_page(addr);
a732 1

d738 1
a738 19
	/* map the kernel space, which will give us virtual_avail */
	*vstart = hppa_round_page(addr + size + totalphysmem * 64);
	if (btlb_insert(kernel_pmap->pmap_space, 0, 0, vstart,
			kernel_pmap->pmap_pid |
			pmap_prot(kernel_pmap, VM_PROT_ALL)) < 0)
		panic("pmap_bootstrap: cannot block map kernel");
	virtual_avail = *vstart;

	/* hack to block map the whole IO space */
	p = HPPA_IOLEN / 4;
	for (i = 4; i--; ) {
		if (btlb_insert(kernel_pmap->pmap_space,
				HPPA_IOBEGIN + i * (HPPA_IOLEN / 4),
				HPPA_IOBEGIN + i * (HPPA_IOLEN / 4),
				&p, kernel_pmap->pmap_pid |
				pmap_prot(kernel_pmap, VM_PROT_ALL)) < 0)
			panic("pmap_bootstrap: cannot block map I/O space");
	}

d740 2
a741 2
	vm_page_physload(atop(virtual_avail), totalphysmem + i,
			 atop(virtual_avail), totalphysmem + i);
d744 1
a744 1
	virtual_steal = addr += size;
d750 1
a750 1
vm_offset_t 
d753 2
a754 2
	vm_offset_t *startp;
	vm_offset_t *endp;
d756 1
a756 1
	vm_offset_t va;
a793 4
#ifdef FORCE_MAP_KERNEL
	extern int kernel_text, etext;
	vm_offset_t end_text, end_data;
#endif
a816 17
#if FORCE_MAP_KERNEL
	end_text = round_page((vm_offset_t)&etext);
	end_data = virtual_avail;

	/* pdc/iodc area; kernel_text is assumed to be page-aligned */
	pmap_map(0, 0, (vm_offset_t)&kernel_text, VM_PROT_ALL, TRUE);
	/* .text */
	pmap_map((vm_offset_t)&kernel_text, (vm_offset_t)&kernel_text,end_text,
#ifdef DDB
		 VM_PROT_WRITE |
#endif
		 VM_PROT_READ | VM_PROT_EXECUTE, TRUE);
	/* .data+.bss */
	pmap_map(end_text, end_text, end_data,
		 VM_PROT_READ | VM_PROT_WRITE, TRUE);
#endif

d864 1
a864 1
	pmap->pmap_space = pmap->pmap_pid;
d973 2
a974 2
	vm_offset_t va;
	vm_offset_t pa;
d985 1
a985 1
	    (pmap_initialized || pmapdebug & PDB_ENTER))
d1074 2
a1075 2
	register vm_offset_t sva;
	register vm_offset_t eva;
d1110 1
a1110 1
vm_offset_t
d1112 3
a1114 3
	vm_offset_t va;
	vm_offset_t spa;
	vm_offset_t epa;
d1140 1
a1140 1
	vm_offset_t pa;
d1207 2
a1208 2
	vm_offset_t sva;
	vm_offset_t eva;
d1273 1
a1273 1
	vm_offset_t	va;
d1310 1
a1310 1
vm_offset_t
d1313 1
a1313 1
	vm_offset_t va;
d1335 1
a1335 1
	register vm_offset_t pa;
d1339 1
a1339 1
	register vm_offset_t pe = pa + PAGE_SIZE;
d1375 2
a1376 2
	vm_offset_t spa;
	vm_offset_t dpa;
d1380 1
a1380 1
	register vm_offset_t spe = spa + PAGE_SIZE;
d1420 1
a1420 1
	vm_offset_t pa;
d1440 1
a1440 1
	vm_offset_t pa;
d1456 1
a1456 1
	vm_offset_t pa;
d1485 1
a1485 1
	vm_offset_t pa;
d1505 1
a1505 1
	vm_offset_t pa;
d1527 1
a1527 1
	if ((vm_offset_t)va < virtual_avail)
d1529 1
a1529 1
	else if ((vm_offset_t)va >= HPPA_IOBEGIN)
d1532 1
a1532 1
		return (int)pmap_extract(pmap_kernel(), (vm_offset_t)va);
@


1.12
log
@s/MAX_PID/HPPA_MAX_PID/g
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1999/01/20 19:29:50 mickey Exp $	*/
d114 1
a114 1
 * CAVAETS:
a141 23
struct {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
} enter_stats;
struct {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
} remove_stats;

a482 1
		enter_stats.firstpv++;
a500 4
#ifdef PMAPDEBUG
		if (!npv)
			enter_stats.secondpv++;
#endif
d693 1
a693 1
#ifdef PMAPPMAPDEBUG
@


1.11
log
@cleanup some rudimentaries
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1999/01/03 18:42:50 mickey Exp $	*/
d905 1
a905 1
		if (pid_counter <= MAX_PID) {
@


1.10
log
@fix uncompilable
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1999/01/03 04:01:36 mickey Exp $	*/
d224 1
a224 1
					u_int, struct pv_entry *));
d479 1
a479 1
pmap_enter_pv(pmap, va, tlbprot, tlbpage, tlbsw, pv)
d482 1
a482 1
	u_int tlbprot, tlbpage, tlbsw;
a534 1
	pv->pv_tlbsw = tlbsw;
d618 1
a618 2
		if (pv->pv_tlbsw & TLB_ICACHE)
			ficache(pv->pv_space, pv->pv_va, NBPG);
d620 2
a621 2
		if (pv->pv_tlbsw & TLB_DCACHE)
			fdcache(pv->pv_space, pv->pv_va, NBPG);
a623 1
		pv->pv_tlbsw &= ~(TLB_ICACHE|TLB_DCACHE);
d1069 1
a1069 1
		pmap_enter_pv(pmap, va, tlbprot, tlbpage, 0, pv);
d1082 1
a1082 1
			pmap_enter_pv(pmap, va, tlbprot, tlbpage, 0, pv);
@


1.9
log
@add pmap_changebit(); fix pmap_enter() so it handles mapping's PA changes
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1998/12/30 02:11:52 mickey Exp $	*/
d1579 1
a1579 1
		pv->vp_tlbprot &= ~reset;
d1582 1
a1582 1
	pmap_clear_pv(pv, NULL);
@


1.8
log
@fix pmap_steal_memory - related things:
shudup  vm_page_bootstrap about "lost pages"
always use pmap_steal_memory() to allocate from block-mapped area
add pmap_steal_memory debugging flag
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.7 1998/12/08 19:49:38 mickey Exp $	*/
d1076 12
a1087 4
		/*
		 * We are just changing the protection.
		 * Flush the current TLB entry to force a fault and reload.
		 */
d1089 2
a1090 2
		if (pmapdebug & PDB_ENTER)
			printf("pmap_enter: changing protection\n");
d1092 1
d1094 4
d1564 20
@


1.7
log
@fix kvtop() according to the recent changes of mapping IO space equally
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 1998/12/05 15:53:53 mickey Exp $	*/
d181 1
d186 1
d781 3
a783 2
	vm_page_physload(atop(virtual_avail), totalphysmem,
			 atop(virtual_avail), totalphysmem);
d811 3
a813 2
		printf("pmap_steal_memory: steal %d bytes (%x+%x,%x)\n",
		       size, virtual_steal, size, virtual_avail);
d817 4
d843 1
d846 1
a846 1
/*	pmapdebug |= PDB_VA | PDB_PV; */
d849 1
a849 4
	/* alloc the rest of steal area for pv_pages */
	for (pvp = (struct pv_page *)virtual_steal;
	     pvp + 1 <= (struct pv_page *)virtual_avail; pvp++)
		pmap_insert_pvp(pvp, 1);
d851 1
a851 1
	printf("pmap_init: allocate %d pv_pages @@ %x\n",
d855 7
a861 1
	virtual_steal = virtual_avail;
@


1.6
log
@block map IO space, check for btlb_insert return; s/DEBUG/PMAPDEBUG/g
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.5 1998/11/24 04:56:36 mickey Exp $	*/
d1546 2
@


1.5
log
@put kvtop() here, since it needs virtual_avail as the end
of equaly mapped area. use pmap_extracet() for the rest ...
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.4 1998/11/11 05:18:12 mickey Exp $	*/
d139 1
d141 1
a141 1
#ifdef DEBUG
d233 1
a233 1
#ifdef DEBUG
d253 1
a253 1
#if defined(DEBUG) || defined(DIAGNOSTIC)
d256 1
a256 1
#ifdef DEBUG
d281 1
a281 1
#ifdef DEBUG
d300 1
a300 1
#ifdef DEBUG
d360 1
a360 1
#ifdef DEBUG
d393 1
a393 1
#ifdef DEBUG
d491 1
a491 1
#ifdef DEBUG
d501 1
a501 1
#ifdef DEBUG
d512 1
a512 1
#ifdef DEBUG
d523 1
a523 1
#ifdef DEBUG
d551 1
a551 1
#ifdef DEBUG
d602 1
a602 1
#ifdef DEBUG
d637 1
a637 1
#ifdef DEBUG
d661 1
a661 1
	vm_offset_t addr;
d666 1
a666 1
#ifdef DEBUG
d722 1
a722 1
#ifdef PMAPDEBUG
d762 4
a765 3
	btlb_insert(0, kernel_pmap->pmap_space, 0, 0, vstart,
		    kernel_pmap->pmap_pid |
		    pmap_prot(kernel_pmap, VM_PROT_ALL));
d768 11
d796 1
a796 1
#ifdef DEBUG
d834 1
a834 1
#ifdef DEBUG
d844 1
a844 1
#ifdef DEBUG
d884 1
a884 1
#ifdef DEBUG
d939 1
a939 1
#ifdef DEBUG
d983 1
a983 1
#ifdef DEBUG
d1034 1
a1034 1
#ifdef DEBUG
d1056 1
a1056 1
#ifdef DEBUG
d1068 1
a1068 1
#ifdef DEBUG
d1096 1
a1096 1
#ifdef DEBUG
d1118 1
a1118 1
#ifdef DEBUG
d1157 1
a1157 1
#ifdef DEBUG
d1186 1
a1186 1
#ifdef DEBUG
d1253 1
a1253 1
#ifdef DEBUG
d1317 1
a1317 1
#ifdef DEBUG
d1355 1
a1355 1
#ifdef DEBUG
d1379 1
a1379 1
#ifdef DEBUG
d1420 1
a1420 1
#ifdef DEBUG
d1462 1
a1462 1
#ifdef DEBUG
d1499 1
a1499 1
#ifdef DEBUG
d1528 1
a1528 1
#ifdef DEBUG
d1548 1
a1548 1
		return (int)pmap_extract(kernel_pmap, (vm_offset_t)va);
d1551 1
a1551 1
#ifdef DEBUG
@


1.4
log
@there is no splvm() in this world
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.3 1998/10/30 22:16:42 mickey Exp $	*/
d1526 10
@


1.3
log
@remove all those "black magic" inspired routines,
use PDC calls instead, which is more MI.
we also don't need pmap_map to be defined any more, unless
kernel mapping enforced through FORCE_MAP_KERNEL definition.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.2 1998/09/12 03:14:49 mickey Exp $	*/
d1520 1
a1520 1
	s = splvm();
@


1.2
log
@typos, thinkos, brainos, buritos and amigos
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.1 1998/08/20 15:46:49 mickey Exp $	*/
d138 1
d235 4
d642 1
a642 1
		return NULL;
d659 1
d749 5
a753 3
	/* Allocate the physical to virtual table. */
	addr = cache_align(addr);
	size = sizeof(struct pv_entry) * atop(*vend - *vstart + 1);
d758 10
a767 8
	bzero ((caddr_t)addr, size);
	virtual_steal = hppa_round_page(addr + size);
	/* align the virtual_avail at power of 2
	   always keep some memory for steal */
	for (virtual_avail = 1; virtual_avail <= virtual_steal;
	     virtual_avail *= 2);
	vm_page_physload(atop(virtual_steal), atop(virtual_end),
			 atop(virtual_avail), atop(virtual_end));
d769 2
a770 2
	vm_physmem[0].pmseg.pvent = (struct pv_entry *) addr;
	addr = virtual_steal;
a773 2
 
	*vstart = virtual_avail;
d787 4
a790 2
	*startp = virtual_avail;
	*endp = virtual_end;
d796 1
a796 2
		       size, vm_physmem[0].start, atop(size),
		       vm_physmem[0].avail_start);
a797 1
		vm_physmem[0].start += atop(size);
d815 6
d824 29
a852 1
/*	pmapdebug |= PDB_ENTER | PDB_VA | PDB_PV; */
d1127 1
d1136 1
a1136 1
pmap_map(va, spa, epa, prot)
d1141 1
d1150 1
a1150 1
		pmap_enter(pmap_kernel(), va, spa, prot, FALSE);
d1156 1
@


1.1
log
@some cmu/osf-derived pmap, hacked all over
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a142 1
	int ptpneeded;	/* needed to allocate a PT page */
d163 21
a183 15
int pmapdebug = 0 /* 0xffff */;
#define	PDB_FOLLOW	0x0001
#define	PDB_INIT	0x0002
#define	PDB_ENTER	0x0004
#define	PDB_REMOVE	0x0008
#define	PDB_CREATE	0x0010
#define	PDB_PTPAGE	0x0020
#define	PDB_CACHE	0x0040
#define	PDB_BITS	0x0080
#define	PDB_COLLECT	0x0100
#define	PDB_PROTECT	0x0200
#define	PDB_PDRTAB	0x0400
#define	PDB_PARANOIA	0x2000
#define	PDB_WIRING	0x4000
#define	PDB_PVDUMP	0x8000
d186 1
a186 2
vm_offset_t	virtual_avail;
vm_offset_t	virtual_end;
a193 16
#ifdef USEALIGNMENT
/*
 * Mask to determine if two VAs naturally align in the data cache:
 *	834/835:	0 (cannot do)
 *	720/730/750:	256k
 *	705/710:	64k
 *      715:            64k
 * Note that this is a mask only for the data cache since we are
 * only attempting to prevent excess cache flushing of RW data.
 */
vm_offset_t	pmap_alignmask;

#define pmap_aligned(va1, va2) \
	(((va1) & pmap_alignmask) == ((va2) & pmap_alignmask))
#endif

a209 20
static const u_int	kern_prot[8] = {
	TLB_AR_NA,	/* VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE */
	TLB_AR_KR,	/* VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE */
	TLB_AR_KRW,	/* VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE */
	TLB_AR_KRW,	/* VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE */
	TLB_AR_KRX,	/* VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE */
	TLB_AR_KRX,	/* VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE */
	TLB_AR_KRWX,	/* VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE */
	TLB_AR_KRWX	/* VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE */
};
static const u_int user_prot[8] = {
	TLB_AR_NA,	/* VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_NONE */
	TLB_AR_UR,	/* VM_PROT_READ | VM_PROT_NONE  | VM_PROT_NONE */
	TLB_AR_URW,	/* VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE */
	TLB_AR_URW,	/* VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE */
	TLB_AR_URX,	/* VM_PROT_NONE | VM_PROT_NONE  | VM_PROT_EXECUTE */
	TLB_AR_URX,	/* VM_PROT_READ | VM_PROT_NONE  | VM_PROT_EXECUTE */
	TLB_AR_URWX,	/* VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE */
	TLB_AR_URWX	/* VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE */
};
d215 1
d231 5
a239 19
#define pmap_prot(pmap, prot)	\
	(((pmap) == kernel_pmap ? kern_prot : user_prot)[prot])

/*
 * Clear the HPT table entry for the corresponding space/offset to reflect
 * the fact that we have possibly changed the mapping, and need to pick
 * up new values from the mapping structure on the next access.
 */
static __inline void
pmap_clear_va(space, va)
	pa_space_t space;
	vm_offset_t va;
{
	register int hash = pmap_hash(space, va);

	hpt_table[hash].hpt_valid = 0;
	hpt_table[hash].hpt_space = -1;
}

d247 8
d256 1
a256 3
	register struct pv_entry *pvp =	hpt->hpt_entry;
	va = btop(va);
	while(pvp && pvp->pv_va != va && pvp->pv_space != space)
d275 6
a280 2
	va = btop(va);
	while(pvp && pvp->pv_va != va && pvp->pv_space != space)
d291 1
a291 1
		&hpt_table[pmap_hash(pv->pv_space, ptob(pv->pv_va))];
d294 5
d304 1
a304 1
		if (pv->pv_va == hpt->hpt_vpn &&
d316 32
d353 5
a357 1
	int i;
d362 1
a362 1
		if (pvp == 0)
d364 8
a371 14
		pvp->pvp_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = NULL;
		pv_nfree += pvp->pvp_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_list);
		}
		pv = pvp->pvp_freelist;
d373 2
a374 2
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
d376 2
a377 2
		pvp->pvp_freelist = pv->pv_next;
	}
d387 5
d402 5
a406 3
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_list);
		FREE((vm_offset_t) pvp, M_VMPVENT);
d486 1
a486 1
	if (pmapdebug & PDB_ENTER)
d496 2
d507 3
d518 1
a518 1
		if (!npv->pv_next)
d524 1
d531 1
a531 1
	pmap_enter_va(pmap->pmap_space, va, pv);
d545 5
d596 5
a618 3
#ifdef USEALIGNMENT
		pv->pv_tlbprot &= ~TLB_DIRTY;
#endif
d631 4
d656 1
d658 4
a661 5
#ifdef PMAPDEBUG
	vm_offset_t saddr;
#endif
#ifdef USEALIGNMENT
	extern int dcache_size;
d669 19
a687 1
		panic("HPPA page != MACH page");
a703 5
#ifdef USEALIGNMENT
	/* If we can take advantage of natural alignments in the cache
	   set that up now. */
	pmap_alignmask = dcache_size - 1;
#endif
d708 1
d710 1
a710 1
	/* here will be a hole due to the HPT alignment XXX */
a711 1
	/* Allocate the HPT */
d713 2
d716 3
a718 1
	saddr =
d720 4
a723 1
	addr = ((u_int)addr + size-1) & ~(size-1);
d733 1
a733 1
	addr = round_page((vm_offset_t)&hpt_table[hpt_hashsize]);
d735 2
a736 2
	printf("hpt_table: 0x%x @@ 0x%x\n", addr - saddr, saddr);
#endif
a737 3
	/* Allocate the physical to virtual table. */
#ifdef PMAPDEBUG
	saddr =
a738 14
	addr = cache_align(addr);
	size = sizeof(struct pv_entry *) * atop(*vend - *vstart + 1);
	addr = *vstart = cache_align(addr + size);
	vm_page_physload(atop(*vstart), atop(*vend),
			 atop(*vstart), atop(*vend));
	/* we have only one initial phys memory segment */
	vm_physmem[0].pmseg.pvent = (struct pv_entry *) addr;
 
#ifdef PMAPDEBUG
	printf("pv_array: 0x%x @@ 0x%x\n", addr - saddr, saddr);
#endif
	virtual_avail = *vstart;
	virtual_end = *vend;

d743 6
a748 2
#ifdef BTLB
	cpu_disable_sid_hashing();
d750 11
d762 4
a765 2
        /* Load the kernel's PID */
	mtctl(HPPA_PID_KERNEL, CR_PIDR1);
d768 3
a770 2
void 
pmap_virtual_space(startp, endp)
d774 5
d781 15
d807 6
d827 7
a833 1
	u_int pid;
d884 5
d928 6
a933 1
	if (pmap == NULL)
d975 1
a975 1
	u_int tlbpage = btop(pa), tlbprot;
d979 8
a986 1
        if (pmap == NULL)
d992 1
d1001 4
d1013 4
a1039 1
	pmap_clear_va(space, pv->pv_va);
d1041 4
d1063 5
a1067 1
	if(pmap == NULL)
a1112 38
#ifdef	BTLB
/*
 * pmap_block_map(pa, size, prot, entry, dtlb)
 *    Block map a physical region. Size must be a power of 2. Address must
 *    must be aligned to the size. Entry is the block TLB entry to use.
 *
 *    S-CHIP: Entries 0,2 and 1,3 must be the same size for each TLB type. 
 */
void
pmap_block_map(pa, size, prot, entry, tlbflag)
	vm_offset_t pa;
	vm_size_t size;
	vm_prot_t prot;
	int entry, tlbflag;
{
	u_int tlbprot;
	
	tlbprot = pmap_prot(kernel_pmap, prot) | kernel_pmap->pmap_pid;

	switch (tlbflag) {
	case BLK_ICACHE:
  		insert_block_itlb(entry, pa, size, tlbprot);
		break;
	case BLK_DCACHE:
		insert_block_dtlb(entry, pa, size, tlbprot);
		break;
	case BLK_COMBINED:
		insert_block_ctlb(entry, pa, size, tlbprot);
		break;
	case BLK_LCOMBINED:
		insert_L_block_ctlb(entry, pa, size, tlbprot);
		break;
	default:
		printf("pmap_block_map routine: unknown flag %d\n",tlbflag);
	}
}
#endif

d1125 1
a1125 1
	u_int tlbprot;
d1128 5
d1195 6
a1200 1
	if (pmap == NULL)
d1259 7
a1265 1
	if (pmap == NULL)
d1297 5
d1305 1
a1305 1
		return ptob(pv->pv_tlbpage) + (va & PGOFSET);
d1321 5
d1362 5
d1404 5
d1415 1
a1415 1
 *	returns TRUE iff the given physical page has been modified 
d1441 5
d1470 5
d1483 21
@

