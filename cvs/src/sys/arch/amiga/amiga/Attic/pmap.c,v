head	1.52;
access;
symbols
	SMP_SYNC_A:1.52
	SMP_SYNC_B:1.52
	UBC_SYNC_A:1.52
	OPENBSD_3_2:1.50.0.2
	OPENBSD_3_2_BASE:1.50
	OPENBSD_3_1:1.48.0.2
	OPENBSD_3_1_BASE:1.48
	UBC_SYNC_B:1.51
	UBC:1.44.0.2
	UBC_BASE:1.44
	OPENBSD_3_0:1.35.0.2
	OPENBSD_3_0_BASE:1.35
	OPENBSD_2_9:1.27.0.2
	OPENBSD_2_9_BASE:1.27
	OPENBSD_2_8:1.23.0.2
	OPENBSD_2_8_BASE:1.23
	OPENBSD_2_7:1.17.0.2
	OPENBSD_2_7_BASE:1.17
	SMP:1.16.0.4
	SMP_BASE:1.16
	kame_19991208:1.16
	OPENBSD_2_6:1.16.0.2
	OPENBSD_2_6_BASE:1.16
	OPENBSD_2_5:1.12.0.2
	OPENBSD_2_5_BASE:1.12
	OPENBSD_2_4:1.11.0.4
	OPENBSD_2_4_BASE:1.11
	OPENBSD_2_3:1.11.0.2
	OPENBSD_2_3_BASE:1.11
	OPENBSD_2_2:1.10.0.2
	OPENBSD_2_2_BASE:1.10
	OPENBSD_2_1:1.8.0.2
	OPENBSD_2_1_BASE:1.8
	OPENBSD_2_0:1.7.0.2
	OPENBSD_2_0_BASE:1.7
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.52
date	2002.12.31.16.35.36;	author miod;	state dead;
branches;
next	1.51;

1.51
date	2002.10.12.01.09.43;	author krw;	state Exp;
branches;
next	1.50;

1.50
date	2002.09.10.18.29.43;	author art;	state Exp;
branches;
next	1.49;

1.49
date	2002.05.28.14.29.05;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	2002.03.25.19.41.52;	author niklas;	state Exp;
branches;
next	1.47;

1.47
date	2002.02.23.00.31.10;	author miod;	state dead;
branches;
next	1.46;

1.46
date	2001.12.22.21.25.59;	author miod;	state Exp;
branches;
next	1.45;

1.45
date	2001.12.20.19.02.24;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2001.11.30.23.14.28;	author miod;	state dead;
branches
	1.44.2.1;
next	1.43;

1.43
date	2001.11.28.16.24.26;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2001.11.28.16.13.27;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2001.11.28.15.34.16;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2001.11.28.14.13.06;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2001.11.28.13.47.37;	author art;	state Exp;
branches;
next	1.38;

1.38
date	2001.11.07.01.18.00;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.11.06.19.53.14;	author miod;	state Exp;
branches;
next	1.36;

1.36
date	2001.11.06.01.47.02;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.09.19.20.50.56;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2001.07.25.13.25.31;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.07.18.10.47.04;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.06.27.03.54.13;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.06.08.08.08.41;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.05.15.14.19.58;	author jj;	state Exp;
branches;
next	1.29;

1.29
date	2001.05.09.15.31.24;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.05.05.21.26.34;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.04.20.11.01.55;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.02.19.17.23.13;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2000.12.15.15.18.36;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2000.11.08.11.42.44;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2000.05.30.10.39.33;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2000.05.28.03.55.21;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2000.05.27.22.12.33;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2000.05.27.21.17.59;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2000.05.27.20.14.18;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2000.05.27.19.42.49;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2000.02.22.19.27.42;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	99.09.03.18.00.29;	author art;	state Exp;
branches
	1.16.4.1;
next	1.15;

1.15
date	99.07.18.18.00.03;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	99.07.18.16.45.43;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	99.07.18.16.23.46;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	99.01.11.05.11.05;	author millert;	state Exp;
branches;
next	1.11;

1.11
date	98.03.30.18.56.10;	author niklas;	state Exp;
branches;
next	1.10;

1.10
date	97.09.18.13.39.36;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	97.09.09.11.05.00;	author niklas;	state Exp;
branches;
next	1.8;

1.8
date	97.01.16.09.23.26;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	96.10.04.23.34.39;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	96.05.29.10.14.31;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	96.05.02.06.43.22;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	95.12.03.10.04.29;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	95.11.28.20.47.34;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	95.11.13.03.53.32;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.49.53;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.49.53;	author deraadt;	state Exp;
branches;
next	;

1.16.4.1
date	2000.03.02.07.04.26;	author niklas;	state Exp;
branches;
next	1.16.4.2;

1.16.4.2
date	2001.04.18.16.01.59;	author niklas;	state Exp;
branches;
next	1.16.4.3;

1.16.4.3
date	2001.07.04.10.14.59;	author niklas;	state Exp;
branches;
next	1.16.4.4;

1.16.4.4
date	2001.10.31.02.52.45;	author nate;	state Exp;
branches;
next	1.16.4.5;

1.16.4.5
date	2001.11.13.21.00.50;	author niklas;	state Exp;
branches;
next	1.16.4.6;

1.16.4.6
date	2001.12.05.00.39.09;	author niklas;	state dead;
branches;
next	;

1.44.2.1
date	2002.01.31.22.55.06;	author niklas;	state Exp;
branches;
next	1.44.2.2;

1.44.2.2
date	2002.06.11.03.34.57;	author art;	state Exp;
branches;
next	1.44.2.3;

1.44.2.3
date	2002.10.29.00.28.01;	author art;	state Exp;
branches;
next	1.44.2.4;

1.44.2.4
date	2003.05.19.21.49.38;	author tedu;	state dead;
branches;
next	;


desc
@@


1.52
log
@amiga and sun3 turned out to not be y2k+3 compliant here. Remove them, as
well as the few userland tools which were only used on these platforms.
@
text
@/*	$OpenBSD: pmap.c,v 1.51 2002/10/12 01:09:43 krw Exp $	*/
/*	$NetBSD: pmap.c,v 1.68 1999/06/19 19:44:09 is Exp $	*/

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/* 
 * Copyright (c) 1991 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)pmap.c	7.5 (Berkeley) 5/10/91
 */

/*
 *	AMIGA physical map management code.
 *	For 68020/68030 machines with 68551, or 68030 MMUs
 *	Don't even pay lip service to multiprocessor support.
 *
 *	will only work for PAGE_SIZE == NBPG
 *	right now because of the assumed one-to-one relationship of PT
 *	pages to STEs.
 */

/*
 *	Manages physical address maps.
 *
 *	In addition to hardware address maps, this
 *	module is called upon to provide software-use-only
 *	maps which may or may not be stored in the same
 *	form as hardware maps.  These pseudo-maps are
 *	used to store intermediate results from copy
 *	operations to and from address spaces.
 *
 *	Since the information managed by this module is
 *	also stored by the logical address mapping module,
 *	this module may throw away valid virtual-to-physical
 *	mappings at almost any time.  However, invalidations
 *	of virtual-to-physical mappings must be done as
 *	requested.
 *
 *	In order to cope with hardware architectures which
 *	make virtual-to-physical map invalidates expensive,
 *	this module may delay invalidate or reduced protection
 *	operations until such time as they are actually
 *	necessary.  This module is given full information as
 *	to which processors are currently using which maps,
 *	and to when physical maps must be made correct.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/msgbuf.h>
#include <sys/user.h>
#include <uvm/uvm.h>
#include <machine/pte.h>
#include <machine/cpu.h>
#include <machine/vmparam.h>
#include <amiga/amiga/memlist.h>
/*
 * Allocate various and sundry SYSMAPs used in the days of old VM
 * and not yet converted.  XXX.
 */

#ifdef DEBUG
struct kpt_stats {
	int collectscans;
	int collectpages;
	int kpttotal;
	int kptinuse;
	int kptmaxuse;
};
struct enter_stats {
	int kernel;	/* entering kernel mapping */
	int user;	/* entering user mapping */
	int ptpneeded;	/* needed to allocate a PT page */
	int pwchange;	/* no mapping change, just wiring or protection */
	int wchange;	/* no mapping change, just wiring */
	int mchange;	/* was mapped but mapping to different page */
	int managed;	/* a managed page */
	int firstpv;	/* first mapping for this PA */
	int secondpv;	/* second mapping for this PA */
	int ci;		/* cache inhibited */
	int unmanaged;	/* not a managed page */
	int flushes;	/* cache flushes */
};
struct remove_stats {
	int calls;
	int removes;
	int pvfirst;
	int pvsearch;
	int ptinvalid;
	int uflushes;
	int sflushes;
};

struct remove_stats remove_stats;
struct enter_stats enter_stats;
struct kpt_stats kpt_stats;

#define PDB_FOLLOW	0x0001
#define PDB_INIT	0x0002
#define PDB_ENTER	0x0004
#define PDB_REMOVE	0x0008
#define PDB_CREATE	0x0010
#define PDB_PTPAGE	0x0020
#define PDB_CACHE	0x0040
#define PDB_BITS	0x0080
#define PDB_COLLECT	0x0100
#define PDB_PROTECT	0x0200
#define PDB_SEGTAB	0x0400
#define PDB_PARANOIA	0x2000
#define PDB_WIRING	0x4000
#define PDB_PVDUMP	0x8000
int debugmap = 0;
int pmapdebug = PDB_PARANOIA;

static void	pmap_check_wiring(char *, vaddr_t);
static void	pmap_pvdump(paddr_t);
#endif

/*
 * Get STEs and PTEs for user/kernel address space
 */
#if defined(M68040) || defined(M68060)
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vaddr_t)(v) >> pmap_ishift]))
#define	pmap_ste1(m, v) (&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
/* XXX assumes physically contiguous ST pages (if more than one) */
#define	pmap_ste2(m, v) \
	(&((m)->pm_stab[(u_int *)(*(u_int *)pmap_ste1(m,v) & SG4_ADDR1) \
			- (m)->pm_stpa + (((v) & SG4_MASK2) >> SG4_SHIFT2)]))
#define	pmap_ste_v(m, v) \
	(mmutype == MMU_68040		\
	? ((*pmap_ste1(m, v) & SG_V) &&	\
	   (*pmap_ste2(m, v) & SG_V))	\
	: (*pmap_ste(m, v) & SG_V))
#else	/* defined(M68040) || defined(M68060) */
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
#define pmap_ste_v(m, v)	(*pmap_ste(m, v) & SG_V)
#endif	/* defined(M68040) || defined(M68060) */

#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))

#define pmap_pte_pa(pte)	(*(u_int *)(pte) & PG_FRAME)

#define pmap_pte_w(pte)		(*(u_int *)(pte) & PG_W)
#define pmap_pte_ci(pte)	(*(u_int *)(pte) & PG_CI)
#define pmap_pte_m(pte)		(*(u_int *)(pte) & PG_M)
#define pmap_pte_u(pte)		(*(u_int *)(pte) & PG_U)
#define pmap_pte_prot(pte)	(*(u_int *)(pte) & PG_PROT)
#define pmap_pte_v(pte)		(*(u_int *)(pte) & PG_V)

#define pmap_pte_set_w(pte, v) \
    do { if (v) *(u_int *)(pte) |= PG_W; else *(u_int *)(pte) &= ~PG_W; \
    } while (0)
#define pmap_pte_set_prot(pte, v) \
    do { if (v) *(u_int *)(pte) |= PG_PROT; else *(u_int *)(pte) &= ~PG_PROT; \
    } while (0)
#define pmap_pte_w_chg(pte, nw)		((nw) ^ pmap_pte_w(pte))
#define pmap_pte_prot_chg(pte, np)	((np) ^ pmap_pte_prot(pte))

#define active_pmap(pm)	\
    ((pm) == pmap_kernel() || (pm) == curproc->p_vmspace->vm_map.pmap)

/*
 * Given a map and a machine independent protection code,
 * convert to a vax protection code.
 */
#define pte_prot(m, p)	(protection_codes[p])
int	protection_codes[8];

/*
 * Kernel page table page management.
 *
 * One additional page of KPT allows for 16 MB of virtual buffer cache.
 * A GENERIC kernel allocates this for 2 MB of real buffer cache,
 * which in turn is allocated for 38 MB of RAM.
 * We add one per 16 MB of RAM to allow for tuning the machine-independent
 * options.
 */
#ifndef NKPTADDSHIFT
#define NKPTADDSHIFT 24
#endif

struct kpt_page {
	struct kpt_page *kpt_next;	/* link on either used or free list */
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
};
struct kpt_page *kpt_free_list, *kpt_used_list;
struct kpt_page *kpt_pages;

/*
 * Kernel segment/page table and page table map.
 * The page table map gives us a level of indirection we need to dynamically
 * expand the page table.  It is essentially a copy of the segment table
 * with PTEs instead of STEs.  All are initialized in locore at boot time.
 * Sysmap will initially contain VM_KERNEL_PT_PAGES pages of PTEs.
 * Segtabzero is an empty segment table which all processes share til they
 * reference something.
 */
u_int	*Sysseg, *Sysseg_pa;
u_int	*Sysmap, *Sysptmap;
u_int	*Segtabzero, *Segtabzeropa;
vsize_t	Sysptsize = VM_KERNEL_PT_PAGES;

struct pmap	kernel_pmap_store;
struct vm_map	*pt_map;
struct vm_map	pt_map_store;

vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
int		page_cnt;	/* number of pages managed by the VM system */
boolean_t	pmap_initialized = FALSE;	/* Has pmap_init completed? */
char		*pmap_attributes;	/* reference and modify bits */
TAILQ_HEAD(pv_page_list, pv_page) pv_page_freelist;
int		pv_nfree;
#if defined(M68040) || defined(M68060)
static int	pmap_ishift;	/* segment table index shift */
int		protostfree;	/* prototype (default) free ST map */
#endif
extern paddr_t	msgbufpa;	/* physical address of the msgbuf */

u_long		noncontig_enable;
extern vaddr_t	amiga_uptbase;

extern paddr_t	z2mem_start;

extern vaddr_t reserve_dumppages(vaddr_t);
  
boolean_t	pmap_testbit(paddr_t, int);
void		pmap_enter_ptpage(pmap_t, vaddr_t);
static void	pmap_ptpage_addref(vaddr_t);
static int	pmap_ptpage_delref(vaddr_t);
static void	pmap_changebit(vaddr_t, int, boolean_t);
struct pv_entry *pmap_alloc_pv(void);
void		pmap_free_pv(struct pv_entry *);
void		pmap_pinit(pmap_t);
void		pmap_release(pmap_t);
static void	pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *, int);

static void	amiga_protection_init(void);
void		pmap_collect1(pmap_t, paddr_t, paddr_t);

/* pmap_remove_mapping flags */
#define		PRM_TFLUSH	0x01
#define		PRM_CFLUSH	0x02 
#define		PRM_KEEPPTPAGE	0x04
  

/*
 * All those kernel PT submaps that BSD is so fond of
 */
caddr_t	CADDR1, CADDR2, vmmap;
u_int	*CMAP1, *CMAP2, *vmpte, *msgbufmap;

#define	PAGE_IS_MANAGED(pa) (pmap_initialized				\
			 && vm_physseg_find(atop((pa)), NULL) != -1)

#define pa_to_pvh(pa) \
({ \
	int bank_, pg_; \
	bank_ = vm_physseg_find(atop((pa)), &pg_); \
	&vm_physmem[bank_].pmseg.pvent[pg_]; \
})

#define pa_to_attribute(pa) \
({ \
	int bank_, pg_; \
	bank_ = vm_physseg_find(atop((pa)), &pg_); \
	&vm_physmem[bank_].pmseg.attrs[pg_]; \
})

/*
 *	Bootstrap the system enough to run with virtual memory.
 *	Map the kernel's code and data, and allocate the system page table.
 *
 *	On the HP this is called after mapping has already been enabled
 *	and just syncs the pmap module with what has already been done.
 *	[We can't call it easily with mapping off since the kernel is not
 *	mapped with PA == VA, hence we would have to relocate every address
 *	from the linked base (virtual) address 0 to the actual (physical)
 *	address of 0xFFxxxxxx.]
 */
void
pmap_bootstrap(firstaddr, loadaddr)
	paddr_t firstaddr;
	paddr_t loadaddr;
{
	vaddr_t va;
	u_int *pte;
	int i;
	struct boot_memseg *sp, *esp;
	paddr_t fromads, toads;

	fromads = firstaddr;
	toads = maxmem << PGSHIFT;

	uvmexp.pagesize = NBPG;
	uvm_setpagesize();

	/* XXX: allow for msgbuf */
	toads -= m68k_round_page(MSGBUFSIZE);
	msgbufpa = toads;
	/*
	 * first segment of memory is always the one loadbsd found
	 * for loading the kernel into.
	 */
	uvm_page_physload(atop(fromads), atop(toads),
		atop(fromads), atop(toads), VM_FREELIST_DEFAULT);

	sp = memlist->m_seg;
	esp = sp + memlist->m_nseg;
	i = 1;
	for (; noncontig_enable && sp < esp; sp++) {
		if ((sp->ms_attrib & MEMF_FAST) == 0)
			continue;		/* skip if not FastMem */
		if (firstaddr >= sp->ms_start &&
		    firstaddr < sp->ms_start + sp->ms_size)
			continue;		/* skip kernel segment */
		if (sp->ms_size == 0)
			continue;		/* skip zero size segments */
		fromads = sp->ms_start;
		toads = sp->ms_start + sp->ms_size;
#ifdef DEBUG_A4000
		/*
		 * My A4000 doesn't seem to like Zorro II memory - this
		 * hack is to skip the motherboard memory and use the
		 * Zorro II memory.  Only for trying to debug the problem.
		 * Michael L. Hitch
		 */
		if (toads == 0x08000000)
			continue;	/* skip A4000 motherboard mem */
#endif
		/*
		 * Deal with Zorro II memory stolen for DMA bounce buffers.
		 * This needs to be handled better.
		 *
		 * XXX is: disabled. This is handled now in amiga_init.c
		 * by removing the stolen memory from the memlist.
		 *
		 * XXX is: enabled again, but check real size and position.
		 * We check z2mem_start is in this segment, and set its end
		 * to the z2mem_start.
		 * 
		 */
		if ((fromads <= z2mem_start) && (toads > z2mem_start))
			toads = z2mem_start;

		uvm_page_physload(atop(fromads), atop(toads),
			atop(fromads), atop(toads), (fromads & 0xff000000) ?
			VM_FREELIST_DEFAULT : VM_FREELIST_ZORROII);
		physmem += (toads - fromads) / NBPG;
		++i;
		if (noncontig_enable == 1)
			break;		/* Only two segments enabled */
	}

	mem_size = physmem << PGSHIFT;
	virtual_avail = VM_MIN_KERNEL_ADDRESS + (firstaddr - loadaddr);
	virtual_end = VM_MAX_KERNEL_ADDRESS;

	/*
	 * Initialize protection array.
	 */
	amiga_protection_init();

	/*
	 * Kernel page/segment table allocated in locore,
	 * just initialize pointers.
	 */
	pmap_kernel()->pm_stpa = Sysseg_pa;
	pmap_kernel()->pm_stab = Sysseg;
	pmap_kernel()->pm_ptab = Sysmap;
#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		pmap_ishift = SG4_SHIFT1;
		pmap_kernel()->pm_stfree = protostfree;
	} else
		pmap_ishift = SG_ISHIFT;
#endif

	simple_lock_init(&pmap_kernel()->pm_lock);
	pmap_kernel()->pm_count = 1;

	/*
	 * Allocate all the submaps we need
	 */
#define	SYSMAP(c, p, v, n)	\
	v = (c)va; va += ((n)*NBPG); p = pte; pte += (n);

	va = virtual_avail;
	pte = pmap_pte(pmap_kernel(), va);

	SYSMAP(caddr_t		,CMAP1		,CADDR1	   ,1		)
	SYSMAP(caddr_t		,CMAP2		,CADDR2	   ,1		)
	SYSMAP(caddr_t		,vmpte		,vmmap	   ,1		)
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
	
	DCIS();
	virtual_avail = reserve_dumppages(va);
}

/*
 *	Initialize the pmap module.
 *	Called by vm_init, to initialize any structures that the pmap
 *	system needs to map virtual memory.
 */
void
pmap_init()
{
	extern vaddr_t	amigahwaddr;
	extern u_int	namigahwpg;
	vaddr_t		addr, addr2;
	paddr_t		paddr;
	vsize_t		s;
	u_int		npg;
	struct pv_entry *pv;
	char		*attr;
	int		rv, bank;
#if defined(M68060)
	struct kpt_page *kptp;
#endif

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_init()\n");
#endif
	/*
	 * Now that kernel map has been allocated, we can mark as
	 * unavailable regions which we have mapped in locore.
	 * XXX in pmap_boostrap() ???
	 */
	addr = (vaddr_t) amigahwaddr;
	if (uvm_map(kernel_map, &addr,
		    ptoa(namigahwpg),
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)))
		goto bogons;
	addr = (vaddr_t) Sysmap;
	if (uvm_map(kernel_map, &addr, AMIGA_KPTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED))) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
bogons:
		panic("pmap_init: bogons in the VM system!");
	}
#ifdef DEBUG
	if (pmapdebug & PDB_INIT) {
		printf("pmap_init: Sysseg %p, Sysmap %p, Sysptmap %p\n",
		    Sysseg, Sysmap, Sysptmap);
		printf(" vstart %lx, vend %lx\n", virtual_avail, virtual_end);
	}
#endif

	/*
	 * Allocate memory for random pmap data structures.  Includes the
	 * initial segment table, pv_head_table and pmap_attributes.
	 */
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++) {
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
#ifdef DEBUG
		printf("pmap_init: %2d: %08lx - %08lx (%10d)\n", bank,
		    vm_physmem[bank].start << PGSHIFT,
		    vm_physmem[bank].end << PGSHIFT, page_cnt << PGSHIFT);
#endif
	}
	s = AMIGA_STSIZE;				/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */

	s = round_page(s);
	addr = uvm_km_zalloc(kernel_map, s);
	Segtabzero = (u_int *)addr;
	pmap_extract(pmap_kernel(), addr, (paddr_t *)&Segtabzeropa);

	addr += AMIGA_STSIZE;

	pv_table = (pv_entry_t)addr;
	addr += page_cnt * sizeof(struct pv_entry);

	pmap_attributes = (char *)addr;
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
		       "tbl %p atr %p\n",
		       s, page_cnt, Segtabzero, Segtabzeropa,
		       pv_table, pmap_attributes);
#endif

        /*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npg = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npg;
		attr += npg;
	}

	/*
	 * Allocate physical memory for kernel PT pages and their management.
	 * we need enough pages to map the page tables for each process 
	 * plus some slop.
	 */
	npg = howmany(((maxproc + 16) * AMIGA_UPTSIZE / NPTEPG), NBPG);
#ifdef NKPTADD
	npg += NKPTADD;
#else
	npg += mem_size >> NKPTADDSHIFT;
#endif
#if 1/*def DEBUG*/
	printf("Maxproc %d, mem_size %ld MB: allocating %d KPT pages\n",
	    maxproc, mem_size>>20, npg);
#endif
	s = ptoa(npg) + round_page(npg * sizeof (struct kpt_page));

	/*
	 * Verify that space will be allocated in region for which
	 * we already have kernel PT pages.
	 */
	addr = 0;
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv || (addr + s) >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	uvm_unmap(kernel_map, addr, addr + s);
	/*
	 * Now allocate the space and link the pages together to
	 * form the KPT free list.
	 */
	addr = uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
	s = ptoa(npg);
	addr2 = addr + s;
	kpt_pages = &((struct kpt_page *)addr2)[npg];
	kpt_free_list = (struct kpt_page *)0;
	do {
		addr2 -= NBPG;
		(--kpt_pages)->kpt_next = kpt_free_list;
		kpt_free_list = kpt_pages;
		kpt_pages->kpt_va = addr2;
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
	} while (addr != addr2);

#ifdef DEBUG
	kpt_stats.kpttotal = atop(s);
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: KPT: %ld pages from %lx to %lx\n", atop(s),
		    addr, addr + s);
#endif

        /*
	 * Allocate the segment table map and the page table map.
	 */
	addr = amiga_uptbase;
	if ((AMIGA_UPTMAXSIZE / AMIGA_UPTSIZE) < maxproc) {
		s = AMIGA_UPTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = AMIGA_UPTMAXSIZE / AMIGA_UPTSIZE;
	} else
		s = (maxproc * AMIGA_UPTSIZE);

	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, VM_MAP_PAGEABLE,
				 TRUE, &pt_map_store);

#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040)
		protostfree = ~1 & ~(-1 << MAXUL2SIZE);
#endif /* defined(M68040) || defined(M68060) */

	/*
	 * Now it is safe to enable pv_table recording.
	 */
	pmap_initialized = TRUE;
	/*
	 * Now that this is done, mark the pages shared with the
	 * hardware page table search as non-CCB (actually, as CI).
	 *
	 * XXX Hm. Given that this is in the kernel map, can't we just
	 * use the va's?
	 */
#ifdef M68060
	if (machineid & AMIGA_68060) {
		kptp = kpt_free_list;
		while (kptp) {
			pmap_changebit(kptp->kpt_pa, PG_CCB, 0);
			pmap_changebit(kptp->kpt_pa, PG_CI, 1);
			kptp = kptp->kpt_next;
		}

		paddr = (paddr_t)Segtabzeropa;
		while (paddr < (paddr_t)Segtabzeropa + AMIGA_STSIZE) {
			pmap_changebit(paddr, PG_CCB, 0);
			pmap_changebit(paddr, PG_CI, 1);
			paddr += NBPG;
		}

		DCIS();
	}
#endif
}

struct pv_entry *
pmap_alloc_pv()
{
	struct pv_page *pvp;
	struct pv_entry *pv;
	int i;

	if (pv_nfree == 0) {
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
		pvp->pvp_pgi.pgi_freelist = pv = &pvp->pvp_pv[1];
		for (i = NPVPPG - 2; i; i--, pv++)
			pv->pv_next = pv + 1;
		pv->pv_next = 0;
		pv_nfree += pvp->pvp_pgi.pgi_nfree = NPVPPG - 1;
		TAILQ_INSERT_HEAD(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		pv = &pvp->pvp_pv[0];
	} else {
		--pv_nfree;
		pvp = pv_page_freelist.tqh_first;
		if (--pvp->pvp_pgi.pgi_nfree == 0) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		}
		pv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
		if (pv == 0)
			panic("pmap_alloc_pv: pgi_nfree inconsistent");
#endif
		pvp->pvp_pgi.pgi_freelist = pv->pv_next;
	}
	return pv;
}

void
pmap_free_pv(pv)
	struct pv_entry *pv;
{
	struct pv_page *pvp;

	pvp = (struct pv_page *)trunc_page((vaddr_t)pv);
	switch (++pvp->pvp_pgi.pgi_nfree) {
	case 1:
		TAILQ_INSERT_TAIL(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
	default:
		pv->pv_next = pvp->pvp_pgi.pgi_freelist;
		pvp->pvp_pgi.pgi_freelist = pv;
		++pv_nfree;
		break;
	case NPVPPG:
		pv_nfree -= NPVPPG - 1;
		TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
		break;
	}
}

/*
 *	Used to map a range of physical addresses into kernel
 *	virtual address space.
 *
 *	For now, VM is already on, we only need to map the
 *	specified memory.
 */
vaddr_t
pmap_map(virt, start, end, prot)
	vaddr_t	virt;
	paddr_t	start;
	paddr_t	end;
	int		prot;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_map(%lx, %lx, %lx, %x)\n", virt, start, end,
		    prot);
#endif
	while (start < end) {
		pmap_enter(pmap_kernel(), virt, start, prot, 0);
		virt += PAGE_SIZE;
		start += PAGE_SIZE;
	}
	return(virt);
}

/*
 *	Create and return a physical map.
 *
 *	If the size specified for the map
 *	is zero, the map is an actual physical
 *	map, and may be referenced by the
 *	hardware.
 *
 *	If the size specified is non-zero,
 *	the map will be used in software only, and
 *	is bounded by that size.
 */
struct pmap *
pmap_create(void)
{
	struct pmap *pmap;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_create()\n");
#endif

	pmap = (struct pmap *)malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
	bzero(pmap, sizeof(*pmap));
	pmap_pinit(pmap);
	return (pmap);
}

/*
 * Initialize a preallocated and zeroed pmap structure,
 * such as one in a vmspace structure.
 */
void
pmap_pinit(pmap)
	pmap_t pmap;
{

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_CREATE))
		printf("pmap_pinit(%p)\n", pmap);
#endif
	/*
	 * No need to allocate page table space yet but we do need a
	 * valid segment table.  Initially, we point everyone at the
	 * "null" segment table.  On the first pmap_enter, a real
	 * segment table will be allocated.
	 */
	pmap->pm_stab = Segtabzero;
	pmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040)
		pmap->pm_stfree = protostfree;
#endif
	pmap->pm_count = 1;
	simple_lock_init(&pmap->pm_lock);
}

/*
 *	Retire the given physical map from service.
 *	Should only be called if the map contains
 *	no valid mappings.
 */
void
pmap_destroy(pmap)
	pmap_t pmap;
{
	int count;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_destroy(%p)\n", pmap);
#endif
	if (pmap == NULL)
		return;

	simple_lock(&pmap->pm_lock);
	count = --pmap->pm_count;
	simple_unlock(&pmap->pm_lock);
	if (count == 0) {
		pmap_release(pmap);
		free((caddr_t)pmap, M_VMPMAP);
	}
}

/*
 * Release any resources held by the given physical map.
 * Called when a pmap initialized by pmap_pinit is being released.
 * Should only be called if the map contains no valid mappings.
 */
void
pmap_release(pmap)
	pmap_t pmap;
{

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_release(%p)\n", pmap);
#endif
#ifdef notdef /* DIAGNOSTIC */
	/* count would be 0 from pmap_destroy... */
	simple_lock(&pmap->pm_lock);
	if (pmap->pm_count != 1)
		panic("pmap_release count");
#endif
	if (pmap->pm_ptab)
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				 AMIGA_UPTSIZE);
	if (pmap->pm_stab != Segtabzero)
		uvm_km_free_wakeup(kernel_map, (vaddr_t)pmap->pm_stab,
				 AMIGA_STSIZE);
}

/*
 *	Add a reference to the specified pmap.
 */
void
pmap_reference(pmap)
	pmap_t	pmap;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_reference(%p)\n", pmap);
#endif
	if (pmap != NULL) {
		simple_lock(&pmap->pm_lock);
		pmap->pm_count++;
		simple_unlock(&pmap->pm_lock);
	}
}

/*
 *	Remove the given range of addresses from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the page size.
 */
void
pmap_remove(pmap, sva, eva)
	pmap_t pmap;
	vaddr_t sva, eva;
{
	paddr_t pa;
	vaddr_t va;
	u_int *pte;
	int flags;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
		printf("pmap_remove(%p, %lx, %lx)\n", pmap, sva, eva);
#endif

	if (pmap == NULL)
		return;

#ifdef DEBUG
	remove_stats.calls++;
#endif
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
	for (va = sva; va < eva; va += PAGE_SIZE) {
		/*
		 * Weed out invalid mappings.
		 * Note: we assume that the segment table is always allocated.
		 */
		if (!pmap_ste_v(pmap, va)) {
			/* XXX: avoid address wrap around */
			if (va >= m68k_trunc_seg((vaddr_t)-1))
				break;
			va = m68k_round_seg(va + PAGE_SIZE) - PAGE_SIZE;
			continue;
		}
		pte = pmap_pte(pmap, va);
		pa = pmap_pte_pa(pte);
		if (pa == 0)
			continue;
		pmap_remove_mapping(pmap, va, pte, flags);
	}
}

/*
 *	pmap_page_protect:
 *
 *	Lower the permission for all mappings to a given page.
 */
void
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
{
	paddr_t pa;
	pv_entry_t pv;
	int s;

	pa = VM_PAGE_TO_PHYS(pg);

#ifdef DEBUG
	if ((pmapdebug & (PDB_FOLLOW|PDB_PROTECT)) ||
	    (prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE)))
		printf("pmap_page_protect(%lx, %x)\n", pa, prot);
#endif
	if (!PAGE_IS_MANAGED(pa))
		return;

	switch (prot) {
	case VM_PROT_ALL:
		break;
	/* copy_on_write */
	case VM_PROT_READ:
	case VM_PROT_READ|VM_PROT_EXECUTE:
		pmap_changebit(pa, PG_RO, TRUE);
		break;
	/* remove_all */
	default:
		pv = pa_to_pvh(pa);
		s = splimp();
		while (pv->pv_pmap != NULL) {
			pt_entry_t	*pte;

			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
#ifdef DEBUG
			if (!pmap_ste_v(pv->pv_pmap,pv->pv_va) ||
			    pmap_pte_pa(pte) != pa)
{
    printf("pmap_page_protect: va %lx, pmap_ste_v %d pmap_pte_pa %08x/%lx\n",
	pv->pv_va, pmap_ste_v(pv->pv_pmap,pv->pv_va),
	pmap_pte_pa(pmap_pte(pv->pv_pmap,pv->pv_va)), pa);
    printf(" pvh %p pv %p pv_next %p\n", pa_to_pvh(pa), pv, pv->pv_next);
			panic("pmap_page_protect: bad mapping");
}
#endif
			if (!pmap_pte_w(pte))
				pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
				    pte, PRM_TFLUSH|PRM_CFLUSH);
			else {
				pv = pv->pv_next;
#ifdef DEBUG
				if (pmapdebug & PDB_PARANOIA)
				printf("%s wired mapping for %lx not removed\n",
					     "pmap_page_protect:", pa);
#endif
				if (pv == NULL)
					break;
			}
		}
		splx(s);
		break;
	}
}

/*
 *	Set the physical protection on the
 *	specified range of this map as requested.
 */
void
pmap_protect(pmap, sva, eva, prot)
	pmap_t pmap;
	vaddr_t	sva, eva;
	vm_prot_t prot;
{
	u_int *pte;
	vaddr_t va;
	boolean_t needtflush;
	int isro;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_PROTECT))
		printf("pmap_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva,
		    prot);
#endif
	if (pmap == NULL)
		return;

	if ((prot & VM_PROT_READ) == VM_PROT_NONE) {
		pmap_remove(pmap, sva, eva);
		return;
	}
	if (prot & VM_PROT_WRITE)
		return;

	pte = pmap_pte(pmap, sva);
	isro = pte_prot(pmap, prot) == PG_RO ? 1 : 0;
	needtflush = active_pmap(pmap);
	for (va = sva; va < eva; va += PAGE_SIZE) {
		/*
		 * Page table page is not allocated.
		 * Skip it, we don't want to force allocation
		 * of unnecessary PTE pages just to set the protection.
		 */
		if (!pmap_ste_v(pmap, va)) {
			/* XXX: avoid address wrap around */
			if (va >= m68k_trunc_seg((vaddr_t)-1))
				break;
			va = m68k_round_seg(va + PAGE_SIZE) - PAGE_SIZE;
			pte = pmap_pte(pmap, va);
			pte++;
			continue;
		}
		/*
		 * skip if page not valid or protection is same
		 */
		if (!pmap_pte_v(pte) || !pmap_pte_prot_chg(pte, isro)) {
			pte++;
			continue;
		}
#if defined(M68040) || defined(M68060)
		/*
		 * Clear caches if making RO (see section
		 * "7.3 Cache Coherency" in the manual).
		 */
		if (isro && mmutype == MMU_68040) {
			paddr_t pa = pmap_pte_pa(pte);

			DCFP(pa);
			ICPP(pa);
		}
#endif
		pmap_pte_set_prot(pte, isro);
		if (needtflush)
			TBIS(va);
		pte++;
	}
}

/*
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte can not be reclaimed.
 *
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */
extern int kernel_copyback;

int
pmap_enter(pmap, va, pa, prot, flags)
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
	vm_prot_t prot;
	int flags;
{
	u_int *pte;
	int npte;
	paddr_t opa;
	boolean_t cacheable = TRUE;
	boolean_t checkpv = TRUE;
	boolean_t wired = (flags & PMAP_WIRED) != 0;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER))
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n", pmap, va, pa,
		    prot, wired);
#endif

#ifdef DEBUG
	if (pmap == pmap_kernel())
		enter_stats.kernel++;
	else
		enter_stats.user++;
#endif
	/*
	 * For user mapping, allocate kernel VM resources if necessary.
	 */
	if (pmap->pm_ptab == NULL)
		pmap->pm_ptab = (pt_entry_t *)
			uvm_km_valloc_wait(pt_map, AMIGA_UPTSIZE);

	/*
	 * Segment table entry not valid, we need a new PT page
	 */
	if (!pmap_ste_v(pmap, va))
		pmap_enter_ptpage(pmap, va);

	pte = pmap_pte(pmap, va);
	opa = pmap_pte_pa(pte);
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: pte %p, *pte %x\n", pte, *(int *)pte);
#endif

	/*
	 * Mapping has not changed, must be protection or wiring change.
	 */
	if (opa == pa) {
#ifdef DEBUG
		enter_stats.pwchange++;
#endif
		/*
		 * Wiring change, just update stats.
		 * We don't worry about wiring PT pages as they remain
		 * resident as long as there are valid mappings in them.
		 * Hence, if a user page is wired, the PT page will be also.
		 */
		if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))){
#ifdef DEBUG
			if (pmapdebug & PDB_ENTER)
				printf("enter: wiring change -> %x\n", wired);
#endif
			if (wired)
				pmap->pm_stats.wired_count++;
			else
				pmap->pm_stats.wired_count--;
#ifdef DEBUG
			enter_stats.wchange++;
#endif
		}
		/*
		 * Retain cache inhibition status
		 */
		checkpv = FALSE;
		if (pmap_pte_ci(pte))
			cacheable = FALSE;
		goto validate;
	}

	/*
	 * Mapping has changed, invalidate old range and fall through to
	 * handle validating new mapping.
	 */
	if (opa) {
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: removing old mapping %lx\n", va);
#endif
		pmap_remove_mapping(pmap, va, pte,
			PRM_TFLUSH|PRM_CFLUSH|PRM_KEEPPTPAGE);
#ifdef DEBUG
		enter_stats.mchange++;
#endif
	}

	/*
	 * If this is a new user mapping, increment the wiring count
	 * on this PT page.  PT pages are wired down as long as there
	 * is a valid mapping in the page.
	 */
	if (pmap != pmap_kernel())
		pmap_ptpage_addref(trunc_page((vaddr_t)pte));

	/*
	 * Enter on the PV list if part of our managed memory
	 * Note that we raise IPL while manipulating pv_table
	 * since pmap_enter can be called at interrupt time.
	 */
	if (PAGE_IS_MANAGED(pa)) {
		pv_entry_t pv, npv;
		int s;

#ifdef DEBUG
		enter_stats.managed++;
#endif
		pv = pa_to_pvh(pa);
		s = splimp();
#ifdef DEBUG
		if (pmapdebug & PDB_ENTER)
			printf("enter: pv at %p: %lx/%p/%p\n", pv, pv->pv_va,
			    pv->pv_pmap, pv->pv_next);
#endif
		/*
		 * No entries yet, use header as the first entry
		 */
		if (pv->pv_pmap == NULL) {
#ifdef DEBUG
			enter_stats.firstpv++;
#endif
			pv->pv_va = va;
			pv->pv_pmap = pmap;
			pv->pv_next = NULL;
			pv->pv_ptste = NULL;
			pv->pv_ptpmap = NULL;
			pv->pv_flags = 0;
		}
		/*
		 * There is at least one other VA mapping this page.
		 * Place this entry after the header.
		 */
		else {
#ifdef DEBUG
			for (npv = pv; npv; npv = npv->pv_next)
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					panic("pmap_enter: already in pv_tab");
#endif
			npv = pmap_alloc_pv();
			npv->pv_va = va;
			npv->pv_pmap = pmap;
			npv->pv_next = pv->pv_next;
			npv->pv_ptste = NULL;
			npv->pv_ptpmap = NULL;
			pv->pv_next = npv;
#ifdef DEBUG
			if (!npv->pv_next)
				enter_stats.secondpv++;
#endif
		}
		splx(s);
	}
	/*
	 * Assumption: if it is not part of our managed memory
	 * then it must be device memory which may be volitile.
	 */
	else if (pmap_initialized) {
		checkpv = cacheable = FALSE;
#ifdef DEBUG
		enter_stats.unmanaged++;
#endif
	}

	/*
	 * Increment counters
	 */
	pmap->pm_stats.resident_count++;
	if (wired)
		pmap->pm_stats.wired_count++;

validate:
	/*
	 * Now validate mapping with desired protection/wiring.
	 * Assume uniform modified and referenced status for all
	 * AMIGA pages in a MACH page.
	 */
#if defined(M68040) || defined(M68060)
#if DEBUG
	if (pmapdebug & 0x10000 && mmutype == MMU_68040 && 
	    pmap == pmap_kernel()) {
		char *s;
		if (va >= amiga_uptbase && 
		    va < (amiga_uptbase + AMIGA_UPTMAXSIZE))
			s = "UPT";
		else if (va >= (u_int)Sysmap && 
		    va < ((u_int)Sysmap + AMIGA_KPTSIZE))
			s = "KPT";
		else if (va >= (u_int)pmap->pm_stab && 
		    va < ((u_int)pmap->pm_stab + AMIGA_STSIZE))
			s = "KST";
		else if (curproc && 
		    va >= (u_int)curproc->p_vmspace->vm_map.pmap->pm_stab &&
		    va < ((u_int)curproc->p_vmspace->vm_map.pmap->pm_stab +
		    AMIGA_STSIZE))
			s = "UST";
		else
			s = "other";
		printf("pmap_init: validating %s kernel page at %lx -> %lx\n",
		    s, va, pa);

	}
#endif
	if (mmutype == MMU_68040 && pmap == pmap_kernel() &&
	    ((va >= amiga_uptbase && va < (amiga_uptbase + AMIGA_UPTMAXSIZE)) ||
	     (va >= (u_int)Sysmap && va < ((u_int)Sysmap + AMIGA_KPTSIZE))))
		cacheable = FALSE;	/* don't cache user page tables */

	/* Don't cache if process can't take it, like SunOS ones.  */
	if (mmutype == MMU_68040 && pmap != pmap_kernel() &&
	    (curproc->p_md.md_flags & MDP_UNCACHE_WX) &&
	    (prot & VM_PROT_EXECUTE) && (prot & VM_PROT_WRITE))
		checkpv = cacheable = FALSE;
#endif
	npte = (pa & PG_FRAME) | pte_prot(pmap, prot) | PG_V;
	npte |= (*(int *)pte & (PG_M|PG_U));
	if (wired)
		npte |= PG_W;
	if (!checkpv && !cacheable)
#if defined(M68060) && defined(NO_SLOW_CIRRUS)
#if defined(M68040) || defined(M68030) || defined(M68020)
		npte |= (cputype == CPU_68060 ? PG_CIN : PG_CI);
#else
		npte |= PG_CIN;
#endif
#else
		npte |= PG_CI;
#endif
#if defined(M68040) || defined(M68060)
	else if (mmutype == MMU_68040 && (npte & PG_PROT) == PG_RW &&
	    (kernel_copyback || pmap != pmap_kernel()))
		npte |= PG_CCB;		/* cache copyback */
#endif
	/*
	 * Remember if this was a wiring-only change.
	 * If so, we need not flush the TLB and caches.
	 */
	wired = ((*(int *)pte ^ npte) == PG_W);
#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040 && !wired) {
		DCFP(pa);
		ICPP(pa);
	}
#endif
#ifdef DEBUG
	if (pmapdebug & PDB_ENTER)
		printf("enter: new pte value %x\n", npte);
#endif
	*(int *)pte++ = npte;
	if (!wired && active_pmap(pmap))
		TBIS(va);
#ifdef DEBUG
	if ((pmapdebug & PDB_WIRING) && pmap != pmap_kernel()) {
		va -= PAGE_SIZE;
		pmap_check_wiring("enter", trunc_page((vaddr_t)pmap_pte(pmap, va)));
	}
#endif

	return (0);
}

/*
 *	Routine:	pmap_unwire
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */
void
pmap_unwire(pmap, va)
	pmap_t	pmap;
	vaddr_t	va;
{
	u_int *pte;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_unwire(%p, %lx)\n", pmap, va);
#endif
	if (pmap == NULL)
		return;

	pte = pmap_pte(pmap, va);
#ifdef DEBUG
	/*
	 * Page table page is not allocated.
	 * Should this ever happen?  Ignore it for now,
	 * we don't want to force allocation of unnecessary PTE pages.
	 */
	if (!pmap_ste_v(pmap, va)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid STE for %lx\n",
			    va);
		return;
	}
	/*
	 * Page not valid.  Should this ever happen?
	 * Just continue and change wiring anyway.
	 */
	if (!pmap_pte_v(pte)) {
		if (pmapdebug & PDB_PARANOIA)
			printf("pmap_unwire: invalid PTE for %lx\n",
			    va);
	}
#endif
	if (pmap_pte_w(pte)) {
			pmap->pm_stats.wired_count--;
	}
	/*
	 * Wiring is not a hardware characteristic so there is no need
	 * to invalidate TLB.
	 */
	pmap_pte_set_w(pte, 0);
}

/*
 *	Routine:	pmap_extract
 *	Function:
 *		Extract the physical page address associated
 *		with the given map/virtual_address pair.
 */

boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t va;
	paddr_t *pap;
{
	paddr_t pa;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_extract(%p, %lx) -> ", pmap, va);
#endif
	if (pmap && pmap_ste_v(pmap, va))
		pa = *(int *)pmap_pte(pmap, va);
	else
		return (FALSE);
	*pap = (pa & PG_FRAME) | (va & ~PG_FRAME);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("%lx\n", *pap);
#endif
	return (TRUE);
}

/*
 *	Copy the range specified by src_addr/len
 *	from the source map to the range dst_addr/len
 *	in the destination map.
 *
 *	This routine is only advisory and need not do anything.
 */
void pmap_copy(dst_pmap, src_pmap, dst_addr, len, src_addr)
	pmap_t	dst_pmap;
	pmap_t	src_pmap;
	vaddr_t	dst_addr;
	vsize_t	len;
	vaddr_t	src_addr;
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n", dst_pmap,
		    src_pmap, dst_addr, len, src_addr);
#endif
}

/*
 *	Routine:	pmap_collect
 *	Function:
 *		Garbage collects the physical map system for
 *		pages which are no longer used.
 *		Success need not be guaranteed -- that is, there
 *		may well be pages which are not referenced, but
 *		others may be collected.
 *	Usage:
 *		Called by the pageout daemon when pages are scarce.
 */
void
pmap_collect(pmap)
	pmap_t		pmap;
{
	int bank, s;

	if (pmap != pmap_kernel())
		return;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_collect(%p)\n", pmap);
	kpt_stats.collectscans++;
#endif
	s = splimp();

	for (bank = 0; bank < vm_nphysseg; bank++)
		pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			      ptoa(vm_physmem[bank].end));

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
       splx(s);
}

/*
 *     Routine:        pmap_collect1()
 *
 *     Function:
 *             Helper function for pmap_collect().  Do the actual
 *             garbage-collection of range of physical addresses.
 */
void
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	paddr_t		startpa, endpa;
{
	paddr_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	paddr_t kpa;
#ifdef DEBUG
	int *ste;
	int opmapdebug = 0;
#endif

	for (pa = startpa; pa < endpa; pa += NBPG) {
		struct kpt_page *kpt, **pkpt;

		/*
		 * Locate physical pages which are being used as kernel
		 * page table pages.
		 */
		pv = pa_to_pvh(pa);
		if (pv->pv_pmap != pmap_kernel() ||
		    !(pv->pv_flags & PV_PTPAGE))
			continue;
		do {
			if (pv->pv_ptste && pv->pv_ptpmap == pmap_kernel())
				break;
		} while ((pv = pv->pv_next) > 0);
		if (pv == NULL)
			continue;
#ifdef DEBUG
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + AMIGA_KPTSIZE)
			printf("collect: kernel PT VA out of range\n");
		else
			goto ok;
		pmap_pvdump(pa);
		continue;
ok:
#endif
		pte = (int *)(pv->pv_va + NBPG);
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
			;
		if (pte >= (pt_entry_t *)pv->pv_va)
			continue;

#ifdef DEBUG
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT)) {
			printf(
			    "collect: freeing KPT page at %lx (ste %x@@%p)\n",
			    pv->pv_va, *(int *)pv->pv_ptste, pv->pv_ptste);
			opmapdebug = pmapdebug;
			pmapdebug |= PDB_PTPAGE;
		}

		ste = (int *)pv->pv_ptste;
#endif
		/*
		 * If all entries were invalid we can remove the page.
		 * We call pmap_remove to take care of invalidating ST
		 * and Sysptmap entries.
		 */
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				PRM_TFLUSH|PRM_CFLUSH);

		/*
		 * Use the physical address to locate the original
		 * (kmem_alloc assigned) address for the page and put
		 * that page back on the free list.
		 */
		for (pkpt = &kpt_used_list, kpt = *pkpt;
		     kpt != (struct kpt_page *)0;
		     pkpt = &kpt->kpt_next, kpt = *pkpt)
			if (kpt->kpt_pa == kpa)
				break;
#ifdef DEBUG
		if (kpt == (struct kpt_page *)0)
			panic("pmap_collect: lost a KPT page");
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			printf("collect: %lx (%lx) to free list\n",
			    kpt->kpt_va, kpa);
#endif
		*pkpt = kpt->kpt_next;
		kpt->kpt_next = kpt_free_list;
		kpt_free_list = kpt;
#ifdef DEBUG
		kpt_stats.kptinuse--;
		kpt_stats.collectpages++;
		if (pmapdebug & (PDB_PTPAGE|PDB_COLLECT))
			pmapdebug = opmapdebug;

		if (*ste)
			printf("collect: kernel STE at %p still valid (%x)\n",
			    ste, *ste);
		ste =
		    (int *)&Sysptmap[(u_int *)ste-pmap_ste(pmap_kernel(), 0)];
		if (*ste)
			printf(
			    "collect: kernel PTmap at %p still valid (%x)\n",
			    ste, *ste);
#endif
	}
}

/*
 *	Mark that a processor is about to be used by a given pmap.
 */
void
pmap_activate(p)
	struct proc *p;
{
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_SEGTAB))
		printf("pmap_activate(%p)\n", p);
#endif
	PMAP_ACTIVATE(pmap, p == curproc);
}

/*
 *	Mark that a processor is no longer in use by a given pmap.
 */
void
pmap_deactivate(p)
	struct proc *p;
{
}

/*
 *	pmap_zero_page zeros the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bzero to clear its contents, one machine dependent page
 *	at a time.
 */
void
pmap_zero_page(struct vm_page *pg)
{
	paddr_t	phys = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_zero_page(%lx)\n", phys);
#endif
	phys >>= PG_SHIFT;
	clearseg(phys);
}

/*
 *	pmap_copy_page copies the specified (machine independent)
 *	page by mapping the page into virtual memory and using
 *	bcopy to copy the page, one machine dependent page at a
 *	time.
 */
void
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
{
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_copy_page(%lx, %lx)\n", src, dst);
#endif
	src >>= PG_SHIFT;
	dst >>= PG_SHIFT;
	physcopyseg(src, dst);
}

/*
 *	Clear the modify bits on the specified physical page.
 */

boolean_t
pmap_clear_modify(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_modify(%lx)\n", pa);
#endif
	ret = pmap_is_modified(pg);

	pmap_changebit(pa, PG_M, FALSE);

	return (ret);
}

/*
 *	pmap_clear_reference:
 *
 *	Clear the reference bit on the specified physical page.
 */

boolean_t
pmap_clear_reference(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_clear_reference(%lx)\n", pa);
#endif
	ret = pmap_is_referenced(pg);
	pmap_changebit(pa, PG_U, FALSE);

	return (ret);
}

/*
 *	pmap_is_referenced:
 *
 *	Return whether or not the specified physical page is referenced
 *	by any physical maps.
 */

boolean_t
pmap_is_referenced(struct vm_page *pg)
{
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_U);
		printf("pmap_is_referenced(%lx) -> %c\n", pa, "FT"[rv]);
		return (rv);
	}
#endif
	return (pmap_testbit(pa, PG_U));
}

/*
 *	pmap_is_modified:
 *
 *	Return whether or not the specified physical page is modified
 *	by any physical maps.
 */

boolean_t
pmap_is_modified(struct vm_page *pg)
{
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW) {
		boolean_t rv = pmap_testbit(pa, PG_M);
		printf("pmap_is_modified(%lx) -> %c\n", pa, "FT"[rv]);
		return (rv);
	}
#endif
	return (pmap_testbit(pa, PG_M));
}

paddr_t
pmap_phys_address(ppn)
	int ppn;
{
	return(m68k_ptob(ppn));
}

/*
 * Miscellaneous support routines follow
 */

/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 *
 *	If (flags & PRM_KEEPPTPAGE), we don't free the page table page
 *	if the reference drops to zero.
 */
static void
pmap_remove_mapping(pmap, va, pte, flags)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	int flags;
{
	paddr_t pa;
	struct pv_entry *pv, *npv;
	pmap_t ptpmap;
	st_entry_t *ste;
	int s, bits;
#if defined(M68040) || defined(M68060)
	int i;
#endif
#ifdef DEBUG
	pt_entry_t opte;
#endif

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
	    printf("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    		pmap, va, pte, flags);
#endif

	/*
	 * PTE not provided, compute it from pmap and va.
	 */
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
	}

	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	opte = *pte;
#endif
	/*
	 * Update statistics
	 */
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf ("remove: invalidating pte at %p\n", pte);
#endif

	bits = *pte & (PG_U|PG_M);
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS(va);
	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.
	 */
	if (pmap != pmap_kernel()) {
		vaddr_t ptpva = trunc_page((vaddr_t)pte);
		int refs = pmap_ptpage_delref(ptpva);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", ptpva);
#endif
		/*
		 * If reference count drops to 1, and we're not instructed
		 * to keep it around, free the PT page.
		 *
		 * Note: refcnt == 1 comes from the fact that we allocate
		 * the page with uvm_fault_wire(), which initially wires
		 * the page.  The first reference we actually add causes
		 * the refcnt to be 2.
		 */
		if (refs == 1 && (flags & PRM_KEEPPTPAGE) == 0) {
			struct pv_entry *pv;
			paddr_t pa;

			pa = pmap_pte_pa(pmap_pte(pmap_kernel(), ptpva));
#ifdef DIAGNOSTIC
			if (PAGE_IS_MANAGED(pa) == 0)
				panic("pmap_remove_mapping: unmanaged PT page");
#endif
			pv = pa_to_pvh(pa);
#ifdef DIAGNOSTIC
			if (pv->pv_ptste == NULL)
				panic("pmap_remove_mapping: ptste == NULL");
			if (pv->pv_pmap != pmap_kernel() ||
			    pv->pv_va != ptpva ||
			    pv->pv_next != NULL)
				panic("pmap_remove_mapping: "
				    "bad PT page pmap %p, va 0x%lx, next %p",
				    pv->pv_pmap, pv->pv_va, pv->pv_next);
#endif
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
			    NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(PHYS_TO_VM_PAGE(pa));
#ifdef DEBUG
			if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
			    printf("remove: PT page 0x%lx (0x%lx) freed\n",
				    ptpva, pa);
#endif
		}
	}

	/*
	 * If this isn't a managed page, we are all done.
	 */
	if (PAGE_IS_MANAGED(pa) == 0)
		return;
	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */
	pv = pa_to_pvh(pa);
	ste = ST_ENTRY_NULL;
	s = splimp();
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptste;
		ptpmap = pv->pv_ptpmap;
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
#ifdef DEBUG
		remove_stats.pvfirst++;
#endif
	} else {
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
#ifdef DEBUG
			remove_stats.pvsearch++;
#endif
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
			pv = npv;
		}
#ifdef DEBUG
		if (npv == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = npv->pv_ptste;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		pmap_free_pv(npv);
		pv = pa_to_pvh(pa);
	}

	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */
	if (ste) {
#ifdef DEBUG
		remove_stats.ptinvalid++;
		if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
		    printf("remove: ste was %x@@%p pte was %x@@%p\n",
			    *ste, ste, opte, pmap_pte(pmap, va));
#endif
#if defined(M68040) || defined(M68060)
		if (mmutype == MMU_68040) {
		    /*
		     * On the 68040, the PT page contains NPTEPG/SG4_LEV3SIZE
		     * page tables, so we need to remove all the associated
		     * segment table entries
		     * (This may be incorrect:  if a single page table is
		     * being removed, the whole page should not be
		     * removed.)
		     */
		    for (i = 0; i < NPTEPG / SG4_LEV3SIZE; ++i)
			*ste++ = SG_NV;
		    ste -= NPTEPG / SG4_LEV3SIZE;
#ifdef DEBUG
		    if (pmapdebug &(PDB_REMOVE|PDB_SEGTAB|0x10000))
			printf("pmap_remove:PT at %lx removed\n", va);
#endif
		} else
#endif /* defined(M68040) || defined(M68060) */
		*ste = SG_NV;
		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */
		if (ptpmap != pmap_kernel()) {
#ifdef DEBUG
			if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
				printf("remove: stab %p, refcnt %d\n",
					ptpmap->pm_stab,
					ptpmap->pm_sref - 1);
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
#ifdef DEBUG
				if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
				    printf("remove: free stab %p\n",
					    ptpmap->pm_stab);
#endif
				uvm_km_free_wakeup(kernel_map,
				    (vaddr_t)ptpmap->pm_stab, AMIGA_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
				if (mmutype == MMU_68040)
					ptpmap->pm_stfree = protostfree;
#endif
				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 */
				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
#if 0
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
#endif
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
	}
	/*
	 * Update saved attributes for managed page
	 */
	*pa_to_attribute(pa) |= bits;
	splx(s);
}

/*
 * pmap_ptpage_addref:
 *
 *	Add a reference to the specified PT page.
 */
void
pmap_ptpage_addref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *m;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	m->wire_count++;
	simple_unlock(&uvm.kernel_object->vmobjlock);
}

/*
 * pmap_ptpage_delref:
 *
 *	Delete a reference to the specified PT page.
 */
int
pmap_ptpage_delref(ptpva)
	vaddr_t ptpva;
{
	struct vm_page *m;
	int rv;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --m->wire_count;
	simple_unlock(&uvm.kernel_object->vmobjlock);
	return (rv);
}

static void
amiga_protection_init()
{
	int *kp, prot;

	kp = protection_codes;
	for (prot = 0; prot < 8; prot++) {
		switch (prot) {
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_NONE:
			*kp++ = 0;
			break;
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_NONE | VM_PROT_EXECUTE:
		case VM_PROT_NONE | VM_PROT_NONE | VM_PROT_EXECUTE:
			*kp++ = PG_RO;
			break;
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_NONE:
		case VM_PROT_NONE | VM_PROT_WRITE | VM_PROT_EXECUTE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_NONE:
		case VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE:
			*kp++ = PG_RW;
			break;
		}
	}
}

/* static */
boolean_t
pmap_testbit(pa, bit)
	paddr_t pa;
	int bit;
{
	pv_entry_t pv;
	int *pte;
	int s;

	if (!PAGE_IS_MANAGED(pa))
		return (FALSE);

	pv = pa_to_pvh(pa);
	s = splimp();
	/*
	 * Check saved info first
	 */
	if (*pa_to_attribute(pa) & bit) {
		splx(s);
		return (TRUE);
	}
	/*
	 * Not found, check current mappings returning
	 * immediately if found.
	 */
	if (pv->pv_pmap != NULL) {
		for (; pv; pv = pv->pv_next) {
			pte = (int *)pmap_pte(pv->pv_pmap, pv->pv_va);
			if (*pte & bit) {
				splx(s);
				return (TRUE);
			}
		}
	}
	splx(s);
	return (FALSE);
}

static void
pmap_changebit(pa, bit, setem)
	paddr_t pa;
	int bit;
	boolean_t setem;
{
	pv_entry_t pv;
	int *pte, npte;
	vaddr_t va;
	boolean_t firstpage;
	int s;

	firstpage = TRUE;

#ifdef DEBUG
	if (pmapdebug & PDB_BITS)
		printf("pmap_changebit(%lx, %x, %s)\n", pa, bit,
		    setem ? "set" : "clear");
#endif
	if (!PAGE_IS_MANAGED(pa))
		return;

	pv = pa_to_pvh(pa);
	s = splimp();
	/*
	 * Clear saved attributes (modify, reference)
	 */
	if (!setem)
		*pa_to_attribute(pa) &= ~bit;
	/*
	 * Loop over all current mappings setting/clearing as appropos
	 * If setting RO do we need to clear the VAC?
	 */
	if (pv->pv_pmap == NULL) {
		splx(s);
		return;
	}
	for (; pv; pv = pv->pv_next) {
		va = pv->pv_va;

		/*
		 * XXX don't write protect pager mappings
		 */
		if (bit == PG_RO) {
			if (va >= uvm.pager_sva && va < uvm.pager_eva)
				continue;
		}

		pte = (int *)pmap_pte(pv->pv_pmap, va);
		if (setem)
			npte = *pte | bit;
		else
			npte = *pte & ~bit;
		if (*pte != npte) {
			/*
			 * If we are changing caching status or
			 * protection make sure the caches are
			 * flushed (but only once).
			 */
#if defined(M68040) || defined(M68060)
			if (firstpage && mmutype == MMU_68040 &&
			    ((bit == PG_RO && setem) || (bit & PG_CMASK))) {
				firstpage = FALSE;
				DCFP(pa);
				ICPP(pa);
			}
#endif
			*pte = npte;
			if (active_pmap(pv->pv_pmap))
				TBIS(va);
		}
	}
	splx(s);
}

/* static */
void
pmap_enter_ptpage(pmap, va)
	pmap_t pmap;
	vaddr_t va;
{
	paddr_t ptpa;
	pv_entry_t pv;
#ifdef M68060
	u_int stpa;
#endif
	u_int *ste;
	int s;

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_ENTER|PDB_PTPAGE))
		printf("pmap_enter_ptpage: pmap %p, va %lx\n", pmap, va);
	enter_stats.ptpneeded++;
#endif
	/*
	 * Allocate a segment table if necessary.  Note that it is allocated
	 * from kernel_map and not pt_map.  This keeps user page tables
	 * aligned on segment boundaries in the kernel address space.
	 * The segment table is wired down.  It will be freed whenever the
	 * reference count drops to zero.
	 */
	if (pmap->pm_stab == Segtabzero) {
		/* XXX Atari uses kernel_map here: */
		pmap->pm_stab = (st_entry_t *)
			uvm_km_zalloc(kernel_map, AMIGA_STSIZE);
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab,
			(paddr_t *)&pmap->pm_stpa);
#if defined(M68040) || defined(M68060)
		if (mmutype == MMU_68040) {
#if defined(M68060)
			stpa = (u_int)pmap->pm_stpa;
			if (machineid & AMIGA_68060) {
				while (stpa < (u_int)pmap->pm_stpa + 
				    AMIGA_STSIZE) {
					pmap_changebit(stpa, PG_CCB, 0);
					pmap_changebit(stpa, PG_CI, 1);
					stpa += NBPG;
				}
				DCIS(); /* XXX */
	 		}
#endif
			pmap->pm_stfree = protostfree;
		}
#endif
		/*
		 * XXX may have changed segment table pointer for current
		 * process so update now to reload hardware.
		 */
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter_pt: pmap %p stab %p(%p)\n", pmap,
			    pmap->pm_stab, pmap->pm_stpa);
#endif
	}

	ste = pmap_ste(pmap, va);

#if defined(M68040) || defined(M68060)
	/*
	 * Allocate level 2 descriptor block if necessary
	 */
	if (mmutype == MMU_68040) {
		if (*ste == SG_NV) {
			int ix;
			caddr_t addr;

			ix = bmtol2(pmap->pm_stfree);
			if (ix == -1)
				panic("enter_pt: out of address space");
			pmap->pm_stfree &= ~l2tobm(ix);
			addr = (caddr_t)&pmap->pm_stab[ix * SG4_LEV2SIZE];
			bzero(addr, SG4_LEV2SIZE * sizeof(st_entry_t));
			addr = (caddr_t)&pmap->pm_stpa[ix * SG4_LEV2SIZE];
			*ste = (u_int) addr | SG_RW | SG_U | SG_V;
#ifdef DEBUG
			if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
				printf("enter_pt: alloc ste2 %d(%p)\n", ix,
				    addr);
#endif
		}
		ste = pmap_ste2(pmap, va);
		/*
		 * Since a level 2 descriptor maps a block of SG4_LEV3SIZE
		 * level 3 descriptors, we need a chunk of NPTEPG/SEG4_LEV3SIZE
		 * (64) such descriptors (NBPG/SG4_LEV3SIZE bytes) to map a
		 * PT page -- the unit of allocation.  We set 'ste' to point
		 * to the first entry of that chunk which is validated in its
		 * entirety below.
		 */
		ste = (u_int *)((int)ste & ~(NBPG / SG4_LEV3SIZE - 1));
#ifdef DEBUG
		if (pmapdebug &  (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter_pt: ste2 %p (%p)\n", pmap_ste2(pmap, va),
			    ste);
#endif
	}
#endif
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));

	/*
	 * In the kernel we allocate a page from the kernel PT page
	 * free list and map it into the kernel page table map (via
	 * pmap_enter).
	 */
	if (pmap == pmap_kernel()) {
		struct kpt_page *kpt;

		s = splimp();
		if ((kpt = kpt_free_list) == (struct kpt_page *)0) {
			/*
			 * No PT pages available.
			 * Try once to free up unused ones.
			 */
#ifdef DEBUG
			if (pmapdebug & PDB_COLLECT)
				printf(
				    "enter_pt: no KPT pages, collecting...\n");
#endif
			pmap_collect(pmap_kernel());
			if ((kpt = kpt_free_list) == (struct kpt_page *)0)
				panic("pmap_enter_ptpage: can't get KPT page");
		}
#ifdef DEBUG
		if (++kpt_stats.kptinuse > kpt_stats.kptmaxuse)
			kpt_stats.kptmaxuse = kpt_stats.kptinuse;
#endif
		kpt_free_list = kpt->kpt_next;
		kpt->kpt_next = kpt_used_list;
		kpt_used_list = kpt;
		ptpa = kpt->kpt_pa;
		bzero((char *)kpt->kpt_va, NBPG);
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
			   VM_PROT_DEFAULT|PMAP_WIRED);
#if defined(M68060)
		if (machineid & AMIGA_68060) {
			pmap_changebit(ptpa, PG_CCB, 0);
			pmap_changebit(ptpa, PG_CI, 1);
			DCIS();
	 	}
#endif
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
			printf(
			    "enter_pt: add &Sysptmap[%d]: %x (KPT page %lx)\n",
			    ste - pmap_ste(pmap, 0),
			    *(int *)&Sysptmap[ste - pmap_ste(pmap, 0)],
			    kpt->kpt_va);
#endif
		splx(s);
	}
	/*
	 * For user processes we just simulate a fault on that location
	 * letting the VM system allocate a zero-filled page.
	 *
	 * Note we use a wire-fault to keep the page off the paging
	 * queues.  This sets our PT page's reference (wire) count to
	 * 1, which is what we use to check if the page can be freed.
	 * See pmap_remove_mapping().
	 */
	else {
		/*
		 * Count the segment table reference now so that we won't
		 * lose the segment table when low on memory.
		 */
		pmap->pm_sref++;
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
			printf("enter_pt: about to fault UPT pg at %lx\n", va);
#endif
		s = uvm_fault_wire(pt_map, va, va + PAGE_SIZE,
		    VM_PROT_READ|VM_PROT_WRITE);
		if (s) {
			printf("uvm_fault_wire(pt_map, 0x%lx, 0%lx, RW) "
				"-> %d\n", va, va + PAGE_SIZE, s);
			panic("pmap_enter: uvm_fault_wire failed");
		}
		ptpa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
#if 0 /* XXXX what is this? XXXX */
		/*
		 * Mark the page clean now to avoid its pageout (and
		 * hence creation of a pager) between now and when it
		 * is wired; i.e. while it is on a paging queue.
		 */
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_CLEAN;
#endif
	}

#ifdef M68060
	if (machineid & M68060) {
		pmap_changebit(ptpa, PG_CCB, 0);
		pmap_changebit(ptpa, PG_CI, 1);
		DCIS();
	}
#endif
	/*
	 * Locate the PV entry in the kernel for this PT page and
	 * record the STE address.  This is so that we can invalidate
	 * the STE when we remove the mapping for the page.
	 */
	pv = pa_to_pvh(ptpa);
	s = splimp();
	if (pv) {
		pv->pv_flags |= PV_PTPAGE;
		do {
			if (pv->pv_pmap == pmap_kernel() && pv->pv_va == va)
				break;
		} while ((pv = pv->pv_next) > 0);
	}
#ifdef DEBUG
	if (pv == NULL) {
		printf("enter_pt: PV entry for PT page %lx not found\n", ptpa);
		panic("pmap_enter_ptpage: PT page not entered");
	}
#endif
	pv->pv_ptste = ste;
	pv->pv_ptpmap = pmap;
#ifdef DEBUG
	if (pmapdebug & (PDB_ENTER|PDB_PTPAGE))
		printf("enter_pt: new PT page at PA %lx, ste at %p\n", ptpa,
		    ste);
#endif

	/*
	 * Map the new PT page into the segment table.
	 * Also increment the reference count on the segment table if this
	 * was a user page table page.  Note that we don't use vm_map_pageable
	 * to keep the count like we do for PT pages, this is mostly because
	 * it would be difficult to identify ST pages in pmap_pageable to
	 * release them.  We also avoid the overhead of vm_map_pageable.
	 */
#if defined(M68040) || defined(M68060)
	if (mmutype == MMU_68040) {
		u_int *este;

		for (este = &ste[NPTEPG / SG4_LEV3SIZE]; ste < este; ++ste) {
			*ste = ptpa | SG_U | SG_RW | SG_V;
			ptpa += SG4_LEV3SIZE * sizeof(st_entry_t);
		}
	}
	else
#endif
		*(int *)ste = (ptpa & SG_FRAME) | SG_RW | SG_V;
	if (pmap != pmap_kernel()) {
#ifdef DEBUG
		if (pmapdebug & (PDB_ENTER|PDB_PTPAGE|PDB_SEGTAB))
			printf("enter_pt: stab %p refcnt %d\n", pmap->pm_stab,
			    pmap->pm_sref);
#endif
	}
	/*
	 * Flush stale TLB info.
	 */
	if (pmap == pmap_kernel())
		TBIAS();
	else
		TBIAU();
	pmap->pm_ptpages++;
	splx(s);
}

#ifdef DEBUG
void
pmap_pvdump(pa)
	paddr_t pa;
{
	pv_entry_t pv;

	printf("pa %lx", pa);
	for (pv = pa_to_pvh(pa); pv; pv = pv->pv_next)
		printf(" -> pmap %p, va %lx, ptste %p, ptpmap %p, flags %x",
		    pv->pv_pmap, pv->pv_va, pv->pv_ptste, pv->pv_ptpmap,
		    pv->pv_flags);
	printf("\n");
}

/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
void
pmap_check_wiring(str, va)
	char *str;
	vaddr_t va;
{
	pt_entry_t *pte;
	paddr_t pa;
	struct vm_page *m;
	int count;

	if (!pmap_ste_v(pmap_kernel(), va) ||
	    !pmap_pte_v(pmap_pte(pmap_kernel(), va)))
		return;

	pa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
	m = PHYS_TO_VM_PAGE(pa);
	if (m->wire_count < 1) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, m->wire_count);
		return;
	}

	count = 0;
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va + NBPG); pte++)
		if (*pte)
			count++;
	if ((m->wire_count - 1) != count)
		printf("*%s*: 0x%lx: w%d/a%d\n",
		    str, va, (m->wire_count-1), count);
}
#endif

/*
 *     Routine:        pmap_virtual_space
 *
 *     Function:
 *             Report the range of available kernel virtual address
 *             space to the VM system during bootstrap.  Called by
 *             vm_bootstrap_steal_memory().
 */
void
pmap_virtual_space(vstartp, vendp)
	vaddr_t	*vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pmap_enter(pmap_kernel(), va, pa, prot,
		VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
}
@


1.51
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.50 2002/09/10 18:29:43 art Exp $	*/
@


1.50
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.49 2002/05/28 14:29:05 deraadt Exp $	*/
d521 1
a521 1
		panic("pmap_init: bogons in the VM system!\n");
@


1.49
log
@__P killing
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.48 2002/03/25 19:41:52 niklas Exp $	*/
d1668 1
a1668 2
pmap_zero_page(phys)
	paddr_t	phys;
d1670 1
d1686 1
a1686 2
pmap_copy_page(src, dst)
	paddr_t	src, dst;
d1688 2
@


1.48
log
@revert the pmap stuff to 0221, so we can boot
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/12/22 21:25:59 miod Exp $	*/
d184 2
a185 2
static void	pmap_check_wiring __P((char *, vaddr_t));
static void	pmap_pvdump __P((paddr_t));
d296 1
a296 1
extern vaddr_t reserve_dumppages __P((vaddr_t));
d298 10
a307 10
boolean_t	pmap_testbit __P((paddr_t, int));
void		pmap_enter_ptpage __P((pmap_t, vaddr_t)); 
static void	pmap_ptpage_addref __P((vaddr_t));
static int	pmap_ptpage_delref __P((vaddr_t));
static void	pmap_changebit __P((vaddr_t, int, boolean_t));
  struct pv_entry * pmap_alloc_pv __P((void));
void		pmap_free_pv __P((struct pv_entry *));
void		pmap_pinit __P((pmap_t));
void		pmap_release __P((pmap_t));
static void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));
d309 2
a310 2
static void	amiga_protection_init __P((void));
void		pmap_collect1	__P((pmap_t, paddr_t, paddr_t));
@


1.47
log
@Bring back pmap_motorola in service for amiga, too.
@
text
@@


1.46
log
@Compile with option DEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.45 2001/12/20 19:02:24 miod Exp $	*/
@


1.45
log
@Temporarily revert the pmap_motorola changes, as they may account for
some problems as well.
Requested by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.43 2001/11/28 16:24:26 art Exp $	*/
d793 1
a793 1
		printf("pmap_create(%lx)\n", size);
@


1.44
log
@Switch to pmap_motorola.
XXX Kernel compiles, but not tested.
@
text
@@


1.44.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.46 2001/12/22 21:25:59 miod Exp $	*/
d793 1
a793 1
		printf("pmap_create()\n");
@


1.44.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44.2.1 2002/01/31 22:55:06 niklas Exp $	*/
d184 2
a185 2
static void	pmap_check_wiring(char *, vaddr_t);
static void	pmap_pvdump(paddr_t);
d296 1
a296 1
extern vaddr_t reserve_dumppages(vaddr_t);
d298 10
a307 10
boolean_t	pmap_testbit(paddr_t, int);
void		pmap_enter_ptpage(pmap_t, vaddr_t);
static void	pmap_ptpage_addref(vaddr_t);
static int	pmap_ptpage_delref(vaddr_t);
static void	pmap_changebit(vaddr_t, int, boolean_t);
struct pv_entry *pmap_alloc_pv(void);
void		pmap_free_pv(struct pv_entry *);
void		pmap_pinit(pmap_t);
void		pmap_release(pmap_t);
static void	pmap_remove_mapping(pmap_t, vaddr_t, pt_entry_t *, int);
d309 2
a310 2
static void	amiga_protection_init(void);
void		pmap_collect1(pmap_t, paddr_t, paddr_t);
@


1.44.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44.2.2 2002/06/11 03:34:57 art Exp $	*/
d521 1
a521 1
		panic("pmap_init: bogons in the VM system!");
d1668 2
a1669 1
pmap_zero_page(struct vm_page *pg)
a1670 1
	paddr_t	phys = VM_PAGE_TO_PHYS(pg);
d1686 2
a1687 1
pmap_copy_page(struct vm_page *srcpg, struct vm_page *dstpg)
a1688 2
	paddr_t src = VM_PAGE_TO_PHYS(srcpg);
	paddr_t dst = VM_PAGE_TO_PHYS(dstpg);
@


1.44.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.44.2.3 2002/10/29 00:28:01 art Exp $	*/
@


1.43
log
@more typedef zapping vm_page_t -> struct vm_page *
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.42 2001/11/28 16:13:27 art Exp $	*/
@


1.42
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.41 2001/11/28 15:34:16 art Exp $	*/
d2069 1
a2069 1
	vm_page_t m;
d2086 1
a2086 1
	vm_page_t m;
d2532 1
a2532 1
	vm_page_t m;
@


1.41
log
@Make pmap_update functions into nops so that we can have a consistent
pmap_update API (right now it's nop).
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.40 2001/11/28 14:13:06 art Exp $	*/
d274 1
a274 1
vm_map_t	pt_map;
@


1.40
log
@pmap_kenter_pgs is not used and not really useful. remove.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.39 2001/11/28 13:47:37 art Exp $	*/
a1482 21
}

/*
 *	Require that all active physical maps contain no
 *	incorrect entries NOW.  [This update includes
 *	forcing updates of any address map caching.]
 *
 *	Generally used to insure that a thread about
 *	to run will see a semantically correct world.
 */
void pmap_update()
{
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_update()\n");
#endif
#if defined(M68060)
	if (machineid & AMIGA_68060)
		DCIA();
#endif
	TBIA();
@


1.39
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.38 2001/11/07 01:18:00 art Exp $	*/
a2598 12
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
@


1.38
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.37 2001/11/06 19:53:14 miod Exp $	*/
d507 1
a507 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
d514 1
a514 1
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
d605 1
a605 1
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d607 1
a607 3
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
d1376 1
a1376 1
	return (KERN_SUCCESS);
d2435 1
a2435 1
		if (s != KERN_SUCCESS) {
@


1.37
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.36 2001/11/06 01:47:02 art Exp $	*/
d504 1
a504 1
		    NULL, UVM_UNKNOWN_OFFSET,
d511 1
a511 1
		    NULL, UVM_UNKNOWN_OFFSET,
d602 1
a602 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
@


1.36
log
@redundant includes.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.35 2001/09/19 20:50:56 mickey Exp $	*/
a121 1
#include <uvm/uvm_extern.h>
@


1.35
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.34 2001/07/25 13:25:31 art Exp $	*/
a119 1
#include <vm/vm.h>
a120 1
#include <vm/vm_page.h>
@


1.34
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.33 2001/07/18 10:47:04 art Exp $	*/
a121 1
#include <vm/vm_kern.h>
d124 1
@


1.33
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.32 2001/06/27 03:54:13 art Exp $	*/
d772 1
a772 1
		pmap_enter(pmap_kernel(), virt, start, prot, FALSE, 0);
d1113 2
a1114 2
void
pmap_enter(pmap, va, pa, prot, wired, access_type)
d1119 1
a1119 2
	boolean_t wired;
	vm_prot_t access_type;
d1126 1
a1132 2
	if (pmap == NULL)
		return;
d1380 2
d2400 2
a2401 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE,
			   VM_PROT_DEFAULT);
d2602 2
a2603 1
	pmap_enter(pmap_kernel(), va, pa, prot, 1, VM_PROT_READ|VM_PROT_WRITE);
d2613 2
a2614 2
			VM_PROT_READ|VM_PROT_WRITE, 1,
			VM_PROT_READ|VM_PROT_WRITE);
@


1.32
log
@No more old VM on amiga.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.31 2001/06/08 08:08:41 art Exp $	*/
d791 2
a792 3
pmap_t
pmap_create(size)
	vsize_t	size;
d794 1
a794 1
	pmap_t pmap;
a799 5
	/*
	 * Software use map does not need a pmap
	 */
	if (size)
		return(NULL);
d801 1
a801 6
	/* XXX: is it ok to wait here? */
	pmap = (pmap_t)malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
#ifdef notifwewait
	if (pmap == NULL)
		panic("pmap_create: cannot allocate a pmap");
#endif
d963 1
a963 3
pmap_page_protect(pa, prot)
	paddr_t	pa;
	vm_prot_t	prot;
d965 1
d969 2
d1728 2
a1729 3
void
pmap_clear_modify(pa)
	paddr_t pa;
d1731 3
d1738 2
d1741 2
d1751 2
a1752 2
void pmap_clear_reference(pa)
	paddr_t	pa;
d1754 2
d1760 1
d1762 2
d1774 1
a1774 2
pmap_is_referenced(pa)
	paddr_t	pa;
d1776 1
d1795 1
a1795 2
pmap_is_modified(pa)
	paddr_t	pa;
d1797 1
d2597 26
@


1.31
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.30 2001/05/15 14:19:58 jj Exp $	*/
a123 1
#if defined(UVM)
a124 1
#endif
a277 1
#if defined(UVM)
a278 1
#endif
a368 1
#if defined(UVM)
a370 1
#endif
a378 1
#if defined(UVM)
a380 4
#else
	vm_page_physload(atop(fromads), atop(toads),
		atop(fromads), atop(toads));
#endif
a419 1
#if defined(UVM)
a422 4
#else
		vm_page_physload(atop(fromads), atop(toads),
			atop(fromads), atop(toads));
#endif
a503 1
#if defined(UVM)
a525 18
#else
	addr = amigahwaddr;
	(void)vm_map_find(kernel_map, NULL, 0, &addr, ptoa(namigahwpg), FALSE);
	if (addr != amigahwaddr)
		panic("pmap_init: bogons in the VM system!");

	addr = (vaddr_t)Sysmap;
	vm_object_reference(kernel_object);
	(void)vm_map_find(kernel_map, kernel_object, addr, &addr,
	    AMIGA_KPTSIZE, FALSE);
	/*
	 * If this fails it is probably because the static portion of
	 * the kernel page table isn't big enough and we overran the
	 * page table map. XXX Need to adjust pmap_size() in amiga_init.c.
	 */
	if (addr != (vaddr_t)Sysmap)
		panic("pmap_init: bogons in the VM system!");
#endif
a550 1
#if defined(UVM)
a551 3
#else
	addr = (vaddr_t)kmem_alloc(kernel_map, s);
#endif
a603 1
#if defined(UVM)
a612 7
#else
	addr = 0;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS || addr + s >= (vaddr_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	vm_map_remove(kernel_map, addr, addr + s);
#endif
a616 1
#if defined(UVM)
a619 3
#else
	addr = (vaddr_t)kmem_alloc(kernel_map, s);
#endif
a638 1
#if defined(UVM)
a656 24
#else
	/*
	 * Slightly modified version of kmem_suballoc() to get page table
	 * map where we want it.
	 */
	addr = amiga_uptbase;
	s = AMIGA_UPTMAXSIZE / AMIGA_UPTSIZE < maxproc ?
	    AMIGA_UPTMAXSIZE : maxproc * AMIGA_UPTSIZE;
	addr2 = addr + s;
	rv = vm_map_find(kernel_map, NULL, 0, &addr, s, TRUE);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot allocate space for PT map");
	pmap_reference(vm_map_pmap(kernel_map));
	pt_map = vm_map_create(vm_map_pmap(kernel_map), addr, addr2, TRUE);
	if (pt_map == NULL)
		panic("pmap_init: cannot create pt_map");
	rv = vm_map_submap(kernel_map, addr, addr2, pt_map);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: cannot map range to pt_map");
#ifdef DEBUG
	if (pmapdebug & PDB_INIT)
		printf("pmap_init: pt_map [%lx - %lx)\n", addr, addr2);
#endif
#endif /* UVM */
a702 1
#if defined(UVM)
a705 5
#else
		pvp = (struct pv_page *)kmem_alloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: kmem_alloc() failed");
#endif
a746 1
#if defined(UVM)
a747 3
#else
		kmem_free(kernel_map, (vaddr_t)pvp, NBPG);
#endif
a893 1
#if defined(UVM)
a899 8
#else
	if (pmap->pm_ptab)
		kmem_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
				 AMIGA_UPTSIZE);
	if (pmap->pm_stab != Segtabzero)
		kmem_free_wakeup(kernel_map, (vaddr_t)pmap->pm_stab,
				 AMIGA_STSIZE);
#endif
a1155 1
#if defined(UVM)
a1157 4
#else
		pmap->pm_ptab = (u_int *)
			kmem_alloc_wait(pt_map, AMIGA_UPTSIZE);
#endif
a2229 1
#if defined(UVM)
a2231 6
#else
			extern vm_offset_t pager_sva, pager_eva;

			if (va >= pager_sva && va < pager_eva)
				continue;
#endif
a2288 1
#if defined(UVM)
a2290 4
#else
		pmap->pm_stab = (u_int *)
			kmem_alloc(kernel_map, AMIGA_STSIZE);
#endif
a2436 1
#if defined(UVM)
a2443 5
#else
		if (vm_fault(pt_map, va, VM_PROT_READ|VM_PROT_WRITE, FALSE)
		    != KERN_SUCCESS)
			panic("pmap_enter: vm_fault failed");
#endif
@


1.30
log
@Sync with NetBSD, different calculation of maxproc. From NetBSD via Aaron@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.29 2001/05/09 15:31:24 art Exp $	*/
d592 1
a592 1
	Segtabzeropa = (u_int *)pmap_extract(pmap_kernel(), addr);
d680 1
a680 1
		kpt_pages->kpt_pa = pmap_extract(pmap_kernel(), addr2);
d1555 2
a1556 2
paddr_t
pmap_extract(pmap, va)
d1559 1
a1566 1
	pa = 0;
d1569 3
a1571 2
	if (pa)
		pa = (pa & PG_FRAME) | (va & ~PG_FRAME);
d1574 1
a1574 1
		printf("%lx\n", pa);
d1576 1
a1576 1
	return (pa);
d1729 1
a1729 1
		kpa = pmap_extract(pmap, pv->pv_va);
d2403 2
a2404 2
		pmap->pm_stpa = (u_int *)pmap_extract(
		    pmap_kernel(), (vaddr_t)pmap->pm_stab);
@


1.29
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.28 2001/05/05 21:26:34 art Exp $	*/
d695 1
a695 1
	if ((AMIGA_UPTMAXSIZE / AMIGA_MAX_PTSIZE) < maxproc) {
d703 1
a703 1
		maxproc = AMIGA_UPTMAXSIZE / AMIGA_MAX_PTSIZE;
d705 1
a705 1
		s = (maxproc * AMIGA_MAX_PTSIZE);
@


1.28
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.27 2001/04/20 11:01:55 art Exp $	*/
d1495 1
a1495 1
 *	Routine:	pmap_change_wiring
d1502 1
a1502 1
pmap_change_wiring(pmap, va, wired)
a1504 1
	boolean_t	wired;
a1507 6
	/*
	 * Never called this way.
	 */
	if (wired)
		panic("pmap_change_wiring: wired");

d1510 1
a1510 1
		printf("pmap_change_wiring(%p, %lx, %x)\n", pmap, va, wired);
d1524 1
a1524 1
			printf("pmap_change_wiring: invalid STE for %lx\n",
d1534 1
a1534 1
			printf("pmap_change_wiring: invalid PTE for %lx\n",
d1538 1
a1538 4
	if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))) {
		if (wired)
			pmap->pm_stats.wired_count++;
		else
d1545 1
a1545 1
	pmap_pte_set_w(pte, wired);
a1830 24
}


/*
 *	Routine:	pmap_pageable
 *	Function:
 *		Make the specified pages (by pmap, offset)
 *		pageable (or not) as requested.
 *
 *		A page which is not pageable may not take
 *		a fault; therefore, its page table entry
 *		must remain valid for the duration.
 *
 *		This routine is merely advisory; pmap_enter
 *		will specify that these pages are to be wired
 *		down (or not) as appropriate.
 */
void
pmap_pageable(pmap, sva, eva, pageable)
	pmap_t		pmap;
	vaddr_t		sva, eva;
	boolean_t	pageable;
{
	/* nothing */
@


1.27
log
@Big upgrade of the pmap. Revised pagetable handling, more careful wiring,
more careful handling of pageability.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/02/19 17:23:13 art Exp $	*/
d817 1
a817 1
	pvp = (struct pv_page *)trunc_page(pv);
d1329 1
a1329 1
		pmap_ptpage_addref(trunc_page(pte));
d1489 1
a1489 1
		pmap_check_wiring("enter", trunc_page(pmap_pte(pmap, va)));
d2028 1
a2028 1
		vaddr_t ptpva = trunc_page(pte);
d2168 1
a2168 1
			    ptpmap->pm_stab != (st_entry_t *)trunc_page(ste))
@


1.26
log
@explicitly set page size for uvm.
@
text
@d1 38
a38 2
/*	$OpenBSD: pmap.c,v 1.25 2000/12/15 15:18:36 art Exp $	*/
/*	$NetBSD: pmap.c,v 1.39 1997/06/10 18:26:41 veego Exp $	*/
d188 3
d197 2
a198 3
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vm_offset_t)(v) >> pmap_ishift]))
#define pmap_ste1(m, v) \
	(&((m)->pm_stab[(vm_offset_t)(v) >> SG4_SHIFT1]))
d200 1
a200 1
#define pmap_ste2(m, v) \
d203 4
a206 4
#define pmap_ste_v(m, v) \
	(mmutype == MMU_68040 \
	? ((*pmap_ste1(m, v) & SG_V) && \
	   (*pmap_ste2(m, v) & SG_V)) \
d208 2
a209 2
#else
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vm_offset_t)(v) >> SG_ISHIFT]))
d211 3
a213 2
#endif
#define pmap_pte(m, v)	(&((m)->pm_ptab[(vm_offset_t)(v) >> PG_SHIFT]))
d258 2
a259 2
	vm_offset_t	kpt_va;		/* always valid kernel VA */
	vm_offset_t	kpt_pa;		/* PA of this page (for speed) */
d273 1
a273 2
u_int	*Sysseg;
u_int	*Sysseg_pa;
d276 1
a276 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES;
d284 3
a286 3
vm_size_t	mem_size;	/* memory size in bytes */
vm_offset_t	virtual_avail;  /* VA of first avail page (after kernel bss)*/
vm_offset_t	virtual_end;	/* VA of last avail page (end of kernel AS) */
d299 1
a299 1
extern vm_offset_t z2mem_start;
d301 1
a301 14
boolean_t	pmap_testbit __P((register vm_offset_t, int));
void		pmap_enter_ptpage __P((register pmap_t, register vm_offset_t));

void		pmap_collect1 __P((pmap_t, vm_offset_t, vm_offset_t));
extern vm_offset_t reserve_dumppages __P((vm_offset_t));
static void amiga_protection_init __P((void));
void pmap_check_wiring __P((char *, vm_offset_t));
static void pmap_changebit __P((register vm_offset_t, int, boolean_t));
struct pv_entry * pmap_alloc_pv __P((void));
void pmap_free_pv __P((struct pv_entry *));

#ifdef DEBUG            
void pmap_pvdump __P((vm_offset_t));  
#endif
d303 21
d331 2
a332 2
#define	PAGE_IS_MANAGED(pa) (pmap_initialized &&	\
			vm_physseg_find(atop((pa)), NULL) != -1)
d361 2
a362 2
	vm_offset_t firstaddr;
	vm_offset_t loadaddr;
d364 1
a364 1
	vm_offset_t va;
d368 1
a368 1
	vm_offset_t fromads, toads;
d498 1
a498 1
	extern vm_offset_t amigahwaddr;
d500 4
a503 3
	vm_offset_t	addr, addr2;
	vm_size_t	npages, s;
	int		rv;
d506 1
a506 1
	int		bank;
d521 1
a521 1
	addr = (vm_offset_t) amigahwaddr;
d529 1
a529 1
	addr = (vm_offset_t) Sysmap;
d549 1
a549 1
	addr = (vm_offset_t)Sysmap;
d558 1
a558 1
	if (addr != (vm_offset_t)Sysmap)
d575 1
d579 1
a580 6

#if 0 /* XXX def DEBUG */
	printf("pmap_init: avail_start %lx phys_segs[0].start %lx npg %ld\n",
	    avail_start, phys_segs[0].start, page_cnt);
#endif

d587 1
a587 1
	addr = (vaddr_t)uvm_km_zalloc(kernel_map, s);
d589 1
a589 1
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);
d615 1
a615 1
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
d618 2
a619 2
		pv += npages;
		attr += npages;
d627 1
a627 1
	npages = howmany(((maxproc + 16) * AMIGA_UPTSIZE / NPTEPG), NBPG);
d629 1
a629 1
	npages += NKPTADD;
d631 1
a631 1
	npages += mem_size >> NKPTADDSHIFT;
d634 2
a635 2
	printf("Maxproc %d, mem_size %ld MB: allocating %ld KPT pages\n",
	    maxproc, mem_size>>20, npages);
d637 1
a637 1
	s = ptoa(npages) + round_page(npages * sizeof (struct kpt_page));
d648 1
a648 1
	if (rv != KERN_SUCCESS || (addr + s) >= (vm_offset_t)Sysmap)
d656 1
a656 1
	if (rv != KERN_SUCCESS || addr + s >= (vm_offset_t)Sysmap)
d665 1
a665 1
	addr = (vm_offset_t) uvm_km_zalloc(kernel_map, s);
d669 1
a669 1
	addr = (vm_offset_t)kmem_alloc(kernel_map, s);
d671 1
a671 1
	s = ptoa(npages);
d673 1
a673 1
	kpt_pages = &((struct kpt_page *)addr2)[npages];
d694 1
a694 1
	addr = AMIGA_UPTBASE;
d703 1
a703 1
		maxproc = (AMIGA_UPTMAXSIZE / AMIGA_MAX_PTSIZE);
d706 1
d714 1
a714 1
	addr = AMIGA_UPTBASE;
d735 1
a735 1
	if (mmutype == MMU_68040) {
d737 1
a737 2
	}
#endif
d759 5
a763 5
		addr2 = (vm_offset_t)Segtabzeropa;
		while (addr2 < (vm_offset_t)Segtabzeropa + AMIGA_STSIZE) {
			pmap_changebit(addr2, PG_CCB, 0);
			pmap_changebit(addr2, PG_CI, 1);
			addr2 += NBPG;
d815 1
a815 1
	register struct pv_page *pvp;
d830 1
a830 1
		uvm_km_free(kernel_map, (vm_offset_t)pvp, NBPG);
d832 1
a832 1
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
a837 66
#ifdef not_used		/* ?? */
void
pmap_collect_pv()
{
	struct pv_page_list pv_page_collectlist;
	struct pv_page *pvp, *npvp;
	struct pv_entry *ph, *ppv, *pv, *npv;
	int s;

	TAILQ_INIT(&pv_page_collectlist);

	for (pvp = pv_page_freelist.tqh_first; pvp; pvp = npvp) {
		if (pv_nfree < NPVPPG)
			break;
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
		if (pvp->pvp_pgi.pgi_nfree > NPVPPG / 3) {
			TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp,
			    pvp_pgi.pgi_list);
			pv_nfree -= pvp->pvp_pgi.pgi_nfree;
			pvp->pvp_pgi.pgi_nfree = -1;
		}
	}

	if (pv_page_collectlist.tqh_first == 0)
		return;

	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
		if (ph->pv_pmap == 0)
			continue;
		s = splimp();
		for (ppv = ph; (pv = ppv->pv_next) != 0; ) {
			pvp = (struct pv_page *)trunc_page(pv);
			if (pvp->pvp_pgi.pgi_nfree == -1) {
				pvp = pv_page_freelist.tqh_first;
				if (--pvp->pvp_pgi.pgi_nfree == 0) {
					TAILQ_REMOVE(&pv_page_freelist, pvp,
					    pvp_pgi.pgi_list);
				}
				npv = pvp->pvp_pgi.pgi_freelist;
#ifdef DIAGNOSTIC
				if (npv == 0)
					panic("pmap_collect_pv: "
					    "pgi_nfree inconsistent");
#endif
				pvp->pvp_pgi.pgi_freelist = npv->pv_next;
				*npv = *pv;
				ppv->pv_next = npv;
				ppv = npv;
			} else
				ppv = pv;
		}
		splx(s);
	}

	for (pvp = pv_page_collectlist.tqh_first; pvp; pvp = npvp) {
		npvp = pvp->pvp_pgi.pgi_list.tqe_next;
#if defined(UVM)
		uvm_km_free(kernel_map, (vm_offset_t)pvp, NBPG);
#else
		kmem_free(kernel_map, (vm_offset_t)pvp, NBPG);
#endif
	}
}
#endif

d845 1
a845 1
vm_offset_t
d847 3
a849 3
	vm_offset_t	virt;
	vm_offset_t	start;
	vm_offset_t	end;
d879 1
a879 1
	vm_size_t	size;
d881 1
a881 1
	register pmap_t pmap;
d910 1
a910 1
	register struct pmap *pmap;
d940 1
a940 1
	register pmap_t pmap;
d967 1
a967 1
	register struct pmap *pmap;
d982 1
a982 1
		uvm_km_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
d985 1
a985 1
		uvm_km_free_wakeup(kernel_map, (vm_offset_t)pmap->pm_stab,
d989 1
a989 1
		kmem_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
d992 1
a992 1
		kmem_free_wakeup(kernel_map, (vm_offset_t)pmap->pm_stab,
d1023 2
a1024 2
	register pmap_t pmap;
	vm_offset_t sva, eva;
d1026 5
a1030 9
	register vm_offset_t pa, va;
	register u_int *pte;
	register pv_entry_t pv, npv;
	pmap_t ptpmap;
	int *ste, s, bits;
	boolean_t flushcache = FALSE;
#if defined(M68040) || defined(M68060)
	int i;
#endif
a1031 2
	u_int opte;

d1042 1
d1050 1
a1050 1
			if (va >= m68k_trunc_seg((vm_offset_t)-1))
d1059 1
a1059 209
#ifdef DEBUG
		opte = *pte;
		remove_stats.removes++;
#endif
		/*
		 * Update statistics
		 */
		if (pmap_pte_w(pte))
			pmap->pm_stats.wired_count--;
		pmap->pm_stats.resident_count--;

		/*
		 * Invalidate the PTEs.
		 * XXX: should cluster them up and invalidate as many
		 * as possible at once.
		 */
#ifdef DEBUG
		if (pmapdebug & PDB_REMOVE)
			printf("remove: invalidating %p\n", pte);
#endif
		bits = *(int *)pte & (PG_U|PG_M);
		*(int *)pte = PG_NV;
		if (active_pmap(pmap))
			TBIS(va);

		/*
		 * For user mappings decrement the wiring count on
		 * the PT page.  We do this after the PTE has been
		 * invalidated because vm_map_pageable winds up in
		 * pmap_pageable which clears the modify bit for the
		 * PT page.
		 */
		if (pmap != pmap_kernel()) {
			pte = pmap_pte(pmap, va);
#if defined(UVM)
			uvm_map_pageable(pt_map, trunc_page(pte),
					 round_page(pte+1), TRUE);
#else
			vm_map_pageable(pt_map, trunc_page(pte),
					round_page(pte+1), TRUE);
#endif
#ifdef DEBUG
			if (pmapdebug & PDB_WIRING)
				pmap_check_wiring("remove", trunc_page(pte));
#endif
		}
		/*
		 * Remove from the PV table (raise IPL since we
		 * may be called at interrupt time).
		 */
		if (!PAGE_IS_MANAGED(pa))
			continue;
		pv = pa_to_pvh(pa);
		ste = (int *)0;
		s = splimp();
		/*
		 * If it is the first entry on the list, it is actually
		 * in the header and we must copy the following entry up
		 * to the header.  Otherwise we must search the list for
		 * the entry.  In either case we free the now unused entry.
		 */
		if (pmap == pv->pv_pmap && va == pv->pv_va) {
			ste = (int *)pv->pv_ptste;
			ptpmap = pv->pv_ptpmap;
			npv = pv->pv_next;
			if (npv) {
				*pv = *npv;
				pmap_free_pv(npv);
			} else
				pv->pv_pmap = NULL;
#ifdef DEBUG
			remove_stats.pvfirst++;
#endif
		} else {
			for (npv = pv->pv_next; npv; npv = npv->pv_next) {
#ifdef DEBUG
				remove_stats.pvsearch++;
#endif
				if (pmap == npv->pv_pmap && va == npv->pv_va)
					break;
				pv = npv;
			}
#if 0 /* XXX def DEBUG */
			if (npv == NULL) {
#ifdef MACHINE_NONCONTIG	/* XXX this need to be fixed */
				printf("pmap_remove: PA %lx index %d\n", pa,
				    pa_index(pa));
#else
				printf("pmap_remove: PA %lx index %ld\n", pa,
				    pa_index(pa));
#endif
				panic("pmap_remove: PA not in pv_tab");
			}
#endif
			ste = (int *)npv->pv_ptste;
			ptpmap = npv->pv_ptpmap;
			pv->pv_next = npv->pv_next;
			pmap_free_pv(npv);
			pv = pa_to_pvh(pa);
		}
		/*
		 * If this was a PT page we must also remove the
		 * mapping from the associated segment table.
		 */
		if (ste) {
#ifdef DEBUG
			remove_stats.ptinvalid++;
			if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE)) {
				printf("remove: ste was %x@@%p pte was %x@@%p\n",
				    *ste, ste, *(int *)&opte,
				    pmap_pte(pmap, va));
			}
#endif
#if defined(M68040) || defined(M68060)
			if (mmutype == MMU_68040) {
			/*
			 * On the 68040, the PT page contains
			 * NPTEPG/SG4_LEV3SIZE page tables, so we need to
			 * remove all the associated segment table entries
			 * (This may be incorrect:  if a single page table is
			 * being removed, the whole page should not be
			 * removed.)
			 */
				for (i = 0; i < NPTEPG / SG4_LEV3SIZE; ++i)
					*ste++ = SG_NV;
				ste -= NPTEPG / SG4_LEV3SIZE;
#ifdef DEBUG
				if (pmapdebug &
				    (PDB_REMOVE|PDB_SEGTAB|0x10000))
					printf(
					    "pmap_remove:PT at %lx removed\n",
					    va);
#endif
			}
			else
#endif /* M68040 || M68060 */
				*ste = SG_NV;
			/*
			 * If it was a user PT page, we decrement the
			 * reference count on the segment table as well,
			 * freeing it if it is now empty.
			 */
			if (ptpmap != pmap_kernel()) {
#ifdef DEBUG
				if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
					printf("remove: stab %p, refcnt %d\n",
					    ptpmap->pm_stab,
					    ptpmap->pm_sref - 1);
				if ((pmapdebug & PDB_PARANOIA) &&
				    ptpmap->pm_stab !=
				    (u_int *)trunc_page(ste))
					panic("remove: bogus ste");
#endif
				if (--(ptpmap->pm_sref) == 0) {
#ifdef DEBUG
					if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
					printf("remove: free stab %p\n",
					     ptpmap->pm_stab);
#endif
#if defined(UVM)
					uvm_km_free_wakeup(kernel_map,
					    (vm_offset_t)ptpmap->pm_stab,
					    AMIGA_STSIZE);
#else
					kmem_free_wakeup(kernel_map,
					    (vm_offset_t)ptpmap->pm_stab,
					    AMIGA_STSIZE);
#endif
					ptpmap->pm_stab = Segtabzero;
					ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
					if (mmutype == MMU_68040)
						ptpmap->pm_stfree =
						    protostfree;
#endif
					/*
					 * XXX may have changed segment table
					 * pointer for current process so
					 * update now to reload hardware.
					 */
					if (active_user_pmap(ptpmap))
						PMAP_ACTIVATE(ptpmap, 1);
				}
			}
			if (ptpmap == pmap_kernel())
				TBIAS();
			else
				TBIAU();
			pv->pv_flags &= ~PV_PTPAGE;
			ptpmap->pm_ptpages--;
		}
		/*
		 * Update saved attributes for managed page
		 */
		*pa_to_attribute(pa) |= bits;
		splx(s);
	}
	if (flushcache) {
		if (pmap == pmap_kernel()) {
			DCIS();
#ifdef DEBUG
			remove_stats.sflushes++;
#endif
		} else {
			DCIU();
#ifdef DEBUG
			remove_stats.uflushes++;
#endif
		}
d1070 1
a1070 1
	vm_offset_t	pa;
d1073 1
a1073 1
	register pv_entry_t pv;
d1097 3
d1102 1
a1102 1
			    pmap_pte_pa(pmap_pte(pv->pv_pmap,pv->pv_va)) != pa)
d1108 1
a1108 1
    panic("pmap_page_protect: bad mapping");
d1111 13
a1123 2
			pmap_remove(pv->pv_pmap, pv->pv_va,
			    pv->pv_va + PAGE_SIZE);
d1136 3
a1138 3
	register pmap_t	pmap;
	vm_offset_t	sva, eva;
	vm_prot_t	prot;
d1140 2
a1141 2
	register u_int *pte;
	register vm_offset_t va;
d1171 1
a1171 1
			if (va >= m68k_trunc_seg((vm_offset_t)-1))
d1191 1
a1191 1
			vm_offset_t pa = pmap_pte_pa(pte);
d1220 3
a1222 3
	register pmap_t pmap;
	vm_offset_t va;
	register vm_offset_t pa;
d1227 3
a1229 3
	register u_int *pte;
	register int npte;
	vm_offset_t opa;
d1285 1
a1285 2
		if ((wired && !pmap_pte_w(pte)) ||
		    (!wired && pmap_pte_w(pte))) {
d1316 2
a1317 1
		pmap_remove(pmap, va, va + PAGE_SIZE);
d1329 1
a1329 7
#if defined(UVM)
		uvm_map_pageable(pt_map, trunc_page(pte),
				 round_page(pte+1), FALSE);
#else
		vm_map_pageable(pt_map, trunc_page(pte),
				round_page(pte+1), FALSE);
#endif
d1337 1
a1337 1
		register pv_entry_t pv, npv;
d1417 2
a1418 2
		if (va >= AMIGA_UPTBASE && 
		    va < (AMIGA_UPTBASE + AMIGA_UPTMAXSIZE))
d1439 2
a1440 3
	    ((va >= AMIGA_UPTBASE &&
	    va < (AMIGA_UPTBASE + AMIGA_UPTMAXSIZE)) ||
	    (va >= (u_int)Sysmap && va < ((u_int)Sysmap + AMIGA_KPTSIZE))))
d1454 7
d1462 1
d1503 2
a1504 2
	register pmap_t	pmap;
	vm_offset_t	va;
d1507 7
a1513 1
	register u_int *pte;
d1565 1
a1565 1
vm_offset_t
d1567 2
a1568 2
	register pmap_t	pmap;
	vm_offset_t va;
d1570 1
a1570 1
	register vm_offset_t pa;
d1596 5
a1600 5
	pmap_t		dst_pmap;
	pmap_t		src_pmap;
	vm_offset_t	dst_addr;
	vm_size_t	len;
	vm_offset_t	src_addr;
d1678 1
a1678 1
	vm_offset_t	startpa, endpa;
d1680 1
a1680 1
	vm_offset_t pa;
d1683 1
a1683 1
	vm_offset_t kpa;
d1690 1
a1690 1
		register struct kpt_page *kpt, **pkpt;
d1707 2
a1708 2
		if (pv->pv_va < (vm_offset_t)Sysmap ||
		    pv->pv_va >= (vm_offset_t)Sysmap + AMIGA_KPTSIZE)
d1739 2
a1740 1
		pmap_remove(pmap, pv->pv_va, pv->pv_va + NBPG);
d1814 1
a1814 1
	register vm_offset_t	phys;
d1832 1
a1832 1
	register vm_offset_t	src, dst;
d1861 1
a1861 1
	vm_offset_t	sva, eva;
d1864 1
a1864 49
#ifdef DEBUG
	if (pmapdebug & PDB_FOLLOW)
		printf("pmap_pageable(%p, %lx, %lx, %x)\n", pmap, sva, eva,
		    pageable);
#endif
	/*
	 * If we are making a PT page pageable then all valid
	 * mappings must be gone from that page.  Hence it should
	 * be all zeros and there is no need to clean it.
	 * Assumptions:
	 *	- we are called with only one page at a time
	 *	- PT pages have only one pv_table entry
	 */
	if (pmap == pmap_kernel() && pageable && sva + PAGE_SIZE == eva) {
		register pv_entry_t pv;
		register vm_offset_t pa;

#ifdef DEBUG
		if ((pmapdebug & (PDB_FOLLOW|PDB_PTPAGE)) == PDB_PTPAGE)
			printf("pmap_pageable(%p, %lx, %lx, %x)\n", pmap, sva,
			    eva, pageable);
#endif
		if (!pmap_ste_v(pmap, sva))
			return;
		pa = pmap_pte_pa(pmap_pte(pmap, sva));
		if (!PAGE_IS_MANAGED(pa))
			return;
		pv = pa_to_pvh(pa);
		if (pv->pv_ptste == NULL)
			return;
#ifdef DEBUG
		if (pv->pv_va != sva || pv->pv_next) {
			printf("pmap_pageable: bad PT page va %lx next %p\n",
			    pv->pv_va, pv->pv_next);
			return;
		}
#endif
		/*
		 * Mark it unmodified to avoid pageout
		 */
		pmap_changebit(pa, PG_M, FALSE);
#ifdef DEBUG
		if (pmapdebug & PDB_PTPAGE)
			printf("pmap_pageable: PT page %lx(%x) unmodified\n",
			    sva, *(int *)pmap_pte(pmap, sva));
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("pageable", sva);
#endif
	}
d1873 1
a1873 1
	vm_offset_t	pa;
d1889 1
a1889 1
	vm_offset_t	pa;
d1907 1
a1907 1
	vm_offset_t	pa;
d1928 1
a1928 1
	vm_offset_t	pa;
d1940 1
a1940 1
vm_offset_t
d1951 303
d2257 1
a2257 1
	register int *kp, prot;
d2283 1
a2283 1
	register vm_offset_t pa;
d2286 2
a2287 2
	register pv_entry_t pv;
	register int *pte;
d2321 1
a2321 1
	register vm_offset_t pa;
d2325 3
a2327 3
	register pv_entry_t pv;
	register int *pte, npte;
	vm_offset_t va;
d2404 2
a2405 2
	register pmap_t pmap;
	register vm_offset_t va;
d2407 2
a2408 2
	register vm_offset_t ptpa;
	register pv_entry_t pv;
d2428 1
d2437 1
a2437 1
		    pmap_kernel(), (vm_offset_t)pmap->pm_stab);
a2486 7
#if 0 /* XXX should be superfluous here: defined(M68060) */
			if (machineid & AMIGA_68060) {
				pmap_changebit(addr, PG_CCB, 0);
				pmap_changebit(addr, PG_CI, 1);
				DCIS(); /* XXX */
			}
#endif
d2510 1
a2510 1
	va = trunc_page((vm_offset_t)pmap_pte(pmap, va));
d2518 1
a2518 1
		register struct kpt_page *kpt;
d2566 5
d2583 7
a2589 3
		if (uvm_fault(pt_map, va, 0, VM_PROT_READ|VM_PROT_WRITE)
		    != KERN_SUCCESS)
			panic("pmap_enter: uvm_fault failed");
d2595 2
a2596 1
		ptpa = pmap_extract(pmap_kernel(), va);
a2602 4
#if !defined(UVM)
#ifdef DEBUG
		PHYS_TO_VM_PAGE(ptpa)->flags |=  PG_PTPAGE;
#endif
d2682 1
a2682 1
	vm_offset_t pa;
d2684 1
a2684 1
	register pv_entry_t pv;
d2694 7
d2704 1
a2704 1
	vm_offset_t va;
d2706 4
a2709 2
	vm_map_entry_t entry;
	register int count, *pte;
a2710 1
	va = trunc_page(va);
d2715 4
a2718 3
#if defined(UVM)
	if (!uvm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
d2721 1
a2721 6
#else
	if (!vm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
		return;
	}
#endif
d2723 1
a2723 1
	for (pte = (int *)va; pte < (int *)(va+PAGE_SIZE); pte++)
d2726 3
a2728 3
	if (entry->wired_count != count)
		printf("*%s*: %lx: w%d/a%d\n", str, va, entry->wired_count,
		    count);
d2742 1
a2742 1
	vm_offset_t	*vstartp, *vendp;
@


1.25
log
@Grow the number of reserved pt pages.
Solves the 64MB problem. Somewhat based on NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.24 2000/11/08 11:42:44 art Exp $	*/
d326 5
@


1.24
log
@Fixes to submap allocation.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.23 2000/05/30 10:39:33 art Exp $	*/
d238 1
a238 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES + 4 / NPTEPG;
@


1.23
log
@Oops. conversion braino.
Noted by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.22 2000/05/28 03:55:21 art Exp $	*/
d241 1
a241 1
vm_map_t	st_map, pt_map;
d243 1
a243 1
struct vm_map	st_map_store, pt_map_store;
a645 4
	s = maxproc * AMIGA_STSIZE;
	st_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
				 FALSE, &st_map_store);

d658 1
a658 1
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, TRUE,
@


1.22
log
@Bugfix pmap_activate.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.21 2000/05/27 22:12:33 art Exp $	*/
d2542 1
a2542 1
		if (uvm_fault(pt_map, va, VM_PROT_READ|VM_PROT_WRITE, FALSE)
@


1.21
log
@Fix a bunch of typos.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.20 2000/05/27 21:17:59 art Exp $	*/
a950 1
	pmap->pm_stchanged = TRUE;
a1260 1
					ptpmap->pm_stchanged = TRUE;
d1267 1
a1267 3
						PMAP_ACTIVATE(ptpmap,
						    (struct pcb *)
						    &curproc->p_addr->u_pcb, 1);
a2000 1
	struct pcb *pcb = &p->p_addr->u_pcb;
d2007 1
a2007 1
	PMAP_ACTIVATE(pmap, pcb, p == curproc);
a2411 1
		pmap->pm_stchanged = TRUE;
d2417 1
a2417 1
			PMAP_ACTIVATE(pmap, &curproc->p_addr->u_pcb, 1);
@


1.20
log
@UVM support.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.19 2000/05/27 20:14:18 art Exp $	*/
d381 1
a381 1
		vm_page_physload(atop(fromads), atop(toads),
d602 1
a602 1
	rv = uvm_unmap(kernel_map, addr, addr + s, FALSE);
d736 1
a736 1
		pvp = (struct pv_page *)kmem_km_zalloc(kernel_map, NBPG);
d2012 1
a2012 1
	PMAP_ACTIVATE(pmap, pcbp, p == curproc);
@


1.19
log
@Make pmap_activate take struct proc * as arguemnt.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.18 2000/05/27 19:42:49 art Exp $	*/
d88 3
d241 4
a244 1
vm_map_t	pt_map;
d334 4
d340 1
d380 5
d387 1
d468 24
d508 1
d538 3
d542 1
d595 11
d611 1
a611 1

d616 5
d622 1
d642 23
d687 1
d735 5
d743 1
d785 3
d789 1
d851 3
d855 1
d1003 8
d1017 1
d1121 4
d1127 1
d1246 5
d1254 1
d1476 4
d1482 1
d1554 4
d1560 1
d2326 4
d2334 1
d2391 4
d2397 1
d2547 5
d2555 1
d2557 7
d2567 1
d2671 6
d2681 1
@


1.18
log
@MACHINE_NEW_NONCONTIG code for amiga. Enabled by default.
Old contig and NONCONTIG code will no longer work.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.17 2000/02/22 19:27:42 deraadt Exp $	*/
a260 1
void pmap_activate __P((register pmap_t, struct pcb *));
d1147 1
a1147 3
					if (curproc &&
					    ptpmap ==
					    curproc->p_vmspace->vm_map.pmap)
d1150 1
a1150 1
						    curproc->p_addr, 1);
d1867 3
d1871 2
a1872 3
pmap_activate(pmap, pcbp)
	register pmap_t pmap;
	struct pcb *pcbp;
d1874 3
d1879 1
a1879 1
		printf("pmap_activate(%p, %p)\n", pmap, pcbp);
d1881 10
a1890 1
	PMAP_ACTIVATE(pmap, pcbp, pmap == curproc->p_vmspace->vm_map.pmap);
d2281 2
a2282 2
		if (pmap == curproc->p_vmspace->vm_map.pmap)
			PMAP_ACTIVATE(pmap, (struct pcb *)curproc->p_addr, 1);
@


1.17
log
@enlarge msgbuf, somewhat line netbsd did
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16 1999/09/03 18:00:29 art Exp $	*/
a239 2
vm_offset_t    	avail_start;	/* PA of first available physical page */
vm_offset_t	avail_end;	/* PA of last available physical page */
d243 1
a243 2
vm_offset_t	vm_first_phys;	/* PA of first managed page */
vm_offset_t	vm_last_phys;	/* PA just past last managed page */
d252 1
d254 1
a254 12
#ifdef MACHINE_NONCONTIG
struct physeg {
	vm_offset_t start;
	vm_offset_t end;
	int first_page;
} phys_segs[16];

static	vm_offset_t avail_next;
static	vm_size_t avail_remaining;
u_long	noncontig_enable;
#endif

d260 1
a260 7
#ifdef MACHINE_NONCONTIG
#define pmap_valid_page(pa)	(pmap_initialized && pmap_page_index(pa) >= 0)
#else
#define pmap_valid_page(pa)	(pmap_initialized && pa >= vm_first_phys && \
				pa < vm_last_phys)
#endif

d280 17
a314 1
#ifdef MACHINE_NONCONTIG
d317 1
a317 1
#endif
d319 2
a320 2
	avail_start = firstaddr;
	avail_end = maxmem << PGSHIFT;
d323 2
a324 2
	avail_end -= m68k_round_page(MSGBUFSIZE);
#ifdef MACHINE_NONCONTIG
d327 1
a327 1
	 * found for loading the kernel into.
d329 3
a331 4
	avail_next = avail_start;
	avail_remaining = (avail_end - avail_start) >> PGSHIFT;
	phys_segs[0].start = avail_start;
	phys_segs[0].end = avail_end;
d338 2
a339 2
		if (avail_start >= sp->ms_start && avail_start <
		    sp->ms_start + sp->ms_size)
d343 2
a344 2
		phys_segs[i].start = sp->ms_start;
		phys_segs[i].end = sp->ms_start + sp->ms_size;
d352 1
a352 1
		if (phys_segs[i].end == 0x08000000)
d367 6
a372 9
		if ((phys_segs[i].start <= z2mem_start) &&
		    (phys_segs[i].end > z2mem_start))
			phys_segs[i].end = z2mem_start;

		phys_segs[i].first_page = phys_segs[i - 1].first_page +
		    (phys_segs[i - 1].end - phys_segs[i - 1].start) / NBPG;
		avail_remaining +=
		    (phys_segs[i].end - phys_segs[i].start) / NBPG;
		physmem += (phys_segs[i].end - phys_segs[i].start) / NBPG;
a376 1
#endif
d418 2
a419 1

a423 33
 * Bootstrap memory allocator. This function allows for early dynamic
 * memory allocation until the virtual memory system has been bootstrapped.
 * After that point, either kmem_alloc or malloc should be used. This
 * function works by stealing pages from the (to be) managed page pool,
 * stealing virtual address space, then mapping the pages and zeroing them.
 *
 * It should be used from pmap_bootstrap till vm_page_startup, afterwards
 * it cannot be used, and will generate a panic if tried. Note that this
 * memory will never be freed, and in essence it is wired down.
 */
void *
pmap_bootstrap_alloc(size)
	int size;
{
	extern boolean_t vm_page_startup_initialized;
	vm_offset_t val;
	
	if (vm_page_startup_initialized)
		panic(
		    "pmap_bootstrap_alloc: called after startup initialized");
	size = round_page(size);
	val = virtual_avail;

	virtual_avail = pmap_map(virtual_avail, avail_start,
	    avail_start + size, VM_PROT_READ|VM_PROT_WRITE);
	avail_start += size;

	bzero((caddr_t)val, size);
	return ((void *)val);
}


/*
a428 1
#ifdef MACHINE_NONCONTIG
a429 4
#else
pmap_init(phys_start, phys_end)
	vm_offset_t	phys_start, phys_end;
#endif
d434 1
a434 1
	vm_size_t	npg, s;
d436 6
d445 1
a445 5
#ifdef MACHINE_NONCONTIG
		printf("pmap_init(%lx, %lx)\n", avail_start, avail_end);
#else
		printf("pmap_init(%lx, %lx)\n", phys_start, phys_end);
#endif
d450 1
d464 1
a464 1
	 * page table map.   Need to adjust pmap_size() in amiga_init.c.
d472 1
a472 2
		printf("  pstart %lx, pend %lx, vstart %lx, vend %lx\n",
		    avail_start, avail_end, virtual_avail, virtual_end);
d480 5
a484 5
#ifdef MACHINE_NONCONTIG
	{
		int i;
		for (npg = 0, i = 0; phys_segs[i].start; ++i)
			npg += atop(phys_segs[i].end - phys_segs[i].start);
d486 2
a487 1
#ifdef DEBUG
d489 1
a489 1
	    avail_start, phys_segs[0].start, npg);
d491 4
a494 4
#else
	npg = atop(phys_end - phys_start);
#endif
	s = (vm_size_t)AMIGA_STSIZE + sizeof(struct pv_entry) * npg + npg;
d500 1
a500 11
#ifdef M68060
	if (machineid & AMIGA_68060) {
		addr2 = addr;
		while (addr2 < addr + AMIGA_STSIZE) {
			pmap_changebit(addr2, PG_CCB, 0);
			pmap_changebit(addr2, PG_CI, 1);
			addr2 += NBPG;
		}
		DCIS();
	}
#endif
d502 1
d504 2
a505 1
	addr += sizeof (struct pv_entry) * npg;
d509 19
a527 4
		printf(
		    "pmap_init: %lx bytes (%lx pgs): seg %p tbl %p attr %p\n",
		    s, npg, Segtabzero, pv_table, pmap_attributes);
#endif
d534 1
a534 1
	npg = howmany(((maxproc + 16) * AMIGA_UPTSIZE / NPTEPG), NBPG);
d536 1
a536 1
	npg += NKPTADD;
d538 1
a538 1
	npg += mem_size >> NKPTADDSHIFT;
d542 1
a542 1
	    maxproc, mem_size>>20, npg);
d544 1
a544 1
	s = ptoa(npg) + round_page(npg * sizeof (struct kpt_page));
d561 1
a561 1
	s = ptoa(npg);
d563 1
a563 1
	kpt_pages = &((struct kpt_page *)addr2)[npg];
a570 7
#ifdef M68060
		if (machineid & AMIGA_68060) {
			pmap_changebit(kpt_pages->kpt_pa, PG_CCB, 0);
			pmap_changebit(kpt_pages->kpt_pa, PG_CI, 1);
			DCIS();
		}
#endif
a611 7
#ifdef MACHINE_NONCONTIG
	vm_first_phys = avail_start;
	vm_last_phys = avail_end;
#else
	vm_first_phys = phys_start;
	vm_last_phys = phys_end;
#endif
d613 15
a627 1
}
d629 6
a634 6
#ifdef MACHINE_NONCONTIG
unsigned int
pmap_free_pages()
{
	return (avail_remaining);
}
d636 2
a637 13
int
pmap_next_page(addrp)
	vm_offset_t *addrp;
{
	static int cur_seg = 0;

	if (phys_segs[cur_seg].start == 0)
		return FALSE;
	if (avail_next == phys_segs[cur_seg].end) {
		avail_next = phys_segs[++cur_seg].start;
#ifdef DEBUG
		printf("pmap_next_page: next %lx remain %ld\n", avail_next,
		    avail_remaining);
a638 8
	}

	if (avail_next == 0)
		return FALSE;
	*addrp = avail_next;
	avail_next += NBPG;
	avail_remaining--;
	return TRUE;
a640 27
int
pmap_page_index(pa)
	vm_offset_t pa;
{

	struct physeg *sep = &phys_segs[0];

	while (sep->start) {
		if (pa >= sep->start && pa < sep->end)
			return (m68k_btop(pa - sep->start) + sep->first_page);
		++sep;
	}
	return -1;
}

void
pmap_virtual_space(startp, endp)
	vm_offset_t	*startp;
	vm_offset_t	*endp;
{
	*startp = virtual_avail;
	*endp = virtual_end;
}
#else
#define pmap_page_index(pa) (pa_index(pa))
#endif	/* MACHINE_NONCONTIG */

d725 1
a725 1
	for (ph = &pv_table[npages - 1]; ph >= &pv_table[0]; ph--) {
d1023 1
a1023 1
		if (!pmap_valid_page(pa))
d1055 1
a1055 1
#ifdef DEBUG
d1166 1
a1166 1
		pmap_attributes[pa_index(pa)] |= bits;
d1202 1
a1202 1
	if (!pmap_valid_page(pa))
d1439 1
a1439 1
	if (pmap_valid_page(pa)) {
d1735 1
a1735 5
	register vm_offset_t pa;
	register pv_entry_t pv;
	register int *pte;
	vm_offset_t kpa;
	int s;
a1736 4
#ifdef DEBUG
	int *ste;
	int opmapdebug = 0;
#endif
d1746 34
a1779 1
	for (pa = vm_first_phys; pa < vm_last_phys; pa += PAGE_SIZE) {
d1807 1
a1807 1
		while (--pte >= (int *)pv->pv_va && *pte == PG_NV)
d1809 1
a1809 1
		if (pte >= (int *)pv->pv_va)
a1867 1
	splx(s);
d1965 1
a1965 1
		if (!pmap_valid_page(pa))
d2111 1
a2111 1
	if (!pmap_valid_page(pa))
d2119 1
a2119 1
	if (pmap_attributes[pa_index(pa)] & bit) {
d2159 1
a2159 1
	if (!pmap_valid_page(pa))
d2168 1
a2168 1
		pmap_attributes[pa_index(pa)] &= ~bit;
d2518 17
@


1.16
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.15 1999/07/18 18:00:03 deraadt Exp $	*/
a297 1
struct msgbuf	*msgbufp;
d326 1
a326 1
	avail_end -= m68k_round_page(sizeof(struct msgbuf));
d425 1
a425 1
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,1		)
@


1.16.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d298 1
d327 1
a327 1
	avail_end -= m68k_round_page(MSGBUFSIZE);
d426 1
a426 1
	SYSMAP(struct msgbuf *	,msgbufmap	,msgbufp   ,btoc(MSGBUFSIZE))
@


1.16.4.2
log
@Update the SMP branch to -current, this breaks the SMP branch though.
But it will be fixed soonish.  Note, nothing new has happened, this is just
a merge of the trunk into this branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.26 2001/02/19 17:23:13 art Exp $	*/
a87 3
#if defined(UVM)
#include <uvm/uvm.h>
#endif
d235 1
a235 1
vm_size_t	Sysptsize = VM_KERNEL_PT_PAGES;
a238 3
#if defined(UVM)
struct vm_map	pt_map_store;
#endif
d240 2
d245 2
a246 1
int		page_cnt;	/* number of pages managed by the VM system */
a254 1
extern paddr_t	msgbufpa;	/* physical address of the msgbuf */
d256 12
a267 1
u_long		noncontig_enable;
d273 8
a280 1
void		pmap_collect1 __P((pmap_t, vm_offset_t, vm_offset_t));
a298 17
#define	PAGE_IS_MANAGED(pa) (pmap_initialized &&	\
			vm_physseg_find(atop((pa)), NULL) != -1)

#define pa_to_pvh(pa) \
({ \
	int bank_, pg_; \
	bank_ = vm_physseg_find(atop((pa)), &pg_); \
	&vm_physmem[bank_].pmseg.pvent[pg_]; \
})

#define pa_to_attribute(pa) \
({ \
	int bank_, pg_; \
	bank_ = vm_physseg_find(atop((pa)), &pg_); \
	&vm_physmem[bank_].pmseg.attrs[pg_]; \
})

d317 1
d320 1
a320 1
	vm_offset_t fromads, toads;
d322 2
a323 7
	fromads = firstaddr;
	toads = maxmem << PGSHIFT;

#if defined(UVM)
	uvmexp.pagesize = NBPG;
	uvm_setpagesize();
#endif
d326 2
a327 2
	toads -= m68k_round_page(MSGBUFSIZE);
	msgbufpa = toads;
d330 1
a330 1
	 * for loading the kernel into.
d332 4
a335 8
#if defined(UVM)
	uvm_page_physload(atop(fromads), atop(toads),
		atop(fromads), atop(toads), VM_FREELIST_DEFAULT);
#else
	vm_page_physload(atop(fromads), atop(toads),
		atop(fromads), atop(toads));
#endif

d342 2
a343 2
		if (firstaddr >= sp->ms_start &&
		    firstaddr < sp->ms_start + sp->ms_size)
d347 2
a348 2
		fromads = sp->ms_start;
		toads = sp->ms_start + sp->ms_size;
d356 1
a356 1
		if (toads == 0x08000000)
d371 9
a379 12
		if ((fromads <= z2mem_start) && (toads > z2mem_start))
			toads = z2mem_start;

#if defined(UVM)
		uvm_page_physload(atop(fromads), atop(toads),
			atop(fromads), atop(toads), (fromads & 0xff000000) ?
			VM_FREELIST_DEFAULT : VM_FREELIST_ZORROII);
#else
		vm_page_physload(atop(fromads), atop(toads),
			atop(fromads), atop(toads));
#endif
		physmem += (toads - fromads) / NBPG;
d384 1
d426 21
d448 12
a459 2
	DCIS();
	virtual_avail = reserve_dumppages(va);
d462 1
d469 1
d471 4
d479 1
a479 1
	vm_size_t	npages, s;
a480 6
	struct pv_entry *pv;
	char		*attr;
	int		bank;
#if defined(M68060)
	struct kpt_page *kptp;
#endif
d484 5
a488 1
		printf("pmap_init()\n");
a492 1
	 * XXX in pmap_boostrap() ???
a493 24
#if defined(UVM)
	addr = (vm_offset_t) amigahwaddr;
	if (uvm_map(kernel_map, &addr,
		    ptoa(namigahwpg),
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS)
		goto bogons;
	addr = (vm_offset_t) Sysmap;
	if (uvm_map(kernel_map, &addr, AMIGA_KPTSIZE,
		    NULL, UVM_UNKNOWN_OFFSET,
		    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE,
				UVM_INH_NONE, UVM_ADV_RANDOM,
				UVM_FLAG_FIXED)) != KERN_SUCCESS) {
		/*
		 * If this fails, it is probably because the static
		 * portion of the kernel page table isn't big enough
		 * and we overran the page table map.
		 */
bogons:
		panic("pmap_init: bogons in the VM system!\n");
	}
#else
d506 1
a506 1
	 * page table map. XXX Need to adjust pmap_size() in amiga_init.c.
a509 1
#endif
d514 2
a515 1
		printf(" vstart %lx, vend %lx\n", virtual_avail, virtual_end);
d523 5
a527 5
	for (page_cnt = 0, bank = 0; bank < vm_nphysseg; bank++) {
		page_cnt += vm_physmem[bank].end - vm_physmem[bank].start;
		printf("pmap_init: %2d: %08lx - %08lx (%10d)\n", bank,
		    vm_physmem[bank].start << PGSHIFT,
		    vm_physmem[bank].end << PGSHIFT, page_cnt << PGSHIFT);
d529 1
a529 2

#if 0 /* XXX def DEBUG */
d531 1
a531 1
	    avail_start, phys_segs[0].start, page_cnt);
d533 4
a536 4

	s = AMIGA_STSIZE;				/* Segtabzero */
	s += page_cnt * sizeof(struct pv_entry);	/* pv table */
	s += page_cnt * sizeof(char);			/* attribute table */
a538 3
#if defined(UVM)
	addr = (vaddr_t)uvm_km_zalloc(kernel_map, s);
#else
a539 1
#endif
d542 11
a552 1

a553 1

d555 1
a555 2
	addr += page_cnt * sizeof(struct pv_entry);

d559 4
a562 19
		printf("pmap_init: %lx bytes: page_cnt %x s0 %p(%p) "
		       "tbl %p atr %p\n",
		       s, page_cnt, Segtabzero, Segtabzeropa,
		       pv_table, pmap_attributes);
#endif

        /*
	 * Now that the pv and attribute tables have been allocated,
	 * assign them to the memory segments.
	 */
	pv = pv_table;
	attr = pmap_attributes;
	for (bank = 0; bank < vm_nphysseg; bank++) {
		npages = vm_physmem[bank].end - vm_physmem[bank].start;
		vm_physmem[bank].pmseg.pvent = pv;
		vm_physmem[bank].pmseg.attrs = attr;
		pv += npages;
		attr += npages;
	}
d569 1
a569 1
	npages = howmany(((maxproc + 16) * AMIGA_UPTSIZE / NPTEPG), NBPG);
d571 1
a571 1
	npages += NKPTADD;
d573 1
a573 1
	npages += mem_size >> NKPTADDSHIFT;
d577 1
a577 1
	    maxproc, mem_size>>20, npages);
d579 1
a579 1
	s = ptoa(npages) + round_page(npages * sizeof (struct kpt_page));
a584 11
#if defined(UVM)
	addr = 0;
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET,
		     UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
				 UVM_ADV_RANDOM, UVM_FLAG_NOMERGE));
	if (rv != KERN_SUCCESS || (addr + s) >= (vm_offset_t)Sysmap)
		panic("pmap_init: kernel PT too small");
	rv = uvm_unmap(kernel_map, addr, addr + s);
	if (rv != KERN_SUCCESS)
		panic("pmap_init: uvm_unmap failed");
#else
d590 1
a590 1
#endif
a594 5
#if defined(UVM)
	addr = (vm_offset_t) uvm_km_zalloc(kernel_map, s);
	if (addr == 0)
		panic("pmap_init: cannot allocate KPT free list");
#else
d596 1
a596 2
#endif
	s = ptoa(npages);
d598 1
a598 1
	kpt_pages = &((struct kpt_page *)addr2)[npages];
d606 7
a621 19
#if defined(UVM)
        /*
	 * Allocate the segment table map and the page table map.
	 */
	addr = AMIGA_UPTBASE;
	if ((AMIGA_UPTMAXSIZE / AMIGA_MAX_PTSIZE) < maxproc) {
		s = AMIGA_UPTMAXSIZE;
		/*
		 * XXX We don't want to hang when we run out of
		 * page tables, so we lower maxproc so that fork()
		 * will fail instead.  Note that root could still raise
		 * this value via sysctl(2).
		 */
		maxproc = (AMIGA_UPTMAXSIZE / AMIGA_MAX_PTSIZE);
	} else
		s = (maxproc * AMIGA_MAX_PTSIZE);
	pt_map = uvm_km_suballoc(kernel_map, &addr, &addr2, s, VM_MAP_PAGEABLE,
				 TRUE, &pt_map_store);
#else
a643 1
#endif /* UVM */
d654 7
d662 37
a698 15
	/*
	 * Now that this is done, mark the pages shared with the
	 * hardware page table search as non-CCB (actually, as CI).
	 *
	 * XXX Hm. Given that this is in the kernel map, can't we just
	 * use the va's?
	 */
#ifdef M68060
	if (machineid & AMIGA_68060) {
		kptp = kpt_free_list;
		while (kptp) {
			pmap_changebit(kptp->kpt_pa, PG_CCB, 0);
			pmap_changebit(kptp->kpt_pa, PG_CI, 1);
			kptp = kptp->kpt_next;
		}
d700 1
a700 6
		addr2 = (vm_offset_t)Segtabzeropa;
		while (addr2 < (vm_offset_t)Segtabzeropa + AMIGA_STSIZE) {
			pmap_changebit(addr2, PG_CCB, 0);
			pmap_changebit(addr2, PG_CI, 1);
			addr2 += NBPG;
		}
d702 4
a705 1
		DCIS();
d707 10
a716 1
#endif
d718 3
a729 5
#if defined(UVM)
		pvp = (struct pv_page *)uvm_km_zalloc(kernel_map, NBPG);
		if (pvp == 0)
			panic("pmap_alloc_pv: uvm_km_zalloc() failed");
#else
a732 1
#endif
a773 3
#if defined(UVM)
		uvm_km_free(kernel_map, (vm_offset_t)pvp, NBPG);
#else
a774 1
#endif
d806 1
a806 1
	for (ph = &pv_table[page_cnt - 1]; ph >= &pv_table[0]; ph--) {
a835 3
#if defined(UVM)
		uvm_km_free(kernel_map, (vm_offset_t)pvp, NBPG);
#else
a836 1
#endif
d932 1
a983 8
#if defined(UVM)
	if (pmap->pm_ptab)
		uvm_km_free_wakeup(pt_map, (vm_offset_t)pmap->pm_ptab,
				 AMIGA_UPTSIZE);
	if (pmap->pm_stab != Segtabzero)
		uvm_km_free_wakeup(kernel_map, (vm_offset_t)pmap->pm_stab,
				 AMIGA_STSIZE);
#else
a989 1
#endif
a1092 4
#if defined(UVM)
			uvm_map_pageable(pt_map, trunc_page(pte),
					 round_page(pte+1), TRUE);
#else
a1094 1
#endif
d1104 1
a1104 1
		if (!PAGE_IS_MANAGED(pa))
d1136 1
a1136 1
#if 0 /* XXX def DEBUG */
a1212 5
#if defined(UVM)
					uvm_km_free_wakeup(kernel_map,
					    (vm_offset_t)ptpmap->pm_stab,
					    AMIGA_STSIZE);
#else
a1215 1
#endif
d1223 1
d1229 6
a1234 2
					if (active_user_pmap(ptpmap))
						PMAP_ACTIVATE(ptpmap, 1);
d1247 1
a1247 1
		*pa_to_attribute(pa) |= bits;
d1283 1
a1283 1
	if (!PAGE_IS_MANAGED(pa))
a1438 4
#if defined(UVM)
		pmap->pm_ptab = (pt_entry_t *)
			uvm_km_valloc_wait(pt_map, AMIGA_UPTSIZE);
#else
a1440 1
#endif
a1511 4
#if defined(UVM)
		uvm_map_pageable(pt_map, trunc_page(pte),
				 round_page(pte+1), FALSE);
#else
a1513 1
#endif
d1520 1
a1520 1
	if (PAGE_IS_MANAGED(pa)) {
d1816 5
a1820 1
	int bank, s;
d1822 4
d1835 1
a1835 34

	for (bank = 0; bank < vm_nphysseg; bank++)
		pmap_collect1(pmap, ptoa(vm_physmem[bank].start),
			      ptoa(vm_physmem[bank].end));

#ifdef notyet
	/* Go compact and garbage-collect the pv_table. */
	pmap_collect_pv();
#endif
       splx(s);
}

/*
 *     Routine:        pmap_collect1()
 *
 *     Function:
 *             Helper function for pmap_collect().  Do the actual
 *             garbage-collection of range of physical addresses.
 */
void
pmap_collect1(pmap, startpa, endpa)
	pmap_t		pmap;
	vm_offset_t	startpa, endpa;
{
	vm_offset_t pa;
	struct pv_entry *pv;
	pt_entry_t *pte;
	vm_offset_t kpa;
#ifdef DEBUG
	int *ste;
	int opmapdebug = 0;
#endif

	for (pa = startpa; pa < endpa; pa += NBPG) {
d1863 1
a1863 1
		while (--pte >= (pt_entry_t *)pv->pv_va && *pte == PG_NV)
d1865 1
a1865 1
		if (pte >= (pt_entry_t *)pv->pv_va)
d1924 1
a1926 3
/*
 *	Mark that a processor is about to be used by a given pmap.
 */
d1928 3
a1930 2
pmap_activate(p)
	struct proc *p;
a1931 2
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

d1934 1
a1934 1
		printf("pmap_activate(%p)\n", p);
d1936 1
a1936 10
	PMAP_ACTIVATE(pmap, p == curproc);
}

/*
 *	Mark that a processor is no longer in use by a given pmap.
 */
void
pmap_deactivate(p)
	struct proc *p;
{
d2022 1
a2022 1
		if (!PAGE_IS_MANAGED(pa))
d2168 1
a2168 1
	if (!PAGE_IS_MANAGED(pa))
d2176 1
a2176 1
	if (*pa_to_attribute(pa) & bit) {
d2216 1
a2216 1
	if (!PAGE_IS_MANAGED(pa))
d2225 1
a2225 1
		*pa_to_attribute(pa) &= ~bit;
a2240 4
#if defined(UVM)
			if (va >= uvm.pager_sva && va < uvm.pager_eva)
				continue;
#else
a2244 1
#endif
a2300 4
#if defined(UVM)
		pmap->pm_stab = (st_entry_t *)
			uvm_km_zalloc(kernel_map, AMIGA_STSIZE);
#else
a2302 1
#endif
d2322 1
d2327 2
a2328 2
		if (active_user_pmap(pmap))
			PMAP_ACTIVATE(pmap, 1);
a2451 5
#if defined(UVM)
		if (uvm_fault(pt_map, va, 0, VM_PROT_READ|VM_PROT_WRITE)
		    != KERN_SUCCESS)
			panic("pmap_enter: uvm_fault failed");
#else
a2454 1
#endif
a2455 7
		/*
		 * Mark the page clean now to avoid its pageout (and
		 * hence creation of a pager) between now and when it
		 * is wired; i.e. while it is on a paging queue.
		 */
		PHYS_TO_VM_PAGE(ptpa)->flags |= PG_CLEAN;
#if !defined(UVM)
a2458 1
#endif
a2561 6
#if defined(UVM)
	if (!uvm_map_lookup_entry(pt_map, va, &entry)) {
		printf("wired_check: entry for %lx not found\n", va);
		return;
	}
#else
a2565 1
#endif
a2574 17

/*
 *     Routine:        pmap_virtual_space
 *
 *     Function:
 *             Report the range of available kernel virtual address
 *             space to the VM system during bootstrap.  Called by
 *             vm_bootstrap_steal_memory().
 */
void
pmap_virtual_space(vstartp, vendp)
	vm_offset_t	*vstartp, *vendp;
{

	*vstartp = virtual_avail;
	*vendp = virtual_end;
}
@


1.16.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 38
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.68 1999/06/19 19:44:09 is Exp $	*/

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */
d88 1
d90 1
a151 3

static void	pmap_check_wiring __P((char *, vaddr_t));
static void	pmap_pvdump __P((paddr_t));
d158 3
a160 2
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vaddr_t)(v) >> pmap_ishift]))
#define	pmap_ste1(m, v) (&((m)->pm_stab[(vaddr_t)(v) >> SG4_SHIFT1]))
d162 1
a162 1
#define	pmap_ste2(m, v) \
d165 4
a168 4
#define	pmap_ste_v(m, v) \
	(mmutype == MMU_68040		\
	? ((*pmap_ste1(m, v) & SG_V) &&	\
	   (*pmap_ste2(m, v) & SG_V))	\
d170 2
a171 2
#else	/* defined(M68040) || defined(M68060) */
#define	pmap_ste(m, v)	(&((m)->pm_stab[(vaddr_t)(v) >> SG_ISHIFT]))
d173 2
a174 3
#endif	/* defined(M68040) || defined(M68060) */

#define pmap_pte(m, v)	(&((m)->pm_ptab[(vaddr_t)(v) >> PG_SHIFT]))
d219 2
a220 2
	vaddr_t		kpt_va;		/* always valid kernel VA */
	paddr_t		kpt_pa;		/* PA of this page (for speed) */
d234 2
a235 1
u_int	*Sysseg, *Sysseg_pa;
d238 1
a238 1
vsize_t	Sysptsize = VM_KERNEL_PT_PAGES;
d242 1
d244 1
d246 3
a248 3
vsize_t		mem_size;	/* memory size in bytes */
vaddr_t		virtual_avail;  /* VA of first avail page (after kernel bss)*/
vaddr_t		virtual_end;	/* VA of last avail page (end of kernel AS) */
d261 1
a261 1
extern vaddr_t	amiga_uptbase;
d263 14
a276 1
extern paddr_t	z2mem_start;
a277 21
extern vaddr_t reserve_dumppages __P((vaddr_t));
  
boolean_t	pmap_testbit __P((paddr_t, int));
void		pmap_enter_ptpage __P((pmap_t, vaddr_t)); 
static void	pmap_ptpage_addref __P((vaddr_t));
static int	pmap_ptpage_delref __P((vaddr_t));
static void	pmap_changebit __P((vaddr_t, int, boolean_t));
  struct pv_entry * pmap_alloc_pv __P((void));
void		pmap_free_pv __P((struct pv_entry *));
void		pmap_pinit __P((pmap_t));
void		pmap_release __P((pmap_t));
static void	pmap_remove_mapping __P((pmap_t, vaddr_t, pt_entry_t *, int));

static void	amiga_protection_init __P((void));
void		pmap_collect1	__P((pmap_t, paddr_t, paddr_t));

/* pmap_remove_mapping flags */
#define		PRM_TFLUSH	0x01
#define		PRM_CFLUSH	0x02 
#define		PRM_KEEPPTPAGE	0x04
  
d285 2
a286 2
#define	PAGE_IS_MANAGED(pa) (pmap_initialized				\
			 && vm_physseg_find(atop((pa)), NULL) != -1)
d315 2
a316 2
	paddr_t firstaddr;
	paddr_t loadaddr;
d318 1
a318 1
	vaddr_t va;
d322 1
a322 1
	paddr_t fromads, toads;
d327 1
d330 1
d339 1
d342 4
d385 1
d389 4
d452 1
a452 1
	extern vaddr_t	amigahwaddr;
d454 3
a456 4
	vaddr_t		addr, addr2;
	paddr_t		paddr;
	vsize_t		s;
	u_int		npg;
d459 1
a459 1
	int		rv, bank;
d473 2
a474 1
	addr = (vaddr_t) amigahwaddr;
d482 1
a482 1
	addr = (vaddr_t) Sysmap;
d496 18
a527 1
#ifdef DEBUG
d531 5
d537 1
a537 1
	}
d543 5
a547 1
	addr = uvm_km_zalloc(kernel_map, s);
d549 1
a549 1
	pmap_extract(pmap_kernel(), addr, (paddr_t *)&Segtabzeropa);
d572 1
a572 1
		npg = vm_physmem[bank].end - vm_physmem[bank].start;
d575 2
a576 2
		pv += npg;
		attr += npg;
d584 1
a584 1
	npg = howmany(((maxproc + 16) * AMIGA_UPTSIZE / NPTEPG), NBPG);
d586 1
a586 1
	npg += NKPTADD;
d588 1
a588 1
	npg += mem_size >> NKPTADDSHIFT;
d591 2
a592 2
	printf("Maxproc %d, mem_size %ld MB: allocating %d KPT pages\n",
	    maxproc, mem_size>>20, npg);
d594 1
a594 1
	s = ptoa(npg) + round_page(npg * sizeof (struct kpt_page));
d600 1
d605 1
a605 1
	if (rv != KERN_SUCCESS || (addr + s) >= (vaddr_t)Sysmap)
d610 7
d621 2
a622 1
	addr = uvm_km_zalloc(kernel_map, s);
d625 4
a628 1
	s = ptoa(npg);
d630 1
a630 1
	kpt_pages = &((struct kpt_page *)addr2)[npg];
d637 1
a637 1
		pmap_extract(pmap_kernel(), addr2, &kpt_pages->kpt_pa);
d647 1
d651 2
a652 2
	addr = amiga_uptbase;
	if ((AMIGA_UPTMAXSIZE / AMIGA_UPTSIZE) < maxproc) {
d660 1
a660 1
		maxproc = AMIGA_UPTMAXSIZE / AMIGA_UPTSIZE;
d662 1
a662 2
		s = (maxproc * AMIGA_UPTSIZE);

d665 24
d691 1
a691 1
	if (mmutype == MMU_68040)
d693 2
a694 1
#endif /* defined(M68040) || defined(M68060) */
d716 5
a720 5
		paddr = (paddr_t)Segtabzeropa;
		while (paddr < (paddr_t)Segtabzeropa + AMIGA_STSIZE) {
			pmap_changebit(paddr, PG_CCB, 0);
			pmap_changebit(paddr, PG_CI, 1);
			paddr += NBPG;
d736 1
d740 5
d772 1
a772 1
	struct pv_page *pvp;
d774 1
a774 1
	pvp = (struct pv_page *)trunc_page((vaddr_t)pv);
d786 5
a790 1
		uvm_km_free(kernel_map, (vaddr_t)pvp, NBPG);
d795 66
d868 1
a868 1
vaddr_t
d870 3
a872 3
	vaddr_t	virt;
	paddr_t	start;
	paddr_t	end;
d902 1
a902 1
	vsize_t	size;
d904 1
a904 1
	pmap_t pmap;
d933 1
a933 1
	pmap_t pmap;
d963 1
a963 1
	pmap_t pmap;
d990 1
a990 1
	pmap_t pmap;
d1003 1
d1005 1
a1005 1
		uvm_km_free_wakeup(pt_map, (vaddr_t)pmap->pm_ptab,
d1008 1
a1008 1
		uvm_km_free_wakeup(kernel_map, (vaddr_t)pmap->pm_stab,
d1010 8
d1046 2
a1047 2
	pmap_t pmap;
	vaddr_t sva, eva;
d1049 11
a1059 4
	paddr_t pa;
	vaddr_t va;
	u_int *pte;
	int flags;
a1060 1
#ifdef DEBUG
a1070 1
	flags = active_pmap(pmap) ? PRM_TFLUSH : 0;
d1078 1
a1078 1
			if (va >= m68k_trunc_seg((vaddr_t)-1))
d1087 209
a1295 1
		pmap_remove_mapping(pmap, va, pte, flags);
d1306 1
a1306 1
	paddr_t	pa;
d1309 1
a1309 1
	pv_entry_t pv;
a1332 3
			pt_entry_t	*pte;

			pte = pmap_pte(pv->pv_pmap, pv->pv_va);
d1335 1
a1335 1
			    pmap_pte_pa(pte) != pa)
d1341 1
a1341 1
			panic("pmap_page_protect: bad mapping");
d1344 2
a1345 13
			if (!pmap_pte_w(pte))
				pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
				    pte, PRM_TFLUSH|PRM_CFLUSH);
			else {
				pv = pv->pv_next;
#ifdef DEBUG
				if (pmapdebug & PDB_PARANOIA)
				printf("%s wired mapping for %lx not removed\n",
					     "pmap_page_protect:", pa);
#endif
				if (pv == NULL)
					break;
			}
d1358 3
a1360 3
	pmap_t pmap;
	vaddr_t	sva, eva;
	vm_prot_t prot;
d1362 2
a1363 2
	u_int *pte;
	vaddr_t va;
d1393 1
a1393 1
			if (va >= m68k_trunc_seg((vaddr_t)-1))
d1413 1
a1413 1
			paddr_t pa = pmap_pte_pa(pte);
d1442 3
a1444 3
	pmap_t pmap;
	vaddr_t va;
	paddr_t pa;
d1449 3
a1451 3
	u_int *pte;
	int npte;
	paddr_t opa;
d1473 1
d1476 4
d1507 2
a1508 1
		if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))){
d1539 1
a1539 2
		pmap_remove_mapping(pmap, va, pte,
			PRM_TFLUSH|PRM_CFLUSH|PRM_KEEPPTPAGE);
d1551 7
a1557 1
		pmap_ptpage_addref(trunc_page((vaddr_t)pte));
d1565 1
a1565 1
		pv_entry_t pv, npv;
d1645 2
a1646 2
		if (va >= amiga_uptbase && 
		    va < (amiga_uptbase + AMIGA_UPTMAXSIZE))
d1667 3
a1669 2
	    ((va >= amiga_uptbase && va < (amiga_uptbase + AMIGA_UPTMAXSIZE)) ||
	     (va >= (u_int)Sysmap && va < ((u_int)Sysmap + AMIGA_KPTSIZE))))
a1682 7
#if defined(M68060) && defined(NO_SLOW_CIRRUS)
#if defined(M68040) || defined(M68030) || defined(M68020)
		npte |= (cputype == CPU_68060 ? PG_CIN : PG_CI);
#else
		npte |= PG_CIN;
#endif
#else
a1683 1
#endif
d1710 1
a1710 1
		pmap_check_wiring("enter", trunc_page((vaddr_t)pmap_pte(pmap, va)));
d1716 1
a1716 1
 *	Routine:	pmap_unwire
d1723 4
a1726 3
pmap_unwire(pmap, va)
	pmap_t	pmap;
	vaddr_t	va;
d1728 1
a1728 1
	u_int *pte;
d1732 1
a1732 1
		printf("pmap_unwire(%p, %lx)\n", pmap, va);
d1746 1
a1746 1
			printf("pmap_unwire: invalid STE for %lx\n",
d1756 1
a1756 1
			printf("pmap_unwire: invalid PTE for %lx\n",
d1760 4
a1763 1
	if (pmap_pte_w(pte)) {
d1770 1
a1770 1
	pmap_pte_set_w(pte, 0);
d1780 4
a1783 5
boolean_t
pmap_extract(pmap, va, pap)
	pmap_t	pmap;
	vaddr_t va;
	paddr_t *pap;
d1785 1
a1785 1
	paddr_t pa;
d1791 1
d1794 2
a1795 3
	else
		return (FALSE);
	*pap = (pa & PG_FRAME) | (va & ~PG_FRAME);
d1798 1
a1798 1
		printf("%lx\n", *pap);
d1800 1
a1800 1
	return (TRUE);
d1811 5
a1815 5
	pmap_t	dst_pmap;
	pmap_t	src_pmap;
	vaddr_t	dst_addr;
	vsize_t	len;
	vaddr_t	src_addr;
d1893 1
a1893 1
	paddr_t		startpa, endpa;
d1895 1
a1895 1
	paddr_t pa;
d1898 1
a1898 1
	paddr_t kpa;
d1905 1
a1905 1
		struct kpt_page *kpt, **pkpt;
d1922 2
a1923 2
		if (pv->pv_va < (vaddr_t)Sysmap ||
		    pv->pv_va >= (vaddr_t)Sysmap + AMIGA_KPTSIZE)
d1953 2
a1954 3
		pmap_extract(pmap, pv->pv_va, &kpa);
		pmap_remove_mapping(pmap, pv->pv_va, PT_ENTRY_NULL,
				PRM_TFLUSH|PRM_CFLUSH);
d2028 1
a2028 1
	paddr_t	phys;
d2046 1
a2046 1
	paddr_t	src, dst;
d2057 72
d2135 1
a2135 1
	paddr_t pa;
d2151 1
a2151 1
	paddr_t	pa;
d2169 1
a2169 1
	paddr_t	pa;
d2190 1
a2190 1
	paddr_t	pa;
d2202 1
a2202 1
paddr_t
a2212 303
/*
 * pmap_remove_mapping:
 *
 *	Invalidate a single page denoted by pmap/va.
 *
 *	If (pte != NULL), it is the already computed PTE for the page.
 *
 *	If (flags & PRM_TFLUSH), we must invalidate any TLB information.
 *
 *	If (flags & PRM_CFLUSH), we must flush/invalidate any cache
 *	information.
 *
 *	If (flags & PRM_KEEPPTPAGE), we don't free the page table page
 *	if the reference drops to zero.
 */
static void
pmap_remove_mapping(pmap, va, pte, flags)
	pmap_t pmap;
	vaddr_t va;
	pt_entry_t *pte;
	int flags;
{
	paddr_t pa;
	struct pv_entry *pv, *npv;
	pmap_t ptpmap;
	st_entry_t *ste;
	int s, bits;
#if defined(M68040) || defined(M68060)
	int i;
#endif
#ifdef DEBUG
	pt_entry_t opte;
#endif

#ifdef DEBUG
	if (pmapdebug & (PDB_FOLLOW|PDB_REMOVE|PDB_PROTECT))
	    printf("pmap_remove_mapping(%p, %lx, %p, %x)\n",
	    		pmap, va, pte, flags);
#endif

	/*
	 * PTE not provided, compute it from pmap and va.
	 */
	if (pte == PT_ENTRY_NULL) {
		pte = pmap_pte(pmap, va);
		if (*pte == PG_NV)
			return;
	}

	pa = pmap_pte_pa(pte);
#ifdef DEBUG
	opte = *pte;
#endif
	/*
	 * Update statistics
	 */
	if (pmap_pte_w(pte))
		pmap->pm_stats.wired_count--;
	pmap->pm_stats.resident_count--;

	/*
	 * Invalidate the PTE after saving the reference modify info.
	 */
#ifdef DEBUG
	if (pmapdebug & PDB_REMOVE)
		printf ("remove: invalidating pte at %p\n", pte);
#endif

	bits = *pte & (PG_U|PG_M);
	*pte = PG_NV;
	if ((flags & PRM_TFLUSH) && active_pmap(pmap))
		TBIS(va);
	/*
	 * For user mappings decrement the wiring count on
	 * the PT page.
	 */
	if (pmap != pmap_kernel()) {
		vaddr_t ptpva = trunc_page((vaddr_t)pte);
		int refs = pmap_ptpage_delref(ptpva);
#ifdef DEBUG
		if (pmapdebug & PDB_WIRING)
			pmap_check_wiring("remove", ptpva);
#endif
		/*
		 * If reference count drops to 1, and we're not instructed
		 * to keep it around, free the PT page.
		 *
		 * Note: refcnt == 1 comes from the fact that we allocate
		 * the page with uvm_fault_wire(), which initially wires
		 * the page.  The first reference we actually add causes
		 * the refcnt to be 2.
		 */
		if (refs == 1 && (flags & PRM_KEEPPTPAGE) == 0) {
			struct pv_entry *pv;
			paddr_t pa;

			pa = pmap_pte_pa(pmap_pte(pmap_kernel(), ptpva));
#ifdef DIAGNOSTIC
			if (PAGE_IS_MANAGED(pa) == 0)
				panic("pmap_remove_mapping: unmanaged PT page");
#endif
			pv = pa_to_pvh(pa);
#ifdef DIAGNOSTIC
			if (pv->pv_ptste == NULL)
				panic("pmap_remove_mapping: ptste == NULL");
			if (pv->pv_pmap != pmap_kernel() ||
			    pv->pv_va != ptpva ||
			    pv->pv_next != NULL)
				panic("pmap_remove_mapping: "
				    "bad PT page pmap %p, va 0x%lx, next %p",
				    pv->pv_pmap, pv->pv_va, pv->pv_next);
#endif
			pmap_remove_mapping(pv->pv_pmap, pv->pv_va,
			    NULL, PRM_TFLUSH|PRM_CFLUSH);
			uvm_pagefree(PHYS_TO_VM_PAGE(pa));
#ifdef DEBUG
			if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
			    printf("remove: PT page 0x%lx (0x%lx) freed\n",
				    ptpva, pa);
#endif
		}
	}

	/*
	 * If this isn't a managed page, we are all done.
	 */
	if (PAGE_IS_MANAGED(pa) == 0)
		return;
	/*
	 * Otherwise remove it from the PV table
	 * (raise IPL since we may be called at interrupt time).
	 */
	pv = pa_to_pvh(pa);
	ste = ST_ENTRY_NULL;
	s = splimp();
	/*
	 * If it is the first entry on the list, it is actually
	 * in the header and we must copy the following entry up
	 * to the header.  Otherwise we must search the list for
	 * the entry.  In either case we free the now unused entry.
	 */
	if (pmap == pv->pv_pmap && va == pv->pv_va) {
		ste = pv->pv_ptste;
		ptpmap = pv->pv_ptpmap;
		npv = pv->pv_next;
		if (npv) {
			npv->pv_flags = pv->pv_flags;
			*pv = *npv;
			pmap_free_pv(npv);
		} else
			pv->pv_pmap = NULL;
#ifdef DEBUG
		remove_stats.pvfirst++;
#endif
	} else {
		for (npv = pv->pv_next; npv; npv = npv->pv_next) {
#ifdef DEBUG
			remove_stats.pvsearch++;
#endif
			if (pmap == npv->pv_pmap && va == npv->pv_va)
				break;
			pv = npv;
		}
#ifdef DEBUG
		if (npv == NULL)
			panic("pmap_remove: PA not in pv_tab");
#endif
		ste = npv->pv_ptste;
		ptpmap = npv->pv_ptpmap;
		pv->pv_next = npv->pv_next;
		pmap_free_pv(npv);
		pv = pa_to_pvh(pa);
	}

	/*
	 * If this was a PT page we must also remove the
	 * mapping from the associated segment table.
	 */
	if (ste) {
#ifdef DEBUG
		remove_stats.ptinvalid++;
		if (pmapdebug & (PDB_REMOVE|PDB_PTPAGE))
		    printf("remove: ste was %x@@%p pte was %x@@%p\n",
			    *ste, ste, opte, pmap_pte(pmap, va));
#endif
#if defined(M68040) || defined(M68060)
		if (mmutype == MMU_68040) {
		    /*
		     * On the 68040, the PT page contains NPTEPG/SG4_LEV3SIZE
		     * page tables, so we need to remove all the associated
		     * segment table entries
		     * (This may be incorrect:  if a single page table is
		     * being removed, the whole page should not be
		     * removed.)
		     */
		    for (i = 0; i < NPTEPG / SG4_LEV3SIZE; ++i)
			*ste++ = SG_NV;
		    ste -= NPTEPG / SG4_LEV3SIZE;
#ifdef DEBUG
		    if (pmapdebug &(PDB_REMOVE|PDB_SEGTAB|0x10000))
			printf("pmap_remove:PT at %lx removed\n", va);
#endif
		} else
#endif /* defined(M68040) || defined(M68060) */
		*ste = SG_NV;
		/*
		 * If it was a user PT page, we decrement the
		 * reference count on the segment table as well,
		 * freeing it if it is now empty.
		 */
		if (ptpmap != pmap_kernel()) {
#ifdef DEBUG
			if (pmapdebug & (PDB_REMOVE|PDB_SEGTAB))
				printf("remove: stab %p, refcnt %d\n",
					ptpmap->pm_stab,
					ptpmap->pm_sref - 1);
			if ((pmapdebug & PDB_PARANOIA) &&
			    ptpmap->pm_stab != (st_entry_t *)trunc_page((vaddr_t)ste))
				panic("remove: bogus ste");
#endif
			if (--(ptpmap->pm_sref) == 0) {
#ifdef DEBUG
				if (pmapdebug&(PDB_REMOVE|PDB_SEGTAB))
				    printf("remove: free stab %p\n",
					    ptpmap->pm_stab);
#endif
				uvm_km_free_wakeup(kernel_map,
				    (vaddr_t)ptpmap->pm_stab, AMIGA_STSIZE);
				ptpmap->pm_stab = Segtabzero;
				ptpmap->pm_stpa = Segtabzeropa;
#if defined(M68040) || defined(M68060)
				if (mmutype == MMU_68040)
					ptpmap->pm_stfree = protostfree;
#endif
				/*
				 * XXX may have changed segment table
				 * pointer for current process so
				 * update now to reload hardware.
				 */
				if (active_user_pmap(ptpmap))
					PMAP_ACTIVATE(ptpmap, 1);
			}
#ifdef DEBUG
			else if (ptpmap->pm_sref < 0)
				panic("remove: sref < 0");
#endif
		}
#if 0
		/*
		 * XXX this should be unnecessary as we have been
		 * flushing individual mappings as we go.
		 */
		if (ptpmap == pmap_kernel())
			TBIAS();
		else
			TBIAU();
#endif
		pv->pv_flags &= ~PV_PTPAGE;
		ptpmap->pm_ptpages--;
	}
	/*
	 * Update saved attributes for managed page
	 */
	*pa_to_attribute(pa) |= bits;
	splx(s);
}

/*
 * pmap_ptpage_addref:
 *
 *	Add a reference to the specified PT page.
 */
void
pmap_ptpage_addref(ptpva)
	vaddr_t ptpva;
{
	vm_page_t m;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	m->wire_count++;
	simple_unlock(&uvm.kernel_object->vmobjlock);
}

/*
 * pmap_ptpage_delref:
 *
 *	Delete a reference to the specified PT page.
 */
int
pmap_ptpage_delref(ptpva)
	vaddr_t ptpva;
{
	vm_page_t m;
	int rv;

	simple_lock(&uvm.kernel_object->vmobjlock);
	m = uvm_pagelookup(uvm.kernel_object, ptpva - vm_map_min(kernel_map));
	rv = --m->wire_count;
	simple_unlock(&uvm.kernel_object->vmobjlock);
	return (rv);
}

d2216 1
a2216 1
	int *kp, prot;
d2242 1
a2242 1
	paddr_t pa;
d2245 2
a2246 2
	pv_entry_t pv;
	int *pte;
d2280 1
a2280 1
	paddr_t pa;
d2284 3
a2286 3
	pv_entry_t pv;
	int *pte, npte;
	vaddr_t va;
d2322 1
d2325 6
d2363 2
a2364 2
	pmap_t pmap;
	vaddr_t va;
d2366 2
a2367 2
	paddr_t ptpa;
	pv_entry_t pv;
d2387 1
a2387 1
		/* XXX Atari uses kernel_map here: */
d2390 6
a2395 2
		pmap_extract(pmap_kernel(), (vaddr_t)pmap->pm_stab,
			(paddr_t *)&pmap->pm_stpa);
d2445 7
d2475 1
a2475 1
	va = trunc_page((vaddr_t)pmap_pte(pmap, va));
d2483 1
a2483 1
		struct kpt_page *kpt;
a2530 5
	 *
	 * Note we use a wire-fault to keep the page off the paging
	 * queues.  This sets our PT page's reference (wire) count to
	 * 1, which is what we use to check if the page can be freed.
	 * See pmap_remove_mapping().
d2542 10
a2551 9
		s = uvm_fault_wire(pt_map, va, va + PAGE_SIZE,
		    VM_PROT_READ|VM_PROT_WRITE);
		if (s != KERN_SUCCESS) {
			printf("uvm_fault_wire(pt_map, 0x%lx, 0%lx, RW) "
				"-> %d\n", va, va + PAGE_SIZE, s);
			panic("pmap_enter: uvm_fault_wire failed");
		}
		ptpa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
#if 0 /* XXXX what is this? XXXX */
d2558 4
d2641 1
a2641 1
	paddr_t pa;
d2643 1
a2643 1
	pv_entry_t pv;
a2652 7
/*
 * pmap_check_wiring:
 *
 *	Count the number of valid mappings in the specified PT page,
 *	and ensure that it is consistent with the number of wirings
 *	to that page that the VM system has.
 */
d2656 1
a2656 1
	vaddr_t va;
d2658 2
a2659 4
	pt_entry_t *pte;
	paddr_t pa;
	vm_page_t m;
	int count;
d2661 1
d2666 3
a2668 4
	pa = pmap_pte_pa(pmap_pte(pmap_kernel(), va));
	m = PHYS_TO_VM_PAGE(pa);
	if (m->wire_count < 1) {
		printf("*%s*: 0x%lx: wire count %d\n", str, va, m->wire_count);
d2671 6
a2676 1

d2678 1
a2678 1
	for (pte = (pt_entry_t *)va; pte < (pt_entry_t *)(va + NBPG); pte++)
d2681 3
a2683 3
	if ((m->wire_count - 1) != count)
		printf("*%s*: 0x%lx: w%d/a%d\n",
		    str, va, (m->wire_count-1), count);
d2697 1
a2697 1
	vaddr_t	*vstartp, *vendp;
@


1.16.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16.4.3 2001/07/04 10:14:59 niklas Exp $	*/
d122 1
a124 1
#include <uvm/uvm_extern.h>
d772 1
a772 1
		pmap_enter(pmap_kernel(), virt, start, prot, 0);
d791 3
a793 2
struct pmap *
pmap_create(void)
d795 1
a795 1
	struct pmap *pmap;
d801 5
d807 6
a812 1
	pmap = (struct pmap *)malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
d974 3
a976 1
pmap_page_protect(struct vm_page *pg, vm_prot_t prot)
a977 1
	paddr_t pa;
a980 2
	pa = VM_PAGE_TO_PHYS(pg);

d1123 2
a1124 2
int
pmap_enter(pmap, va, pa, prot, flags)
d1129 2
a1130 1
	int flags;
a1136 1
	boolean_t wired = (flags & PMAP_WIRED) != 0;
d1143 2
a1391 2

	return (KERN_SUCCESS);
d1738 3
a1740 2
boolean_t
pmap_clear_modify(struct vm_page *pg)
a1741 3
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;

a1745 2
	ret = pmap_is_modified(pg);

a1746 2

	return (ret);
d1755 2
a1756 2
boolean_t
pmap_clear_reference(struct vm_page *pg)
a1757 2
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
	boolean_t ret;
a1761 1
	ret = pmap_is_referenced(pg);
a1762 2

	return (ret);
d1773 2
a1774 1
pmap_is_referenced(struct vm_page *pg)
a1775 1
	paddr_t pa = VM_PAGE_TO_PHYS(pg);
d1794 2
a1795 1
pmap_is_modified(struct vm_page *pg)
a1796 1
	paddr_t	pa = VM_PAGE_TO_PHYS(pg);
d2399 2
a2400 2
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT,
			   VM_PROT_DEFAULT|PMAP_WIRED);
a2595 27
}

void
pmap_kenter_pa(vaddr_t va, paddr_t pa, vm_prot_t prot)
{
	pmap_enter(pmap_kernel(), va, pa, prot,
		VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
}

void
pmap_kenter_pgs(vaddr_t va, struct vm_page **pgs, int npgs)
{
	int i;

	for (i = 0; i < npgs; i++, va += PAGE_SIZE) {
		pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pgs[i]),
			VM_PROT_READ|VM_PROT_WRITE,
			VM_PROT_READ|VM_PROT_WRITE|PMAP_WIRED);
	}
}

void
pmap_kremove(vaddr_t va, vsize_t len)
{
	for (len >>= PAGE_SHIFT; len > 0; len--, va += PAGE_SIZE) {
		pmap_remove(pmap_kernel(), va, va + PAGE_SIZE);
	}
@


1.16.4.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d120 1
d122 1
d124 1
d507 1
a507 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d514 1
a514 1
		    NULL, UVM_UNKNOWN_OFFSET, 0,
d605 1
a605 1
	rv = uvm_map(kernel_map, &addr, s, NULL, UVM_UNKNOWN_OFFSET, 0,
@


1.16.4.6
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.16.4.5 2001/11/13 21:00:50 niklas Exp $	*/
@


1.15
log
@pmap_activate() and pmap_deactivate() are MD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/01/11 05:11:05 millert Exp $	*/
d862 1
a862 1
		pmap_enter(pmap_kernel(), virt, start, prot, FALSE);
d1408 1
a1408 1
pmap_enter(pmap, va, pa, prot, wired)
d1414 1
d2420 2
a2421 1
		pmap_enter(pmap, va, ptpa, VM_PROT_DEFAULT, TRUE);
@


1.14
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.13 1999/07/18 16:23:46 deraadt Exp $	*/
d280 1
d1928 3
a1930 2
pmap_activate(p)
	struct proc *p;
a1931 3
	struct pcb *pcbp = &p->p_addr->u_pcb;
	pmap_t pmap = p->p_vmspace->vm_map.pmap;

@


1.13
log
@fix pmap_activate() and pmap_deactivate() arguments after art changed things
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.12 1999/01/11 05:11:05 millert Exp $	*/
a279 1
void pmap_activate __P((register pmap_t));
d1927 2
a1928 2
pmap_activate(pmap)
	register pmap_t pmap;
d1931 1
@


1.12
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.11 1998/03/30 18:56:10 niklas Exp $	*/
d280 1
a280 1
void pmap_activate __P((register pmap_t, struct pcb *));
d1928 1
a1928 1
pmap_activate(pmap, pcbp)
a1929 1
	struct pcb *pcbp;
d1931 2
@


1.11
log
@Size the KPT area depending on physmem; from is@@netbsd.org.  KNF too
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.10 1997/09/18 13:39:36 niklas Exp $	*/
d498 1
a498 1
		panic("pmap_init: bogons in the VM system!\n");
d510 1
a510 1
		panic("pmap_init: bogons in the VM system!\n");
@


1.10
log
@Merge of NetBSD changes upto last week or so, with the exception of stand/
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.9 1997/09/09 11:05:00 niklas Exp $	*/
d203 6
d210 4
d271 1
a271 1
void		pmap_enter_ptpage __P((register pmap_t, register vm_offset_t)); 
d378 2
a379 1
		avail_remaining += (phys_segs[i].end - phys_segs[i].start) / NBPG;
d450 2
a451 1
		panic("pmap_bootstrap_alloc: called after startup initialized");
d456 1
a456 1
		avail_start + size, VM_PROT_READ|VM_PROT_WRITE);
d459 2
a460 2
	bzero((caddr_t) val, size);
	return((void *) val);
d478 1
a478 1
	extern u_int namigahwpg;
d500 1
a500 1
	addr = (vm_offset_t) Sysmap;
d502 1
a502 1
	(void) vm_map_find(kernel_map, kernel_object, addr, &addr,
d537 1
a537 2
	s = (vm_size_t)AMIGA_STSIZE +
	    sizeof(struct pv_entry) * npg + npg;
d540 3
a542 3
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
	Segtabzero = (u_int *) addr;
	Segtabzeropa = (u_int *) pmap_extract(pmap_kernel(), addr);
d555 3
a557 3
	pv_table = (pv_entry_t) addr;
	addr += sizeof(struct pv_entry) * npg;
	pmap_attributes = (char *) addr;
d560 2
a561 1
		printf("pmap_init: %lx bytes (%lx pgs): seg %p tbl %p attr %p\n",
d571 10
a580 2
	npg = min(atop(AMIGA_MAX_KPTSIZE), npg);
	s = ptoa(npg) + round_page(npg * sizeof(struct kpt_page));
d596 1
a596 1
	addr = (vm_offset_t) kmem_alloc(kernel_map, s);
d600 1
a600 1
	kpt_free_list = (struct kpt_page *) 0;
d669 1
a669 2

	return avail_remaining;
d763 1
a763 1
	pvp = (struct pv_page *) trunc_page(pv);
d797 2
a798 1
			TAILQ_INSERT_TAIL(&pv_page_collectlist, pvp, pvp_pgi.pgi_list);
d812 1
a812 1
			pvp = (struct pv_page *) trunc_page(pv);
d816 2
a817 1
					TAILQ_REMOVE(&pv_page_freelist, pvp, pvp_pgi.pgi_list);
d822 2
a823 1
					panic("pmap_collect_pv: pgi_nfree inconsistent");
d898 1
a898 1
	pmap = (pmap_t) malloc(sizeof *pmap, M_VMPMAP, M_WAITOK);
d1171 3
a1173 3
			 * On the 68040, the PT page contains NPTEPG/SG4_LEV3SIZE
			 * page tables, so we need to remove all the associated
			 * segment table entries
d1182 4
a1185 2
				if (pmapdebug &(PDB_REMOVE|PDB_SEGTAB|0x10000))
					printf("pmap_remove:PT at %lx removed\n",
d1204 2
a1205 1
				    ptpmap->pm_stab != (u_int *)trunc_page(ste))
d1221 2
a1222 1
						ptpmap->pm_stfree = protostfree;
d1231 2
a1232 1
					    ptpmap == curproc->p_vmspace->vm_map.pmap)
d1234 2
a1235 1
							(struct pcb *)curproc->p_addr, 1);
d1304 1
a1304 1
    printf ("pmap_page_protect: va %lx, pmap_ste_v %d pmap_pte_pa %08x/%lx\n",
d1307 1
a1307 1
    printf (" pvh %p pv %p pv_next %p\n", pa_to_pvh(pa), pv, pv->pv_next);
d1468 2
a1469 1
		if ((wired && !pmap_pte_w(pte)) || (!wired && pmap_pte_w(pte))) {
d1756 1
a1756 1
	return(pa);
d1843 2
a1844 1
		if (pv->pv_pmap != pmap_kernel() || !(pv->pv_flags & PV_PTPAGE))
d1870 2
a1871 1
			printf("collect: freeing KPT page at %lx (ste %x@@%p)\n",
d1886 1
d1916 2
a1917 1
		ste = (int *)&Sysptmap[(u_int *)ste-pmap_ste(pmap_kernel(), 0)];
d1919 2
a1920 1
			printf("collect: kernel PTmap at %p still valid (%x)\n",
d2094 1
a2094 1
		return(rv);
d2097 1
a2097 1
	return(pmap_testbit(pa, PG_U));
d2115 1
a2115 1
		return(rv);
d2118 1
a2118 1
	return(pmap_testbit(pa, PG_M));
d2169 1
a2169 1
		return(FALSE);
d2178 1
a2178 1
		return(TRUE);
d2186 1
a2186 1
			pte = (int *) pmap_pte(pv->pv_pmap, pv->pv_va);
d2189 1
a2189 1
				return(TRUE);
d2194 1
a2194 1
	return(FALSE);
d2247 1
a2247 1
		pte = (int *) pmap_pte(pv->pv_pmap, va);
d2303 1
a2303 1
		pmap->pm_stpa = (u_int *) pmap_extract(
d2403 2
a2404 1
				printf("enter_pt: no KPT pages, collecting...\n");
d2429 2
a2430 1
			printf("enter_pt: add &Sysptmap[%d]: %x (KPT page %lx)\n",
@


1.9
log
@s/amiga_/m68k_/; from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.8 1997/01/16 09:23:26 niklas Exp $	*/
d2091 1
a2091 1
	return(amiga_ptob(ppn));
@


1.8
log
@Sync to NetBSD 970110
@
text
@d1 2
a2 2
/*	$OpenBSD: pmap.c,v 1.7 1996/10/04 23:34:39 niklas Exp $	*/
/*	$NetBSD: pmap.c,v 1.34.4.1 1996/08/03 00:53:58 jtc Exp $	*/
d317 1
a317 1
	avail_end -= amiga_round_page(sizeof(struct msgbuf));
d686 1
a686 1
			return (amiga_btop(pa - sep->start) + sep->first_page);
d1029 1
a1029 1
			if (va >= amiga_trunc_seg((vm_offset_t)-1))
d1031 1
a1031 1
			va = amiga_round_seg(va + PAGE_SIZE) - PAGE_SIZE;
d1332 1
a1332 1
			if (va >= amiga_trunc_seg((vm_offset_t)-1))
d1334 1
a1334 1
			va = amiga_round_seg(va + PAGE_SIZE) - PAGE_SIZE;
@


1.7
log
@Merge of NetBSD 1.2 (961004)
@
text
@d1 1
a1 1
/*	$OpenBSD: pmap.c,v 1.6 1996/05/29 10:14:31 niklas Exp $	*/
d502 1
a502 1
		       Sysseg, Sysmap, Sysptmap);
d504 1
a504 1
		       avail_start, avail_end, virtual_avail, virtual_end);
d519 2
a520 2
	printf ("pmap_init: avail_start %lx phys_segs[0].start %lx npg %ld\n",
		avail_start, phys_segs[0].start, npg);
d550 1
a550 1
		       s, npg, Segtabzero, pv_table, pmap_attributes);
d599 2
a600 2
		printf("pmap_init: KPT: %ld pages from %lx to %lx\n",
		       atop(s), addr, addr + s);
d664 2
a665 2
		printf ("pmap_next_page: next %lx remain %ld\n",
		    avail_next, avail_remaining);
d836 2
a837 1
		printf("pmap_map(%lx, %lx, %lx, %x)\n", virt, start, end, prot);
d1118 2
a1119 2
				printf("pmap_remove: PA %lx index %d\n",
				    pa, pa_index(pa));
d1121 2
a1122 2
				printf("pmap_remove: PA %lx index %ld\n",
				    pa, pa_index(pa));
d1142 2
a1143 2
				       *ste, ste,
				       *(int *)&opte, pmap_pte(pmap, va));
d1177 2
a1178 2
					       ptpmap->pm_stab,
					       ptpmap->pm_sref - 1);
d1187 1
a1187 1
					       ptpmap->pm_stab);
d1190 2
a1191 2
						  (vm_offset_t)ptpmap->pm_stab,
						  AMIGA_STSIZE);
d1276 5
a1280 5
  printf ("pmap_page_protect: va %lx, pmap_ste_v %d pmap_pte_pa %08x/%lx\n",
    pv->pv_va, pmap_ste_v(pv->pv_pmap,pv->pv_va),
    pmap_pte_pa(pmap_pte(pv->pv_pmap,pv->pv_va)),pa);
  printf (" pvh %p pv %p pv_next %p\n", pa_to_pvh(pa), pv, pv->pv_next);
				panic("pmap_page_protect: bad mapping");
d1284 1
a1284 1
				    pv->pv_va + PAGE_SIZE);
d1308 2
a1309 1
		printf("pmap_protect(%p, %lx, %lx, %x)\n", pmap, sva, eva, prot);
d1395 2
a1396 2
		printf("pmap_enter(%p, %lx, %lx, %x, %x)\n",
		       pmap, va, pa, prot, wired);
d1502 2
a1503 2
			printf("enter: pv at %p: %lx/%p/%p\n",
			       pv, pv->pv_va, pv->pv_pmap, pv->pv_next);
d1673 2
a1674 1
			printf("pmap_change_wiring: invalid STE for %lx\n", va);
d1683 2
a1684 1
			printf("pmap_change_wiring: invalid PTE for %lx\n", va);
d1746 2
a1747 2
		printf("pmap_copy(%p, %p, %lx, %lx, %lx)\n",
		       dst_pmap, src_pmap, dst_addr, len, src_addr);
d1841 1
a1841 1
			       pv->pv_va, *(int *)pv->pv_ptste, pv->pv_ptste);
d1870 1
a1870 1
			       kpt->kpt_va, kpa);
d1883 1
a1883 1
			       ste, *ste);
d1887 1
a1887 1
			       ste, *ste);
d1965 2
a1966 2
		printf("pmap_pageable(%p, %lx, %lx, %x)\n",
		       pmap, sva, eva, pageable);
d1982 2
a1983 2
			printf("pmap_pageable(%p, %lx, %lx, %x)\n",
			       pmap, sva, eva, pageable);
d1996 1
a1996 1
			       pv->pv_va, pv->pv_next);
d2007 1
a2007 1
			       sva, *(int *)pmap_pte(pmap, sva));
d2179 2
a2180 2
		printf("pmap_changebit(%lx, %x, %s)\n",
		       pa, bit, setem ? "set" : "clear");
d2297 2
a2298 2
			printf("enter_pt: pmap %p stab %p(%p)\n",
			       pmap, pmap->pm_stab, pmap->pm_stpa);
d2330 2
a2331 1
				printf("enter_pt: alloc ste2 %d(%p)\n", ix, addr);
d2346 2
a2347 2
			printf("enter_pt: ste2 %p (%p)\n",
			    pmap_ste2(pmap, va), ste);
d2395 3
a2397 3
			       ste - pmap_ste(pmap, 0),
			       *(int *)&Sysptmap[ste - pmap_ste(pmap, 0)],
			       kpt->kpt_va);
d2455 2
a2456 1
		printf("enter_pt: new PT page at PA %lx, ste at %p\n", ptpa, ste);
d2482 2
a2483 2
			printf("enter_pt: stab %p refcnt %d\n",
			       pmap->pm_stab, pmap->pm_sref);
d2507 2
a2508 2
		       pv->pv_pmap, pv->pv_va, pv->pv_ptste, pv->pv_ptpmap,
		       pv->pv_flags);
d2534 2
a2535 2
		printf("*%s*: %lx: w%d/a%d\n",
		       str, va, entry->wired_count, count);
@


1.6
log
@Merge of 960526 NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pmap.c,v 1.34 1996/05/19 21:04:24 veego Exp $	*/
d258 2
d336 2
d353 8
d362 4
a365 2
		if (phys_segs[i].start == 0x00200000)
			phys_segs[i].end -= MAXPHYS;
@


1.5
log
@Sync with NetBSD 9600430.  The port has gone over a major -Wall treat
@
text
@d2 1
a2 1
/*	$NetBSD: pmap.c,v 1.30 1996/04/28 06:59:08 mhitch Exp $	*/
d154 1
a154 1
#ifdef M68040
d239 3
a241 1
#ifdef M68040
d273 3
d378 1
a378 1
#ifdef M68040
d462 1
a462 1
		printf("pmap_init(%x, %x)\n", phys_start, phys_end);
d518 11
d573 7
d581 1
d612 1
a612 1
#ifdef M68040
d690 116
d892 1
a892 1
#ifdef M68040
d952 2
a953 1
		kmem_free(kernel_map, (vm_offset_t)pmap->pm_stab, AMIGA_STSIZE);
d989 1
a989 1
	int *ste, i, s, bits;
d991 3
d1085 1
a1085 1
				free((caddr_t)npv, M_VMPVENT);
d1102 7
a1108 1
printf ("pmap_remove: PA %lx index %d\n", pa, pa_index(pa));
d1115 1
a1115 1
			free((caddr_t)npv, M_VMPVENT);
d1131 1
a1131 1
#ifdef M68040
d1151 1
a1151 1
#endif /* M68040 */
d1174 1
a1174 1
					kmem_free(kernel_map,
d1179 1
a1179 1
#ifdef M68040
d1330 1
a1330 1
#ifdef M68040
d1361 2
d1513 1
a1513 4
			npv = (pv_entry_t)
				malloc(sizeof *npv, M_VMPVENT, M_NOWAIT);
			if (npv == NULL)
				panic("pmap_enter: PV allocation failure");
d1551 26
a1576 1
#ifdef M68040
d1578 3
a1580 1
	    va >= AMIGA_UPTBASE && va < (AMIGA_UPTBASE + AMIGA_UPTMAXSIZE))
d1595 3
a1597 2
#ifdef M68040
	else if (mmutype == MMU_68040 && (npte & PG_PROT) == PG_RW)
d1605 1
a1605 1
#ifdef M68040
d1747 4
d2206 1
a2206 1
#ifdef M68040
d2230 3
d2253 1
a2253 1
#ifdef M68040
d2255 12
d2286 1
a2286 1
#ifdef M68040
d2303 7
d2366 7
d2405 7
d2447 1
a2447 1
#ifdef M68040
@


1.4
log
@Rearrange user page table size calculation so that silent overflow
cannot occur
@
text
@d1 2
a2 1
/*	$NetBSD: pmap.c,v 1.27.2.1 1995/11/10 16:10:00 chopps Exp $	*/
d256 2
a257 2
boolean_t	pmap_testbit();
void		pmap_enter_ptpage();
d266 1
d268 7
d455 1
a455 1
		printf("pmap_init(%x, %x)\n", avail_start, avail_end);
d482 1
a482 1
		printf("pmap_init: Sysseg %x, Sysmap %x, Sysptmap %x\n",
d484 1
a484 1
		printf("  pstart %x, pend %x, vstart %x, vend %x\n",
d500 1
a500 1
	printf ("pmap_init: avail_start %08x phys_segs[0].start %08x npg %d\n",
d519 1
a519 1
		printf("pmap_init: %x bytes (%x pgs): seg %x tbl %x attr %x\n",
d561 1
a561 1
		printf("pmap_init: KPT: %d pages from %x to %x\n",
d585 1
a585 1
		printf("pmap_init: pt_map [%x - %x)\n", addr, addr2);
d626 1
a626 1
		printf ("pmap_next_page: next %08x remain %d\n",
d682 1
a682 1
		printf("pmap_map(%x, %x, %x, %x)\n", virt, start, end, prot);
d712 1
a712 1
		printf("pmap_create(%x)\n", size);
d742 1
a742 1
		printf("pmap_pinit(%x)\n", pmap);
d774 1
a774 1
		printf("pmap_destroy(%x)\n", pmap);
d800 1
a800 1
		printf("pmap_release(%x)\n", pmap);
d824 1
a824 1
		printf("pmap_reference(%x)\n", pmap);
d854 1
a854 1
		printf("pmap_remove(%x, %x, %x)\n", pmap, sva, eva);
d897 1
a897 1
			printf("remove: invalidating %x\n", pte);
d958 1
a958 1
printf ("pmap_remove: PA %08x index %d\n", pa, pa_index(pa));
d976 1
a976 1
				printf("remove: ste was %x@@%x pte was %x@@%x\n",
d996 1
a996 1
					printf("pmap_remove:PT at %x removed\n",
d1011 1
a1011 1
					printf("remove: stab %x, refcnt %d\n",
d1021 1
a1021 1
					printf("remove: free stab %x\n",
d1088 2
a1089 2
	    prot == VM_PROT_NONE && (pmapdebug & PDB_REMOVE))
		printf("pmap_page_protect(%x, %x)\n", pa, prot);
d1111 1
a1111 1
  printf ("pmap_page_protect: va %08x, pmap_ste_v %d pmap_pte_pa %08x/%08x\n",
d1114 1
a1114 1
  printf (" pvh %08x pv %08x pv_next %08x\n", pa_to_pvh(pa), pv, pv->pv_next);
d1143 1
a1143 1
		printf("pmap_protect(%x, %x, %x, %x)\n", pmap, sva, eva, prot);
d1227 1
a1227 1
		printf("pmap_enter(%x, %x, %x, %x, %x)\n",
d1256 1
a1256 1
		printf("enter: pte %x, *pte %x\n", pte, *(int *)pte);
d1272 1
a1272 1
		if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
d1301 1
a1301 1
			printf("enter: removing old mapping %x\n", va);
d1334 1
a1334 1
			printf("enter: pv at %x: %x/%x/%x\n",
d1466 1
a1466 1
		printf("pmap_change_wiring(%x, %x, %x)\n", pmap, va, wired);
d1480 1
a1480 1
			printf("pmap_change_wiring: invalid STE for %x\n", va);
d1489 1
a1489 1
			printf("pmap_change_wiring: invalid PTE for %x\n", va);
d1492 1
a1492 1
	if (wired && !pmap_pte_w(pte) || !wired && pmap_pte_w(pte)) {
d1521 1
a1521 1
		printf("pmap_extract(%x, %x) -> ", pmap, va);
d1530 1
a1530 1
		printf("%x\n", pa);
d1551 1
a1551 1
		printf("pmap_copy(%x, %x, %x, %x, %x)\n",
d1596 1
a1596 1
	int opmapdebug;
d1603 1
a1603 1
		printf("pmap_collect(%x)\n", pmap);
d1620 1
a1620 1
		} while (pv = pv->pv_next);
d1641 1
a1641 1
			printf("collect: freeing KPT page at %x (ste %x@@%x)\n",
d1670 1
a1670 1
			printf("collect: %x (%x) to free list\n",
d1683 1
a1683 1
			printf("collect: kernel STE at %x still valid (%x)\n",
d1687 1
a1687 1
			printf("collect: kernel PTmap at %x still valid (%x)\n",
d1701 1
a1701 1
		printf("pmap_activate(%x, %x)\n", pmap, pcbp);
d1718 1
a1718 1
		printf("pmap_zero_page(%x)\n", phys);
d1736 1
a1736 1
		printf("pmap_copy_page(%x, %x)\n", src, dst);
d1766 1
a1766 1
		printf("pmap_pageable(%x, %x, %x, %x)\n",
d1783 1
a1783 1
			printf("pmap_pageable(%x, %x, %x, %x)\n",
d1796 1
a1796 1
			printf("pmap_pageable: bad PT page va %x next %x\n",
d1807 1
a1807 1
			printf("pmap_pageable: PT page %x(%x) unmodified\n",
d1825 1
a1825 1
		printf("pmap_clear_modify(%x)\n", pa);
d1841 1
a1841 1
		printf("pmap_clear_reference(%x)\n", pa);
d1860 1
a1860 1
		printf("pmap_is_referenced(%x) -> %c\n", pa, "FT"[rv]);
d1881 1
a1881 1
		printf("pmap_is_modified(%x) -> %c\n", pa, "FT"[rv]);
d1899 1
a1899 1
/* static */
d1964 1
a1964 1
/* static */
d1980 1
a1980 1
		printf("pmap_changebit(%x, %x, %s)\n",
d2027 1
a2027 1
			    (bit == PG_RO && setem || (bit & PG_CMASK))) {
a2050 1
	u_int sg_proto, *sg;
d2054 1
a2054 1
		printf("pmap_enter_ptpage: pmap %x, va %x\n", pmap, va);
d2083 1
a2083 1
			printf("enter_pt: pmap %x stab %x(%x)\n",
d2109 1
a2109 1
				printf("enter_pt: alloc ste2 %d(%x)\n", ix, addr);
d2124 1
a2124 1
			printf("enter_pt: ste2 %x (%x)\n",
d2165 1
a2165 1
			printf("enter_pt: add &Sysptmap[%d]: %x (KPT page %x)\n",
d2184 1
a2184 1
			printf("enter_pt: about to fault UPT pg at %x\n", va);
d2207 1
a2207 1
		} while (pv = pv->pv_next);
d2211 1
a2211 1
		printf("enter_pt: PV entry for PT page %x not found\n", ptpa);
d2219 1
a2219 1
		printf("enter_pt: new PT page at PA %x, ste at %x\n", ptpa, ste);
d2245 1
a2245 1
			printf("enter_pt: stab %x refcnt %d\n",
d2261 1
d2267 1
a2267 1
	printf("pa %x", pa);
d2269 1
a2269 1
		printf(" -> pmap %x, va %x, ptste %x, ptpmap %x, flags %x",
d2275 1
d2289 1
a2289 1
		printf("wired_check: entry for %x not found\n", va);
d2297 1
a2297 1
		printf("*%s*: %x: w%d/a%d\n",
@


1.3
log
@Fix SunOS emulation on 040
@
text
@d561 2
a562 1
	s = min(AMIGA_UPTMAXSIZE, maxproc * AMIGA_UPTSIZE);
@


1.2
log
@update to netbsd
@
text
@d1393 2
a1394 2
	if (mmutype == MMU_68040 && pmap == pmap_kernel() && va >= AMIGA_UPTBASE && 
	    va < (AMIGA_UPTBASE + AMIGA_UPTMAXSIZE))
d1396 6
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: pmap.c,v 1.27 1995/10/09 04:34:02 chopps Exp $	*/
d240 1
d367 1
a367 1
		pmap_kernel()->pm_stfree = 0xfffffff8;	/* XXXX */
d578 6
d744 1
a744 1
		pmap->pm_stfree = 0x0000fffe;	/* XXXX */
d1021 1
a1021 1
						ptpmap->pm_stfree = 0x0000fffe; /* XXXX */
d2056 1
a2056 1
			pmap->pm_stfree = 0x0000fffe;	/* XXXX */
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
