head	1.2;
access;
symbols
	SMP_SYNC_A:1.2
	SMP_SYNC_B:1.2
	UBC_SYNC_A:1.2
	UBC_SYNC_B:1.2
	UBC:1.1.0.2
	UBC_BASE:1.1;
locks; strict;
comment	@ * @;


1.2
date	2001.12.19.08.58.06;	author art;	state dead;
branches;
next	1.1;

1.1
date	2001.12.10.04.45.31;	author art;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2002.06.11.04.13.39;	author art;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2002.11.04.18.02.31;	author art;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2003.05.20.04.04.46;	author tedu;	state Exp;
branches;
next	1.1.2.5;

1.1.2.5
date	2004.02.21.01.22.40;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.2
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@/*	$OpenBSD: genfs_vnops.c,v 1.1 2001/12/10 04:45:31 art Exp $	*/
/*
 * Copyright (c) 1982, 1986, 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/kernel.h>
#include <sys/mount.h>
#include <sys/namei.h>
#include <sys/vnode.h>
#include <sys/fcntl.h>
#include <sys/malloc.h>
#include <sys/poll.h>
#include <sys/mman.h>
#include <sys/pool.h>

#include <miscfs/genfs/genfs.h>
#include <miscfs/specfs/specdev.h>

#include <uvm/uvm.h>
#include <uvm/uvm_pager.h>

/*
 * generic VM getpages routine.
 * Return PG_BUSY pages for the given range,
 * reading from backing store if necessary.
 */

int
genfs_getpages(v)
	void *v;
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		vm_page_t *a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t newsize, diskeof, memeof;
	off_t offset, origoffset, startoffset, endoffset, raoffset;
	daddr_t lbn, blkno;
	int s, i, error, npages, orignpages, npgs, run, ridx, pidx, pcount;
	int fs_bshift, fs_bsize, dev_bshift;
	int flags = ap->a_flags;
	size_t bytes, iobytes, tailbytes, totalbytes, skipbytes;
	vaddr_t kva;
	struct buf *bp, *mbp;
	struct vnode *vp = ap->a_vp;
	struct uvm_object *uobj = &vp->v_uobj;
	struct vm_page *pgs[16];			/* XXXUBC 16 */
	struct genfs_node *gp = VTOG(vp);
	struct ucred *cred = curproc->p_ucred;		/* XXXUBC curproc */
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	boolean_t sawhole = FALSE;
	struct proc *p = curproc;
	UVMHIST_FUNC("genfs_getpages"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p off 0x%x/%x count %d",
		    vp, ap->a_offset >> 32, ap->a_offset, *ap->a_count);

	/* XXXUBC temp limit */
	if (*ap->a_count > 16) {
		return EINVAL;
	}

	error = 0;
	origoffset = ap->a_offset;
	orignpages = *ap->a_count;
	GOP_SIZE(vp, vp->v_size, &diskeof);
	if (flags & PGO_PASTEOF) {
		newsize = MAX(vp->v_size,
		    origoffset + (orignpages << PAGE_SHIFT));
		GOP_SIZE(vp, newsize, &memeof);
	} else {
		memeof = diskeof;
	}
	KASSERT(ap->a_centeridx >= 0 || ap->a_centeridx <= orignpages);
	KASSERT((origoffset & (PAGE_SIZE - 1)) == 0 && origoffset >= 0);
	KASSERT(orignpages > 0);

	/*
	 * Bounds-check the request.
	 */

	if (origoffset + (ap->a_centeridx << PAGE_SHIFT) >= memeof) {
		if ((flags & PGO_LOCKED) == 0) {
			simple_unlock(&uobj->vmobjlock);
		}
		UVMHIST_LOG(ubchist, "off 0x%x count %d goes past EOF 0x%x",
			    origoffset, *ap->a_count, memeof,0);
		return EINVAL;
	}

	/*
	 * For PGO_LOCKED requests, just return whatever's in memory.
	 */

	if (flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
			      UFP_NOWAIT|UFP_NOALLOC|UFP_NORDONLY);

		return ap->a_m[ap->a_centeridx] == NULL ? EBUSY : 0;
	}

	/* vnode is VOP_LOCKed, uobj is locked */

	if (write && (vp->v_bioflag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}

	/*
	 * find the requested pages and make some simple checks.
	 * leave space in the page array for a whole block.
	 */

	fs_bshift = vp->v_mount->mnt_fs_bshift;
	fs_bsize = 1 << fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;

	orignpages = MIN(orignpages,
	    round_page(memeof - origoffset) >> PAGE_SHIFT);
	npages = orignpages;
	startoffset = origoffset & ~(fs_bsize - 1);
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT)
				+ fs_bsize - 1) & ~(fs_bsize - 1));
	endoffset = MIN(endoffset, round_page(memeof));
	ridx = (origoffset - startoffset) >> PAGE_SHIFT;

	memset(pgs, 0, sizeof(pgs));
	uvn_findpages(uobj, origoffset, &npages, &pgs[ridx], UFP_ALL);

	/*
	 * if PGO_OVERWRITE is set, don't bother reading the pages.
	 * PGO_OVERWRITE also means that the caller guarantees
	 * that the pages already have backing store allocated.
	 */

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				uvm_pagezero(pg);
				pg->flags &= ~(PG_FAKE);
			}
			pg->flags &= ~(PG_RDONLY);
		}
		npages += ridx;
		goto out;
	}

	/*
	 * if the pages are already resident, just return them.
	 */

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[ridx + i];

		if ((pg->flags & PG_FAKE) ||
		    (write && (pg->flags & PG_RDONLY))) {
			break;
		}
	}
	if (i == npages) {
		UVMHIST_LOG(ubchist, "returning cached pages", 0,0,0,0);
		raoffset = origoffset + (orignpages << PAGE_SHIFT);
		npages += ridx;
		goto raout;
	}

	/*
	 * the page wasn't resident and we're not overwriting,
	 * so we're going to have to do some i/o.
	 * find any additional pages needed to cover the expanded range.
	 */

	npages = (endoffset - startoffset) >> PAGE_SHIFT;
	if (startoffset != origoffset || npages != orignpages) {

		/*
		 * XXXUBC we need to avoid deadlocks caused by locking
		 * additional pages at lower offsets than pages we
		 * already have locked.  for now, unlock them all and
		 * start over.
		 */

		for (i = 0; i < orignpages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				pg->flags |= PG_RELEASED;
			}
		}
		uvm_page_unbusy(&pgs[ridx], orignpages);
		memset(pgs, 0, sizeof(pgs));

		UVMHIST_LOG(ubchist, "reset npages start 0x%x end 0x%x",
			    startoffset, endoffset, 0,0);
		npgs = npages;
		uvn_findpages(uobj, startoffset, &npgs, pgs, UFP_ALL);
	}
	simple_unlock(&uobj->vmobjlock);

	/*
	 * read the desired page(s).
	 */

	totalbytes = npages << PAGE_SHIFT;
	bytes = MIN(totalbytes, MAX(diskeof - startoffset, 0));
	tailbytes = totalbytes - bytes;
	skipbytes = 0;

	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WAITOK |
			     UVMPAGER_MAPIN_READ);

	s = splbio();
	mbp = pool_get(&bufpool, PR_WAITOK);
	splx(s);
	mbp->b_bufsize = totalbytes;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	/*
	 * if EOF is in the middle of the range, zero the part past EOF.
	 */

	if (tailbytes > 0) {
		memset((void *)(kva + bytes), 0, tailbytes);
	}

	/*
	 * now loop over the pages, reading as needed.
	 */

	if (write) {
		lockmgr(&gp->g_glock, LK_EXCLUSIVE, NULL, p);
	} else {
		lockmgr(&gp->g_glock, LK_SHARED, NULL, p);
	}

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {

		/*
		 * skip pages which don't need to be read.
		 */

		pidx = (offset - startoffset) >> PAGE_SHIFT;
		while ((pgs[pidx]->flags & (PG_FAKE|PG_RDONLY)) == 0) {
			size_t b;

			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			b = MIN(PAGE_SIZE, bytes);
			offset += b;
			bytes -= b;
			skipbytes += b;
			pidx++;
			UVMHIST_LOG(ubchist, "skipping, new offset 0x%x",
				    offset, 0,0,0);
			if (bytes == 0) {
				goto loopdone;
			}
		}

		/*
		 * bmap the file to find out the blkno to read from and
		 * how much we can read in one i/o.  if bmap returns an error,
		 * skip the rest of the top-level i/o.
		 */

		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP lbn 0x%x -> %d\n",
				    lbn, error,0,0);
			skipbytes += bytes;
			goto loopdone;
		}

		/*
		 * see how many pages can be read with this i/o.
		 * reduce the i/o size if necessary to avoid
		 * overwriting pages with valid data.
		 */

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (offset + iobytes > round_page(offset)) {
			pcount = 1;
			while (pidx + pcount < npages &&
			       pgs[pidx + pcount]->flags & PG_FAKE) {
				pcount++;
			}
			iobytes = MIN(iobytes, (pcount << PAGE_SHIFT) -
				      (offset - trunc_page(offset)));
		}

		/*
		 * if this block isn't allocated, zero it instead of reading it.
		 * if this is a read access, mark the pages we zeroed PG_RDONLY.
		 */

		if (blkno < 0) {
			int holepages = (round_page(offset + iobytes) - 
					 trunc_page(offset)) >> PAGE_SHIFT;
			UVMHIST_LOG(ubchist, "lbn 0x%x -> HOLE", lbn,0,0,0);

			sawhole = TRUE;
			memset((char *)kva + (offset - startoffset), 0,
			       iobytes);
			skipbytes += iobytes;

			for (i = 0; i < holepages; i++) {
				if (write) {
					pgs[pidx + i]->flags &= ~PG_CLEAN;
				} else {
					pgs[pidx + i]->flags |= PG_RDONLY;
				}
			}
			continue;
		}

		/*
		 * allocate a sub-buf for this piece of the i/o
		 * (or just use mbp if there's only 1 piece),
		 * and start it going.
		 */

		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			bp = pool_get(&bufpool, PR_WAITOK);
			splx(s);
			bp->b_data = (char *)kva + offset - startoffset;
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_READ|B_CALL;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);

		UVMHIST_LOG(ubchist, "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    bp, offset, iobytes, bp->b_blkno);

		VOP_STRATEGY(bp);
	}

loopdone:
	if (skipbytes) {
		s = splbio();
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}

	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)",0,0,0,0);
		lockmgr(&gp->g_glock, LK_RELEASE, NULL, p);
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}
	s = splbio();
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	raoffset = startoffset + totalbytes;

	/*
	 * if this we encountered a hole then we have to do a little more work.
	 * for read faults, we marked the page PG_RDONLY so that future
	 * write accesses to the page will fault again.
	 * for write faults, we must make sure that the backing store for
	 * the page is completely allocated while the pages are locked.
	 */

	if (error == 0 && sawhole && write) {
		error = GOP_ALLOC(vp, startoffset, npages << PAGE_SHIFT, 0,
			   cred);
		if (error) {
			UVMHIST_LOG(ubchist, "balloc lbn 0x%x -> %d",
				    lbn, error,0,0);
			lockmgr(&gp->g_glock, LK_RELEASE, NULL, p);
			simple_lock(&uobj->vmobjlock);
			goto out;
		}
	}
	lockmgr(&gp->g_glock, LK_RELEASE, NULL, p);
	simple_lock(&uobj->vmobjlock);

	/*
	 * see if we want to start any readahead.
	 * XXXUBC for now, just read the next 128k on 64k boundaries.
	 * this is pretty nonsensical, but it is 50% faster than reading
	 * just the next 64k.
	 */

raout:
	if (!error && !async && !write && ((int)raoffset & 0xffff) == 0 &&
	    PAGE_SHIFT <= 16) {
		int racount;

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset + 0x10000, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);
	}

	/*
	 * we're almost done!  release the pages...
	 * for errors, we free the pages.
	 * otherwise we activate them and mark them as valid and clean.
	 * also, unbusy pages that were not actually requested.
	 */

out:
	if (error) {
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
				    pgs[i], pgs[i]->flags, 0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			if (pgs[i]->flags & PG_FAKE) {
				uvm_pagefree(pgs[i]);
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
		uvm_unlock_pageq();
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(ubchist, "returning error %d", error,0,0,0);
		return error;
	}

	UVMHIST_LOG(ubchist, "succeeding, npages %d", npages,0,0,0);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		if (pgs[i] == NULL) {
			continue;
		}
		UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
			    pgs[i], pgs[i]->flags, 0,0);
		if (pgs[i]->flags & PG_FAKE) {
			UVMHIST_LOG(ubchist, "unfaking pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			pgs[i]->flags &= ~(PG_FAKE);
			pmap_clear_modify(pgs[i]);
			pmap_clear_reference(pgs[i]);
		}
		if (write) {
			pgs[i]->flags &= ~(PG_RDONLY);
		}
		if (i < ridx || i >= ridx + orignpages || async) {
			UVMHIST_LOG(ubchist, "unbusy pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	if (ap->a_m != NULL) {
		memcpy(ap->a_m, &pgs[ridx],
		       orignpages * sizeof(struct vm_page *));
	}
	return 0;
}

/*
 * generic VM putpages routine.
 * Write the given range of pages to backing store.
 */

int
genfs_putpages(v)
	void *v;
{
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		struct vm_page **a_m;
		int a_count;
		int a_flags;
		int *a_rtvals;
	} */ *ap = v;

	int s, error, npages, run;
	int fs_bshift, dev_bshift;
	vaddr_t kva;
	off_t eof, offset, startoffset;
	size_t bytes, iobytes, skipbytes;
	daddr_t lbn, blkno;
	struct vm_page *pg;
	struct buf *mbp, *bp;
	struct vnode *vp = ap->a_vp;
	boolean_t async = (ap->a_flags & PGO_SYNCIO) == 0;
	UVMHIST_FUNC("genfs_putpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p offset 0x%x count %d",
		    vp, ap->a_m[0]->offset, ap->a_count, 0);

	simple_unlock(&vp->v_uobj.vmobjlock);

	GOP_SIZE(vp, vp->v_size, &eof);

	error = 0;
	npages = ap->a_count;
	fs_bshift = vp->v_mount->mnt_fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;

	pg = ap->a_m[0];
	startoffset = pg->offset;
	bytes = MIN(npages << PAGE_SHIFT, eof - startoffset);
	skipbytes = 0;
	KASSERT(bytes != 0);

	kva = uvm_pagermapin(ap->a_m, npages, UVMPAGER_MAPIN_WAITOK);

	s = splbio();
	vp->v_numoutput += 2;
	mbp = pool_get(&bufpool, PR_WAITOK);
	UVMHIST_LOG(ubchist, "vp %p mbp %p num now %d bytes 0x%x",
		    vp, mbp, vp->v_numoutput, bytes);
	splx(s);
	mbp->b_bufsize = npages << PAGE_SHIFT;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE |
		(async ? B_CALL : 0) |
		(curproc == uvm.pagedaemon_proc ? B_PDAEMON : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {
		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP() -> %d", error,0,0,0);
			skipbytes += bytes;
			bytes = 0;
			break;
		}

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (blkno == (daddr_t)-1) {
			skipbytes += iobytes;
			continue;
		}

		/* if it's really one i/o, don't make a second buf */
		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			vp->v_numoutput++;
			bp = pool_get(&bufpool, PR_WAITOK);
			UVMHIST_LOG(ubchist, "vp %p bp %p num now %d",
				    vp, bp, vp->v_numoutput, 0);
			splx(s);
			bp->b_data = (char *)kva +
				(vaddr_t)(offset - pg->offset);
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);
		UVMHIST_LOG(ubchist, "vp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    vp, offset, bp->b_bcount, bp->b_blkno);
		VOP_STRATEGY(bp);
	}
	if (skipbytes) {
		UVMHIST_LOG(ubchist, "skipbytes %d", skipbytes, 0,0,0);
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)", 0,0,0,0);
		return 0;
	}
	if (bp != NULL) {
		UVMHIST_LOG(ubchist, "waiting for mbp %p", mbp,0,0,0);
		error = biowait(mbp);
	}
	if (bioops.io_pageiodone) {
		(*bioops.io_pageiodone)(mbp);
	}
	s = splbio();
	if (mbp->b_vp) {
		vwakeup(mbp->b_vp);
	}
	buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	UVMHIST_LOG(ubchist, "returning, error %d", error,0,0,0);
	return error;
}

void
genfs_size(struct vnode *vp, off_t size, off_t *eobp)
{
	int bsize;

	bsize = 1 << vp->v_mount->mnt_fs_bshift;
	*eobp = (size + bsize - 1) & ~(bsize - 1);
}

void
genfs_node_init(struct vnode *vp, struct genfs_ops *ops)
{
	struct genfs_node *gp = VTOG(vp);

	lockinit(&gp->g_glock, PINOD, "glock", 0, 0);
	gp->g_op = ops;
}
@


1.1
log
@Big cleanup inspired by NetBSD with some parts of the code from NetBSD.
 - get rid of VOP_BALLOCN and VOP_SIZE
 - move the generic getpages and putpages into miscfs/genfs
 - create a genfs_node which must be added to the top of the private portion
   of each vnode for filsystems that want to use genfs_{get,put}pages
 - rename genfs_mmap to vop_generic_mmap
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.1.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: genfs_vnops.c,v 1.1 2001/12/10 04:45:31 art Exp $	*/
d85 2
a86 1
	struct vnode *devvp;
a87 2
	struct uvm_object *uobj = &vp->v_uobj;
	struct vm_page *pg, *pgs[16];			/* XXXUBC 16 */
a91 1
	boolean_t overwrite = (flags & PGO_OVERWRITE) != 0;
d100 1
a100 1
		panic("genfs_getpages: too many pages");
d153 1
a153 7
	if (vp->v_type == VREG) {
		fs_bshift = vp->v_mount->mnt_fs_bshift;
		dev_bshift = vp->v_mount->mnt_dev_bshift;
	} else {
		fs_bshift = DEV_BSHIFT;
		dev_bshift = DEV_BSHIFT;
	}
d155 1
d170 22
a210 16
	 * if PGO_OVERWRITE is set, don't bother reading the pages.
	 */

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			pg->flags &= ~(PG_RDONLY|PG_CLEAN);
		}
		npages += ridx;
		goto out;
	}

	/*
d220 1
a220 1
		 * we need to avoid deadlocks caused by locking
d222 2
a223 1
		 * already have locked. unlock them all and start over.
d262 1
a262 1
	mbp->b_iodone = (async ? uvm_aio_biodone : 0);
a271 2
		UVMHIST_LOG(ubchist, "tailbytes %p 0x%x 0x%x",
			    kva, bytes, tailbytes,0);
d318 1
a318 1
		error = VOP_BMAP(vp, lbn, &devvp, &blkno, &run);
a385 1
			bp->b_proc = NULL;
a389 3
		if (devvp->v_type == VBLK) {
			bp->b_dev = devvp->v_rdev;
		}
d438 9
a446 7
	if (!error && sawhole && write) {
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			pgs[i]->flags &= ~PG_CLEAN;
			UVMHIST_LOG(ubchist, "mark dirty pg %p", pgs[i],0,0,0);
a447 4
		error = GOP_ALLOC(vp, startoffset, npages << PAGE_SHIFT, 0,
				  cred);
		UVMHIST_LOG(ubchist, "gop_alloc off 0x%x/0x%x -> %d",
		    startoffset, npages << PAGE_SHIFT, error,0);
d482 1
d484 1
d491 9
d501 2
a502 1
				pgs[i]->flags |= PG_RELEASED;
d504 3
a507 2
		uvm_lock_pageq();
		uvm_page_unbusy(pgs, npages);
a513 1
out:
d517 1
a517 2
		pg = pgs[i];
		if (pg == NULL) {
d521 7
a527 4
			    pg, pg->flags, 0,0);
		if (pg->flags & PG_FAKE && !overwrite) {
			pg->flags &= ~(PG_FAKE);
			pmap_clear_modify(pg);
d530 1
a530 1
			pg->flags &= ~(PG_RDONLY);
d534 8
a541 10
				    pg, pg->offset,0,0);
			if (pg->flags & PG_WANTED) {
				wakeup(pg);
			}
			if (pg->flags & PG_FAKE) {
				KASSERT(overwrite);
				uvm_pagezero(pg);
			}
			if (pg->flags & PG_RELEASED) {
				uvm_pagefree(pg);
d544 3
a546 3
			uvm_pageactivate(pg);
			pg->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pg, NULL);
a560 46
 *
 * => "offhi == 0" means flush all pages at or after "offlo".
 * => object should be locked by caller.   we may _unlock_ the object
 *	if (and only if) we need to clean a page (PGO_CLEANIT), or
 *	if PGO_SYNCIO is set and there are pages busy.
 *	we return with the object locked.
 * => if PGO_CLEANIT or PGO_SYNCIO is set, we may block (due to I/O).
 *	thus, a caller might want to unlock higher level resources
 *	(e.g. vm_map) before calling flush.
 * => if neither PGO_CLEANIT nor PGO_SYNCIO is set, then we will neither
 *	unlock the object nor block.
 * => if PGO_ALLPAGES is set, then all pages in the object will be processed.
 * => NOTE: we rely on the fact that the object's memq is a TAILQ and
 *	that new pages are inserted on the tail end of the list.   thus,
 *	we can make a complete pass through the object in one go by starting
 *	at the head and working towards the tail (new pages are put in
 *	front of us).
 * => NOTE: we are allowed to lock the page queues, so the caller
 *	must not be holding the page queue lock.
 *
 * note on "cleaning" object and PG_BUSY pages:
 *	this routine is holding the lock on the object.   the only time
 *	that it can run into a PG_BUSY page that it does not own is if
 *	some other process has started I/O on the page (e.g. either
 *	a pagein, or a pageout).    if the PG_BUSY page is being paged
 *	in, then it can not be dirty (!PG_CLEAN) because no one has
 *	had a chance to modify it yet.    if the PG_BUSY page is being
 *	paged out then it means that someone else has already started
 *	cleaning the page for us (how nice!).    in this case, if we 
 *	have syncio specified, then after we make our pass through the
 *	object we need to wait for the other PG_BUSY pages to clear 
 *	off (i.e. we need to do an iosync).   also note that once a
 *	page is PG_BUSY it must stay in its object until it is un-busyed.
 *
 * note on page traversal:
 *	we can traverse the pages in an object either by going down the
 *	linked list in "uobj->memq", or we can go over the address range
 *	by page doing hash table lookups for each address.    depending
 *	on how many pages are in the object it may be cheaper to do one 
 *	or the other.   we set "by_list" to true if we are using memq.
 *	if the cost of a hash lookup was equal to the cost of the list
 *	traversal we could compare the number of pages in the start->stop
 *	range to the total number of pages in the object.   however, it
 *	seems that a hash table lookup is more expensive than the linked
 *	list traversal, so we multiply the number of pages in the 
 *	range by an estimate of the relatively higher cost of the hash lookup.
d569 2
a570 2
		voff_t a_offlo;
		voff_t a_offhi;
d572 1
a573 253
	struct vnode *vp = ap->a_vp;
	struct uvm_object *uobj = &vp->v_uobj;
	off_t startoff = ap->a_offlo;
	off_t endoff = ap->a_offhi;
	off_t off;
	int flags = ap->a_flags;
	int n = MAXBSIZE >> PAGE_SHIFT;
	int i, s, error, npages, nback;
	int freeflag;
	struct vm_page *pgs[n], *pg, *nextpg, *tpg, curmp, endmp;
	boolean_t wasclean, by_list, needs_clean;
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	UVMHIST_FUNC("genfs_putpages"); UVMHIST_CALLED(ubchist);

	KASSERT(flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE));
	KASSERT((startoff & PAGE_MASK) == 0 && (endoff & PAGE_MASK) == 0);
	KASSERT(startoff < endoff || endoff == 0);

	UVMHIST_LOG(ubchist, "vp %p pages %d off 0x%x len 0x%x",
	    vp, uobj->uo_npages, startoff, endoff - startoff);
	if (uobj->uo_npages == 0) {
		s = splbio();
		if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
		    (vp->v_bioflag & VBIOONSYNCLIST)) {
			vp->v_bioflag &= ~VBIOONSYNCLIST;
			LIST_REMOVE(vp, v_synclist);
		}
		splx(s);
		simple_unlock(&uobj->vmobjlock);
		return 0;
	}

	/*
	 * the vnode has pages, set up to process the request.
	 */

	error = 0;
	wasclean = TRUE;
	off = startoff;
	if (endoff == 0 || flags & PGO_ALLPAGES) {
		endoff = trunc_page(LLONG_MAX);
	}
	by_list = (uobj->uo_npages <=
	    ((endoff - startoff) >> PAGE_SHIFT) * UVM_PAGE_HASH_PENALTY);

	/*
	 * start the loop.  when scanning by list, hold the last page
	 * in the list before we start.  pages allocated after we start
	 * will be added to the end of the list, so we can stop at the
	 * current last page.
	 */

	freeflag = (curproc == uvm.pagedaemon_proc) ? PG_PAGEOUT : PG_RELEASED;
	curmp.uobject = uobj;
	curmp.offset = (voff_t)-1;
	curmp.flags = PG_BUSY;
	endmp.uobject = uobj;
	endmp.offset = (voff_t)-1;
	endmp.flags = PG_BUSY;
	if (by_list) {
		pg = TAILQ_FIRST(&uobj->memq);
		TAILQ_INSERT_TAIL(&uobj->memq, &endmp, listq);
		PHOLD(curproc);
	} else {
		pg = uvm_pagelookup(uobj, off);
	}
	nextpg = NULL;
	while (by_list || off < endoff) {

		/*
		 * if the current page is not interesting, move on to the next.
		 */

		KASSERT(pg == NULL || pg->uobject == uobj);
		KASSERT(pg == NULL ||
			(pg->flags & (PG_RELEASED|PG_PAGEOUT)) == 0 ||
			(pg->flags & PG_BUSY) != 0);
		if (by_list) {
			if (pg == &endmp) {
				break;
			}
			if (pg->offset < startoff || pg->offset >= endoff ||
			    pg->flags & (PG_RELEASED|PG_PAGEOUT)) {
				pg = TAILQ_NEXT(pg, listq);
				continue;
			}
			off = pg->offset;
		} else if (pg == NULL || pg->flags & (PG_RELEASED|PG_PAGEOUT)) {
			off += PAGE_SIZE;
			if (off < endoff) {
				pg = uvm_pagelookup(uobj, off);
			}
			continue;
		}

		/*
		 * if the current page needs to be cleaned and it's busy,
		 * wait for it to become unbusy.
		 */

		if (flags & PGO_FREE) {
			pmap_page_protect(pg, VM_PROT_NONE);
		}
		if (flags & PGO_CLEANIT) {
			needs_clean = pmap_clear_modify(pg) ||
				(pg->flags & PG_CLEAN) == 0;
			pg->flags |= PG_CLEAN;
		} else {
			needs_clean = FALSE;
		}
		if (needs_clean && pg->flags & PG_BUSY) {
			KASSERT(curproc != uvm.pagedaemon_proc);
			UVMHIST_LOG(ubchist, "busy %p", pg,0,0,0);
			if (by_list) {
				TAILQ_INSERT_BEFORE(pg, &curmp, listq);
				UVMHIST_LOG(ubchist, "curmp next %p",
					    TAILQ_NEXT(&curmp, listq), 0,0,0);
			}
			pg->flags |= PG_WANTED;
			pg->flags &= ~PG_CLEAN;
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
			    "genput", 0);
			simple_lock(&uobj->vmobjlock);
			if (by_list) {
				UVMHIST_LOG(ubchist, "after next %p",
					    TAILQ_NEXT(&curmp, listq), 0,0,0);
				pg = TAILQ_NEXT(&curmp, listq);
				TAILQ_REMOVE(&uobj->memq, &curmp, listq);
			} else {
				pg = uvm_pagelookup(uobj, off);
			}
			continue;
		}

		/*
		 * if we're cleaning, build a cluster.
		 * the cluster will consist of pages which are currently dirty,
		 * but they will be returned to us marked clean.
		 * if not cleaning, just operate on the one page.
		 */

		if (needs_clean) {
			wasclean = FALSE;
			memset(pgs, 0, sizeof(pgs));
			pg->flags |= PG_BUSY;
			UVM_PAGE_OWN(pg, "genfs_putpages");

			/*
			 * first look backward.
			 */

			npages = MIN(n >> 1, off >> PAGE_SHIFT);
			nback = npages;
			uvn_findpages(uobj, off - PAGE_SIZE, &nback, &pgs[0],
			    UFP_NOWAIT|UFP_NOALLOC|UFP_DIRTYONLY|UFP_BACKWARD);
			if (nback) {
				int i;

				for (i = 0; i < nback; i++)
					pgs[i] = pgs[npages - nback + i];
			}
			n -= nback;

			/*
			 * then plug in our page of interest.
			 */

			pgs[nback] = pg;

			/*
			 * then look forward to fill in the remaining space in
			 * the array of pages.
			 */

			npages = MIN(n, (endoff - off) >> PAGE_SHIFT) - 1;
			uvn_findpages(uobj, off + PAGE_SIZE, &npages,
			    &pgs[nback + 1],
			    UFP_NOWAIT|UFP_NOALLOC|UFP_DIRTYONLY);
			npages += nback + 1;
		} else {
			pgs[0] = pg;
			npages = 1;
		}

		/*
		 * apply FREE or DEACTIVATE options if requested.
		 */

		if (flags & (PGO_DEACTIVATE|PGO_FREE)) {
			uvm_lock_pageq();
		}
		for (i = 0; i < npages; i++) {
			tpg = pgs[i];
			KASSERT(tpg->uobject == uobj);
			if (flags & PGO_DEACTIVATE &&
			    (tpg->pqflags & PQ_INACTIVE) == 0 &&
			    tpg->wire_count == 0) {
				(void) pmap_clear_reference(tpg);
				uvm_pagedeactivate(tpg);
			} else if (flags & PGO_FREE) {
				pmap_page_protect(tpg, VM_PROT_NONE);
				if (tpg->flags & PG_BUSY) {
					tpg->flags |= freeflag;
					if (freeflag == PG_PAGEOUT) {
						uvmexp.paging++;
						uvm_pagedequeue(tpg);
					}
				} else {
					nextpg = TAILQ_NEXT(tpg, listq);
					uvm_pagefree(tpg);
				}
			}
		}
		if (flags & (PGO_DEACTIVATE|PGO_FREE)) {
			uvm_unlock_pageq();
		}
		if (needs_clean) {

			/*
			 * start the i/o.  if we're traversing by list,
			 * keep our place in the list with a marker page.
			 */

			if (by_list) {
				TAILQ_INSERT_AFTER(&uobj->memq, pg, &curmp,
				    listq);
			}
			simple_unlock(&uobj->vmobjlock);
			error = GOP_WRITE(vp, pgs, npages, flags);
			simple_lock(&uobj->vmobjlock);
			if (by_list) {
				pg = TAILQ_NEXT(&curmp, listq);
				TAILQ_REMOVE(&uobj->memq, &curmp, listq);
			}
			if (error == ENOMEM) {
				for (i = 0; i < npages; i++) {
					tpg = pgs[i];
					if (tpg->flags & PG_PAGEOUT) {
						tpg->flags &= ~PG_PAGEOUT;
						uvmexp.paging--;
					}
					tpg->flags &= ~PG_CLEAN;
					uvm_pageactivate(tpg);
				}
				uvm_page_unbusy(pgs, npages);
			}
			if (error) {
				break;
			}
			if (by_list) {
				continue;
			}
		}
d575 1
a575 56
		/*
		 * find the next page and continue if there was no error.
		 */

		if (by_list) {
			if (nextpg) {
				pg = nextpg;
				nextpg = NULL;
			} else {
				pg = TAILQ_NEXT(pg, listq);
			}
		} else {
			off += PAGE_SIZE;
			if (off < endoff) {
				pg = uvm_pagelookup(uobj, off);
			}
		}
	}
	if (by_list) {
		TAILQ_REMOVE(&uobj->memq, &endmp, listq);
		PRELE(curproc);
	}

	/*
	 * if we're cleaning and there was nothing to clean,
	 * take us off the syncer list.  if we started any i/o
	 * and we're doing sync i/o, wait for all writes to finish.
	 */

	s = splbio();
	if ((flags & PGO_CLEANIT) && wasclean &&
	    startoff == 0 && endoff == trunc_page(LLONG_MAX) &&
	    LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
	    (vp->v_bioflag & VBIOONSYNCLIST)) {
		vp->v_bioflag &= ~VBIOONSYNCLIST;
		LIST_REMOVE(vp, v_synclist);
	}
	splx(s);
	if (!wasclean && !async) {
		s = splbio();
		while (vp->v_numoutput != 0) {
			vp->v_bioflag |= VBIOWAIT;
			UVM_UNLOCK_AND_WAIT(&vp->v_numoutput, &uobj->vmobjlock,
					    FALSE, "genput2",0);
			simple_lock(&uobj->vmobjlock);
		}
		splx(s);
	}
	simple_unlock(&uobj->vmobjlock);
	return error;
}

int
genfs_gop_write(struct vnode *vp, struct vm_page **pgs, int npages, int flags)
{
	int s, error, run;
d583 7
a589 5
	struct vnode *devvp;
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	UVMHIST_FUNC("genfs_do_putpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p pgs %p npages %d flags 0x%x",
	    vp, pgs, npages, flags);
d593 4
a596 7
	if (vp->v_type == VREG) {
		fs_bshift = vp->v_mount->mnt_fs_bshift;
		dev_bshift = vp->v_mount->mnt_dev_bshift;
	} else {
		fs_bshift = DEV_BSHIFT;
		dev_bshift = DEV_BSHIFT;
	}
d598 1
a598 2
	error = 0;
	pg = pgs[0];
d604 1
a604 2
	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WRITE | 
		UVMPAGER_MAPIN_WAITOK);
d615 3
a617 1
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE | (async ? B_CALL : 0);
d628 1
a628 1
		error = VOP_BMAP(vp, lbn, &devvp, &blkno, &run);
d656 1
a656 1
			bp->b_flags = B_BUSY|B_WRITE|B_CALL;
a662 3
		if (devvp->v_type == VBLK) {
			bp->b_dev = devvp->v_rdev;
		}
d674 1
a678 1
		mbp->b_resid -= skipbytes;
d688 15
a702 4

	UVMHIST_LOG(ubchist, "waiting for mbp %p", mbp,0,0,0);
	error = biowait(mbp);
	uvm_aio_aiodone(mbp);
@


1.1.2.2
log
@protect bgetvp with splbio
@
text
@d1 1
a1 1
/*	$OpenBSD: genfs_vnops.c,v 1.1.2.1 2002/02/02 03:28:25 art Exp $	*/
a264 1
	s = splbio();
a265 1
	splx(s);
a970 1
	s = splbio();
a971 1
	splx(s);
@


1.1.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 1
a1 3
/*	$OpenBSD: genfs_vnops.c,v 1.1.2.2 2002/06/11 04:13:39 art Exp $	*/
/*	$NetBSD: genfs_vnops.c,v 1.67 2002/10/25 05:44:41 yamt Exp $	*/

a46 1
#include <sys/file.h>
a53 26
static __inline void genfs_rel_pages(struct vm_page **, int);

#define MAX_READ_AHEAD	16 	/* XXXUBC 16 */
int genfs_rapages = MAX_READ_AHEAD; /* # of pages in each chunk of readahead */
int genfs_racount = 2;		/* # of page chunks to readahead */
int genfs_raskip = 2;		/* # of busy page chunks allowed to skip */

static __inline void
genfs_rel_pages(struct vm_page **pgs, int npages)
{
	int i;

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[i];

		if (pg == NULL)
			continue;
		if (pg->flags & PG_FAKE) {
			pg->flags |= PG_RELEASED;
		}
	}
	uvm_lock_pageq();
	uvm_page_unbusy(pgs, npages);
	uvm_unlock_pageq();
}

d61 2
a62 1
genfs_getpages(void *v)
d67 1
a67 1
		struct vm_page *a_m;
d88 2
a89 3
	struct vm_page *pg, *pgs[MAX_READ_AHEAD];
	struct proc *p = curproc;
	struct ucred *cred = p->p_ucred;		/* XXXUBC curproc */
d94 1
d98 1
a98 1
	    vp, ap->a_offset >> 32, ap->a_offset, *ap->a_count);
d101 1
a101 1
	if (*ap->a_count > MAX_READ_AHEAD) {
d129 2
a130 2
		    origoffset, *ap->a_count, memeof,0);
		return (EINVAL);
d139 1
a139 1
		    UFP_NOWAIT|UFP_NOALLOC| (write ? UFP_NORDONLY : 0));
d141 1
a141 1
		return (ap->a_m[ap->a_centeridx] == NULL ? EBUSY : 0);
d168 2
a169 2
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT) +
	    fs_bsize - 1) & ~(fs_bsize - 1));
d174 1
a174 10
	UVMHIST_LOG(ubchist, "ridx %d npages %d startoff %ld endoff %ld",
	    ridx, npages, startoffset, endoffset);
	KASSERT(&pgs[ridx + npages] <= &pgs[MAX_READ_AHEAD]);
	if (uvn_findpages(uobj, origoffset, &npages, &pgs[ridx],
	    async ? UFP_NOWAIT : UFP_ALL) != orignpages) {
		KASSERT(async != 0);
		genfs_rel_pages(&pgs[ridx], orignpages);
		simple_unlock(&uobj->vmobjlock);
		return (EBUSY);
	}
d223 1
a223 1
		 * already have locked.  unlock them all and start over.
d226 8
a233 1
		genfs_rel_pages(&pgs[ridx], orignpages);
d237 1
a237 1
		    startoffset, endoffset, 0,0);
d239 1
a239 7
		if (uvn_findpages(uobj, startoffset, &npgs, pgs,
		    async ? UFP_NOWAIT : UFP_ALL) != npages) {
			KASSERT(async != 0);
			genfs_rel_pages(pgs, npages);
			simple_unlock(&uobj->vmobjlock);
			return (EBUSY);
		}
d252 2
a253 2
	kva = uvm_pagermapin(pgs, npages,
	    UVMPAGER_MAPIN_READ | UVMPAGER_MAPIN_WAITOK);
d261 1
a261 1
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL|B_ASYNC : 0);
a273 6
		size_t tailstart = bytes;

		if ((pgs[bytes >> PAGE_SHIFT]->flags & PG_FAKE) == 0) {
			tailstart = round_page(tailstart);
			tailbytes -= tailstart - bytes;
		}
d275 2
a276 2
		    kva, tailstart, tailbytes,0);
		memset((void *)(kva + tailstart), 0, tailbytes);
d291 2
a292 2
	    bytes > 0;
	    offset += iobytes, bytes -= iobytes) {
d309 1
a309 1
			    offset, 0,0,0);
d325 1
a325 1
			    lbn, error,0,0);
d341 1
a341 1
			    pgs[pidx + pcount]->flags & PG_FAKE) {
d345 1
a345 1
			    (offset - trunc_page(offset)));
d349 2
a350 3
		 * if this block isn't allocated, zero it instead of
		 * reading it.  if this is a read access, mark the
		 * pages we zeroed PG_RDONLY.
d354 2
a355 2
			int holepages = (round_page(offset + iobytes) -
			    trunc_page(offset)) >> PAGE_SHIFT;
d360 1
a360 1
			    iobytes);
d387 1
a387 1
			bp->b_flags = B_BUSY|B_READ|B_CALL|B_ASYNC;
d401 1
a401 1
		    dev_bshift);
d403 2
a404 3
		UVMHIST_LOG(ubchist,
		    "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
		    bp, offset, iobytes, bp->b_blkno);
d426 1
a426 1
		return (0);
d455 1
a455 1
		    cred);
d472 1
a472 2
		off_t rasize;
		int rapages, err, i, skipped;
d474 9
a482 16
		/* XXXUBC temp limit, from above */
		rapages = MIN(MIN(1 << (16 - PAGE_SHIFT), MAX_READ_AHEAD),
		    genfs_rapages);
		rasize = rapages << PAGE_SHIFT;
		for (i = skipped = 0; i < genfs_racount; i++) {
			err = VOP_GETPAGES(vp, raoffset, NULL, &rapages, 0,
			    VM_PROT_READ, 0, 0);
			simple_lock(&uobj->vmobjlock);
			if (err) {
				if (err != EBUSY ||
				    skipped++ == genfs_raskip)
					break;
			}
			raoffset += rasize;
			rapages = rasize >> PAGE_SHIFT;
		}
d498 1
a498 1
			    pgs[i], pgs[i]->flags, 0,0);
d508 1
a508 1
		return (error);
d520 1
a520 1
		    pg, pg->flags, 0,0);
d523 1
a523 1
			pmap_clear_modify(pgs[i]);
d530 1
a530 1
			    pg, pg->offset,0,0);
d543 1
a543 1
			pg->flags &= ~(PG_WANTED|PG_BUSY|PG_FAKE);
d551 1
a551 1
		    orignpages * sizeof(struct vm_page *));
d553 1
a553 1
	return (0);
d587 1
a587 1
 *	cleaning the page for us (how nice!).    in this case, if we
d589 1
a589 1
 *	object we need to wait for the other PG_BUSY pages to clear
d597 1
a597 1
 *	on how many pages are in the object it may be cheaper to do one
d603 1
a603 1
 *	list traversal, so we multiply the number of pages in the
d608 2
a609 1
genfs_putpages(void *v)
a618 1
	struct simplelock *slock = &uobj->vmobjlock;
d623 1
a623 1
	const int maxpages = MAXBSIZE >> PAGE_SHIFT;
d626 2
a627 2
	struct vm_page *pgs[maxpages], *pg, *nextpg, *tpg, curmp, endmp;
	boolean_t wasclean, by_list, needs_clean, yield;
a628 1
	boolean_t pagedaemon = curproc == uvm.pagedaemon_proc;
d645 2
a646 2
		simple_unlock(slock);
		return (0);
d654 1
a654 3
	s = splbio();
	wasclean = (vp->v_numoutput == 0);
	splx(s);
d669 1
a669 1
	freeflag = pagedaemon ? PG_PAGEOUT : PG_RELEASED;
d692 2
a693 2
		    (pg->flags & (PG_RELEASED|PG_PAGEOUT)) == 0 ||
		    (pg->flags & PG_BUSY) != 0);
d704 1
a704 2
		} else if (pg == NULL ||
		    pg->flags & (PG_RELEASED|PG_PAGEOUT)) {
d717 12
a728 4
		yield = (curproc->p_schedflags & PSCHED_SHOULDYIELD)
		    && !pagedaemon;
		if (pg->flags & PG_BUSY || yield) {
			KASSERT(!pagedaemon);
d733 1
a733 10
				    TAILQ_NEXT(&curmp, listq), 0,0,0);
			}
			if (yield) {
				simple_unlock(slock);
				preempt(NULL);
				simple_lock(slock);
			} else {
				pg->flags |= PG_WANTED;
				UVM_UNLOCK_AND_WAIT(pg, slock, 0, "genput", 0);
				simple_lock(slock);
d735 5
d742 1
a742 1
				    TAILQ_NEXT(&curmp, listq), 0,0,0);
a751 16
		 * if we're freeing, remove all mappings of the page now.
		 * if we're cleaning, check if the page is needs to be cleaned.
		 */

		if (flags & PGO_FREE) {
			pmap_page_protect(pg, VM_PROT_NONE);
		}
		if (flags & PGO_CLEANIT) {
			needs_clean = pmap_clear_modify(pg) ||
			    (pg->flags & PG_CLEAN) == 0;
			pg->flags |= PG_CLEAN;
		} else {
			needs_clean = FALSE;
		}

		/*
d768 1
a768 1
			npages = MIN(maxpages >> 1, off >> PAGE_SHIFT);
a776 6
				if (npages - nback < nback)
					memset(&pgs[nback], 0,
					    (npages - nback) * sizeof(pgs[0]));
				else
					memset(&pgs[npages - nback], 0,
					    nback * sizeof(pgs[0]));
d778 1
d791 1
a791 1
			npages = maxpages - nback - 1;
a798 1
			nback = 0;
a810 4
			if (by_list && tpg == TAILQ_NEXT(pg, listq))
				pg = tpg;
			if (tpg->offset < startoff || tpg->offset >= endoff)
				continue;
d820 1
a820 1
					if (pagedaemon) {
a824 7

					/*
					 * ``page is not busy''
					 * implies that npages is 1
					 * and needs_clean is false.
					 */

d844 1
a844 1
			simple_unlock(slock);
d846 1
a846 1
			simple_lock(slock);
d851 12
d883 1
a883 1
			off += (npages - nback) << PAGE_SHIFT;
d913 3
a915 3
			UVM_UNLOCK_AND_WAIT(&vp->v_numoutput, slock, FALSE,
			    "genput2",0);
			simple_lock(slock);
d920 1
a920 1
	return (error);
d936 1
a936 1
	UVMHIST_FUNC("genfs_gop_write"); UVMHIST_CALLED(ubchist);
d941 1
d949 1
d957 2
a958 2
	kva = uvm_pagermapin(pgs, npages,
	    UVMPAGER_MAPIN_WRITE | UVMPAGER_MAPIN_WAITOK);
d964 1
a964 1
	    vp, mbp, vp->v_numoutput, bytes);
d969 1
a969 1
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE| (async ? (B_CALL|B_ASYNC) : 0);
d979 2
a980 2
	    bytes > 0;
	    offset += iobytes, bytes -= iobytes) {
d1005 1
a1005 1
			    vp, bp, vp->v_numoutput, 0);
d1008 1
a1008 1
			    (vaddr_t)(offset - pg->offset);
d1010 1
a1010 1
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
d1023 3
a1025 4
		    dev_bshift);
		UVMHIST_LOG(ubchist,
		    "vp %p offset 0x%x bcount 0x%x blkno 0x%x",
		    vp, offset, bp->b_bcount, bp->b_blkno);
d1043 1
a1043 1
		return (0);
d1045 1
a1047 1
	s = splbio();
a1048 1
	splx(s);
d1050 1
a1050 1
	return (error);
d1053 2
a1054 2
int
genfs_null_putpages(void *v)
d1056 1
a1056 7
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		voff_t a_offlo;
		voff_t a_offhi;
		int a_flags;
	} */ *ap = v;
	struct vnode *vp = ap->a_vp;
d1058 2
a1059 3
	KASSERT(vp->v_uobj.uo_npages == 0);
	simple_unlock(&vp->v_interlock);
	return (0);
a1068 150
}

void
genfs_size(struct vnode *vp, off_t size, off_t *eobp)
{
	int bsize;

	bsize = 1 << vp->v_mount->mnt_fs_bshift;
	*eobp = (size + bsize - 1) & ~(bsize - 1);
}

int
genfs_compat_getpages(void *v)
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		struct vm_page **a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t origoffset;
	struct vnode *vp = ap->a_vp;
	struct uvm_object *uobj = &vp->v_uobj;
	struct vm_page *pg, **pgs;
	vaddr_t kva;
	int i, error, orignpages, npages;
	struct iovec iov;
	struct uio uio;
	struct ucred *cred = curproc->p_ucred;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	int s;

	error = 0;
	origoffset = ap->a_offset;
	orignpages = *ap->a_count;
	pgs = ap->a_m;

	s = splbio();
	if (write && (vp->v_flag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}
	splx(s);
	if (ap->a_flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
		    UFP_NOWAIT|UFP_NOALLOC| (write ? UFP_NORDONLY : 0));

		return (ap->a_m[ap->a_centeridx] == NULL ? EBUSY : 0);
	}
	if (origoffset + (ap->a_centeridx << PAGE_SHIFT) >= vp->v_size) {
		simple_unlock(&uobj->vmobjlock);
		return (EINVAL);
	}
	npages = orignpages;
	uvn_findpages(uobj, origoffset, &npages, pgs, UFP_ALL);
	simple_unlock(&uobj->vmobjlock);
	kva = uvm_pagermapin(pgs, npages,
	    UVMPAGER_MAPIN_READ | UVMPAGER_MAPIN_WAITOK);
	for (i = 0; i < npages; i++) {
		pg = pgs[i];
		if ((pg->flags & PG_FAKE) == 0) {
			continue;
		}
		iov.iov_base = (char *)kva + (i << PAGE_SHIFT);
		iov.iov_len = PAGE_SIZE;
		uio.uio_iov = &iov;
		uio.uio_iovcnt = 1;
		uio.uio_offset = origoffset + (i << PAGE_SHIFT);
		uio.uio_segflg = UIO_SYSSPACE;
		uio.uio_rw = UIO_READ;
		uio.uio_resid = PAGE_SIZE;
		uio.uio_procp = curproc;
		error = VOP_READ(vp, &uio, 0, cred);
		if (error) {
			break;
		}
		if (uio.uio_resid) {
			memset(iov.iov_base, 0, uio.uio_resid);
		}
	}
	uvm_pagermapout(kva, npages);
	simple_lock(&uobj->vmobjlock);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		pg = pgs[i];
		if (error && (pg->flags & PG_FAKE) != 0) {
			pg->flags |= PG_RELEASED;
		} else {
			pmap_clear_modify(pg);
			uvm_pageactivate(pg);
		}
	}
	if (error) {
		uvm_page_unbusy(pgs, npages);
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	return (error);
}

int
genfs_compat_gop_write(struct vnode *vp, struct vm_page **pgs, int npages,
    int flags)
{
	off_t offset;
	struct iovec iov;
	struct uio uio;
	struct ucred *cred = curproc->p_ucred;
	struct buf *bp;
	vaddr_t kva;
	int s, error;

	offset = pgs[0]->offset;
	kva = uvm_pagermapin(pgs, npages,
	    UVMPAGER_MAPIN_WRITE | UVMPAGER_MAPIN_WAITOK);

	iov.iov_base = (void *)kva;
	iov.iov_len = npages << PAGE_SHIFT;
	uio.uio_iov = &iov;
	uio.uio_iovcnt = npages;
	uio.uio_offset = offset;
	uio.uio_segflg = UIO_SYSSPACE;
	uio.uio_rw = UIO_WRITE;
	uio.uio_resid = npages << PAGE_SHIFT;
	uio.uio_procp = curproc;
	error = VOP_WRITE(vp, &uio, 0, cred);

	s = splbio();
	vp->v_numoutput++;
	bp = pool_get(&bufpool, PR_WAITOK);
	splx(s);

	bp->b_flags = B_BUSY | B_WRITE | B_AGE;
	bp->b_vp = vp;
	bp->b_lblkno = offset >> vp->v_mount->mnt_fs_bshift;
	bp->b_data = (char *)kva;
	bp->b_bcount = npages << PAGE_SHIFT;
	bp->b_bufsize = npages << PAGE_SHIFT;
	bp->b_resid = 0;
	LIST_INIT(&bp->b_dep);
	if (error) {
		bp->b_flags |= B_ERROR;
		bp->b_error = error;
	}
	uvm_aio_aiodone(bp);
	return (error);
@


1.1.2.4
log
@add genfs_lock functions
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a62 82

/*
 * Lock the node.
 */
int
genfs_lock(void *v)
{
	struct vop_lock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;
	struct vnode *vp = ap->a_vp;

	return (lockmgr(&vp->v_lock, ap->a_flags, &vp->v_interlock, ap->a_p));
}

/*
 * Unlock the node.
 */
int
genfs_unlock(void *v)
{
	struct vop_unlock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;
	struct vnode *vp = ap->a_vp;

	return (lockmgr(&vp->v_lock, ap->a_flags | LK_RELEASE,
	    &vp->v_interlock, ap->a_p));
}

/*
 * Return whether or not the node is locked.
 */
int
genfs_islocked(void *v)
{
	struct vop_islocked_args /* {
		struct vnode *a_vp;
	} */ *ap = v;
	struct vnode *vp = ap->a_vp;

	return (lockstatus(&vp->v_lock));
}

/*
 * Stubs to use when there is no locking to be done on the underlying object.
 */
int
genfs_nolock(void *v)
{
	struct vop_lock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

	/*
	 * Since we are not using the lock manager, we must clear
	 * the interlock here.
	 */
	if (ap->a_flags & LK_INTERLOCK)
		simple_unlock(&ap->a_vp->v_interlock);
	return (0);
}

int
genfs_nounlock(void *v)
{

	return (0);
}

int
genfs_noislocked(void *v)
{

	return (0);
}
@


1.1.2.5
log
@trick gcc into non-DIAG compile
@
text
@d1 1
a1 1
/*	$OpenBSD: genfs_vnops.c,v 1.1.2.4 2003/05/20 04:04:46 tedu Exp $	*/
d1217 2
a1218 3
	struct vnode *vp;
	
	vp = ap->a_vp;
@


