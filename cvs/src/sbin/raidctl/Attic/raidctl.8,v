head	1.44;
access;
symbols
	OPENBSD_5_1_BASE:1.43
	OPENBSD_5_1:1.43.0.2
	OPENBSD_5_0:1.42.0.6
	OPENBSD_5_0_BASE:1.42
	OPENBSD_4_9:1.42.0.4
	OPENBSD_4_9_BASE:1.42
	OPENBSD_4_8:1.42.0.2
	OPENBSD_4_8_BASE:1.42
	OPENBSD_4_7:1.41.0.6
	OPENBSD_4_7_BASE:1.41
	OPENBSD_4_6:1.41.0.8
	OPENBSD_4_6_BASE:1.41
	OPENBSD_4_5:1.41.0.4
	OPENBSD_4_5_BASE:1.41
	OPENBSD_4_4:1.41.0.2
	OPENBSD_4_4_BASE:1.41
	OPENBSD_4_3:1.38.0.2
	OPENBSD_4_3_BASE:1.38
	OPENBSD_4_2:1.37.0.2
	OPENBSD_4_2_BASE:1.37
	OPENBSD_4_1:1.36.0.2
	OPENBSD_4_1_BASE:1.36
	OPENBSD_4_0:1.35.0.6
	OPENBSD_4_0_BASE:1.35
	OPENBSD_3_9:1.35.0.4
	OPENBSD_3_9_BASE:1.35
	OPENBSD_3_8:1.35.0.2
	OPENBSD_3_8_BASE:1.35
	OPENBSD_3_7:1.33.0.2
	OPENBSD_3_7_BASE:1.33
	OPENBSD_3_6:1.32.0.4
	OPENBSD_3_6_BASE:1.32
	OPENBSD_3_5:1.32.0.2
	OPENBSD_3_5_BASE:1.32
	OPENBSD_3_4:1.30.0.2
	OPENBSD_3_4_BASE:1.30
	OPENBSD_3_3:1.28.0.2
	OPENBSD_3_3_BASE:1.28
	OPENBSD_3_2:1.25.0.2
	OPENBSD_3_2_BASE:1.25
	OPENBSD_3_1:1.23.0.2
	OPENBSD_3_1_BASE:1.23
	OPENBSD_3_0:1.17.0.2
	OPENBSD_3_0_BASE:1.17
	OPENBSD_2_9:1.16.0.2
	OPENBSD_2_9_BASE:1.16
	OPENBSD_2_8:1.15.0.4
	OPENBSD_2_8_BASE:1.15
	OPENBSD_2_7:1.15.0.2
	OPENBSD_2_7_BASE:1.15
	OPENBSD_2_6:1.8.0.2
	OPENBSD_2_6_BASE:1.8
	OPENBSD_2_5:1.3.0.2
	OPENBSD_2_5_BASE:1.3;
locks; strict;
comment	@.\" @;


1.44
date	2012.04.06.15.54.58;	author jsing;	state dead;
branches;
next	1.43;

1.43
date	2011.10.06.22.22.10;	author jmc;	state Exp;
branches;
next	1.42;

1.42
date	2010.04.01.17.06.55;	author jmc;	state Exp;
branches;
next	1.41;

1.41
date	2008.06.26.05.42.06;	author ray;	state Exp;
branches;
next	1.40;

1.40
date	2008.06.03.19.51.02;	author jmc;	state Exp;
branches;
next	1.39;

1.39
date	2008.03.23.23.28.46;	author krw;	state Exp;
branches;
next	1.38;

1.38
date	2008.01.26.23.07.55;	author jmc;	state Exp;
branches;
next	1.37;

1.37
date	2007.05.31.19.19.47;	author jmc;	state Exp;
branches;
next	1.36;

1.36
date	2007.02.21.16.33.09;	author jmc;	state Exp;
branches;
next	1.35;

1.35
date	2005.08.27.06.43.42;	author jmc;	state Exp;
branches;
next	1.34;

1.34
date	2005.08.24.22.47.28;	author jmc;	state Exp;
branches;
next	1.33;

1.33
date	2005.03.12.12.21.08;	author jmc;	state Exp;
branches;
next	1.32;

1.32
date	2003.12.04.11.43.27;	author henning;	state Exp;
branches;
next	1.31;

1.31
date	2003.09.18.09.21.49;	author jmc;	state Exp;
branches;
next	1.30;

1.30
date	2003.06.03.13.16.09;	author jmc;	state Exp;
branches;
next	1.29;

1.29
date	2003.04.02.19.00.26;	author jmc;	state Exp;
branches;
next	1.28;

1.28
date	2003.02.13.08.23.40;	author jmc;	state Exp;
branches;
next	1.27;

1.27
date	2003.02.11.07.26.20;	author jmc;	state Exp;
branches;
next	1.26;

1.26
date	2002.10.15.11.59.01;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2002.08.08.10.55.13;	author tdeval;	state Exp;
branches;
next	1.24;

1.24
date	2002.06.09.08.13.09;	author todd;	state Exp;
branches;
next	1.23;

1.23
date	2002.02.18.22.57.05;	author tdeval;	state Exp;
branches;
next	1.22;

1.22
date	2002.02.17.20.23.36;	author tdeval;	state Exp;
branches;
next	1.21;

1.21
date	2002.02.11.18.43.51;	author mpech;	state Exp;
branches;
next	1.20;

1.20
date	2002.01.05.13.47.37;	author tdeval;	state Exp;
branches;
next	1.19;

1.19
date	2001.12.30.00.31.03;	author brad;	state Exp;
branches;
next	1.18;

1.18
date	2001.12.29.21.54.57;	author tdeval;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.20.18.07.11;	author mpech;	state Exp;
branches;
next	1.16;

1.16
date	2000.11.09.17.53.04;	author aaron;	state Exp;
branches;
next	1.15;

1.15
date	2000.04.24.05.12.19;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	2000.04.15.02.15.16;	author aaron;	state Exp;
branches;
next	1.13;

1.13
date	2000.04.12.21.47.59;	author aaron;	state Exp;
branches;
next	1.12;

1.12
date	2000.03.18.22.56.03;	author aaron;	state Exp;
branches;
next	1.11;

1.11
date	2000.03.14.21.31.42;	author aaron;	state Exp;
branches;
next	1.10;

1.10
date	2000.03.05.00.28.56;	author aaron;	state Exp;
branches;
next	1.9;

1.9
date	2000.01.07.14.51.41;	author peter;	state Exp;
branches;
next	1.8;

1.8
date	99.09.23.04.12.02;	author alex;	state Exp;
branches;
next	1.7;

1.7
date	99.07.30.17.24.32;	author peter;	state Exp;
branches;
next	1.6;

1.6
date	99.07.30.14.45.31;	author peter;	state Exp;
branches;
next	1.5;

1.5
date	99.07.03.02.11.08;	author aaron;	state Exp;
branches;
next	1.4;

1.4
date	99.06.04.02.45.25;	author aaron;	state Exp;
branches;
next	1.3;

1.3
date	99.04.02.15.12.18;	author aaron;	state Exp;
branches;
next	1.2;

1.2
date	99.02.16.21.51.39;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	99.01.11.14.49.44;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.44
log
@Put raidctl in the attic.
@
text
@.\"	$OpenBSD: raidctl.8,v 1.43 2011/10/06 22:22:10 jmc Exp $
.\"     $NetBSD: raidctl.8,v 1.24 2001/07/10 01:30:52 lukem Exp $
.\"
.\" Copyright (c) 1998 The NetBSD Foundation, Inc.
.\" All rights reserved.
.\"
.\" This code is derived from software contributed to The NetBSD Foundation
.\" by Greg Oster
.\"
.\" Redistribution and use in source and binary forms, with or without
.\" modification, are permitted provided that the following conditions
.\" are met:
.\" 1. Redistributions of source code must retain the above copyright
.\"    notice, this list of conditions and the following disclaimer.
.\" 2. Redistributions in binary form must reproduce the above copyright
.\"    notice, this list of conditions and the following disclaimer in the
.\"    documentation and/or other materials provided with the distribution.
.\"
.\" THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
.\" ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
.\" TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
.\" PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
.\" BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
.\" CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
.\" SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
.\" INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
.\" CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
.\" ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
.\" POSSIBILITY OF SUCH DAMAGE.
.\"
.\"
.\" Copyright (c) 1995 Carnegie-Mellon University.
.\" All rights reserved.
.\"
.\" Author: Mark Holland
.\"
.\" Permission to use, copy, modify and distribute this software and
.\" its documentation is hereby granted, provided that both the copyright
.\" notice and this permission notice appear in all copies of the
.\" software, derivative works or modified versions, and any portions
.\" thereof, and that both notices appear in supporting documentation.
.\"
.\" CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
.\" CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
.\" FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
.\"
.\" Carnegie Mellon requests users of this software to return to
.\"
.\"  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
.\"  School of Computer Science
.\"  Carnegie Mellon University
.\"  Pittsburgh PA 15213-3890
.\"
.\" any improvements or extensions that they make and grant Carnegie the
.\" rights to redistribute these changes.
.\"
.Dd $Mdocdate: October 6 2011 $
.Dt RAIDCTL 8
.Os
.Sh NAME
.Nm raidctl
.Nd configuration utility for the RAIDframe disk driver
.Sh SYNOPSIS
.Nm raidctl
.Bk -words
.Op Fl BGiPpSsuv
.Op Fl A Op yes | no | root
.Op Fl Cc Ar config_file
.Op Fl I Ar serial_number
.Op Fl aFfgRr Ar component
.Ar dev
.Ek
.Sh DESCRIPTION
.Nm
is the user-land control program for
.Xr raid 4 ,
the RAIDframe disk device.
.Nm
is primarily used to dynamically configure and unconfigure RAIDframe disk
devices.
For more information about the RAIDframe disk device, see
.Xr raid 4 .
.Pp
This document assumes the reader has at least rudimentary knowledge of
RAID and RAID concepts.
.Pp
The device used by
.Nm
is specified by
.Ar dev .
.Ar dev
may be either the full name of the device, e.g.\&
.Pa /dev/rraid0c ,
or just simply raid0 (for
.Pa /dev/rraid0c ) .
.Pp
For several commands
.Pq Fl BGipPsSu ,
.Nm
can accept the word
.Ic all
as the
.Ar dev
argument.
If
.Ic all
is used,
.Nm
will execute the requested action for all the configured
.Xr raid 4
devices.
.Pp
The command-line options for
.Nm
are as follows:
.Bl -tag -width indent
.It Fl A Ic yes Ar dev
Make the RAID set auto-configurable.
The RAID set will be automatically configured at boot
.Em before
the root file system is
mounted.
Note that all components of the set must be of type RAID in the disklabel.
.It Fl A Ic no Ar dev
Turn off auto-configuration for the RAID set.
.It Fl A Ic root Ar dev
Make the RAID set auto-configurable, and also mark the set as being
eligible to contain the root partition.
A RAID set configured this way will
.Em override
the use of the boot disk as the root device.
All components of the set must be of type RAID in the disklabel.
Note that the kernel being booted must currently reside on a non-RAID set and,
in order to have the root file system correctly mounted from it,
the RAID set must have its
.Sq a
partition (aka raid[0..n]a) set up.
.It Fl a Ar component Ar dev
Add
.Ar component
as a hot spare for the device
.Ar dev .
.It Fl B Ar dev
Initiate a copyback of reconstructed data from a spare disk to
its original disk.
This is performed after a component has failed,
and the failed drive has been reconstructed onto a spare drive.
.It Fl C Ar config_file Ar dev
As for
.Fl c ,
but forces the configuration to take place.
This is required the first time a RAID set is configured.
.It Fl c Ar config_file Ar dev
Configure the RAIDframe device
.Ar dev
according to the configuration given in
.Ar config_file .
A description of the contents of
.Ar config_file
is given later.
.It Fl F Ar component Ar dev
Fails the specified
.Ar component
of the device, and immediately begin a reconstruction of the failed
disk onto an available hot spare.
This is one of the mechanisms used to start the reconstruction process
if a component does have a hardware failure.
.It Fl f Ar component Ar dev
This marks the specified
.Ar component
as having failed, but does not initiate a reconstruction of that
component.
.It Fl G Ar dev
Generate the configuration of the RAIDframe device in a format suitable for
use with
.Nm
.Fl c
or
.Fl C .
.It Fl g Ar component Ar dev
Get the component label for the specified component.
.It Fl I Ar serial_number Ar dev
Initialize the component labels on each component of the device.
.Ar serial_number
is used as one of the keys in determining whether a
particular set of components belong to the same RAID set.
While not strictly enforced, different serial numbers should be used for
different RAID sets.
This step
.Em MUST
be performed when a new RAID set is created.
.It Fl i Ar dev
Initialize the RAID device.
In particular, (re-write) the parity on the selected device.
This
.Em MUST
be done for
.Em all
RAID sets before the RAID device is labeled and before
file systems are created on the RAID device.
.It Fl P Ar dev
Check the status of the parity on the RAID set, and initialize
(re-write) the parity if the parity is not known to be up-to-date.
This is normally used after a system crash (and before a
.Xr fsck 8 )
to ensure the integrity of the parity.
.It Fl p Ar dev
Check the status of the parity on the RAID set.
Displays a status message, and returns successfully if the parity
is up-to-date.
.It Fl R Ar component Ar dev
Fails the specified
.Ar component ,
if necessary, and immediately begins a reconstruction back to
.Ar component .
This is useful for reconstructing back onto a component after
it has been replaced following a failure.
.It Fl r Ar component Ar dev
Remove the spare disk specified by
.Ar component
from the set of available spare components.
.It Fl S Ar dev
Check the status of parity re-writing, component reconstruction, and
component copyback.
The output indicates the amount of progress achieved in each of these areas.
.It Fl s Ar dev
Display the status of the RAIDframe device for each of the components
and spares.
.It Fl u Ar dev
Unconfigure the RAIDframe device.
.It Fl v
Be more verbose.
For operations such as reconstructions, parity re-writing,
and copybacks, provide a progress indicator.
.El
.Ss Configuration file
The format of the configuration file is complex, and
only an abbreviated treatment is given here.
In the configuration files, a
.Sq #
indicates the beginning of a comment.
.Pp
There are 4 required sections of a configuration file, and 2
optional sections.
Each section begins with a
.Sq START ,
followed by
the section name, and the configuration parameters associated with that
section.
The first section is the
.Sq array
section, and it specifies
the number of rows, columns, and spare disks in the RAID set.
For example:
.Bd -unfilled -offset indent
START array
1 3 0
.Ed
.Pp
indicates an array with 1 row, 3 columns, and 0 spare disks.
Note that although multi-dimensional arrays may be specified, they are
.Em NOT
supported in the driver.
.Pp
The second section, the
.Sq disks
section, specifies the actual
components of the device.
For example:
.Bd -unfilled -offset indent
START disks
/dev/sd0e
/dev/sd1e
/dev/sd2e
.Ed
.Pp
specifies the three component disks to be used in the RAID device.
If any of the specified drives cannot be found when the RAID device is
configured, then they will be marked as
.Sq failed ,
and the system will
operate in degraded mode.
Note that it is
.Em imperative
that the order of the components in the configuration file does not
change between configurations of a RAID device.
Changing the order of the components will result in data loss if the set
is configured with the
.Fl C
option.
In normal circumstances, the RAID set will not configure if only
.Fl c
is specified, and the components are out-of-order.
.Pp
The next section, which is the
.Sq spare
section, is optional, and, if
present, specifies the devices to be used as
.Sq hot spares
-- devices
which are on-line, but are not actively used by the RAID driver unless
one of the main components fail.
A simple
.Sq spare
section might be:
.Bd -unfilled -offset indent
START spare
/dev/sd3e
.Ed
.Pp
for a configuration with a single spare component.
If no spare drives are to be used in the configuration, then the
.Sq spare
section may be omitted.
.Pp
The next section is the
.Sq layout
section.
This section describes the general layout parameters for the RAID device,
and provides such information as sectors per stripe unit,
stripe units per parity unit, stripe units per reconstruction unit,
and the parity configuration to use.
This section might look like:
.Bd -unfilled -offset indent
START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level
32 1 1 5
.Ed
.Pp
The sectors per stripe unit specifies, in blocks, the interleave
factor; i.e. the number of contiguous sectors to be written to each
component for a single stripe.
Appropriate selection of this value (32 in this example) is the subject
of much research in RAID architectures.
The stripe units per parity unit and stripe units per reconstruction unit
are normally each set to 1.
While certain values above 1 are permitted, a discussion of valid
values and the consequences of using anything other than 1 are outside
the scope of this document.
The last value in this section (5 in this example) indicates the
parity configuration desired.
Valid entries include:
.Bl -tag -width inde
.It 0
RAID level 0.
No parity, only simple striping.
.It 1
RAID level 1.
Mirroring.
The parity is the mirror.
.It 4
RAID level 4.
Striping across components, with parity stored on the last component.
.It 5
RAID level 5.
Striping across components, parity distributed across all components.
.El
.Pp
There are other valid entries here, including those for Even-Odd
parity, RAID level 5 with rotated sparing, Chained declustering,
and Interleaved declustering, but as of this writing the code for
those parity operations has not been tested with
.Ox .
.Pp
The next required section is the
.Sq queue
section.
This is most often specified as:
.Bd -unfilled -offset indent
START queue
fifo 100
.Ed
.Pp
where the queuing method is specified as FIFO (First-In, First-Out),
and the size of the per-component queue is limited to 100 requests.
Other queuing methods may also be specified, but a discussion of them
is beyond the scope of this document.
.Pp
The final section, the
.Sq debug
section, is optional.
For more details on this the reader is referred to the RAIDframe
documentation discussed in the
.Sx HISTORY
section.
See
.Sx EXAMPLES
for a more complete configuration file example.
.Sh EXAMPLES
It is highly recommended that before using the RAID driver for real
file systems that the system administrator(s) become quite familiar
with the use of
.Nm raidctl ,
and that they understand how the component reconstruction process
works.
The examples in this section will focus on configuring a
number of different RAID sets of varying degrees of redundancy.
By working through these examples, administrators should be able to
develop a good feel for how to configure a RAID set, and how to
initiate reconstruction of failed components.
.Pp
In the following examples
.Sq raid0
will be used to denote the RAID device.
.Sq Pa /dev/rraid0c
may be used in place of
.Sq raid0 .
.Ss Initialization and Configuration
The initial step in configuring a RAID set is to identify the components
that will be used in the RAID set.
All components should be the same size.
Each component should have a disklabel type of
.Dv FS_RAID ,
and a typical disklabel entry for a RAID component might look like:
.Bd -unfilled -offset indent
f:  1800000  200495     RAID              # (Cyl.  405*- 4041*)
.Ed
.Pp
While
.Dv FS_BSDFFS
(e.g. 4.2BSD) will also work as the component type, the type
.Dv FS_RAID
(e.g. RAID) is preferred for RAIDframe use, as it is required for
features such as auto-configuration.
As part of the initial configuration of each RAID set, each component
will be given a
.Sq component label .
A
.Sq component label
contains important information about the component, including a
user-specified serial number, the row and column of that component in
the RAID set, the redundancy level of the RAID set, a 'modification
counter', and whether the parity information (if any) on that
component is known to be correct.
Component labels are an integral part of the RAID set, since they are used
to ensure that components are configured in the correct order, and used
to keep track of other vital information about the RAID set.
Component labels are also required for the auto-detection and
auto-configuration of RAID sets at boot time.
For a component label to be considered valid, that particular component label
must be in agreement with the other component labels in the set.
For example, the serial number,
.Sq modification counter ,
number of rows and number of columns must all
be in agreement.
If any of these are different, then the component is not considered to be
part of the set.
See
.Xr raid 4
for more information about component labels.
.Pp
Once the components have been identified, and the disks have
appropriate labels,
.Nm
is then used to configure the
.Xr raid 4
device.
To configure the device, a configuration file which looks something like:
.Bd -unfilled -offset indent
START array
# numRow numCol numSpare
1 3 1

START disks
/dev/sd1e
/dev/sd2e
/dev/sd3e

START spare
/dev/sd4e

START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level_5
32 1 1 5

START queue
fifo 100
.Ed
.Pp
is created in a file.
The above configuration file specifies a RAID 5 set consisting of
the components
.Pa /dev/sd1e , /dev/sd2e ,
and
.Pa /dev/sd3e ,
with
.Pa /dev/sd4e
available as a
.Sq hot spare
in case one of
the three main drives should fail.
A RAID 0 set would be specified in a similar way:
.Bd -unfilled -offset indent
START array
# numRow numCol numSpare
1 4 0

START disks
/dev/sd10e
/dev/sd11e
/dev/sd12e
/dev/sd13e

START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level_0
64 1 1 0

START queue
fifo 100
.Ed
.Pp
In this case, devices
.Pa /dev/sd10e , /dev/sd11e , /dev/sd12e ,
and
.Pa /dev/sd13e
are the components that make up this RAID set.
Note that there are no hot spares for a RAID 0 set, since there is no way
to recover data if any of the components fail.
.Pp
For a RAID 1 (mirror) set, the following configuration might be used:
.Bd -unfilled -offset indent
START array
# numRow numCol numSpare
1 2 0

START disks
/dev/sd20e
/dev/sd21e

START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level_1
128 1 1 1

START queue
fifo 100
.Ed
.Pp
In this case,
.Pa /dev/sd20e
and
.Pa /dev/sd21e
are the two components of the
mirror set.
While no hot spares have been specified in this configuration,
they easily could be, just as they were specified in the RAID 5 case above.
Note as well that RAID 1 sets are currently limited to only 2 components.
At present, n-way mirroring is not possible.
.Pp
The first time a RAID set is configured, the
.Fl C
option must be used:
.Bd -unfilled -offset indent
# raidctl -C raid0.conf raid0
.Ed
.Pp
where
.Sq raid0.conf
is the name of the RAID configuration file.
The
.Fl C
forces the configuration to succeed, even if any of the component
labels are incorrect.
The
.Fl C
option should not be used lightly in
situations other than initial configurations, as if
the system is refusing to configure a RAID set, there is probably a
very good reason for it.
After the initial configuration is done (and appropriate component labels
are added with the
.Fl I
option) then raid0 can be configured normally with:
.Bd -unfilled -offset indent
# raidctl -c raid0.conf raid0
.Ed
.Pp
When the RAID set is configured for the first time, it is
necessary to initialize the component labels, and to initialize the
parity on the RAID set.
Initializing the component labels is done with:
.Bd -unfilled -offset indent
# raidctl -I 112341 raid0
.Ed
.Pp
where
.Sq 112341
is a user-specified serial number for the RAID set.
This initialization step is
.Em required
for all RAID sets.
Also, using different serial numbers between RAID sets is
.Em strongly encouraged ,
as using the same serial number for all RAID sets will only serve to
decrease the usefulness of the component label checking.
.Pp
Initializing the RAID set is done via the
.Fl i
option.
This initialization
.Em MUST
be done for
.Em all
RAID sets, since among other things it verifies that the parity (if
any) on the RAID set is correct.
Since this initialization may be quite time-consuming, the
.Fl v
option may be also used in conjunction with
.Fl i :
.Bd -unfilled -offset indent
# raidctl -iv raid0
.Ed
.Pp
This will give more verbose output on the
status of the initialization:
.Bd -unfilled -offset indent
Initiating re-write of parity
Parity Re-write status:
 10% |****                                   | ETA:    06:03 /
.Ed
.Pp
The output provides a
.Sq Percent Complete
in both a numeric and graphical format, as well as an estimated time
to completion of the operation.
.Pp
Since it is the parity that provides the
.Sq redundancy
part of RAID, it is critical that the parity is correct
as much as possible.
If the parity is not correct, then there is no guarantee that data will not
be lost if a component fails.
.Pp
Once the parity is known to be correct, it is then safe to perform
.Xr disklabel 8 ,
.Xr newfs 8 ,
or
.Xr fsck 8
on the device or its filesystems, and then to mount the filesystems
for use.
.Pp
Under certain circumstances (e.g. the additional component has not
arrived, or data is being migrated off of a disk destined to become a
component) it may be desirable to configure a RAID 1 set with only
a single component.
This can be achieved by configuring the set with a physically existing
component (as either the first or second component) and with a
.Sq fake
component.
In the following:
.Bd -unfilled -offset indent
START array
# numRow numCol numSpare
1 2 0

START disks
/dev/sd6e
/dev/sd0e

START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level_1
128 1 1 1

START queue
fifo 100
.Ed
.Pp
.Pa /dev/sd0e
is the real component, and will be the second disk of a RAID 1
set.
The component
.Pa /dev/sd6e ,
which must exist, but have no physical
device associated with it, is simply used as a placeholder.
Configuration (using
.Fl C
and
.Fl I Ar 12345
as above) proceeds normally, but initialization of the RAID set will
have to wait until all physical components are present.
After configuration, this set can be used normally, but will be operating
in degraded mode.
Once a second physical component is obtained, it can be hot-added,
the existing data mirrored, and normal operation resumed.
.Ss Maintenance of the RAID set
After the parity has been initialized for the first time, the command:
.Bd -unfilled -offset indent
# raidctl -p raid0
.Ed
.Pp
can be used to check the current status of the parity.
To check the parity and rebuild it necessary (for example, after an unclean
shutdown) the command:
.Bd -unfilled -offset indent
# raidctl -P raid0
.Ed
.Pp
is used.
Note that re-writing the parity can be done while other operations on the
RAID set are taking place (e.g. while doing an
.Xr fsck 8
on a file system on the RAID set).
However: for maximum effectiveness of the RAID set, the parity should be
known to be correct before any data on the set is modified.
.Pp
To see how the RAID set is doing, the following command can be used to
show the RAID set's status:
.Bd -unfilled -offset indent
# raidctl -s raid0
.Ed
.Pp
The output will look something like:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: optimal
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: spare
Parity status: clean
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.
.Ed
.Pp
This indicates that all is well with the RAID set.
Of importance here are the component lines which read
.Sq optimal ,
and the
.Sq Parity status
line which indicates that the parity is up-to-date.
Note that if there are file systems open on the RAID set,
the individual components will not be
.Sq clean
but the set as a whole can still be clean.
.Pp
The
.Fl v
option may be also used in conjunction with
.Fl s :
.Bd -unfilled -offset indent
# raidctl -sv raid0
.Ed
.Pp
In this case, the components' label information (see the
.Fl g
option) will be given as well:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: optimal
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: spare
Component label for /dev/sd1e:
   Row: 0 Column: 0 Num Rows: 1 Num Columns: 3
   Version: 2 Serial Number: 13432 Mod Counter: 65
   Clean: No Status: 0
   sectPerSU: 32 SUsPerPU: 1 SUsPerRU: 1
   RAID Level: 5  blocksize: 512 numBlocks: 1799936
   Autoconfig: No
   Last configured as: raid0
Component label for /dev/sd2e:
   Row: 0 Column: 1 Num Rows: 1 Num Columns: 3
   Version: 2 Serial Number: 13432 Mod Counter: 65
   Clean: No Status: 0
   sectPerSU: 32 SUsPerPU: 1 SUsPerRU: 1
   RAID Level: 5  blocksize: 512 numBlocks: 1799936
   Autoconfig: No
   Last configured as: raid0
Component label for /dev/sd3e:
   Row: 0 Column: 2 Num Rows: 1 Num Columns: 3
   Version: 2 Serial Number: 13432 Mod Counter: 65
   Clean: No Status: 0
   sectPerSU: 32 SUsPerPU: 1 SUsPerRU: 1
   RAID Level: 5  blocksize: 512 numBlocks: 1799936
   Autoconfig: No
   Last configured as: raid0
Parity status: clean
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.
.Ed
.Pp
To check the component label of /dev/sd1e, the following is used:
.Bd -unfilled -offset indent
# raidctl -g /dev/sd1e raid0
.Ed
.Pp
The output of this command will look something like:
.Bd -unfilled -offset indent
Component label for /dev/sd1e:
   Row: 0 Column: 0 Num Rows: 1 Num Columns: 3
   Version: 2 Serial Number: 13432 Mod Counter: 65
   Clean: No Status: 0
   sectPerSU: 32 SUsPerPU: 1 SUsPerRU: 1
   RAID Level: 5  blocksize: 512 numBlocks: 1799936
   Autoconfig: No
   Last configured as: raid0
.Ed
.Ss Dealing with Component Failures
If for some reason
(perhaps to test reconstruction) it is necessary to pretend a drive
has failed, the following will perform that function:
.Bd -unfilled -offset indent
# raidctl -f /dev/sd2e raid0
.Ed
.Pp
The system will then be performing all operations in degraded mode,
where missing data is re-computed from existing data and the parity.
In this case, obtaining the status of raid0 will return (in part):
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: failed
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: spare
.Ed
.Pp
Note that with the use of
.Fl f
a reconstruction has not been started.
To both fail the disk and start a reconstruction, the
.Fl F
option must be used:
.Bd -unfilled -offset indent
# raidctl -F /dev/sd2e raid0
.Ed
.Pp
The
.Fl f
option may be used first, and then the
.Fl F
option used later, on the same disk, if desired.
Immediately after the reconstruction is started, the status will report:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: reconstructing
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: used_spare
[...]
Parity status: clean
Reconstruction is 10% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.
.Ed
.Pp
This indicates that a reconstruction is in progress.
To find out how the reconstruction is progressing the
.Fl S
option may be used.
This will indicate the progress in terms of the percentage of the
reconstruction that is completed.
When the reconstruction is finished the
.Fl s
option will show:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: spared
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: used_spare
[...]
Parity status: clean
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.
.Ed
.Pp
At this point there are at least two options.
First, if
.Pa /dev/sd2e
is known to be good (i.e. the failure was either caused by
.Fl f
or
.Fl F ,
or the failed disk was replaced), then a copyback of the data can
be initiated with the
.Fl B
option.
In this example, this would copy the entire contents of
.Pa /dev/sd4e
to
.Pa /dev/sd2e .
Once the copyback procedure is complete, the
status of the device would be (in part):
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: optimal
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: spare
.Ed
.Pp
and the system is back to normal operation.
.Pp
The second option after the reconstruction is to simply use
.Pa /dev/sd4e
in place of
.Pa /dev/sd2e
in the configuration file.
For example, the configuration file (in part) might now look like:
.Bd -unfilled -offset indent
START array
1 3 0

START drives
/dev/sd1e
/dev/sd4e
/dev/sd3e
.Ed
.Pp
This can be done as
.Pa /dev/sd4e
is completely interchangeable with
.Pa /dev/sd2e
at this point.
Note that extreme care must be taken when changing the order of the drives
in a configuration.
This is one of the few instances where the devices and/or their orderings
can be changed without loss of data!
In general, the ordering of components in a configuration file should
.Em never
be changed.
.Pp
If a component fails and there are no hot spares
available on-line, the status of the RAID set might (in part) look like:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: failed
           /dev/sd3e: optimal
No spares.
.Ed
.Pp
In this case there are a number of options.
The first option is to add a hot spare using:
.Bd -unfilled -offset indent
# raidctl -a /dev/sd4e raid0
.Ed
.Pp
After the hot add, the status would then be:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: failed
           /dev/sd3e: optimal
Spares:
           /dev/sd4e: spare
.Ed
.Pp
Reconstruction could then take place using
.Fl F
as describe above.
.Pp
A second option is to rebuild directly onto
.Pa /dev/sd2e .
Once the disk containing
.Pa /dev/sd2e
has been replaced, one can simply use:
.Bd -unfilled -offset indent
# raidctl -R /dev/sd2e raid0
.Ed
.Pp
to rebuild the
.Pa /dev/sd2e
component.
As the rebuilding is in progress, the status will be:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: reconstructing
           /dev/sd3e: optimal
No spares.
.Ed
.Pp
and when completed, will be:
.Bd -unfilled -offset indent
Components:
           /dev/sd1e: optimal
           /dev/sd2e: optimal
           /dev/sd3e: optimal
No spares.
.Ed
.Pp
In circumstances where a particular component is completely
unavailable after a reboot, a special component name will be used to
indicate the missing component.
For example:
.Bd -unfilled -offset indent
Components:
           /dev/sd2e: optimal
          component1: failed
No spares.
.Ed
.Pp
indicates that the second component of this RAID set was not detected
at all by the auto-configuration code.
The name
.Sq component1
can be used anywhere a normal component name would be used.
For example, to add a hot spare to the above set, and rebuild to that hot
spare, the following could be done:
.Bd -unfilled -offset indent
# raidctl -a /dev/sd3e raid0
# raidctl -F component1 raid0
.Ed
.Pp
at which point the data missing from
.Sq component1
would be reconstructed onto
.Pa /dev/sd3e .
.Ss RAID on RAID
RAID sets can be layered to create more complex and much larger RAID
sets.
A RAID 0 set, for example, could be constructed from four RAID 5 sets.
The following configuration file shows such a setup:
.Bd -unfilled -offset indent
START array
# numRow numCol numSpare
1 4 0

START disks
/dev/raid1e
/dev/raid2e
/dev/raid3e
/dev/raid4e

START layout
# sectPerSU SUsPerParityUnit SUsPerReconUnit RAID_level_0
128 1 1 0

START queue
fifo 100
.Ed
.Pp
A similar configuration file might be used for a RAID 0 set
constructed from components on RAID 1 sets.
In such a configuration, the mirroring provides a high degree of redundancy,
while the striping provides additional speed benefits.
.Ss Auto-configuration and Root on RAID
RAID sets can also be auto-configured at boot.
To make a set auto-configurable, simply prepare the RAID set as above,
and then do a:
.Pp
.Dl # raidctl -A yes raid0
.Pp
to turn on auto-configuration for that set.
To turn off auto-configuration, use:
.Pp
.Dl # raidctl -A no raid0
.Pp
RAID sets which are auto-configurable will be configured before the
root file system is mounted.
These RAID sets are thus available for use as a root file system,
or for any other file system.
A primary advantage of using the auto-configuration is that RAID components
become more independent of the disks they reside on.
For example, SCSI ID's can change, but auto-configured sets will always be
configured correctly, even if the SCSI ID's of the component disks
have become scrambled.
.Pp
Having a system's root file system
.Pq Pa /
on a RAID set is also allowed,
with the
.Sq a
partition of such a RAID set being used for
.Pa / .
To use raid0a as the root file system, simply use:
.Bd -unfilled -offset indent
# raidctl -A root raid0
.Ed
.Pp
To return raid0 to be just an auto-configuring set simply use the
.Fl A Ar yes
arguments.
.Pp
.\" Note that kernels can only be directly read from RAID 1 components on
.\" alpha and pmax architectures.
.\" On those architectures, the
.\" .Dv FS_RAID
.\" file system is recognized by the bootblocks, and will properly load the
.\" kernel directly from a RAID 1 component.
.\" For other architectures, or
Note that kernels can't be directly read from a RAID component.
To support the root file system on RAID sets, some mechanism must be
used to get a kernel booting.
For example, a small partition containing only the secondary boot-blocks
and an alternate kernel (or two) could be used.
Once a kernel is booting however, and an auto-configured RAID
set is found that is eligible to be root, then that RAID set will be
auto-configured and its
.Sq a
partition (aka raid[0..n]a) will be used as the root file system.
If two or more RAID sets claim to be root devices, then the user will be
prompted to select the root device.
At this time, RAID 0, 1, 4, and 5 sets are all supported as root devices.
.Pp
A typical RAID 1 setup with root on RAID might be as follows:
.Bl -enum
.It
wd0a - a small partition, which contains a complete, bootable, basic
.Ox
installation.
.It
wd1a - also contains a complete, bootable, basic
.Ox
installation.
.It
wd0e and wd1e - a RAID 1 set, raid0, used for the root file system.
.It
wd0f and wd1f - a RAID 1 set, raid1, which will be used only for
swap space.
.It
wd0g and wd1g - a RAID 1 set, raid2, used for
.Pa /usr ,
.Pa /home ,
or other data, if desired.
.It
wd0h and wd1h - a RAID 1 set, raid3, if desired.
.El
.Pp
RAID sets raid0, raid1, and raid2 are all marked as
auto-configurable.
raid0 is marked as being a root-able raid.
When new kernels are installed, the kernel is not only copied to
.Pa / ,
but also to wd0a and wd1a.
The kernel on wd0a is required, since that is the kernel the system
boots from.
The kernel on wd1a is also required, since that will be the kernel used
should wd0 fail.
The important point here is to have redundant copies of the kernel
available, in the event that one of the drives fail.
.Pp
There is no requirement that the root file system be on the same disk
as the kernel.
For example, obtaining the kernel from wd0a, and using
sd0e and sd1e for raid0, and the root file system, is fine.
It
.Em is
critical, however, that there be multiple kernels available, in the
event of media failure.
.Pp
Multi-layered RAID devices (such as a RAID 0 set made
up of RAID 1 sets) are
.Em not
supported as root devices or auto-configurable devices at this point.
(Multi-layered RAID devices
.Em are
supported in general, however, as mentioned earlier.)
Note that in
order to enable component auto-detection and auto-configuration of
RAID devices, the line:
.Bd -unfilled -offset indent
option	RAID_AUTOCONFIG
.Ed
.Pp
must be in the kernel configuration file.
See
.Xr raid 4
for more details.
.Ss Unconfiguration
The final operation performed by
.Nm
is to unconfigure a
.Xr raid 4
device.
This is accomplished via a simple:
.Pp
.Dl # raidctl -u raid0
.Pp
at which point the device is ready to be reconfigured.
.Ss Performance Tuning
Selection of the various parameter values which result in the best
performance can be quite tricky, and often requires a bit of
trial-and-error to get those values most appropriate for a given system.
A whole range of factors come into play, including:
.Bl -enum
.It
Types of components (e.g. SCSI vs. IDE) and their bandwidth
.It
Types of controller cards and their bandwidth
.It
Distribution of components among controllers
.It
I/O bandwidth
.It
File system access patterns
.It
CPU speed
.El
.Pp
As with most performance tuning, benchmarking under real-life loads
may be the only way to measure expected performance.
Understanding some of the underlying technology is also useful in tuning.
The goal of this section is to provide pointers to those parameters which may
make significant differences in performance.
.Pp
For a RAID 1 set, a SectPerSU value of 64 or 128 is typically sufficient.
Since data in a RAID 1 set is arranged in a linear
fashion on each component, selecting an appropriate stripe size is
somewhat less critical than it is for a RAID 5 set.
However: a stripe size that is too small will cause large I/Os to be
broken up into a number of smaller ones, hurting performance.
At the same time, a large stripe size may cause problems with concurrent
accesses to stripes, which may also affect performance.
Thus values in the range of 32 to 128 are often the most effective.
.Pp
Tuning RAID 5 sets is trickier.
In the best case, I/O is presented to the RAID set one stripe at a time.
Since the entire stripe is available at the beginning of the I/O,
the parity of that stripe can be calculated before the stripe is written,
and then the stripe data and parity can be written in parallel.
When the amount of data being written is less than a full stripe worth, the
.Sq small write
problem occurs.
Since a
.Sq small write
means only a portion of the stripe on the components is going to
change, the data (and parity) on the components must be updated
slightly differently.
First, the
.Sq old parity
and
.Sq old data
must be read from the components.
Then the new parity is constructed, using the new data to be written,
and the old data and old parity.
Finally, the new data and new parity are written.
All this extra data shuffling results in a serious loss of performance,
and is typically 2 to 4 times slower than a full stripe write (or read).
To combat this problem in the real world, it may be useful to ensure that
stripe sizes are small enough that a
.Sq large I/O
from the system will use exactly one large stripe write.
As is seen later, there are some file system dependencies which may come
into play here as well.
.Pp
Since the size of a
.Sq large I/O
is often (currently) only 32K or 64K, on a 5-drive RAID 5 set it may
be desirable to select a SectPerSU value of 16 blocks (8K) or 32
blocks (16K).
Since there are 4 data sectors per stripe, the maximum
data per stripe is 64 blocks (32K) or 128 blocks (64K).
Again, empirical measurement will provide the best indicators of which
values will yield better performance.
.Pp
The parameters used for the file system are also critical to good
performance.
For
.Xr newfs 8 ,
for example, increasing the block size to 32K or 64K may improve
performance dramatically.
Also, changing the cylinders-per-group parameter from 16 to 32 or higher
is often not only necessary for larger file systems, but may also have
positive performance implications.
.Ss Summary
Despite the length of this man-page, configuring a RAID set is a
relatively straight-forward process.
All that needs to be done is the following steps:
.Bl -enum
.It
Use
.Xr disklabel 8
to create the components (of type RAID).
.It
Construct a RAID configuration file: e.g.\&
.Sq raid0.conf
.It
Configure the RAID set with:
.Bd -unfilled -offset indent
# raidctl -C raid0.conf raid0
.Ed
.Pp
.It
Initialize the component labels with:
.Bd -unfilled -offset indent
# raidctl -I 123456 raid0
.Ed
.Pp
.It
Initialize other important parts of the set with:
.Bd -unfilled -offset indent
# raidctl -i raid0
.Ed
.Pp
.It
Get the default label for the RAID set:
.Bd -unfilled -offset indent
# disklabel raid0 > /tmp/label
.Ed
.Pp
.It
Edit the label:
.Bd -unfilled -offset indent
# vi /tmp/label
.Ed
.Pp
.It
Put the new label on the RAID set:
.Bd -unfilled -offset indent
# disklabel -R raid0 /tmp/label
.Ed
.Pp
.It
Create the file system:
.Bd -unfilled -offset indent
# newfs /dev/rraid0e
.Ed
.Pp
.It
Mount the file system:
.Bd -unfilled -offset indent
# mount /dev/raid0e /mnt
.Ed
.Pp
.It
Use:
.Bd -unfilled -offset indent
# raidctl -c raid0.conf raid0
.Ed
.Pp
to re-configure the RAID set the next time it is needed, or put
raid0.conf into
.Pa /etc
where it will automatically be started by the
.Pa /etc/rc
scripts.
.El
.Sh FILES
.Bl -tag -width "/etc/raid[0-3].conf" -compact
.It Pa /dev/{,r}raid*
.Cm raid
device special files.
.It Pa /etc/raid[0-3].conf
Files used by
.Xr rc 8
to configure arrays at startup.
.El
.Sh SEE ALSO
.Xr raid 4 ,
.Xr rc 8
.Sh HISTORY
RAIDframe is a framework for rapid prototyping of RAID structures
developed by the folks at the Parallel Data Laboratory at Carnegie
Mellon University (CMU).
A more complete description of the internals and functionality of
RAIDframe is found in the paper "RAIDframe: A Rapid Prototyping Tool
for RAID Systems", by William V. Courtright II, Garth Gibson, Mark
Holland, LeAnn Neal Reilly, and Jim Zelenka, and published by the
Parallel Data Laboratory of Carnegie Mellon University.
.Pp
The
.Nm
command first appeared as a program in CMU's RAIDframe v1.1 distribution.
This version of
.Nm
is a complete re-write, and first appeared in
.Nx 1.4
from where it was ported to
.Ox 2.5 .
.Sh CAVEATS
Certain RAID levels (1, 4, 5, 6, and others) can protect against some
data loss due to component failure.
However the loss of two components of a RAID 4 or 5 system, or the loss
of a single component of a RAID 0 system will result in the entire
filesystem being lost.
RAID is
.Em NOT
a substitute for good backup practices.
.Pp
Recomputation of parity
.Em MUST
be performed whenever there is a chance that it may have been
compromised.
This includes after system crashes, or before a RAID
device has been used for the first time.
Failure to keep parity correct will be catastrophic should a component
ever fail -- it is better to use RAID 0 and get the additional space
and speed, than it is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
.Sh BUGS
Hot-spare removal is currently not available.
@


1.43
log
@no more ccd(4);
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.42 2010/04/01 17:06:55 jmc Exp $
d57 1
a57 1
.Dd $Mdocdate: April 1 2010 $
@


1.42
log
@WARNINGS -> CAVEATS, and a little neccessary cleanup;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.41 2008/06/26 05:42:06 ray Exp $
d57 1
a57 1
.Dd $Mdocdate: June 26 2008 $
a1347 1
.Xr ccd 4 ,
@


1.41
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.40 2008/06/03 19:51:02 jmc Exp $
d57 1
a57 1
.Dd $Mdocdate: June 3 2008 $
a1336 20
.Sh WARNINGS
Certain RAID levels (1, 4, 5, 6, and others) can protect against some
data loss due to component failure.
However the loss of two components of a RAID 4 or 5 system, or the loss
of a single component of a RAID 0 system will result in the entire
filesystem being lost.
RAID is
.Em NOT
a substitute for good backup practices.
.Pp
Recomputation of parity
.Em MUST
be performed whenever there is a chance that it may have been
compromised.
This includes after system crashes, or before a RAID
device has been used for the first time.
Failure to keep parity correct will be catastrophic should a component
ever fail -- it is better to use RAID 0 and get the additional space
and speed, than it is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
d1370 20
@


1.40
log
@fix some spacing issues;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.39 2008/03/23 23:28:46 krw Exp $
a17 7
.\" 3. All advertising materials mentioning features or use of this software
.\"    must display the following acknowledgement:
.\"        This product includes software developed by the NetBSD
.\"        Foundation, Inc. and its contributors.
.\" 4. Neither the name of The NetBSD Foundation nor the names of its
.\"    contributors may be used to endorse or promote products derived
.\"    from this software without specific prior written permission.
d57 1
a57 1
.Dd $Mdocdate: March 23 2008 $
@


1.39
log
@Remove two stray references to the departed -r argument to disklabel.

Pointed out by jmc@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.38 2008/01/26 23:07:55 jmc Exp $
d64 1
a64 1
.Dd $Mdocdate: January 26 2008 $
d1163 2
a1164 1
supported in general, however, as mentioned earlier.)  Note that in
@


1.38
log
@the kids want I/O;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.37 2007/05/31 19:19:47 jmc Exp $
d64 1
a64 1
.Dd $Mdocdate: May 31 2007 $
d1315 1
a1315 1
# disklabel -R -r raid0 /tmp/label
@


1.37
log
@convert to new .Dd format;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.36 2007/02/21 16:33:09 jmc Exp $
d64 1
a64 1
.Dd $Mdocdate$
d1198 1
a1198 1
IO bandwidth
d1215 1
a1215 1
However: a stripe size that is too small will cause large IO's to be
d1222 2
a1223 2
In the best case, IO is presented to the RAID set one stripe at a time.
Since the entire stripe is available at the beginning of the IO,
d1246 1
a1246 1
.Sq large IO
d1252 1
a1252 1
.Sq large IO
@


1.36
log
@sort options; sync synopsis and usage()
from Igor Sobrado
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.35 2005/08/27 06:43:42 jmc Exp $
d64 1
a64 1
.Dd July 10, 2001
@


1.35
log
@no need to display the copyright in the formatted page;
ok deraadt@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.34 2005/08/24 22:47:28 jmc Exp $
d73 1
a73 4
.Op Fl v
.Op Fl afFgrR Ar component
.Op Fl BGipPsSu
.Op Fl cC Ar config_file
d75 1
d77 1
a123 5
.It Fl a Ar component Ar dev
Add
.Ar component
as a hot spare for the device
.Ar dev .
d145 5
d155 5
a167 10
.It Fl C Ar config_file Ar dev
As for
.Fl c ,
but forces the configuration to take place.
This is required the first time a RAID set is configured.
.It Fl f Ar component Ar dev
This marks the specified
.Ar component
as having failed, but does not initiate a reconstruction of that
component.
d175 5
a179 2
.It Fl g Ar component Ar dev
Get the component label for the specified component.
d187 2
a188 9
.It Fl i Ar dev
Initialize the RAID device.
In particular, (re-write) the parity on the selected device.
This
.Em MUST
be done for
.Em all
RAID sets before the RAID device is labeled and before
file systems are created on the RAID device.
d199 9
a207 4
.It Fl p Ar dev
Check the status of the parity on the RAID set.
Displays a status message, and returns successfully if the parity
is up-to-date.
d214 4
a217 4
.It Fl r Ar component Ar dev
Remove the spare disk specified by
.Ar component
from the set of available spare components.
d225 4
a228 3
.It Fl s Ar dev
Display the status of the RAIDframe device for each of the components
and spares.
d233 3
@


1.34
log
@document /etc/raid[0-3].conf in FILES;
from rohee@@, but adjusted by me;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.33 2005/03/12 12:21:08 jmc Exp $
a1398 28
.Sh COPYRIGHT
.Bd -unfilled
The RAIDframe Copyright is as follows:

Copyright (c) 1994-1996 Carnegie-Mellon University.
All rights reserved.

Permission to use, copy, modify and distribute this software and
its documentation is hereby granted, provided that both the copyright
notice and this permission notice appear in all copies of the
software, derivative works or modified versions, and any portions
thereof, and that both notices appear in supporting documentation.

CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.

Carnegie Mellon requests users of this software to return to

 Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 School of Computer Science
 Carnegie Mellon University
 Pittsburgh PA 15213-3890

any improvements or extensions that they make and grant Carnegie the
rights to redistribute these changes.

.Ed
@


1.33
log
@escape `.' at eol to avoid double spacing issue;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.32 2003/12/04 11:43:27 henning Exp $
d1365 1
a1365 1
.Bl -tag -width /dev/XXrXraidX -compact
d1369 4
@


1.32
log
@no, you cannot form a raid from wd0h and wd0h.
found by Joel Knight
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.31 2003/09/18 09:21:49 jmc Exp $
d100 1
a100 1
may be either the full name of the device, e.g.
d1281 1
a1281 1
Construct a RAID configuration file: e.g.
@


1.31
log
@add .Bk/.Ek to SYNOPSIS;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.30 2003/06/03 13:16:09 jmc Exp $
d1133 1
a1133 1
wd0h and wd0h - a RAID 1 set, raid3, if desired.
@


1.30
log
@- section reorder
- some mdoc fixes
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.29 2003/04/02 19:00:26 jmc Exp $
d72 1
d80 1
@


1.29
log
@fifo -> FIFO

ok millert@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.28 2003/02/13 08:23:40 jmc Exp $
d86 2
a87 1
devices.  For more information about the RAIDframe disk device, see
d103 2
a104 3
For several commands (
.Fl BGipPsSu
),
d129 3
a131 3
Make the RAID set auto-configurable.  The RAID set will be
automatically configured at boot
.Ar before
d133 2
a134 2
mounted.  Note that all components of the set must be of type RAID in the
disklabel.
d139 8
a146 7
eligible to contain the root partition.  A RAID set configured this way
will
.Ar override
the use of the boot disk as the root device.  All components of the
set must be of type RAID in the disklabel.  Note that the kernel being
booted must currently reside on a non-RAID set and, in order to have the root
file system correctly mounted from it, the RAID set must have its
d151 2
a152 1
its original disk.  This is performed after a component has failed,
d165 2
a166 2
but forces the configuration to take place.  This is required the
first time a RAID set is configured.
d176 3
a178 2
disk onto an available hot spare.  This is one of the mechanisms used to start
the reconstruction process if a component does have a hardware failure.
d189 4
a192 3
Initialize the RAID device.  In particular, (re-write) the parity on
the selected device.  This
.Ar MUST
d194 1
a194 1
.Ar all
d201 5
a205 4
particular set of components belong to the same RAID set.  While not
strictly enforced, different serial numbers should be used for
different RAID sets.  This step
.Ar MUST
d208 3
a210 2
Check the status of the parity on the RAID set.  Displays a status
message, and returns successfully if the parity is up-to-date.
d233 2
a234 2
component copyback.  The output indicates the amount of progress
achieved in each of these areas.
d238 3
a240 2
Be more verbose.  For operations such as reconstructions, parity
re-writing, and copybacks, provide a progress indicator.
a241 1
.Pp
d244 2
a245 2
only an abbreviated treatment is given here.  In the configuration
files, a
d250 2
a251 1
optional sections.  Each section begins with a
d255 2
a256 1
section.  The first section is the
d259 2
a260 2
the number of rows, columns, and spare disks in the RAID set.  For
example:
d266 3
a268 3
indicates an array with 1 row, 3 columns, and 0 spare disks.  Note
that although multi-dimensional arrays may be specified, they are
.Ar NOT
d274 2
a275 1
components of the device.  For example:
d283 2
a284 2
specifies the three component disks to be used in the RAID device.  If
any of the specified drives cannot be found when the RAID device is
d288 3
a290 2
operate in degraded mode.  Note that it is
.Ar imperative
d292 3
a294 3
change between configurations of a RAID device.  Changing the order
of the components will result in data loss if the set is configured
with the
d296 2
a297 2
option.  In normal circumstances, the RAID set will not configure if
only
d308 2
a309 1
one of the main components fail.  A simple
d317 2
a318 2
for a configuration with a single spare component.  If no spare drives
are to be used in the configuration, then the
d324 6
a329 5
section.  This section describes the
general layout parameters for the RAID device, and provides such
information as sectors per stripe unit, stripe units per parity unit,
stripe units per reconstruction unit, and the parity configuration to
use.  This section might look like:
d338 5
a342 4
component for a single stripe.  Appropriate selection of this value
(32 in this example) is the subject of much research in RAID
architectures.  The stripe units per parity unit and
stripe units per reconstruction unit are normally each set to 1.
d345 4
a348 3
the scope of this document.  The last value in this section (5 in this
example) indicates the parity configuration desired.  Valid entries
include:
d351 2
a352 1
RAID level 0.  No parity, only simple striping.
d354 3
a356 1
RAID level 1.  Mirroring.  The parity is the mirror.
d358 2
a359 2
RAID level 4.  Striping across components, with parity stored on the
last component.
d361 2
a362 2
RAID level 5.  Striping across components, parity distributed across
all components.
d373 2
a374 2
section.  This is most often
specified as:
d387 3
a389 3
section, is optional.  For more details
on this the reader is referred to the RAIDframe documentation
discussed in the
a391 1

a394 1

a395 1

d401 2
a402 1
works.  The examples in this section will focus on configuring a
a413 1
.Pp
d416 3
a418 2
that will be used in the RAID set.  All components should be the same
size.  Each component should have a disklabel type of
d420 1
a420 2
and a typical disklabel entry for a RAID component
might look like:
d430 3
a432 2
features such as auto-configuration.  As part of the initial
configuration of each RAID set, each component will be given a
d440 9
a448 8
component is known to be correct.  Component labels are an integral
part of the RAID set, since they are used to ensure that components
are configured in the correct order, and used to keep track of other
vital information about the RAID set.  Component labels are also
required for the auto-detection and auto-configuration of RAID sets at
boot time.  For a component label to be considered valid, that
particular component label must be in agreement with the other
component labels in the set.  For example, the serial number,
d451 4
a454 2
be in agreement.  If any of these are different, then the component is
not considered to be part of the set.  See
d463 2
a464 2
device.  To configure the device, a configuration
file which looks something like:
d486 3
a488 2
is created in a file.  The above configuration file specifies a RAID 5
set consisting of the components
d497 2
a498 2
the three main drives should fail. A RAID 0 set would be specified in
a similar way:
d522 3
a524 3
are the components that make up this RAID set.  Note that there are no
hot spares for a RAID 0 set, since there is no way to recover data if
any of the components fail.
d549 5
a553 5
mirror set.  While no hot spares have been specified in this
configuration, they easily could be, just as they were specified in
the RAID 5 case above.  Note as well that RAID 1 sets are currently
limited to only 2 components.  At present, n-way mirroring is not
possible.
d564 2
a565 1
is the name of the RAID configuration file.  The
d568 2
a569 1
labels are incorrect.  The
d574 3
a576 2
very good reason for it.  After the initial configuration is done (and
appropriate component labels are added with the
d585 2
a586 1
parity on the RAID set.  Initializing the component labels is done with:
d593 6
a598 6
is a user-specified serial number for the RAID set.  This
initialization step is
.Ar required
for all RAID sets.  Also, using different
serial numbers between RAID sets is
.Ar strongly encouraged ,
d604 3
a606 2
option.  This initialization
.Ar MUST
d608 1
a608 1
.Ar all
d610 2
a611 2
any) on the RAID set is correct.  Since this initialization may be
quite time-consuming, the
d635 3
a637 2
as much as possible.  If the parity is not correct, then there is no
guarantee that data will not be lost if a component fails.
d639 1
a639 2
Once the parity is known to be correct,
it is then safe to perform
d650 3
a652 3
a single component.  This can be achieved by configuring the set with
a physically existing component (as either the first or second
component) and with a
d654 2
a655 1
component.  In the following:
d675 2
a676 1
set.  The component
d685 5
a689 6
have to wait until all physical components are present.  After
configuration, this set can be used normally, but will be operating
in degraded mode.  Once a second physical component is obtained, it
can be hot-added, the existing data mirrored, and normal operation
resumed.
.Pp
d696 2
a697 2
can be used to check the current status of the parity.  To check the
parity and rebuild it necessary (for example, after an unclean
d703 3
a705 2
is used.  Note that re-writing the parity can be done while
other operations on the RAID set are taking place (e.g. while doing a
d707 3
a709 3
on a file system on the RAID set).  However: for maximum effectiveness
of the RAID set, the parity should be known to be correct before any
data on the set is modified.
d731 2
a732 2
This indicates that all is well with the RAID set.  Of importance here
are the component lines which read
d736 3
a738 3
line which indicates that the parity is up-to-date.  Note that if
there are file systems open on the RAID set, the individual components
will not be
a805 1
.Pp
d828 2
a829 2
a reconstruction has not been started.  To both fail the disk and
start a reconstruction, the
d856 2
a857 2
This indicates that a reconstruction is in progress.  To find out how
the reconstruction is progressing the
d859 4
a862 3
option may be used.  This will indicate the progress in terms of the
percentage of the reconstruction that is completed.  When the
reconstruction is finished the
d879 2
a880 1
At this point there are at least two options.  First, if
d889 2
a890 1
option.  In this example, this would copy the entire contents of
d911 2
a912 2
in the configuration file.  For example, the
configuration file (in part) might now look like:
d927 7
a933 6
at this point.  Note that extreme care must be taken when
changing the order of the drives in a configuration.  This is one of
the few instances where the devices and/or their orderings can be
changed without loss of data!  In general, the ordering of components
in a configuration file should
.Ar never
d946 2
a947 2
In this case there are a number of options.  The first option is to add a hot
spare using:
d977 2
a978 1
component.  As the rebuilding is in progress, the status will be:
d998 2
a999 1
indicate the missing component.  For example:
d1008 2
a1009 1
at all by the auto-configuration code.  The name
d1011 2
a1012 2
can be used anywhere a normal component name would be used.  For
example, to add a hot spare to the above set, and rebuild to that hot
a1022 1
.Pp
d1025 3
a1027 2
sets.  A RAID 0 set, for example, could be constructed from four RAID
5 sets.  The following configuration file shows such a setup:
d1048 9
a1056 3
constructed from components on RAID 1 sets.  In such a configuration,
the mirroring provides a high degree of redundancy, while the striping
provides additional speed benefits.
d1058 2
a1059 7
.Ss Auto-configuration and Root on RAID
RAID sets can also be auto-configured at boot.  To make a set
auto-configurable, simply prepare the RAID set as above, and then do
a:
.Bd -unfilled -offset indent
# raidctl -A yes raid0
.Ed
d1061 1
a1061 5
to turn on auto-configuration for that set.  To turn off
auto-configuration, use:
.Bd -unfilled -offset indent
# raidctl -A no raid0
.Ed
d1064 6
a1069 5
root file system is mounted.  These RAID sets are thus available for
use as a root file system, or for any other file system.  A primary
advantage of using the auto-configuration is that RAID components
become more independent of the disks they reside on.  For example,
SCSI ID's can change, but auto-configured sets will always be
d1090 2
a1091 1
.\" alpha and pmax architectures.  On those architectures, the
d1098 4
a1101 3
used to get a kernel booting.  For example, a small partition containing
only the secondary boot-blocks and an alternate kernel (or two) could be
used.  Once a kernel is booting however, and an auto-configured RAID
d1105 4
a1108 4
partition (aka raid[0..n]a) will be used as the root file system.  If two or
more RAID sets claim to be root devices, then the user will be prompted to
select the root device.  At this time, RAID 0, 1, 4, and 5 sets are all
supported as root devices.
d1135 2
a1136 1
auto-configurable.  raid0 is marked as being a root-able raid.
d1139 6
a1144 4
but also to wd0a and wd1a.  The kernel on wd0a is required, since that
is the kernel the system boots from.  The kernel on wd1a is also
required, since that will be the kernel used should wd0 fail.  The
important point here is to have redundant copies of the kernel
d1148 5
a1152 3
as the kernel.  For example, obtaining the kernel from wd0a, and using
sd0e and sd1e for raid0, and the root file system, is fine.  It
.Ar is
d1158 1
a1158 1
.Ar not
d1161 1
a1161 1
.Ar are
d1169 2
a1170 1
must be in the kernel configuration file.  See
a1172 1
.Pp
d1178 4
a1181 4
device.  This is accomplished via a simple:
.Bd -unfilled -offset indent
# raidctl -u raid0
.Ed
a1183 1
.Pp
d1205 3
a1207 3
may be the only way to measure expected performance.  Understanding
some of the underlying technology is also useful in tuning.  The goal
of this section is to provide pointers to those parameters which may
d1210 2
a1211 2
For a RAID 1 set, a SectPerSU value of 64 or 128 is typically
sufficient.  Since data in a RAID 1 set is arranged in a linear
d1213 13
a1225 13
somewhat less critical than it is for a RAID 5 set.  However: a stripe
size that is too small will cause large IO's to be broken up into a
number of smaller ones, hurting performance.  At the same time, a
large stripe size may cause problems with concurrent accesses to
stripes, which may also affect performance.  Thus values in the range
of 32 to 128 are often the most effective.
.Pp
Tuning RAID 5 sets is trickier.  In the best case, IO is presented to
the RAID set one stripe at a time.  Since the entire stripe is
available at the beginning of the IO, the parity of that stripe can
be calculated before the stripe is written, and then the stripe data
and parity can be written in parallel.  When the amount of data being
written is less than a full stripe worth, the
d1227 2
a1228 1
problem occurs.  Since a
d1232 2
a1233 1
slightly differently.  First, the
d1237 8
a1244 7
must be read from the components.  Then the new parity is constructed,
using the new data to be written, and the old data and old parity.
Finally, the new data and new parity are written.  All this extra data
shuffling results in a serious loss of performance, and is typically 2
to 4 times slower than a full stripe write (or read).  To combat this
problem in the real world, it may be useful to ensure that stripe
sizes are small enough that a
d1246 3
a1248 3
from the system will use exactly one large stripe write. As is seen
later, there are some file system dependencies which may come into play
here as well.
d1254 4
a1257 3
blocks (16K).  Since there are 4 data sectors per stripe, the maximum
data per stripe is 64 blocks (32K) or 128 blocks (64K).  Again,
empirical measurement will provide the best indicators of which
d1261 2
a1262 1
performance.  For
d1265 4
a1268 5
performance dramatically.  Also, changing the cylinders-per-group
parameter from 16 to 32 or higher is often not only necessary for
larger file systems, but may also have positive performance
implications.
.Pp
d1271 2
a1272 2
relatively straight-forward process.  All that needs to be done is the
following steps:
a1341 1
.Pp
d1344 4
a1347 3
data loss due to component failure.  However the loss of two
components of a RAID 4 or 5 system, or the loss of a single component
of a RAID 0 system will result in the entire filesystem being lost.
d1349 1
a1349 1
.Ar NOT
d1353 1
a1353 1
.Ar MUST
d1355 7
a1361 7
compromised.  This includes after system crashes, or before a RAID
device has been used for the first time.  Failure to keep parity
correct will be catastrophic should a component ever fail -- it is
better to use RAID 0 and get the additional space and speed, than it
is to use parity, but not keep the parity correct.  At least with RAID
0 there is no perception of increased data security.
.Pp
a1367 1
.Pp
a1371 2
.Sh BUGS
Hot-spare removal is currently not available.
d1391 2
@


1.28
log
@typos;

setextattr(8): example markus@@
spamd(8): someone else found some of these on bugs/misc, but for the life
	  of me i can't find out who
pf.conf(5): from openbsd@@davidkrause.com
raidctl(8): from ian@@darwinsys.com
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.27 2003/02/11 07:26:20 jmc Exp $
d363 1
a363 1
where the queuing method is specified as fifo (first-in, first-out),
@


1.27
log
@typos;

raidctl(8) ok millert@@
mknetid(8) help maja@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.26 2002/10/15 11:59:01 deraadt Exp $
d1295 1
a1295 1
To re-configure the RAID set the next time it is needed, or put
@


1.26
log
@option, not options; hamajima@@nagoya.ydc.co.jp
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.25 2002/08/08 10:55:13 tdeval Exp $
d97 4
a100 2
may be either the full name of the device, e.g. /dev/rraid0c,
or just simply raid0 (for /dev/rraid0c).
d162 1
a162 1
.Ar -c ,
d352 1
a352 1
.Nx .
d385 1
a385 1
.Nm "" ,
d396 1
a396 1
.Sq /dev/rraid0c
d442 1
a442 1
.Nm ""
d469 7
a475 2
set consisting of the components /dev/sd1e, /dev/sd2e, and /dev/sd3e,
with /dev/sd4e available as a
d499 4
a502 1
In this case, devices /dev/sd10e, /dev/sd11e, /dev/sd12e, and /dev/sd13e
d525 5
a529 1
In this case, /dev/sd20e and /dev/sd21e are the two components of the
d573 1
a573 1
for all RAID sets.  As well, using different
d648 5
a652 2
/dev/sd0e is the real component, and will be the second disk of a RAID 1
set.  The component /dev/sd6e, which must exist, but have no physical
d853 3
a855 2
At this point there are at least two options.  First, if /dev/sd2e is
known to be good (i.e. the failure was either caused by
d863 4
a866 1
/dev/sd4e to /dev/sd2e.  Once the copyback procedure is complete, the
d879 5
a883 2
The second option after the reconstruction is to simply use /dev/sd4e
in place of /dev/sd2e in the configuration file.  For example, the
d895 5
a899 2
This can be done as /dev/sd4e is completely interchangeable with
/dev/sd2e at this point.  Note that extreme care must be taken when
d937 5
a941 2
A second option is to rebuild directly onto /dev/sd2e.  Once the disk
containing /dev/sd2e has been replaced, one can simply use:
d946 3
a948 2
to rebuild the /dev/sd2e component.  As the rebuilding is in progress,
the status will be:
d989 2
a990 1
would be reconstructed onto /dev/sd3e.
d1163 1
a1163 1
file system access patterns
d1218 1
a1218 1
values will yeild better performance.
d1224 1
a1224 1
performance dramatically.  As well, changing the cylinders-per-group
d1296 5
a1300 2
raid0.conf into /etc where it will automatically be started by
the /etc/rc scripts.
d1330 1
a1331 1
.Xr ccd 4 ,
@


1.25
log
@Insist on having the raid[0..n]a set up, when using the root on raid feature.
(to avoid problems as in PR#2816)
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.24 2002/06/09 08:13:09 todd Exp $
d1097 1
a1097 1
options    RAID_AUTOCONFIG
@


1.24
log
@rm trailing whitespace
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.23 2002/02/18 22:57:05 tdeval Exp $
d137 1
a137 1
eligible to be the root partition.  A RAID set configured this way
d142 4
a145 1
booted must currently reside on a non-RAID set.
d1023 1
a1023 1
To return raid0a to be just an auto-configuring set simply use the
d1037 1
a1037 1
used.  Once a kernel is booting however, and an auto-configuring RAID
d1039 6
a1044 4
auto-configured and used as the root device.  If two or more RAID sets
claim to be root devices, then the user will be prompted to select the
root device.  At this time, RAID 0, 1, 4, and 5 sets are all supported
as root devices.
d1071 1
a1071 1
auto-configurable.  raid0 is marked as being a root file system.
@


1.23
log
@sync SYNOPSIS with usage:
and some cleanups
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.22 2002/02/17 20:23:36 tdeval Exp $
d117 1
a117 1
The command-line options for 
d128 1
a128 1
automatically configured at boot 
d138 1
a138 1
will 
d145 1
a145 1
its original disk.  This is performed after a component has failed, 
d151 2
a152 2
.Ar config_file . 
A description of the contents of 
d182 1
a182 1
the selected device.  This 
d184 2
a185 2
be done for 
.Ar all 
d194 1
a194 1
different RAID sets.  This step 
d234 1
a234 1
files, a 
d239 2
a240 2
optional sections.  Each section begins with a 
.Sq START , 
d243 1
a243 1
section.  The first section is the 
d247 1
a247 1
example: 
d254 1
a254 1
that although multi-dimensional arrays may be specified, they are 
d272 1
a272 1
.Sq failed , 
d274 1
a274 1
operate in degraded mode.  Note that it is 
d279 1
a279 1
with the 
d284 1
a284 1
is specified, and the components are out-of-order.  
d286 1
a286 1
The next section, which is the 
d289 1
a289 1
present, specifies the devices to be used as 
d293 1
a293 1
one of the main components fail.  A simple 
d302 1
a302 1
are to be used in the configuration, then the 
d329 1
a329 1
include: 
d359 1
a359 1
and the size of the per-component queue is limited to 100 requests.  
d367 1
a367 1
discussed in the 
d384 1
a384 1
By working through these examples, administrators should be able to 
d390 2
a391 2
will be used to denote the RAID device.  
.Sq /dev/rraid0c 
d407 1
a407 1
.Dv FS_BSDFFS 
d409 1
a409 1
.Dv FS_RAID 
d428 1
a428 1
.Sq modification counter , 
d436 1
a436 1
appropriate labels, 
d439 1
a439 1
.Xr raid 4 
d519 1
a519 1
The first time a RAID set is configured, the 
d526 1
a526 1
where 
d528 1
a528 1
is the name of the RAID configuration file.  The 
d537 1
a537 1
appropriate component labels are added with the 
d554 2
a555 2
initialization step is 
.Ar required 
d557 2
a558 2
serial numbers between RAID sets is 
.Ar strongly encouraged , 
d564 1
a564 1
option.  This initialization 
d566 1
a566 1
be done for 
d586 1
a586 1
The output provides a 
d597 1
a597 1
Once the parity is known to be correct, 
d634 1
a634 1
Configuration (using 
d636 1
a636 1
and 
d640 1
a640 1
configuration, this set can be used normally, but will be operating 
d688 1
a688 1
and the 
d692 1
a692 1
will not be 
d784 1
a784 1
start a reconstruction, the 
d812 1
a812 1
the reconstruction is progressing the 
d834 1
a834 1
known to be good (i.e. the failure was either caused by 
d869 1
a869 1
/dev/sd2e at this point.  Note that extreme care must be taken when 
d873 2
a874 2
in a configuration file should 
.Ar never 
d907 1
a907 1
A second option is to rebuild directly onto /dev/sd2e.  Once the disk 
d953 2
a954 2
at which point the data missing from 
.Sq component1 
d1011 1
a1011 1
with the 
d1025 1
a1025 1
.\" alpha and pmax architectures.  On those architectures, the 
d1028 1
a1028 1
.\" kernel directly from a RAID 1 component. 
d1030 1
a1030 1
Note that kernels can't be directly read from a RAID component. 
d1043 1
a1043 1
.It 
d1046 1
a1046 1
installation. 
d1051 1
a1051 1
.It 
d1055 1
a1055 1
swap space. 
d1061 1
a1061 1
.It 
d1068 1
a1068 1
.Pa / , 
d1077 1
a1077 1
sd0e and sd1e for raid0, and the root file system, is fine.  It 
d1086 1
a1086 1
(Multi-layered RAID devices 
d1127 1
a1127 1
.It 
d1154 1
a1154 1
problem occurs.  Since a 
d1158 1
a1158 1
slightly differently.  First, the 
d1160 1
a1160 1
and 
d1174 1
a1174 1
Since the size of a 
d1184 2
a1185 2
performance.  For 
.Xr newfs 8 , 
d1197 3
a1199 3
.It 
Use 
.Xr disklabel 8 
d1201 5
a1205 5
.It 
Construct a RAID configuration file: e.g. 
.Sq raid0.conf 
.It 
Configure the RAID set with: 
d1210 2
a1211 2
.It 
Initialize the component labels with: 
d1216 2
a1217 2
.It 
Initialize other important parts of the set with: 
d1223 1
a1223 1
Get the default label for the RAID set: 
d1228 2
a1229 2
.It 
Edit the label: 
d1234 2
a1235 2
.It 
Put the new label on the RAID set: 
d1240 2
a1241 2
.It 
Create the file system: 
d1243 1
a1243 1
# newfs /dev/rraid0e 
d1247 1
a1247 1
Mount the file system: 
d1259 1
a1259 1
raid0.conf into /etc where it will automatically be started by 
@


1.22
log
@raidctl can do some actions on 'all' configured raids now :
     -B : CopyBack
     -G : Generate config
     -i : Initialize
     -p : Check parity
     -P : Check and re-write parity
     -s : Status
     -S : Progress status
     -u : Unconfigure
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.21 2002/02/11 18:43:51 mpech Exp $
d73 5
a77 4
.Fl a Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl A Op yes | no | root
a78 48
.Nm raidctl
.Op Fl v
.Fl B Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl c Ar config_file Ar dev
.Nm raidctl
.Op Fl v
.Fl C Ar config_file Ar dev
.Nm raidctl
.Op Fl v
.Fl f Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl F Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl g Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl G Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl i Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl I Ar serial_number Ar dev
.Nm raidctl
.Op Fl v
.Fl p Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl P Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl r Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl R Ar component Ar dev
.Nm raidctl
.Op Fl v
.Fl s Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl S Ar dev | Ic all
.Nm raidctl
.Op Fl v
.Fl u Ar dev | Ic all
d92 25
d143 1
a143 1
.It Fl B Ar dev | Ic all
d173 1
a173 1
.It Fl G Ar dev | Ic all
d180 1
a180 1
.It Fl i Ar dev | Ic all
d197 1
a197 1
.It Fl p Ar dev | Ic all
d200 1
a200 1
.It Fl P Ar dev | Ic all
d217 1
a217 1
.It Fl s Ar dev | Ic all
d220 1
a220 1
.It Fl S Ar dev | Ic all
d224 1
a224 1
.It Fl u Ar dev | Ic all
a229 23
.Pp
The device used by
.Nm
is specified by
.Ar dev .
.Ar dev
may be either the full name of the device, e.g. /dev/rraid0c,
or just simply raid0 (for /dev/rraid0c).
.Pp
For several commands,
.Nm
can accept the word
.Ic all
as the
.Ar dev
argument.
If
.Ic all
is used,
.Nm
will execute the requested action for all the configured
.Xr raid 4
devices.
@


1.21
log
@Initial patch.
When you give command examples in a manual page prefix them with
	$ command
or
	# command

deraadt@@ ok
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.20 2002/01/05 13:47:37 tdeval Exp $
d78 1
a78 1
.Nm raifctl
d80 1
a80 1
.Fl B Ar dev
d98 1
a98 1
.Fl G Ar dev 
d101 1
a101 1
.Fl i Ar dev
d107 1
a107 1
.Fl p Ar dev
d110 1
a110 1
.Fl P Ar dev
d119 1
a119 1
.Fl s Ar dev
d122 1
a122 1
.Fl S Ar dev
d125 1
a125 1
.Fl u Ar dev
d165 1
a165 1
.It Fl B Ar dev
d195 1
a195 1
.It Fl G Ar dev
d202 1
a202 1
.It Fl i Ar dev
d219 1
a219 1
.It Fl p Ar dev
d222 1
a222 1
.It Fl P Ar dev
d239 1
a239 1
.It Fl s Ar dev
d242 1
a242 1
.It Fl S Ar dev
d246 1
a246 1
.It Fl u Ar dev
d260 15
@


1.20
log
@Documentation update.  Adapted from NetBSD.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.19 2001/12/30 00:31:03 brad Exp $
d553 1
a553 1
raidctl -C raid0.conf raid0
d571 1
a571 1
raidctl -c raid0.conf raid0
d578 1
a578 1
raidctl -I 112341 raid0
d605 1
a605 1
raidctl -iv raid0
d678 1
a678 1
raidctl -p raid0
d685 1
a685 1
raidctl -P raid0
d698 1
a698 1
raidctl -s raid0
d731 1
a731 1
raidctl -sv raid0
d776 1
a776 1
raidctl -g /dev/sd1e raid0
d796 1
a796 1
raidctl -f /dev/sd2e raid0
d818 1
a818 1
raidctl -F /dev/sd2e raid0
d920 1
a920 1
raidctl -a /dev/sd4e raid0
d940 1
a940 1
raidctl -R /dev/sd2e raid0
d979 2
a980 2
raidctl -a /dev/sd3e raid0
raidctl -F component1 raid0
d1020 1
a1020 1
raidctl -A yes raid0
d1026 1
a1026 1
raidctl -A no raid0
d1047 1
a1047 1
raidctl -A root raid0
d1136 1
a1136 1
raidctl -u raid0
d1237 1
a1237 1
raidctl -C raid0.conf raid0
d1243 1
a1243 1
raidctl -I 123456 raid0
d1249 1
a1249 1
raidctl -i raid0
d1255 1
a1255 1
disklabel raid0 > /tmp/label
d1261 1
a1261 1
vi /tmp/label
d1267 1
a1267 1
disklabel -R -r raid0 /tmp/label
d1273 1
a1273 1
newfs /dev/rraid0e 
d1279 1
a1279 1
mount /dev/raid0e /mnt
d1285 1
a1285 1
raidctl -c raid0.conf raid0
@


1.19
log
@NetBSD -> OpenBSD (Ox)
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.18 2001/12/29 21:54:57 tdeval Exp $
d173 2
a174 2
.Ar config_file .
A description of the contents of
d438 1
a438 1
will also work as the component type, the type
d440 3
a442 3
is preferred for RAIDframe use, as it is required for features such as
auto-configuration.  As part of the initial configuration of each RAID
set, each component will be given a
d638 1
a638 1
component) it may be desirable to to configure a RAID 1 set with only
d709 35
a773 11
This indicates that all is well with the RAID set.  Of importance here
are the component lines which read
.Sq optimal ,
and the 
.Sq Parity status
line which indicates that the parity is up-to-date.  Note that if
there are file systems open on the RAID set, the individual components
will not be 
.Sq clean
but the set as a whole can still be clean.
.Pp
d1054 16
a1069 14
Note that kernels can only be directly read from RAID 1 components on
alpha and pmax architectures.  On those architectures, the 
.Dv FS_RAID
file system is recognized by the bootblocks, and will properly load the
kernel directly from a RAID 1 component.  For other architectures, or
to support the root file system on other RAID sets, some other
mechanism must be used to get a kernel booting.  For example, a small
partition containing only the secondary boot-blocks and an alternate
kernel (or two) could be used.  Once a kernel is booting however, and
an auto-configuring RAID set is found that is eligible to be root,
then that RAID set will be auto-configured and used as the root
device.  If two or more RAID sets claim to be root devices, then the
user will be prompted to select the root device.  At this time, RAID
0, 1, 4, and 5 sets are all supported as root devices.
d1341 3
a1343 1
.Nx 1.4 .
@


1.18
log
@Changes to support the new RAID_AUTOCONF option (-A) of RAIDFrame.
raidctl(8) man page update.

ok drahn@@, deraadt@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.17 2001/07/20 18:07:11 mpech Exp $
d1049 2
a1050 1
NetBSD installation. 
d1052 3
a1054 1
wd1a - also contains a complete, bootable, basic NetBSD installation.
@


1.17
log
@we don't like:
o) .Pp before/after .Sh;
o) .Pp before/after .Rs/.Re;
o) .Nm without argument in SYNOPSIS;
@
text
@d1 2
a2 2
.\"	$OpenBSD: raidctl.8,v 1.16 2000/11/09 17:53:04 aaron Exp $
.\"     $NetBSD: raidctl.8,v 1.11 2000/01/05 03:02:41 oster Exp $
d64 1
a64 1
.Dd November 6, 1998
d73 1
a73 1
.Op Fl a Ar component Ar dev
d76 5
a80 1
.Op Fl B Ar dev
d83 1
a83 1
.Op Fl c Ar config_file Ar dev
d86 1
a86 1
.Op Fl C Ar config_file Ar dev
d89 1
a89 1
.Op Fl f Ar component Ar dev
d92 1
a92 1
.Op Fl F Ar component Ar dev
d95 1
a95 1
.Op Fl g Ar component Ar dev
d98 1
a98 1
.Op Fl i Ar dev
d101 1
a101 1
.Op Fl I Ar serial_number Ar dev
d104 1
a104 1
.Op Fl p Ar dev
d107 1
a107 1
.Op Fl P Ar dev
d110 1
a110 1
.Op Fl r Ar component Ar dev
d113 1
a113 1
.Op Fl R Ar component Ar dev
d116 1
a116 1
.Op Fl s Ar dev
d119 1
a119 1
.Op Fl S Ar dev
d122 4
a125 1
.Op Fl u Ar dev
d133 1
a133 2
devices.
For more information about the RAIDframe disk device, see
d139 4
a142 2
The options are as follows:
.Bl -tag -width Ds
d148 17
d167 1
a167 2
it's original disk.
This is performed after a component has failed,
d180 2
a181 2
but forces the configuration to take place.
This is required the first time a RAID set is configured.
d191 1
a191 2
disk onto an available hot spare.
This is one of the mechanisms used to start
d195 7
d203 7
a209 8
Initialize (re-write) the parity on the device.
This
.Em must
be done before the RAID device is labeled and before
filesystems are created on the RAID device, and is normally used after
a system crash (and before a
.Xr fsck 8 )
to ensure the integrity of the parity.
d214 5
a218 3
particular set of components belong to the same RAID set.
While not strictly enforced, different serial numbers should be used for
different RAID sets.
d220 2
a221 2
Check the status of the parity on the RAID set.
Displays a status message; returns successfully if the parity is up-to-date.
d225 3
d237 2
a238 2
This is another mechanism for starting the reconstruction process if a
component has a hardware failure.
d243 3
a245 3
Check the status of component reconstruction.
The output indicates
the amount of progress achieved in reconstructing a failed component.
d249 1
a249 2
Be more verbose.
For operations such as reconstructions, parity
d258 2
a259 4
may be either the full name of the device (e.g.,
.Pa /dev/rraid0c ) ,
or just simply raid0 (for
.Pa /dev/rraid0c ) .
d261 1
d263 3
a265 3
only an abbreviated treatment is given here.
In the configuration files, a
.Ql #
d269 2
a270 3
optional components.
Each section begins with a
.Dq START ,
d272 3
a274 4
the section name, and the confuration parameters associated with that
section.
The first section is the
.Dq array
d276 2
a277 2
the number of rows, columns, and spare disks in the RAID array.
For example:
d283 3
a285 3
indicates an array with 1 row, 3 columns, and 0 spare disks.
Note that although multi-dimensional arrays may be specified, they are
.Em not
d289 1
a289 1
.Dq disks
d291 1
a291 2
components of the device.
For example:
d299 2
a300 2
specifies the three component disks to be used in the RAID device.
If any of the specified drives cannot be found when the RAID device is
d302 1
a302 1
.Dq failed ,
d304 2
a305 3
operate in degraded mode.
Note that it is
.Em imperative
d307 14
a320 9
change between configurations of a RAID device.
Changing the order
of the components (at least at the time of this writing) will result in
data loss.
.Pp
The next section,
.Dq spare ,
is optional, and if present specifies the devices to be used as
.Dq hot spares
d323 2
a324 3
one of the main components fail.
A simple
.Dq spare
d331 3
a333 3
for a configuration with a single spare component.
If no spare drives are to be used in the configuration, then the
.Dq spare
d337 2
a338 3
.Dq layout
section.
This section describes the
d342 1
a342 2
use.
This section might look like:
d350 2
a351 3
factor; i.e., the number of contiguous sectors to be written to each
component for a single stripe.
Appropriate selection of this value
d353 1
a353 2
architectures.
The stripe units per parity unit and
d357 3
a359 4
the scope of this document.
The last value in this section (5 in this
example) indicates the parity configuration desired.
Valid entries include:
d362 1
a362 2
RAID level 0.
No parity, only simple striping.
d364 1
a364 2
RAID level 1.
Mirroring.
d366 2
a367 2
RAID level 4.
Striping across components, with parity stored on the last component.
d369 2
a370 2
RAID level 5.
Striping across components, parity distributed across all components.
d377 1
a377 1
.Ox .
d380 3
a382 3
.Dq queue
section.
This is most often specified as:
d385 1
a385 1
fifo 1
d388 2
a389 4
where the queuing method is specified as FIFO (first-in, first-out),
and the size of the per-component queue is limited to 1 request.
A value of 1 is quite conservative here, and values of 100 or more may
been used to increase the driver performance.
d394 4
a397 4
.Dq debug
section, is optional.
For more details on this the reader is referred to the RAIDframe documentation
dissussed in the
d400 1
d404 1
d406 58
a463 12
The examples in this section will focus on a RAID 5 configuration.
Other RAID configurations will behave similarly.
It is highly
recommended that before using the RAID driver for real file systems
that the system administrator(s) have used
.Em all
of the options for
.Nm raidctl ,
and that they understand how the component reconstruction process works.
While this example is not created as a tutorial, the steps
shown here can be easily duplicated using four equal-sized partitions
from any number of disks (including all four from a single disk).
d465 7
a471 6
The primary uses of
.Nm
is to configure and unconfigure
.Xr raid 4
devices.
To configure a device, a configuration file which looks something like:
d493 4
a496 11
is first created.
In short, this configuration file specifies a RAID
5 configuration consisting of the disks
.Pa /dev/sd1e ,
.Pa /dev/sd2e ,
and
.Pa /dev/sd3e ,
with
.Pa /dev/sd4e
available as a
.Dq hot spare
d498 2
a499 4
the three main drives should fail.
If the above configuration is in a file called
.Pa rfconfig ,
raid device 0 in the normal case can be configured with:
d501 16
a516 1
raidctl -c rfconfig raid0
d519 6
a524 1
The above is equivalent to the following:
d526 14
a539 1
raidctl -c rfconfig /dev/rraid0c
d542 6
a547 12
A RAID set will not configure with
.Fl c
if the component labels are not correct.
A
.Sq component label
contains important information about the component, including a
user-specified serial number, the row and column of that component in the RAID
set, and whether the data (and parity) on the component is
.Sq clean .
See
.Xr raid 4
for more information about component labels.
d549 1
a549 2
Since new RAID sets will not have correct component labels, the first
configuration of a RAID set must use
d551 1
a551 2
instead of
.Fl c :
d553 1
a553 1
raidctl -C rfconfig raid0
d556 3
a558 1
The
d561 3
a563 2
labels are incorrect.
This option should not be used lightly in
d566 7
a572 1
very good reason for it.
d576 1
a576 2
parity on the RAID set.
Initializing the component labels is done with:
d583 43
a625 10
is a user-specified serial number for the RAID set.
Using different
serial numbers between RAID sets is strongly encouraged, as using the
same serial number for all RAID sets will only serve to decrease the
usefulness of the component label checking.
.Pp
Initializing the parity on the RAID set is done via:
.Bd -unfilled -offset indent
raidctl -i raid0
.Ed
d627 2
a628 3
Initializing the parity in this way may also be required after an
unclean shutdown.
Once the parity is known to be correct, it is then safe to perform
d636 40
d681 3
a683 2
can be used to check the current status of the parity.
To check the parity and rebuild it if necessary the command:
d688 2
a689 3
is used.
Note that re-writing the parity can be done while
other operations on the RAID set are taking place (e.g., while doing a
d691 1
a691 2
on a filesystem on the RAID set).
However: for maximum effectiveness
d709 40
a748 3
.Ed
.Pp
This indicates that all is well with the RAID set.
d757 9
a765 21
Component label for /dev/sd2e:
Version: 1
Serial Number: 112341
Mod counter: 6
Row: 0
Column: 1
Num Rows: 1
Num Columns: 3
Clean: 0
Status: optimal
.Ed
.Pp
For a component label to be considered valid, that particular
component label must be in agreement with the other component labels
in the set.
For example, the serial number,
.Dq modification counter ,
number of rows, and number of columns must all be in agreement.
If any
of these are different, then the component is not considered to be
part of the set.
d767 1
d777 1
a777 1
In this case, obtaining the status of raid0 will return:
d789 2
a790 2
a reconstruction has not been started.
To both fail the disk and start a reconstruction, the
d810 5
d817 2
a818 2
This indicates that a reconstruction is in progress.
To find out how the reconstruction is progressing the
d820 3
a822 4
option may be used.
This will indicate the progress in terms of the
percentage of the reconstruction that is completed.
When the reconstruction is finished the
d832 5
d839 2
a840 4
At this point there are at least two options.
First, if
.Pa /dev/sd2e
is known to be good (i.e., the failure was either caused by
d847 3
a849 6
option.
In this example, this would copy the entire contents of
.Pa /dev/sd4e
to
.Pa /dev/sd2e .
Once the copyback procedure is complete, the status of the device would be:
d861 3
a863 6
The second option after the reconstruction is to simply use
.Pa /dev/sd4e
in place of
.Pa /dev/sd2e
in the configuration file.
For example, the configuration file (in part) might now look like:
d874 3
a876 8
This can be done as
.Pa /dev/sd4e
is completely interchangeable with
.Pa /dev/sd2e
at this point.
Note that extreme care must be taken when
changing the order of the drives in a configuration.
This is one of
d878 3
a880 3
changed without loss of data!
In general, the ordering of components in a configuration file should
.Em never
d884 1
a884 1
available on-line, the status of the RAID set might look like:
d893 2
a894 2
In this case there are a number of options.
The first option is to add a hot spare using:
d913 2
a914 6
A second option is to rebuild directly onto
.Pa /dev/sd2e .
Once the disk
containing
.Pa /dev/sd2e
has been replaced, one can simply use:
d919 2
a920 4
to rebuild the
.Pa /dev/sd2e
component.
As the rebuilding is in progress, the status will be:
d938 163
d1105 1
a1105 2
device.
This is accomplished via a simple:
d1111 153
d1266 1
a1266 2
data loss due to component failure.
However the loss of two
d1270 1
a1270 1
.Em not
d1274 1
a1274 1
.Em must
d1276 2
a1277 4
compromised.
This includes after system crashes, or before a RAID
device has been used for the first time.
Failure to keep parity
d1280 3
a1282 2
is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
d1287 1
a1287 1
device special files
d1289 1
d1291 1
a1292 1
.Xr raid 4 ,
d1294 2
a1312 2
.Sh BUGS
Hot-spare removal is currently not available.
a1314 1

@


1.16
log
@Change all option list specifications to ".Bl -tag -width Ds". Most man
pages just needed their -width parameter tweaked to "Ds", which provides
a nice width of 6 constant characters. For consistency more than anything.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.15 2000/04/24 05:12:19 deraadt Exp $
a786 1
.Pp
a792 1
.Pp
@


1.15
log
@properly label whole disk partition; op21@@squish.org
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.14 2000/04/15 02:15:16 aaron Exp $
d134 1
a134 1
.Bl -tag -width indent
@


1.14
log
@Mostly punctuation fixes.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.13 2000/04/12 21:47:59 aaron Exp $
d226 1
a226 4
.Pa /dev/rraid0d
for the i386 architecture, and
.Pa /dev/rraid0c
for all others),
d228 1
a228 1
.Pa /dev/rraid0d ) .
d443 1
a443 1
raidctl -c rfconfig /dev/rraid0d
a444 6
.Pp
on the i386 architecture.
On all other architectures,
.Pa /dev/rraid0c
is used in place of
.Pa /dev/rraid0d .
@


1.13
log
@Trailing whitespace begone!
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.12 2000/03/18 22:56:03 aaron Exp $
d179 2
a180 2
.Xr fsck 8
) to ensure the integrity of the parity.
@


1.12
log
@Remove hard sentence breaks, and some other cleanup along the way.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.11 2000/03/14 21:31:42 aaron Exp $
d76 1
a76 1
.Op Fl B Ar dev 
d136 1
a136 1
Add 
d138 1
a138 1
as a hot spare for the device 
d141 1
a141 1
Initiate a copyback of reconstructed data from a spare disk to 
d143 1
a143 1
This is performed after a component has failed, 
d174 1
a174 1
This 
d183 1
a183 1
.Ar serial_number 
d195 2
a196 2
Remove the spare disk specified by 
.Ar component 
d199 3
a201 3
Fails the specified 
.Ar component , 
if necessary, and immediately begins a reconstruction back to 
d210 1
a210 1
The output indicates 
d214 1
a214 1
.It Fl v 
d439 1
a439 1
raid device 0 in the normal case can be configured with: 
d455 1
a455 1
A RAID set will not configure with 
d462 1
a462 1
set, and whether the data (and parity) on the component is 
d464 1
a464 1
See 
d477 1
a477 1
The 
d486 1
a486 1
When the RAID set is configured for the first time, it is 
d494 1
a494 1
where 
d553 1
a553 1
This indicates that all is well with the RAID set. 
d613 1
a613 1
The 
d729 1
a729 1
Reconstruction could then take place using 
d735 2
a736 2
Once the disk 
containing 
d743 1
a743 1
to rebuild the 
d800 1
a800 1
.Cm raid 
@


1.11
log
@Suffix "i.e." and "e.g." with a comma. Just another really picky man page
commit, as we want our documentation to be as consistently formatted as
possible (it's getting there :-)).
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.10 2000/03/05 00:28:56 aaron Exp $
d126 2
a127 1
devices.  For more information about the RAIDframe disk device, see
d142 2
a143 1
it's original disk.  This is performed after a component has failed, 
d156 2
a157 2
but forces the configuration to take place.  This is required the
first time a RAID set is configured.
d167 2
a168 1
disk onto an available hot spare.  This is one of the mechanisms used to start
d173 2
a174 1
Initialize (re-write) the parity on the device.  This 
d182 1
a182 1
Initialize the component labels on each component of the device.  
d185 2
a186 2
particular set of components belong to the same RAID set.  While not
strictly enforced, different serial numbers should be used for
d189 2
a190 2
Check the status of the parity on the RAID set.  Displays a status
message, and returns successfully if the parity is up-to-date.
d209 2
a210 1
Check the status of component reconstruction.  The output indicates 
d215 2
a216 1
Be more verbose.  For operations such as reconstructions, parity
d234 3
a236 3
only an abbreviated treatment is given here.  In the configuration
files, a
.Sq #
d240 2
a241 1
optional components.  Each section begins with a
d245 2
a246 1
section.  The first section is the
d249 2
a250 2
the number of rows, columns, and spare disks in the RAID array.  For
example:
d256 2
a257 2
indicates an array with 1 row, 3 columns, and 0 spare disks.  Note
that although multi-dimensional arrays may be specified, they are
d264 2
a265 1
components of the device.  For example:
d273 2
a274 2
specifies the three component disks to be used in the RAID device.  If
any of the specified drives cannot be found when the RAID device is
d278 2
a279 1
operate in degraded mode.  Note that it is
d282 2
a283 1
change between configurations of a RAID device.  Changing the order
d293 2
a294 1
one of the main components fail.  A simple
d302 2
a303 2
for a configuration with a single spare component.  If no spare drives
are to be used in the configuration, then the
d309 2
a310 1
section.  This section describes the
d314 2
a315 1
use.  This section might look like:
d324 2
a325 1
component for a single stripe.  Appropriate selection of this value
d327 2
a328 1
architectures.  The stripe units per parity unit and
d332 4
a335 3
the scope of this document.  The last value in this section (5 in this
example) indicates the parity configuration desired.  Valid entries
include:
d338 2
a339 1
RAID level 0.  No parity, only simple striping.
d341 2
a342 1
RAID level 1.  Mirroring.
d344 2
a345 2
RAID level 4.  Striping across components, with parity stored on the
last component.
d347 2
a348 2
RAID level 5.  Striping across components, parity distributed across
all components.
d359 2
a360 2
section.  This is most often
specified as:
d367 2
a368 2
and the size of the per-component queue is limited to 1 request.  A
value of 1 is quite conservative here, and values of 100 or more may
d375 2
a376 2
section, is optional.  For more details
on this the reader is referred to the RAIDframe documentation
d385 3
a387 2
Other RAID configurations will behave similarly.  It is highly
recommended that before using the RAID driver for real filesystems
d392 2
a393 2
and that they understand how the component reconstruction process
works.  While this example is not created as a tutorial, the steps
d401 2
a402 2
devices.  To configure a device, a configuration
file which looks something like:
d424 2
a425 1
is first created.  In short, this configuration file specifies a RAID
d436 2
a437 2
the three main drives should fail.  If the above configuration is in a
file called
d449 2
a450 1
on the i386 architecture.  On all other architectures,
d457 2
a458 1
if the component labels are not correct.  A
d480 2
a481 1
labels are incorrect.  This option should not be used lightly in
d488 2
a489 1
parity on the RAID set.  Initializing the component labels is done with:
d496 2
a497 1
is a user-specified serial number for the RAID set.  Using different
d508 2
a509 2
unclean shutdown.  Once the parity is known to be correct, 
it is then safe to perform
d522 2
a523 2
can be used to check the current status of the parity.  To check the
parity and rebuild it if necessary the command:
d528 2
a529 1
is used.  Note that re-writing the parity can be done while
d532 2
a533 1
on a filesystem on the RAID set).  However: for maximum effectiveness
d576 5
a580 2
in the set.  For example, the serial number, 'modification counter',
number of rows and number of columns must all be in agreement.  If any
d605 2
a606 2
a reconstruction has not been started.  To both fail the disk and
start a reconstruction, the
d628 2
a629 2
This indicates that a reconstruction is in progress.  To find out how
the reconstruction is progressing the
d631 4
a634 3
option may be used.  This will indicate the progress in terms of the
percentage of the reconstruction that is completed.  When the
reconstruction is finished the
d646 2
a647 1
At this point there are at least two options.  First, if
d656 2
a657 1
option.  In this example, this would copy the entire contents of
d677 2
a678 2
in the configuration file.  For example, the
configuration file (in part) might now look like:
d693 4
a696 2
at this point.  Note that extreme care must be taken when
changing the order of the drives in a configuration.  This is one of
d698 2
a699 2
changed without loss of data!  In general, the ordering of components
in a configuration file should
d713 2
a714 2
In this case there are a number of options.  The first option is to add a hot
spare using:
d745 2
a746 2
component.  As the rebuilding is in progress,
the status will be:
a763 2

.Pp
d768 2
a769 1
device.  This is accomplished via a simple:
d777 2
a778 1
data loss due to component failure.  However the loss of two
d788 4
a791 2
compromised.  This includes after system crashes, or before a RAID
device has been used for the first time.  Failure to keep parity
d794 2
a795 2
is to use parity, but not keep the parity correct.  At least with RAID
0 there is no perception of increased data security.
a807 2
.Sh BUGS
Hot-spare removal is currently not available.
d820 2
a821 2
command first appeared as a program in CMU's RAIDframe v1.1 distribution.  This
version of
d825 2
@


1.10
log
@Finish standardizing options list introduction.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.9 2000/01/07 14:51:41 peter Exp $
d503 1
a503 1
other operations on the RAID set are taking place (e.g. while doing a
@


1.9
log
@sync with Greg Oster's NetBSD chages to Jan 4th 2000.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.8 1999/09/23 04:12:02 alex Exp $
d132 1
a132 3
The command-line options for
.Nm
are as follows:
@


1.8
log
@Typo fixes.
@
text
@d1 2
a2 3
.\"	$OpenBSD: raidctl.8,v 1.7 1999/07/30 17:24:32 peter Exp $
.\"
.\"     $NetBSD: raidctl.8,v 1.8 1999/03/24 06:18:30 mycroft Exp $
d72 1
d75 1
d78 1
d81 1
d84 1
d87 1
d90 1
d93 1
d96 1
d99 7
d108 1
d111 1
d114 1
d117 1
d177 2
a178 2
.Xr fsck 8 ) Ns
to ensure the integrity of the parity.
d186 6
d211 3
d492 18
@


1.7
log
@A better, but still not perfect, merge of raidctl(8)
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.5 1999/07/03 02:11:08 aaron Exp $
d348 1
a348 1
shown here can be easily dupilicated using four equal-sized partitions
@


1.6
log
@Update RAIDframe from NetBSD-current as of 1999/07/26.

Please note that you *must* follow the upgrade instructions at

	http://www.cs.usask.ca/staff/oster/clabel_upgrade.html

before installing the new raidctl and new kernel using this code.
@
text
@d1 2
d42 1
a42 1
.\" 
d44 1
a44 1
.\" 
d50 1
a50 1
.\" 
d54 1
a54 1
.\" 
d56 1
a56 1
.\" 
d61 1
a61 1
.\" 
d64 1
a64 1
.\" 
d67 1
a67 1
.Os NetBSD
d72 28
a99 28
.Nm ""
.Fl a Ar component Ar dev
.Nm ""
.Fl B Ar dev 
.Nm ""
.Fl c Ar config_file Ar dev
.Nm ""
.Fl C Ar config_file Ar dev
.Nm ""
.Fl f Ar component Ar dev
.Nm ""
.Fl F Ar component Ar dev
.Nm ""
.Fl g Ar component Ar dev
.Nm ""
.Fl i Ar dev
.Nm ""
.Fl I Ar serial_number Ar dev
.Nm ""
.Fl r Ar component Ar dev
.Nm ""
.Fl R Ar component Ar dev
.Nm ""
.Fl s Ar dev 
.Nm ""
.Fl S Ar dev
.Nm ""
.Fl u Ar dev
d101 1
a101 1
.Nm ""
d104 2
a105 2
the RAIDframe disk device.  
.Nm ""
d113 1
a113 1
The command-line options for 
d127 1
a127 1
Configure the RAIDframe device 
d131 1
a131 1
A description of the contents of 
d140 1
a140 1
This marks the specified 
d143 1
a143 1
component.  
d145 1
a145 1
Fails the specified 
d154 1
a154 1
.Ar MUST
d157 2
a158 2
a system crash (and before a 
.Xr fsck 8 )
d180 1
a180 1
and spares.  
d188 1
a188 1
The device used by 
d190 2
a191 2
is specified by 
.Ar dev .  
d193 7
a199 3
may be either the full name of the device, e.g. /dev/rraid0d,
for the i386 architecture, and /dev/rraid0c
for all others, or just simply raid0 (for /dev/rraid0d).
d203 1
a203 1
files, a 
d208 2
a209 2
optional components.  Each section begins with a 
.Sq START , 
d212 2
a213 2
section.  The first section is the 
.Sq array
d215 2
a216 2
the number of rows, columns, and spare disks in the RAID set.  For
example: 
d223 2
a224 2
that although multi-dimenstional arrays may be specified, they are 
.Ar NOT
d227 2
a228 2
The second section, the 
.Sq disks
d240 2
a241 2
configured, then they will be marked as 
.Sq failed , 
d243 2
a244 2
operate in degraded mode.  Note that it is 
.Ar imperative
d248 1
a248 1
data loss.  
d250 4
a253 5
The next section, which is the 
.Sq spare
section, is optional, and, if
present, specifies the devices to be used as 
.Sq hot spares
d256 2
a257 2
one of the main components fail.  A simple 
.Sq spare
d260 1
a260 1
START spare 
d265 3
a267 3
are to be used in the configuration, then the 
.Sq spare
section may be omitted.  
d269 2
a270 2
The next section is the 
.Sq layout
d283 1
a283 1
factor; i.e. the number of contiguous sectors to be written to each
d292 1
a292 1
include: 
d294 1
a294 1
.It 0 
d307 1
a307 1
parity, RAID level 5 with rotated sparing, Chained declustering, 
d309 2
a310 2
those parity operations has not been tested with 
.Nx .
d312 2
a313 2
The next required section is the 
.Sq queue
d321 2
a322 2
where the queueing method is specified as fifo (first-in, first-out),
and the size of the per-component queue is limited to 1 requests.  A
d324 1
a324 1
been used to increase the driver performance. 
d328 2
a329 2
The final section, the 
.Sq debug
d332 1
a332 1
dissussed in the 
a334 1

a337 1

a338 1

d342 4
a345 4
that the system administrator(s) have used 
.Ar all
of the options for 
.Nm "" ,
d349 1
a349 1
from any number of disks (including all four from a single disk).  
d351 2
a352 2
The primary uses of 
.Nm ""
d354 2
a355 2
.Xr raid 4 
devices.  To configure the device, a configuration
d379 9
a387 3
5 configuration consisting of the components /dev/sd1e,
/dev/sd2e, and /dev/sd3e, with /dev/sd4e available as a 
.Sq hot spare
d390 2
a391 2
file called 
.Sq rfconfig , 
d402 4
a405 2
on the i386 architecture.  On all other architectures, /dev/rraid0c
is used in place of /dev/rraid0d.
d517 1
a517 1
were missing data is re-computed from existing data and the parity.
d528 1
a528 1
Note that with the use of 
d531 1
a531 1
start a reconstruction, the 
d542 1
a542 1
option used later, on the same disk, if desired.  
d554 1
a554 1
the reconstruction is progressing the 
d570 3
a572 2
At this point there are at least two options.  First, if /dev/sd2e is
known to be good (i.e. the failure was either caused by 
d574 1
a574 1
or 
d576 2
a577 2
or the failed disk was replaced), then a copyback of the data can 
be initiated with the 
d580 4
a583 2
/dev/sd4e to /dev/sd2e.  Once the copyback procedure is complete, the
status of the device would be:
d595 5
a599 2
The second option after the reconstruction is to simply use /dev/sd4e
in place of /dev/sd2e in the configuration file.  For example, the
d611 5
a615 2
This can be done as /dev/sd4e is completely interchangeable with
/dev/sd2e at this point.  Note that extreme care must be taken when 
d619 2
a620 2
in a configuration file should 
.Ar never 
d653 6
a658 2
A second option is to rebuild directly onto /dev/sd2e.  Once the disk 
containing /dev/sd2e has been replaced, one can simply use:
d663 3
a665 1
to rebuild the /dev/sd2e component.  As the rebuilding is in progress,
d686 1
a686 1
The final operation performed by 
d688 1
a688 1
is to unconfigure a 
d701 2
a702 2
RAID is 
.Ar NOT
d705 2
a706 2
Recomputation of parity 
.Ar MUST
d719 1
a719 1
device special files.
d723 1
a724 1
.Xr ccd 4 ,
d731 1
a731 1
Mellon University (CMU).  
d747 1
d772 1
@


1.5
log
@remove redundant .Pp macros
@
text
@d1 1
a1 3
.\"	$OpenBSD: raidctl.8,v 1.4 1999/06/04 02:45:25 aaron Exp $
.\"
.\"     $NetBSD: raidctl.8,v 1.3 1999/02/04 14:50:31 oster Exp $
d40 1
a40 1
.\"
d42 1
a42 1
.\"
d48 1
a48 1
.\"
d52 1
a52 1
.\"
d54 1
a54 1
.\"
d59 1
a59 1
.\"
d62 1
a62 1
.\"
d65 1
a65 1
.Os
d70 5
a74 1
.Nm raidctl
d76 3
a78 3
.Nm raidctl
.Fl C Ar dev
.Nm raidctl
d80 1
a80 1
.Nm raidctl
d82 15
a96 7
.Nm raidctl
.Fl r Ar dev
.Nm raidctl
.Fl R Ar dev
.Nm raidctl
.Fl s Ar dev
.Nm raidctl
d99 1
a99 1
.Nm
d102 2
a103 2
the RAIDframe disk device.
.Nm
d111 1
a111 1
The command-line options for
d115 9
d125 1
a125 1
Configure the RAIDframe device
d129 1
a129 1
A description of the contents of
d132 5
a136 4
.It Fl C Ar dev
Initiate a copyback of reconstructed data from a spare disk to
its original disk.  This is performed after a component has failed,
and the failed drive has been reconstructed onto a spare drive.
d138 1
a138 1
This marks the specified
d141 1
a141 1
component.
d143 1
a143 1
Fails the specified
d145 2
a146 2
of the device, and immediately beginis a reconstruction of the failed
disk onto an available hot spare.  This is the mechanism used to start
d148 5
a152 3
.It Fl r Ar dev
Re-write the parity on the device.  This
.Em must
d155 2
a156 2
a system crash (and before a
.Xr fsck 8 ) Ns
d158 18
a175 3
.It Fl R Ar dev
Check the status of component reconstruction.  The output indicates
the amount of progress achieved in reconstructing a failed component.
d178 4
a181 1
and spares.
d186 1
a186 1
The device used by
d188 2
a189 2
is specified by
.Ar dev .
d191 3
a193 7
may be either the full name of the device (e.g.,
.Pa /dev/rraid0d
for the i386 architecture, and
.Pa /dev/rraid0c
for all others),
or just simply raid0 (for
.Pa /dev/rraid0d ) .
d197 1
a197 1
files, a
d202 2
a203 2
optional components.  Each section begins with a
.Dq START ,
d206 2
a207 2
section.  The first section is the
.Dq array
d209 2
a210 2
the number of rows, columns, and spare disks in the RAID array.  For
example:
d217 2
a218 2
that although multi-dimensional arrays may be specified, they are
.Em not
d221 2
a222 2
The second section, the
.Dq disks
d234 2
a235 2
configured, then they will be marked as
.Dq failed ,
d237 2
a238 2
operate in degraded mode.  Note that it is
.Em imperative
d242 1
a242 1
data loss.
d244 5
a248 4
The next section,
.Dq spare ,
is optional, and if present specifies the devices to be used as
.Dq hot spares
d251 2
a252 2
one of the main components fail.  A simple
.Dq spare
d255 1
a255 1
START spare
d260 3
a262 3
are to be used in the configuration, then the
.Dq spare
section may be omitted.
d264 2
a265 2
The next section is the
.Dq layout
d278 1
a278 1
factor; i.e., the number of contiguous sectors to be written to each
d287 1
a287 1
include:
d289 1
a289 1
.It 0
d302 1
a302 1
parity, RAID level 5 with rotated sparing, Chained declustering,
d304 2
a305 2
those parity operations has not been tested with
.Ox .
d307 2
a308 2
The next required section is the
.Dq queue
d316 2
a317 2
where the queuing method is specified as FIFO (first-in, first-out),
and the size of the per-component queue is limited to 1 request.  A
d319 1
a319 1
been used to increase the driver performance.
d323 2
a324 2
The final section, the
.Dq debug
d327 1
a327 1
dissussed in the
d330 1
d334 1
d336 1
d340 4
a343 4
that the system administrator(s) have used
.Em all
of the options for
.Nm ,
d347 1
a347 1
from any number of disks (including all four from a single disk).
d349 2
a350 2
The primary use of
.Nm
d352 2
a353 2
.Xr raid 4
devices.  To configure a device, a configuration
d377 3
a379 9
5 configuration consisting of the disks
.Pa /dev/sd1e ,
.Pa /dev/sd2e ,
and
.Pa /dev/sd3e ,
with
.Pa /dev/sd4e
available as a
.Dq hot spare
d382 3
a384 3
file called
.Pa rfconfig ,
raid device 0 can be configured with:
d394 45
a438 4
on the i386 architecture.  On all other architectures,
.Pa /dev/rraid0c
is used in place of
.Pa /dev/rraid0d .
d440 17
a456 1
To see how the device is doing, the following will show the status:
d468 8
a475 1
           /dev/sd4e [0][0]: spare
d478 1
a478 4
This indicates that all is well with the RAID array.  If this is the first
time this RAID array has been configured, or the system is just being
brought up after an unclean shutdown, it is necessary to
ensure that the parity values are correct.  This can be done via:
d480 10
a489 1
raidctl -r raid0
d492 6
a497 6
Once this is done, it is then safe to perform
.Xr disklabel 8 , Ns
.Xr newfs 8 , Ns
or
.Xr fsck 8
on the device or its filesystems.
d507 1
a507 1
where missing data is re-computed from existing data and the parity.
d515 1
a515 1
           /dev/sd4e [0][0]: spare
d518 1
a518 1
Note that with the use of
d521 1
a521 1
start a reconstruction, the
d523 6
a528 1
option must be used.  (The
d532 1
a532 1
option used later, on the same disk, if desired.)
d540 1
a540 1
           /dev/sd4e [0][0]: used_spare
d544 2
a545 2
the reconstruction is progressing the
.Fl R
d557 1
a557 1
           /dev/sd4e [0][0]: used_spare
d560 2
a561 3
At this point there are at least two options.  First, if
.Pa /dev/sd2e
is known to be good (i.e., the failure was either caused by
d563 1
a563 1
or
d565 3
a567 3
or the failed disk was replaced), then a copyback of the data can
be initiated with the
.Fl C
d569 2
a570 4
.Pa /dev/sd4e
to
.Pa /dev/sd2e .
Once the copyback procedure is complete, the status of the device would be:
d577 1
a577 1
           /dev/sd4e [0][0]: spare
d582 2
a583 5
The second option after the reconstruction is to simply use
.Pa /dev/sd4e
in place of
.Pa /dev/sd2e
in the configuration file.  For example, the
d595 2
a596 5
This can be done as
.Pa /dev/sd4e
is completely interchangeable with
.Pa /dev/sd2e
at this point.  Note that extreme care must be taken when
d600 2
a601 2
in a configuration file should
.Em never
d604 58
a661 1
The final operation performed by
d663 1
a663 1
is to unconfigure a
d676 2
a677 2
RAID is
.Em not
d680 2
a681 2
Recomputation of parity
.Em must
d689 1
d693 2
a694 2
.Nm
device special files
d696 1
d698 1
a699 1
.Xr raid 4 ,
d701 2
d706 1
a706 1
Mellon University (CMU).
a721 1

a745 1

@


1.4
log
@start to remove non-escaped trailing whitespace, it can confuse troff; pjanzen@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.3 1999/04/02 15:12:18 aaron Exp $
a531 1
.Pp
a537 1
.Pp
@


1.3
log
@all .Nm macros should have an argument in SYNOPSIS; also misc cleanup
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.2 1999/02/16 21:51:39 niklas Exp $
d42 1
a42 1
.\" 
d44 1
a44 1
.\" 
d50 1
a50 1
.\" 
d54 1
a54 1
.\" 
d56 1
a56 1
.\" 
d61 1
a61 1
.\" 
d64 1
a64 1
.\" 
d85 1
a85 1
.Fl s Ar dev 
d92 2
a93 2
the RAIDframe disk device.  
.Nm 
d101 1
a101 1
The command-line options for 
d106 1
a106 1
Configure the RAIDframe device 
d110 1
a110 1
A description of the contents of 
d114 2
a115 2
Initiate a copyback of reconstructed data from a spare disk to 
its original disk.  This is performed after a component has failed, 
d118 1
a118 1
This marks the specified 
d121 1
a121 1
component.  
d123 1
a123 1
Fails the specified 
d129 1
a129 1
Re-write the parity on the device.  This 
d133 1
a133 1
a system crash (and before a 
d137 1
a137 1
Check the status of component reconstruction.  The output indicates 
d141 1
a141 1
and spares.  
d146 1
a146 1
The device used by 
d148 2
a149 2
is specified by 
.Ar dev .  
d161 1
a161 1
files, a 
d166 2
a167 2
optional components.  Each section begins with a 
.Dq START , 
d170 1
a170 1
section.  The first section is the 
d174 1
a174 1
example: 
d181 1
a181 1
that although multi-dimensional arrays may be specified, they are 
d185 1
a185 1
The second section, the 
d198 2
a199 2
configured, then they will be marked as 
.Dq failed , 
d206 1
a206 1
data loss.  
d214 1
a214 1
one of the main components fail.  A simple 
d218 1
a218 1
START spare 
d223 1
a223 1
are to be used in the configuration, then the 
d225 1
a225 1
section may be omitted.  
d227 1
a227 1
The next section is the 
d250 1
a250 1
include: 
d252 1
a252 1
.It 0 
d265 1
a265 1
parity, RAID level 5 with rotated sparing, Chained declustering, 
d267 1
a267 1
those parity operations has not been tested with 
d270 1
a270 1
The next required section is the 
d282 1
a282 1
been used to increase the driver performance. 
d286 1
a286 1
The final section, the 
d290 1
a290 1
dissussed in the 
d300 1
a300 1
that the system administrator(s) have used 
d302 1
a302 1
of the options for 
d307 1
a307 1
from any number of disks (including all four from a single disk).  
d309 1
a309 1
The primary use of 
d312 1
a312 1
.Xr raid 4 
d344 1
a344 1
available as a 
d348 3
a350 3
file called 
.Pa rfconfig , 
raid device 0 can be configured with: 
d414 1
a414 1
Note that with the use of 
d417 1
a417 1
start a reconstruction, the 
d419 1
a419 1
option must be used.  (The 
d423 1
a423 1
option used later, on the same disk, if desired.)  
d435 1
a435 1
the reconstruction is progressing the 
d453 1
a453 1
is known to be good (i.e., the failure was either caused by 
d455 1
a455 1
or 
d457 2
a458 2
or the failed disk was replaced), then a copyback of the data can 
be initiated with the 
d496 1
a496 1
at this point.  Note that extreme care must be taken when 
d500 2
a501 2
in a configuration file should 
.Em never 
d504 1
a504 1
The final operation performed by 
d506 1
a506 1
is to unconfigure a 
d519 1
a519 1
RAID is 
d523 1
a523 1
Recomputation of parity 
d547 1
a547 1
Mellon University (CMU).  
@


1.2
log
@merge from netbsd 990213 + make compile with our includes
@
text
@d1 1
a1 1
.\"	$OpenBSD: raidctl.8,v 1.1 1999/01/11 14:49:44 niklas Exp $
d72 1
a72 1
.Nm
d74 1
a74 1
.Nm ""
d76 1
a76 1
.Nm ""
d78 1
a78 1
.Nm ""
d80 1
a80 1
.Nm ""
d82 1
a82 1
.Nm ""
d84 1
a84 1
.Nm ""
d86 1
a86 1
.Nm ""
d91 1
a91 1
.Xr raid 4 , Ns
d115 1
a115 1
it's original disk.  This is performed after a component has failed, 
d125 1
a125 1
of the device, and immediately begin a reconstruction of the failed
d130 1
a130 1
.Ar MUST
d151 7
a157 3
may be either the full name of the device, e.g. /dev/rraid0d,
for the i386 architecture, and /dev/rraid0c
for all others, or just simply raid0 (for /dev/rraid0d).
d167 1
a167 1
.Sq START , 
d171 1
a171 1
.Sq array
d181 2
a182 2
that although multi-dimenstional arrays may be specified, they are 
.Ar NOT
d186 1
a186 1
.Sq disks
d199 1
a199 1
.Sq failed , 
d201 2
a202 2
operate in degraded mode.  Note that it is 
.Ar imperative
d208 4
a211 5
The next section, which is the 
.Sq spare
section, is optional, and, if
present, specifies the devices to be used as 
.Sq hot spares
d215 1
a215 1
.Sq spare
d224 2
a225 2
.Sq spare
section may be ommitted.  
d228 1
a228 1
.Sq layout
d241 1
a241 1
factor; i.e. the number of contiguous sectors to be written to each
d271 1
a271 1
.Sq queue
d279 2
a280 2
where the queueing method is specified as fifo (first-in, first-out),
and the size of the per-component queue is limited to 1 requests.  A
d287 2
a288 1
.Sq debug section, is optional.  For more details
a292 1

a295 1

a296 1

d301 1
a301 1
.Ar all
d309 1
a309 1
The primary uses of 
d313 1
a313 1
devices.  To configure the device, a configuration
d337 9
a345 3
5 configuration consisting of the disks /dev/sd1e, /dev/sd2e, and
/dev/sd3e, with /dev/sd4e available as a 
.Sq hot spare
d349 1
a349 1
.Sq rfconfig , 
d360 4
a363 2
on the i386 architecture.  On all other architectures, /dev/rraid0c
is used in place of /dev/rraid0d.
d403 1
a403 1
were missing data is re-computed from existing data and the parity.
d451 3
a453 2
At this point there are at least two options.  First, if /dev/sd2e is
known to be good (i.e. the failure was either caused by 
d461 4
a464 2
/dev/sd4e to /dev/sd2e.  Once the copyback procedure is complete, the
status of the device would be:
d476 5
a480 2
The second option after the reconstruction is to simply use /dev/sd4e
in place of /dev/sd2e in the configuration file.  For example, the
d492 5
a496 2
This can be done as /dev/sd4e is completely interchangeable with
/dev/sd2e at this point.  Note that extreme care must be taken when 
d501 1
a501 1
.Ar never 
d520 1
a520 1
.Ar NOT
d524 1
a524 1
.Ar MUST
d537 1
a537 1
device special files.
d541 1
a542 1
.Xr ccd 4 ,
@


1.1
log
@Control RAIDframe
@
text
@d1 1
a1 1
.\"	$OpenBSD$
d3 1
d177 1
a177 1
that allthough multi-dimenstional arrays may be specified, they are 
@

