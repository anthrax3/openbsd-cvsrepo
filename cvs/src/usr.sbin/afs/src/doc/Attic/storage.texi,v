head	1.4;
access;
symbols
	OPENBSD_5_2:1.3.0.38
	OPENBSD_5_2_BASE:1.3
	OPENBSD_5_1_BASE:1.3
	OPENBSD_5_1:1.3.0.36
	OPENBSD_5_0:1.3.0.34
	OPENBSD_5_0_BASE:1.3
	OPENBSD_4_9:1.3.0.32
	OPENBSD_4_9_BASE:1.3
	OPENBSD_4_8:1.3.0.30
	OPENBSD_4_8_BASE:1.3
	OPENBSD_4_7:1.3.0.26
	OPENBSD_4_7_BASE:1.3
	OPENBSD_4_6:1.3.0.28
	OPENBSD_4_6_BASE:1.3
	OPENBSD_4_5:1.3.0.24
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.3.0.22
	OPENBSD_4_4_BASE:1.3
	OPENBSD_4_3:1.3.0.20
	OPENBSD_4_3_BASE:1.3
	OPENBSD_4_2:1.3.0.18
	OPENBSD_4_2_BASE:1.3
	OPENBSD_4_1:1.3.0.16
	OPENBSD_4_1_BASE:1.3
	OPENBSD_4_0:1.3.0.14
	OPENBSD_4_0_BASE:1.3
	OPENBSD_3_9:1.3.0.12
	OPENBSD_3_9_BASE:1.3
	OPENBSD_3_8:1.3.0.10
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.8
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.3.0.6
	OPENBSD_3_6_BASE:1.3
	OPENBSD_3_5:1.3.0.4
	OPENBSD_3_5_BASE:1.3
	OPENBSD_3_4:1.3.0.2
	OPENBSD_3_4_BASE:1.3
	arla-20030805:1.1.1.2
	OPENBSD_3_3:1.2.0.4
	OPENBSD_3_3_BASE:1.2
	OPENBSD_3_2:1.2.0.2
	OPENBSD_3_2_BASE:1.2
	arla-0-35-7:1.1.1.1
	arla:1.1.1
	OPENBSD_3_1:1.1.0.8
	OPENBSD_3_1_BASE:1.1
	OPENBSD_3_0:1.1.0.6
	OPENBSD_3_0_BASE:1.1
	OPENBSD_2_9_BASE:1.1
	OPENBSD_2_9:1.1.0.4
	OPENBSD_2_8:1.1.0.2
	OPENBSD_2_8_BASE:1.1;
locks; strict;
comment	@# @;


1.4
date	2012.08.23.06.21.51;	author deraadt;	state dead;
branches;
next	1.3;

1.3
date	2003.08.05.09.16.15;	author hin;	state Exp;
branches;
next	1.2;

1.2
date	2002.06.07.04.39.57;	author hin;	state Exp;
branches;
next	1.1;

1.1
date	2000.09.11.14.40.53;	author art;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2002.06.07.04.14.27;	author hin;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2003.08.05.08.21.02;	author hin;	state Exp;
branches;
next	;


desc
@@


1.4
log
@the afs src tree can go away
@
text
@@@c Copyright (c) 1998 - 2001 Kungliga Tekniska Högskolan
@@c (Royal Institute of Technology, Stockholm, Sweden).
@@c All rights reserved.

@@c $arla: storage.texi,v 1.9 2003/04/24 11:50:42 lha Exp $

@@node Organization of data, AFS and the real world, AFS infrastructure, Top
@@comment  node-name,  next,  previous,  up

@@chapter Organization of data

This chapter describes how data is stored and how AFS is different from,
for example, NFS. It also describes how data is kept consistent and what
the requirements were and how that inpacted on the design.

@@menu
* Requirements::  
* Data organization::
* Callbacks::
* Volume management::
* Relationship between pts uid and unix uid::
@@end menu

@@node Requirements, Data organization, Organization of data, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Requirements

@@itemize @@bullet
@@item Scalability

It should be possible to use AFS with hundred-thousands of users without
problems.

Writes that are done to different parts of the filesystem should not
affect each other. It should be possible to distribute out the reads and
writes over many fileservers. If you have a file that is accessed by
many clients, it should be possible to distribute the load.

@@comment What has this to do with requirements?
@@comment If there is multiple writes to the same file, are you sure that isn't a
@@comment database.

@@item Transparent to users

Users should not need to know where their files are stored. It should be
possible to move their files while they are using their files.

@@item Easy to admin

It should be easy for a administrator to make changes to the
filesystem. For example to change quota for a user or project. It should
also be possible to move the users data for a fileserver to a less
loaded one, or one with more diskspace available.

Some benefits of using AFS are:

@@itemize @@bullet
@@item user-transparent data migration
@@item an ability for on-line backups;
@@item data replication that provides both load balancing and robustness of
critical data
@@item global name space without automounters and other add-ons;
@@item @@@@sys variables for platform-independent paths to binary location;
@@item enhanced security;
@@item client-side caching;
@@end itemize
@@end itemize

@@section Anti-requirements

@@itemize @@bullet
@@item No databases

AFS isn't constructed for storing databases. It would be possible to use
AFS for storing a database if a layer above for locking and synchronizing
data would be provided.

One of the problems is that AFS doesn't include mandatory byte-range
locks. AFS uses advisory locking on whole files.

If you need a real database, use one, they are much more efficent on
solving a database problem. Don't use AFS.

@@end itemize

@@node Data organization, Callbacks, Requirements, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Volume

A volume is a unit that is smaller than a partition. It is usually (or should
be) a well defined area, like a user's home directory, a project work
area, or a program distribution.

Quota is controlled on volume-level. All day-to-day management is done
on volumes.

@@section Partition

In AFS a partition is what normally is named a partition. All partions
that afs is using are named a special way, @@file{/vicepNN}, where NN is
ranged from a to z, continuing with aa to zz. The fileserver (and
volser) automaticly picks upp all partitions starting with @@file{/vicep}

Volumes are stored in a partition. Volumes can't span several
partitions. Partitions are added when the fileserver is created or when
a new disk is added to a filesystem. 

@@section Volume cloning and read-only clones

A clone of a volume is often needed for volume operations. A clone is
a copy-on-write copy of a volume, the clone is the read-only version.

Two special versions of a clone are the read-only volume and the backup
volume. The read-only volume is a snapshot of a read-write volume (that
is what a clone is) that can be replicated to several fileservers to
distribute the load. Each fileserver plus partition where a read-only
clone is located is called a replication-site. It usually does not make
sense to have more than one read-only clone on each fileserver.

The backup volume is a clone that typically is made (with @@code{vos
backupsys}) each night to enable the user to retrieve yesterday's data
when they happen to remove a file. This is a very useful feature, since
it lessens the load on the system-administrators to restore files from
backup. The volume is usually mounted in the root user's home directory
under the name OldFiles. A special feature of the backup volume is that
you can't follow mountpoints out of a backup volume.

@@section Mountpoints

Volumes are independent of each other. To glue together the file tree
there are @@samp{mountpoint}s. Mountpoints are really symlinks that are
formated in a special way so that they point out a volume and an
optional cell. An AFS-cache-manager will show a mountpoint as directory
and in fact it will be the root directory of the target volume.

@@node Callbacks, Volume management, Data organization, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Callbacks

Callbacks are messages that enable the AFS-cache-manager to keep the
files without asking the server if there is newer version of the file.

A callback is a promise from the fileserver that it will notify the
client if the file (or directory) changes within the timelimit of the
callback.

For contents of read-only volumes there is only one callback per volume
called a volume callback and it will be broken when the read-only volume
is updated.

The time range of callbacks is from 5 to 60 minutes depending on
how many users of the file exist.

@@node Volume management, Relationship between pts uid and unix uid, Callbacks, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Volume management

All volume management is done with the @@code{vos} command. To get a list
of all commands @@code{vos help} can be used. For help on a specific vos
subcommand, @@code{vos subcommand -h} can be used.

@@itemize @@bullet
@@item Create

@@example
vos create mim c HO.staff.lha.fluff -quota 400000
@@end example

@@item Move

Volumes can be moved from a server to another, even when users are using
the volume.

@@item Replicate

Read-only volumes can be replicated over several servers, they are first
added with @@code{vos addsite}, and the replicated with @@code{vos
release} over the servers.

@@item Release

When you want to distribute the changes in the readwrite volume to the
read-only clones.

@@item Remove

Volumes can be removed

Note that you shouldn't remove the last readonly volume since this makes
clients misbehave. If you are moving the volume you should rather add a
new RO to the new server and then remove it from the old server.

@@item Backup and restoration of volumes.

@@code{vos backup} and @@code{vos backupsys} creates the backup volume.

To stream a volume out to a @@file{file} or @@file{stdout} you use
@@code{vos dump}. The opposite command is named @@code{vos restore}.

@@end itemize

@@node Relationship between pts uid and unix uid, , Volume management, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Relationship between pts uid and unix uid

@@cindex pts
@@cindex uid

Files in AFS are created with the pts uid of the token that was valid at
the time. The pts uid number is then by commands like @@code{ls -l}
interpreted as a unix uid and translated into a username. If the pts and
the unix uids differ, this might confuse the user as it looks like as
her files are owned by someone else. This is however not the case.
Complications can occur if programs impose further access restrictions
based on these wrongly interpreted uids instead of using the
@@code{access()} system call for that purpose. Graphical file browsers
are typically prone to that problem with the effect that the users are
not able to see their own files in these tools.
@


1.3
log
@Merge
@
text
@@


1.2
log
@merge
@
text
@d1 1
a1 1
@@c Copyright (c) 1998 - 2000 Kungliga Tekniska Högskolan
d5 1
a5 1
@@c $KTH: storage.texi,v 1.7 2000/11/25 11:23:56 lha Exp $
d12 3
a14 3
This chapter how data is stored and how AFS diffrent from, for example,
NFS. It also describes how data kept consistent and what the
requirements was and how that inpacted on the design.
d34 1
a34 1
Writes that are done to diffrent parts of the filesystem should not
d36 2
a37 2
writes over many file-servers. So if you have a file that is accessed by
many clients, it should be possible to distribute out the load.
d39 3
a41 2
If there is multiple writes to the same file, are you sure that isn't a
database.
d55 1
a55 1
Some benefits of using AFS is:
d75 2
a76 2
AFS for storing a database if a layer above provided locking and
synchronizing of data.
d90 1
a90 1
A volume is a unit that is smaller then a partition. Its usually (should
d94 1
a94 1
Quota is controlled on volume-level. All day-to-day management are done
d100 1
a100 1
that afs isusing is named a special way, @@file{/vicepNN}, where NN is
d102 1
a102 1
volser) automaticly picks upp all partition starting with @@file{/vicep}
d104 1
a104 1
Volumes are stored in a partition. Volumes can't overlap
d110 2
a111 2
A clone of volume is often needed for the volume operations. A clone is
copy-on-write copy of a volume, the clone is the read-only version.
d113 1
a113 1
A two special versions of a clone is the read-only volume and the backup
d115 4
a118 3
is what a clone is) that can be replicated to several fileserver to
distribute the load. Each fileserver plus partition where the read-only
is located is called a replication-site.
d121 1
a121 1
backupsys}) each night to enable the user to retrieve yestoday's data
d123 1
a123 1
it lessen the load on the system-administrators to restore files from
d126 1
a126 1
inside it you can't follow mountpoint.
d130 5
a134 5
The volumes are independent of each other. To clue the together there is
a @@samp{mountpoint}s. Mountpoints are really symlink that is formated a
special way that points out a volume (and a optional cell). A
AFS-cache-manager will show a mountpoint as directory, in fact it will
be the root directory of the target volume.
d140 2
a141 2
Callbacks are what enable the AFS-cache-manager to keep the files
without asking the server if there is newer version of the file.
d147 3
a149 2
For read-only callbacks there is only callback given its called a volume
callback and it will be broken when the read-only volume is updated.
d151 2
a152 2
The time range of callbacks range from 1 hour to 5 minutes depending of
how many user of the file exists.
d158 3
a160 3
All volume managment is done with the vos-command. To get a list of all
commands `vos help' can be used. For help on a specific vos subcommand,
`vos subcommand -h' can be used.
d178 1
a178 1
release} out over the servers.
d182 2
a183 1
When you want to distribute out the changes in the readwrite volume.
d189 1
a189 1
Note that you shouldn't remove the last readonly volume since this make
d206 2
a207 2
foo

d209 10
@


1.1
log
@New Arla userland from Arla between 0.34.2 and current in arla cvs.
Too many new features and fixes to mention here.
@
text
@d1 3
a3 1
@@c $Id: storage.texi,v 1.2 2000/08/13 13:59:37 lha Exp $
d5 3
a7 1
@@node Organization of data, Parts of Arla, Description of AFS infrastructure, Top
d21 1
d26 1
a26 1
@@heading Requirements
d68 1
a68 1
@@heading Anti-requirements
d87 1
a87 1
@@heading Volume
d96 1
a96 1
@@heading Partition
d107 1
a107 1
@@heading Volume cloning and read-only clones
d118 7
a124 4
The backup volume is a clone that typically is made each night to enable
the user to retrieve yestoday's data when they happen to remove a
file. This is a very useful feature, since it lessen the load on the
system-administrators to restore files from backup.
d126 1
a126 1
@@heading Mountpoints
d136 1
a136 1
@@heading Callbacks
d148 4
a151 1
@@node Volume management, , Callbacks, Organization of data
d153 5
a157 1
@@heading Volume management
d161 10
d172 5
d178 18
a195 2
@@item Delete
@@item Backup
d197 8
@


1.1.1.1
log
@Import of arla-0.35.7
@
text
@d1 1
a1 3
@@c Copyright (c) 1998 - 2000 Kungliga Tekniska Högskolan
@@c (Royal Institute of Technology, Stockholm, Sweden).
@@c All rights reserved.
d3 1
a3 3
@@c $KTH: storage.texi,v 1.7 2000/11/25 11:23:56 lha Exp $

@@node Organization of data, AFS and the real world, AFS infrastructure, Top
a16 1
* Relationship between pts uid and unix uid::
d21 1
a21 1
@@section Requirements
d63 1
a63 1
@@section Anti-requirements
d82 1
a82 1
@@section Volume
d91 1
a91 1
@@section Partition
d102 1
a102 1
@@section Volume cloning and read-only clones
d113 4
a116 7
The backup volume is a clone that typically is made (with @@code{vos
backupsys}) each night to enable the user to retrieve yestoday's data
when they happen to remove a file. This is a very useful feature, since
it lessen the load on the system-administrators to restore files from
backup. The volume is usually mounted in the root user's home directory
under the name OldFiles. A special feature of the backup volume is that
inside it you can't follow mountpoint.
d118 1
a118 1
@@section Mountpoints
d128 1
a128 1
@@section Callbacks
d140 1
a140 4
The time range of callbacks range from 1 hour to 5 minutes depending of
how many user of the file exists.

@@node Volume management, Relationship between pts uid and unix uid, Callbacks, Organization of data
d142 1
a142 5
@@section Volume management

All volume managment is done with the vos-command. To get a list of all
commands `vos help' can be used. For help on a specific vos subcommand,
`vos subcommand -h' can be used.
a145 10

@@example
vos create mim c HO.staff.lha.fluff -quota 400000
@@end example

@@item Move

Volumes can be moved from a server to another, even when users are using
the volume.

a146 5

Read-only volumes can be replicated over several servers, they are first
added with @@code{vos addsite}, and the replicated with @@code{vos
release} out over the servers.

d148 2
a149 18

When you want to distribute out the changes in the readwrite volume.

@@item Remove

Volumes can be removed

Note that you shouldn't remove the last readonly volume since this make
clients misbehave. If you are moving the volume you should rather add a
new RO to the new server and then remove it from the old server.

@@item Backup and restoration of volumes.

@@code{vos backup} and @@code{vos backupsys} creates the backup volume.

To stream a volume out to a @@file{file} or @@file{stdout} you use
@@code{vos dump}. The opposite command is named @@code{vos restore}.

a150 8

@@node Relationship between pts uid and unix uid, , Volume management, Organization of data
@@comment  node-name,  next,  previous,  up
@@section Relationship between pts uid and unix uid

foo


@


1.1.1.2
log
@Import of arla -current as of Aug 5 2003

ok todd@@ deraadt@@
@
text
@d1 1
a1 1
@@c Copyright (c) 1998 - 2001 Kungliga Tekniska Högskolan
d5 1
a5 1
@@c $arla: storage.texi,v 1.9 2003/04/24 11:50:42 lha Exp $
d12 3
a14 3
This chapter describes how data is stored and how AFS is different from,
for example, NFS. It also describes how data is kept consistent and what
the requirements were and how that inpacted on the design.
d34 1
a34 1
Writes that are done to different parts of the filesystem should not
d36 2
a37 2
writes over many fileservers. If you have a file that is accessed by
many clients, it should be possible to distribute the load.
d39 2
a40 3
@@comment What has this to do with requirements?
@@comment If there is multiple writes to the same file, are you sure that isn't a
@@comment database.
d54 1
a54 1
Some benefits of using AFS are:
d74 2
a75 2
AFS for storing a database if a layer above for locking and synchronizing
data would be provided.
d89 1
a89 1
A volume is a unit that is smaller than a partition. It is usually (or should
d93 1
a93 1
Quota is controlled on volume-level. All day-to-day management is done
d99 1
a99 1
that afs is using are named a special way, @@file{/vicepNN}, where NN is
d101 1
a101 1
volser) automaticly picks upp all partitions starting with @@file{/vicep}
d103 1
a103 1
Volumes are stored in a partition. Volumes can't span several
d109 2
a110 2
A clone of a volume is often needed for volume operations. A clone is
a copy-on-write copy of a volume, the clone is the read-only version.
d112 1
a112 1
Two special versions of a clone are the read-only volume and the backup
d114 3
a116 4
is what a clone is) that can be replicated to several fileservers to
distribute the load. Each fileserver plus partition where a read-only
clone is located is called a replication-site. It usually does not make
sense to have more than one read-only clone on each fileserver.
d119 1
a119 1
backupsys}) each night to enable the user to retrieve yesterday's data
d121 1
a121 1
it lessens the load on the system-administrators to restore files from
d124 1
a124 1
you can't follow mountpoints out of a backup volume.
d128 5
a132 5
Volumes are independent of each other. To glue together the file tree
there are @@samp{mountpoint}s. Mountpoints are really symlinks that are
formated in a special way so that they point out a volume and an
optional cell. An AFS-cache-manager will show a mountpoint as directory
and in fact it will be the root directory of the target volume.
d138 2
a139 2
Callbacks are messages that enable the AFS-cache-manager to keep the
files without asking the server if there is newer version of the file.
d145 2
a146 3
For contents of read-only volumes there is only one callback per volume
called a volume callback and it will be broken when the read-only volume
is updated.
d148 2
a149 2
The time range of callbacks is from 5 to 60 minutes depending on
how many users of the file exist.
d155 3
a157 3
All volume management is done with the @@code{vos} command. To get a list
of all commands @@code{vos help} can be used. For help on a specific vos
subcommand, @@code{vos subcommand -h} can be used.
d175 1
a175 1
release} over the servers.
d179 1
a179 2
When you want to distribute the changes in the readwrite volume to the
read-only clones.
d185 1
a185 1
Note that you shouldn't remove the last readonly volume since this makes
d202 2
a203 2
@@cindex pts
@@cindex uid
a204 10
Files in AFS are created with the pts uid of the token that was valid at
the time. The pts uid number is then by commands like @@code{ls -l}
interpreted as a unix uid and translated into a username. If the pts and
the unix uids differ, this might confuse the user as it looks like as
her files are owned by someone else. This is however not the case.
Complications can occur if programs impose further access restrictions
based on these wrongly interpreted uids instead of using the
@@code{access()} system call for that purpose. Graphical file browsers
are typically prone to that problem with the effect that the users are
not able to see their own files in these tools.
@


