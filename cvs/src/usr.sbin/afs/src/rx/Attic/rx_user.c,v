head	1.7;
access;
symbols
	OPENBSD_5_2:1.6.0.36
	OPENBSD_5_2_BASE:1.6
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.34
	OPENBSD_5_0:1.6.0.32
	OPENBSD_5_0_BASE:1.6
	OPENBSD_4_9:1.6.0.30
	OPENBSD_4_9_BASE:1.6
	OPENBSD_4_8:1.6.0.28
	OPENBSD_4_8_BASE:1.6
	OPENBSD_4_7:1.6.0.24
	OPENBSD_4_7_BASE:1.6
	OPENBSD_4_6:1.6.0.26
	OPENBSD_4_6_BASE:1.6
	OPENBSD_4_5:1.6.0.22
	OPENBSD_4_5_BASE:1.6
	OPENBSD_4_4:1.6.0.20
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.6.0.18
	OPENBSD_4_3_BASE:1.6
	OPENBSD_4_2:1.6.0.16
	OPENBSD_4_2_BASE:1.6
	OPENBSD_4_1:1.6.0.14
	OPENBSD_4_1_BASE:1.6
	OPENBSD_4_0:1.6.0.12
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.6.0.10
	OPENBSD_3_9_BASE:1.6
	OPENBSD_3_8:1.6.0.8
	OPENBSD_3_8_BASE:1.6
	OPENBSD_3_7:1.6.0.6
	OPENBSD_3_7_BASE:1.6
	OPENBSD_3_6:1.6.0.4
	OPENBSD_3_6_BASE:1.6
	OPENBSD_3_5:1.6.0.2
	OPENBSD_3_5_BASE:1.6
	OPENBSD_3_4:1.5.0.2
	OPENBSD_3_4_BASE:1.5
	arla-20030805:1.1.1.3
	OPENBSD_3_3:1.4.0.4
	OPENBSD_3_3_BASE:1.4
	OPENBSD_3_2:1.4.0.2
	OPENBSD_3_2_BASE:1.4
	arla-0-35-7:1.1.1.2
	OPENBSD_3_1:1.3.0.8
	OPENBSD_3_1_BASE:1.3
	OPENBSD_3_0:1.3.0.6
	OPENBSD_3_0_BASE:1.3
	OPENBSD_2_9_BASE:1.3
	OPENBSD_2_9:1.3.0.4
	OPENBSD_2_8:1.3.0.2
	OPENBSD_2_8_BASE:1.3
	OPENBSD_2_7:1.2.0.4
	OPENBSD_2_7_BASE:1.2
	OPENBSD_2_6:1.2.0.2
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.1.1.1.0.4
	OPENBSD_2_5_BASE:1.1.1.1
	OPENBSD_2_4:1.1.1.1.0.2
	OPENBSD_2_4_BASE:1.1.1.1
	arla-0_9:1.1.1.1
	arla:1.1.1;
locks; strict;
comment	@ * @;


1.7
date	2012.08.23.06.21.54;	author deraadt;	state dead;
branches;
next	1.6;

1.6
date	2003.12.16.20.13.56;	author beck;	state Exp;
branches;
next	1.5;

1.5
date	2003.08.05.09.11.12;	author hin;	state Exp;
branches;
next	1.4;

1.4
date	2002.06.07.04.43.30;	author hin;	state Exp;
branches;
next	1.3;

1.3
date	2000.09.11.14.41.23;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.04.30.01.59.15;	author art;	state Exp;
branches;
next	1.1;

1.1
date	98.09.14.21.53.17;	author art;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	98.09.14.21.53.17;	author art;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2002.06.07.04.14.35;	author hin;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2003.08.05.08.21.07;	author hin;	state Exp;
branches;
next	;


desc
@@


1.7
log
@the afs src tree can go away
@
text
@/*
****************************************************************************
*        Copyright IBM Corporation 1988, 1989 - All Rights Reserved        *
*                                                                          *
* Permission to use, copy, modify, and distribute this software and its    *
* documentation for any purpose and without fee is hereby granted,         *
* provided that the above copyright notice appear in all copies and        *
* that both that copyright notice and this permission notice appear in     *
* supporting documentation, and that the name of IBM not be used in        *
* advertising or publicity pertaining to distribution of the software      *
* without specific, written prior permission.                              *
*                                                                          *
* IBM DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL *
* IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL IBM *
* BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY      *
* DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER  *
* IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING   *
* OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.    *
****************************************************************************
*/

/*
 * rx_user.c contains routines specific to the user space UNIX
 * implementation of rx
 */

#include "rx_locl.h"

RCSID("$arla: rx_user.c,v 1.21 2003/04/08 22:14:04 lha Exp $");

#ifndef	IPPORT_USERRESERVED
/*
 * If in.h doesn't define this, define it anyway.  Unfortunately, defining
 * this doesn't put the code into the kernel to restrict kernel assigned
 * port numbers to numbers below IPPORT_USERRESERVED...
 */
#define IPPORT_USERRESERVED 5000
#endif

static osi_socket *rx_sockets = NULL;
static int num_rx_sockets = 0;

static fd_set rx_selectMask;
static int rx_maxSocketNumber = -1;    /* Maximum socket number represented
				        * in the select mask */
static PROCESS rx_listenerPid;	       /* LWP process id of socket listener
				        * process */
void rxi_Listener(void);

/*
 * This routine will get called by the event package whenever a new,
 * earlier than others, event is posted.  If the Listener process
 * is blocked in selects, this will unblock it.  It also can be called
 * to force a new trip through the rxi_Listener select loop when the set
 * of file descriptors it should be listening to changes...
 */
void
rxi_ReScheduleEvents(void)
{
    if (rx_listenerPid)
	IOMGR_Cancel(rx_listenerPid);
}

void
rxi_StartListener(void)
{
    /* Initialize LWP & IOMGR in case no one else has */
    PROCESS junk;

    LWP_InitializeProcessSupport(LWP_NORMAL_PRIORITY, &junk);
    IOMGR_Initialize();

    /* Priority of listener should be high, so it can keep conns alive */
#define	RX_LIST_STACK	24000
    LWP_CreateProcess(rxi_Listener, RX_LIST_STACK, LWP_MAX_PRIORITY, 0,
		      "rx_Listener", &rx_listenerPid);
}

/*
 * Called by rx_StartServer to start up lwp's to service calls.
 * NExistingProcs gives the number of procs already existing, and which
 * therefore needn't be created.
 */
void
rxi_StartServerProcs(int nExistingProcs)
{
    struct rx_service *service;
    int i;
    int maxdiff = 0;
    int nProcs = 0;
    PROCESS scratchPid;

    /*
     * For each service, reserve N processes, where N is the "minimum" number
     * of processes that MUST be able to execute a request in parallel, at
     * any time, for that process.  Also compute the maximum difference
     * between any service's maximum number of processes that can run (i.e.
     * the maximum number that ever will be run, and a guarantee that this
     * number will run if other services aren't running), and its minimum
     * number.  The result is the extra number of processes that we need in
     * order to provide the latter guarantee
     */
    for (i = 0; i < RX_MAX_SERVICES; i++) {
	int diff;

	service = rx_services[i];
	if (service == (struct rx_service *) 0)
	    break;
	nProcs += service->minProcs;
	diff = service->maxProcs - service->minProcs;
	if (diff > maxdiff)
	    maxdiff = diff;
    }
    nProcs += maxdiff;		       /* Extra processes needed to allow max
				        * number requested to run in any
				        * given service, under good
				        * conditions */
    nProcs -= nExistingProcs;	       /* Subtract the number of procs that
				        * were previously created for use as
				        * server procs */
    for (i = 0; i < nProcs; i++) {
	LWP_CreateProcess(rx_ServerProc, rx_stackSize, RX_PROCESS_PRIORITY, 0,
			  "rx_ServerProc", &scratchPid);
    }
}

/*
 * Make a socket for receiving/sending IP packets.  Set it into non-blocking
 * and large buffering modes.  If port isn't specified, the kernel will pick
 * one.  Returns the socket (>= 0) on success.  Returns OSI_NULLSOCKET on
 * failure.
 *
 * Port must be in network byte order.
 */

osi_socket
rxi_GetUDPSocket(uint16_t port, uint16_t *retport)
{
    int code;
    osi_socket socketFd = OSI_NULLSOCKET;
    osi_socket *sockets;
    struct sockaddr_in taddr;
    char *name = "rxi_GetUDPSocket: ";
    int sa_size;

    sockets = realloc(rx_sockets, (num_rx_sockets + 1) * sizeof(*rx_sockets));
    if (sockets == NULL) {
	perror("socket");
	osi_Msg(("%sunable to allocated memory for UDP socket\n", name));
	return OSI_NULLSOCKET;
    }
    rx_sockets = sockets;

    socketFd = socket(AF_INET, SOCK_DGRAM, 0);
    if (socketFd < 0) {
	perror("socket");
	osi_Msg(("%sunable to create UDP socket\n", name));
	return OSI_NULLSOCKET;
    }

    if (socketFd >= FD_SETSIZE) {
	osi_Msg(("socket fd too large\n"));
	close(socketFd);
	return OSI_NULLSOCKET;
    }

    rx_sockets[num_rx_sockets] = socketFd;
    num_rx_sockets++;

#ifdef SO_BSDCOMPAT
    {
	int one = 1;
	setsockopt (socketFd, SOL_SOCKET, SO_BSDCOMPAT, &one, sizeof(one));
    }
#endif    

    if (rx_maxSocketNumber < 0)
	FD_ZERO(&rx_selectMask);

    FD_SET(socketFd, &rx_selectMask);
    if (socketFd > rx_maxSocketNumber)
	rx_maxSocketNumber = socketFd;

    memset (&taddr, 0, sizeof(taddr));
    taddr.sin_family = AF_INET;
    taddr.sin_port   = port;

    code = bind(socketFd, (struct sockaddr *) &taddr, sizeof(taddr));
    if (code < 0) {
	perror("bind");
	osi_Msg(("%sunable to bind UDP socket\n", name));
	goto error;
    }

    sa_size = sizeof(taddr);
    code = getsockname(socketFd, (struct sockaddr *) &taddr, &sa_size);
    if (code < 0) {
	perror("getsockname");
	osi_Msg(("%sunable to bind UDP socket\n", name));
	goto error;
    }
    if (retport)
	*retport = taddr.sin_port;

    /*
     * Use one of three different ways of getting a socket buffer expanded to
     * a reasonable size
     */
    {
	int len1, len2;

	len1 = len2 = 32766;

	rx_stats.socketGreedy =
	    (setsockopt(socketFd, SOL_SOCKET, SO_SNDBUF,
			&len1, sizeof(len1)) >= 0) &&
	    (setsockopt(socketFd, SOL_SOCKET, SO_RCVBUF,
			&len2, sizeof(len2)) >= 0);
    }

    if (!rx_stats.socketGreedy)
	osi_Msg(("%s*WARNING* Unable to increase buffering on socket\n",name));

    /*
     * Put it into non-blocking mode so that rx_Listener can do a polling
     * read before entering select
     */
    if (fcntl(socketFd, F_SETFL, FNDELAY) == -1) {
	perror("fcntl");
	osi_Msg(("%sunable to set non-blocking mode on socket\n", name));
	goto error;
    }
    return socketFd;

error:
    num_rx_sockets--;
    rx_sockets[num_rx_sockets] = OSI_NULLSOCKET;

    close(socketFd);

    return OSI_NULLSOCKET;
}

/*
 * The main loop which listens to the net for datagrams, and handles timeouts
 * and retransmissions, etc.  It also is responsible for scheduling the
 * execution of pending events (in conjunction with event.c).
 *
 * Note interaction of nextPollTime and lastPollWorked.  The idea is
 * that if rx is not keeping up with the incoming stream of packets
 * (because there are threads that are interfering with its running
 * sufficiently often), rx does a polling select using IOMGR_Select
 * (setting tv_sec = tv_usec = 0). Old code is a system select, but
 * this was bad since we didn't know what calling conversion the
 * system select() was using (on win32 hosts it was PASCAL, and you
 * lost your $sp)
 *
 * So, our algorithm is that if the last poll on the file descriptor found
 * useful data, or we're at the time nextPollTime (which is advanced so that
 * it occurs every 3 or 4 seconds),
 * then we try the polling select.  If we eventually
 * catch up (which we can tell by the polling select returning no input
 * packets ready), then we don't do a polling select again until several
 * seconds later (via nextPollTime mechanism).
 */

#ifndef FD_COPY
#define FD_COPY(f, t)	memcpy((t), (f), sizeof(*(f)))
#endif

void
rxi_Listener(void)
{
    uint32_t host;
    uint16_t port;
    struct rx_packet *p = NULL;
    fd_set rfds;
    int socket;
    int fds;
    struct clock cv;
    long nextPollTime;		       /* time to next poll FD before
				        * sleeping */
    int lastPollWorked, doingPoll;     /* true iff last poll was useful */
    struct timeval tv, *tvp;

    clock_NewTime();
    lastPollWorked = 0;
    nextPollTime = 0;
    for (;;) {

	/*
	 * Grab a new packet only if necessary (otherwise re-use the old one)
	 */
	if (p == NULL) {
	    if ((p = rxi_AllocPacket(RX_PACKET_CLASS_RECEIVE)) == NULL)
		osi_Panic("rxi_Listener: no packets!");	/* Shouldn't happen */
	}
	/* Wait for the next event time or a packet to arrive. */
	/*
	 * event_RaiseEvents schedules any events whose time has come and
	 * then atomically computes the time to the next event, guaranteeing
	 * that this is positive.  If there is no next event, it returns 0
	 */
	if (!rxevent_RaiseEvents(&cv))
	    tvp = NULL;
	else {

	    /*
	     * It's important to copy cv to tv, because the 4.3 documentation
	     * for select threatens that *tv may be updated after a select,
	     * in future editions of the system, to indicate how much of the
	     * time period has elapsed.  So we shouldn't rely on tv not being
	     * altered.
	     */
	    tv.tv_sec = cv.sec;	       /* Time to next event */
	    tv.tv_usec = cv.usec;
	    tvp = &tv;
	}
	rx_stats.selects++;
	FD_COPY(&rx_selectMask, &rfds);
	if (lastPollWorked || nextPollTime < clock_Sec()) {
	    /* we're catching up, or haven't tried to for a few seconds */
	    doingPoll = 1;
	    nextPollTime = clock_Sec() + 4;	/* try again in 4 seconds no
						 * matter what */
	    tv.tv_sec = tv.tv_usec = 0;/* make sure we poll */
	    tvp = &tv;
	} else {
	    doingPoll = 0;
	}
	lastPollWorked = 0;	       /* default is that it didn't find
				        * anything */

	fds = IOMGR_Select (rx_maxSocketNumber + 1, &rfds, 0, 0, tvp);
	clock_NewTime();
	if (fds > 0) {
	    if (doingPoll)
		lastPollWorked = 1;

	    for (socket = 0; socket < num_rx_sockets; socket++) {
		if (p == NULL)
		    break;
		if (FD_ISSET(rx_sockets[socket], &rfds) &&
		    rxi_ReadPacket(rx_sockets[socket], p, &host, &port))
		{
		    p = rxi_ReceivePacket(p, rx_sockets[socket], host, port);
		}
	    }
	}
    }
    /* NOTREACHED */
}


void
osi_Panic(const char *fmt, ...)
{
    va_list ap;
    va_start(ap, fmt);
    fprintf(stderr, "Fatal Rx error: ");
    vfprintf(stderr, fmt, ap);
    va_end(ap);
    fflush(stderr);
    fflush(stdout);
    exit(-1);
}

void
osi_vMsg(const char *fmt, ...)
{
    va_list ap;
    va_start(ap, fmt);
    vfprintf(stderr, fmt, ap);
    va_end(ap);
    fflush(stderr);
}


#define	ADDRSPERSITE	256

#ifdef ADAPT_MTU

static u_long myNetAddrs[ADDRSPERSITE];
static int myNetMTUs[ADDRSPERSITE];
static int myNetFlags[ADDRSPERSITE];
static int numMyNetAddrs;

static void
GetIFInfo(void)
{
    int s;
    int len, res;
    struct ifconf ifc;
    struct ifreq ifs[ADDRSPERSITE];
    struct sockaddr_in *a;
    char *p;
    struct ifreq ifreq;
    size_t sz = 0;

    numMyNetAddrs = 0;
    memset(myNetAddrs, 0,  sizeof(myNetAddrs));
    memset(myNetMTUs, 0,  sizeof(myNetMTUs));
    memset(myNetFlags, 0,  sizeof(myNetFlags));

    s = socket(AF_INET, SOCK_DGRAM, 0);
    if (s < 0)
	return;

    ifc.ifc_len = sizeof(ifs);
    ifc.ifc_buf = (caddr_t) & ifs[0];
    memset(&ifs[0], 0,  sizeof(ifs));

    res = ioctl(s, SIOCGIFCONF, &ifc);
    if (res < 0) {
	close(s);
	return;
    }
    len = ifc.ifc_len / sizeof(struct ifreq);
    if (len > ADDRSPERSITE)
	len = ADDRSPERSITE;

    ifreq.ifr_name[0] = '\0';
    for (p = ifc.ifc_buf; p < ifc.ifc_buf + ifc.ifc_len; p += sz) {
	struct ifreq *ifr = (struct ifreq *)p;

	sz = sizeof(*ifr);
#ifdef SOCKADDR_HAS_SA_LEN
	sz = max(sz, sizeof(ifr->ifr_name) + ifr->ifr_addr.sa_len);
#endif
	  if (strncmp (ifreq.ifr_name,
		       ifr->ifr_name,
		       sizeof(ifr->ifr_name))) {
	      res = ioctl(s, SIOCGIFFLAGS, ifr);
	      if (res < 0)
		  continue;
	      if (!(ifr->ifr_flags & IFF_UP))
		  continue;
	      if (ifr->ifr_flags & IFF_LOOPBACK)
		  continue;
	      myNetFlags[numMyNetAddrs] = ifr->ifr_flags;

	      res = ioctl(s, SIOCGIFADDR, ifr);
	      if (res < 0)
		  continue;
	      a = (struct sockaddr_in *)&ifr->ifr_addr;
	      if (a->sin_family != AF_INET)
		  continue;
	      myNetAddrs[numMyNetAddrs] = ntohl(a->sin_addr.s_addr);

	      res = -1;
#ifdef SIOCGIFMTU
	      res = ioctl(s, SIOCGIFMTU, ifr);
#elif SIOCRIFMTU
	      res = ioctl(s, SIOCRIFMTU, ifr);
#else
	      res = -1;
#endif
	      if (res == 0) {
		  myNetMTUs[numMyNetAddrs] = ifr->ifr_metric;
		  if (rx_maxReceiveSize < (myNetMTUs[numMyNetAddrs]
					   - RX_IPUDP_SIZE))
		      rx_maxReceiveSize = MIN(RX_MAX_PACKET_SIZE,
					      (myNetMTUs[numMyNetAddrs]
					       - RX_IPUDP_SIZE));

		  if (rx_MyMaxSendSize < myNetMTUs[numMyNetAddrs]
					   - RX_IPUDP_SIZE)
		      rx_MyMaxSendSize = myNetMTUs[numMyNetAddrs]
			  - RX_IPUDP_SIZE;
	      
	      } else {
		  myNetMTUs[numMyNetAddrs] = OLD_MAX_PACKET_SIZE;
		  res = 0;
	      }
	      ++numMyNetAddrs;
	      ifreq = *ifr;
	  }
    }

	{
#if 0
	    RETSIGTYPE (*old)(int);

	    old = signal(SIGSYS, SIG_IGN);
	    if (syscall(31 /* AFS_SYSCALL */ , 28 /* AFSCALL_CALL */ ,
			20 /* AFSOP_GETMTU */ , myNetAddrs[numMyNetAddrs],
			&(myNetMTUs[numMyNetAddrs])));
	    myNetMTUs[numMyNetAddrs] = OLD_MAX_PACKET_SIZE;
	    signal(SIGSYS, old);
#endif
	}

    close(s);

    /*
     * have to allocate at least enough to allow a single packet to reach its
     * maximum size, so ReadPacket will work.  Allocate enough for a couple
     * of packets to do so, for good measure
     */
    /* MTUXXX before shipping, change this 8 to a 4 */
    {
	int npackets, ncbufs;

	ncbufs = (rx_maxReceiveSize - RX_FIRSTBUFFERSIZE);
	if (ncbufs > 0) {
	    ncbufs = ncbufs / RX_CBUFFERSIZE;
	    npackets = (rx_Window / 8);
	    npackets = (npackets > 2 ? npackets : 2);
	    rxi_MoreCbufs(npackets * (ncbufs + 1));
	}
    }
}

#endif				       /* ADAPT_MTU */

/*
 * Called from rxi_FindPeer, when initializing a clear rx_peer structure,
 * to get interesting information.
 */

void
rxi_InitPeerParams(struct rx_peer * pp)
{
    uint32_t ppaddr, msk, net;
    int rxmtu;
    int ix, nlix = 0, nlcount;
    static int Inited = 0;

#ifdef ADAPT_MTU

    if (!Inited) {
	GetIFInfo();
	Inited = 1;
    }
    /*
     * try to second-guess IP, and identify which link is most likely to
     * be used for traffic to/from this host.
     */
    ppaddr = ntohl(pp->host);
    if (IN_CLASSA(ppaddr))
	msk = IN_CLASSA_NET;
    else if (IN_CLASSB(ppaddr))
	msk = IN_CLASSB_NET;
    else if (IN_CLASSC(ppaddr))
	msk = IN_CLASSC_NET;
    else
	msk = 0;
    net = ppaddr & msk;

    for (nlcount = 0, ix = 0; ix < numMyNetAddrs; ++ix) {
#ifdef IFF_LOOPBACK
	if (!(myNetFlags[ix] & IFF_LOOPBACK)) {
	    nlix = ix;
	    ++nlcount;
	}
#endif				       /* IFF_LOOPBACK */
	if ((myNetAddrs[ix] & msk) == net)
	    break;
    }

    pp->rateFlag = 2;		       /* start timing after two full packets */
    /*
     * I don't initialize these, because I presume they are bzero'd...
     * pp->burstSize pp->burst pp->burstWait.sec pp->burstWait.usec
     * pp->timeout.usec
     */

    pp->maxWindow = rx_Window;
    if (ix >= numMyNetAddrs) {	       /* not local */
	pp->timeout.sec = 3;
	pp->packetSize = RX_REMOTE_PACKET_SIZE;
    } else {
	pp->timeout.sec = 2;
	pp->packetSize = MIN(RX_MAX_PACKET_SIZE,
			     (rx_MyMaxSendSize + RX_HEADER_SIZE));
    }

    /* Now, maybe get routing interface and override parameters. */
    if (ix >= numMyNetAddrs && nlcount == 1)
	ix = nlix;

    if (ix < numMyNetAddrs) {
#ifdef IFF_POINTOPOINT
	if (myNetFlags[ix] & IFF_POINTOPOINT) {
	    /* wish we knew the bit rate and the chunk size, sigh. */
	    pp->maxWindow = 10;
	    pp->timeout.sec = 4;
	    /* pp->timeout.usec = 0; */
	    pp->packetSize = RX_PP_PACKET_SIZE;
	}
#endif				       /* IFF_POINTOPOINT */

	/*
	 * Reduce the packet size to one based on the MTU given by the
	 * interface.
	 */
	if (myNetMTUs[ix] > (RX_IPUDP_SIZE + RX_HEADER_SIZE)) {
	    rxmtu = myNetMTUs[ix] - RX_IPUDP_SIZE;
	    if (rxmtu < pp->packetSize)
		pp->packetSize = rxmtu;
	}
    }
#else				       /* ADAPT_MTU */
    pp->rateFlag = 2;		       /* start timing after two full packets */
    pp->maxWindow = rx_Window;
    pp->timeout.sec = 2;
    pp->packetSize = OLD_MAX_PACKET_SIZE;
#endif				       /* ADAPT_MTU */
}
@


1.6
log
@afs cleanup, realloc strings, abort, etc.
testing by beck, hin, jose, fries. ok deraadt@@
@
text
@@


1.5
log
@Merge
@
text
@d365 1
a365 1
    abort();
@


1.4
log
@merge
@
text
@d29 1
a29 1
RCSID("$KTH: rx_user.c,v 1.16 2000/10/08 17:49:48 assar Exp $");
d40 2
a41 7
/* Don't set this to anything else right now; it is currently broken */
static int (*rx_select) (int, fd_set *, fd_set *, 
		  fd_set *, struct timeval *) = IOMGR_Select;
                                       /* 
					* Can be set to "select", in some
				        * cases 
					*/
d87 2
a88 2
    register struct rx_service *service;
    register int i;
d137 1
a137 1
rxi_GetUDPSocket(u_short port)
d140 2
a141 1
    int socketFd = OSI_NULLSOCKET;
d144 9
d157 1
a157 1
	(osi_Msg "%sunable to create UDP socket\n", name);
d162 2
a163 1
	(osi_Msg "socket fd too large\n");
d167 3
d177 1
a177 1
    if (rx_maxSocketNumber < 0) {
a178 1
    }
d188 1
a188 1
    code = bind(socketFd, (const struct sockaddr *) &taddr, sizeof(taddr));
d191 1
a191 1
	(osi_Msg "%sunable to bind UDP socket\n", name);
d195 10
a220 1
/*#ifdef notdef*/
d222 1
a222 2
	(osi_Msg "%s*WARNING* Unable to increase buffering on socket\n", name);
/*#endif*/
d230 1
a230 1
	(osi_Msg "%sunable to set non-blocking mode on socket\n", name);
d236 5
a240 2
    if (socketFd >= 0)
	close(socketFd);
d249 8
a256 6
 * Note interaction of nextPollTime and lastPollWorked.  The idea is that
 * if rx is not keeping up with the incoming stream of packets (because
 * there are threads that are interfering with its running sufficiently
 * often), rx does a polling select before doing a real IOMGR_Select system
 * call.  Doing a real select means that we don't have to let other processes
 * run before processing more packets.
d261 1
a261 1
 * then we try the polling select before the IOMGR_Select.  If we eventually
d274 3
a276 3
    u_long host;
    u_short port;
    register struct rx_packet *p = (struct rx_packet *) 0;
d294 2
a295 2
	if (!p) {
	    if (!(p = rxi_AllocPacket(RX_PACKET_CLASS_RECEIVE)))
d305 1
a305 1
	    tvp = (struct timeval *) 0;
a322 1
	    rx_select = select;
a328 1
	    rx_select = IOMGR_Select;
d334 1
a334 1
	fds = (*rx_select) (rx_maxSocketNumber + 1, &rfds, 0, 0, tvp);
d339 3
a341 2
	    for(socket = 0; socket < rx_maxSocketNumber + 1; ++socket) {
		if(p == NULL)
d343 4
a346 3
		if(FD_ISSET(socket, &rfds) && 
		   rxi_ReadPacket(socket, p, &host, &port)) {
		    p = rxi_ReceivePacket(p, socket, host, port);
a367 16
#ifdef	AFS_AIX32_ENV
#ifndef osi_Alloc
static char memZero;
char *
osi_Alloc(long x)
{

    /*
     * 0-length allocs may return NULL ptr from osi_kalloc, so we
     * special-case things so that NULL returned iff an error occurred
     */
    if (x == 0)
	return &memZero;
    return ((char *) malloc(x));
}

d369 1
a369 1
osi_Free(char *x, long size)
d371 5
a375 3
    if (x == &memZero)
	return;
    free((char *) x);
a377 2
#endif
#endif				       /* AFS_AIX32_ENV */
d438 2
d465 6
d522 1
a522 1
rxi_InitPeerParams(register struct rx_peer * pp)
d524 2
a525 2
    register u_long ppaddr, msk, net;
    u_short rxmtu;
@


1.3
log
@New Arla userland from Arla between 0.34.2 and current in arla cvs.
Too many new features and fixes to mention here.
@
text
@d29 1
a29 1
RCSID("$Id: rx_user.c,v 1.14 1999/03/05 04:18:16 assar Exp $");
d41 1
a41 1
int (*rx_select) (int, fd_set *, fd_set *, 
d48 4
a51 3
fd_set rx_selectMask = { {0,0,0,0,0,0,0,0} }; /* XXX */

PROCESS rx_listenerPid;		       /* LWP process id of socket listener
d156 5
d168 4
d374 1
a374 1
#define	ADDRSPERSITE	16
@


1.2
log
@upgrade Arla to fresher code. Too many new features and bugfixes.
@
text
@a0 1
/*	$OpenBSD$	*/
d29 1
a29 1
RCSID("$KTH: rx_user.c,v 1.14 1999/03/05 04:18:16 assar Exp $");
@


1.1
log
@Initial revision
@
text
@d30 1
a30 1
RCSID("$KTH: rx_user.c,v 1.10 1998/04/08 05:19:34 lha Exp $");
d144 2
a145 2
    int binds, code;
    register int socketFd = OSI_NULLSOCKET;
d149 5
a153 6
#if 0				       /* We dont want to have this error
				        * message, don't bother us */
    if (port >= IPPORT_RESERVED && port < IPPORT_USERRESERVED) {
	(osi_Msg "%s*WARNING* port number %d is not a reserved port number."
	 "Use port numbers above %d\n",
	 name, port, IPPORT_USERRESERVED);
a154 1
#endif				       /* 0 */
d156 4
a159 5
    if (port > 0 && port < IPPORT_RESERVED && geteuid() != 0) {
	(osi_Msg "%sport number %d is a reserved port number which may only"
	 " be used by root.  Use port numbers above %d\n",
	 name, port, IPPORT_USERRESERVED);
	goto error;
d161 1
a161 1
    socketFd = socket(AF_INET, SOCK_DGRAM, 0);
a162 5
    /* IOMGR_Select doesn't deal with arbitrarily large select masks */
    if (socketFd > 31) {
	(osi_Msg "%ssocket descriptor > 31\n", name);
	goto error;
    }
d167 1
a167 1
    taddr.sin_addr.s_addr = 0;
d169 4
a172 9
    taddr.sin_port = port;
#define MAX_RX_BINDS 10
    for (binds = 0; binds < MAX_RX_BINDS; binds++) {
	code = bind(socketFd, (const struct sockaddr *) & taddr, sizeof(taddr));
	if (!code)
	    break;
	sleep(10);
    }
    if (code) {
d174 1
a174 1
	(osi_Msg "%sbind failed\n", name);
d306 1
a306 1
	fds = (*rx_select) (FD_SETSIZE, &rfds, 0, 0, tvp);
d311 1
a311 1
	    for(socket = 0; socket < FD_SETSIZE; ++socket) {
@


1.1.1.1
log
@Initial version of Arla, a free implementation of an AFS cache-manager.
(second try, hopefully into the correct repository this time)
@
text
@@


1.1.1.2
log
@Import of arla-0.35.7
@
text
@d1 1
d30 1
a30 1
RCSID("$KTH: rx_user.c,v 1.16 2000/10/08 17:49:48 assar Exp $");
d42 1
a42 1
static int (*rx_select) (int, fd_set *, fd_set *, 
d49 3
a51 4
static fd_set rx_selectMask;
static int rx_maxSocketNumber = -1;    /* Maximum socket number represented
				        * in the select mask */
static PROCESS rx_listenerPid;	       /* LWP process id of socket listener
d144 2
a145 2
    int code;
    int socketFd = OSI_NULLSOCKET;
d149 6
a154 5
    socketFd = socket(AF_INET, SOCK_DGRAM, 0);
    if (socketFd < 0) {
	perror("socket");
	(osi_Msg "%sunable to create UDP socket\n", name);
	return OSI_NULLSOCKET;
d156 1
d158 5
a162 3
    if (socketFd >= FD_SETSIZE) {
	(osi_Msg "socket fd too large\n");
	return OSI_NULLSOCKET;
d164 1
d166 4
a169 4
#ifdef SO_BSDCOMPAT
    {
	int one = 1;
	setsockopt (socketFd, SOL_SOCKET, SO_BSDCOMPAT, &one, sizeof(one));
a170 6
#endif    

    if (rx_maxSocketNumber < 0) {
	FD_ZERO(&rx_selectMask);
    }

d175 1
a175 1
    memset (&taddr, 0, sizeof(taddr));
d177 9
a185 4
    taddr.sin_port   = port;

    code = bind(socketFd, (const struct sockaddr *) &taddr, sizeof(taddr));
    if (code < 0) {
d187 1
a187 1
	(osi_Msg "%sunable to bind UDP socket\n", name);
d319 1
a319 1
	fds = (*rx_select) (rx_maxSocketNumber + 1, &rfds, 0, 0, tvp);
d324 1
a324 1
	    for(socket = 0; socket < rx_maxSocketNumber + 1; ++socket) {
d378 1
a378 1
#define	ADDRSPERSITE	256
@


1.1.1.3
log
@Import of arla -current as of Aug 5 2003

ok todd@@ deraadt@@
@
text
@d29 1
a29 1
RCSID("$arla: rx_user.c,v 1.21 2003/04/08 22:14:04 lha Exp $");
d40 7
a46 2
static osi_socket *rx_sockets = NULL;
static int num_rx_sockets = 0;
d92 2
a93 2
    struct rx_service *service;
    int i;
d142 1
a142 1
rxi_GetUDPSocket(uint16_t port, uint16_t *retport)
d145 1
a145 2
    osi_socket socketFd = OSI_NULLSOCKET;
    osi_socket *sockets;
a147 9
    int sa_size;

    sockets = realloc(rx_sockets, (num_rx_sockets + 1) * sizeof(*rx_sockets));
    if (sockets == NULL) {
	perror("socket");
	osi_Msg(("%sunable to allocated memory for UDP socket\n", name));
	return OSI_NULLSOCKET;
    }
    rx_sockets = sockets;
d152 1
a152 1
	osi_Msg(("%sunable to create UDP socket\n", name));
d157 1
a157 2
	osi_Msg(("socket fd too large\n"));
	close(socketFd);
a160 3
    rx_sockets[num_rx_sockets] = socketFd;
    num_rx_sockets++;

d168 1
a168 1
    if (rx_maxSocketNumber < 0)
d170 1
d180 1
a180 1
    code = bind(socketFd, (struct sockaddr *) &taddr, sizeof(taddr));
d183 1
a183 1
	osi_Msg(("%sunable to bind UDP socket\n", name));
a186 10
    sa_size = sizeof(taddr);
    code = getsockname(socketFd, (struct sockaddr *) &taddr, &sa_size);
    if (code < 0) {
	perror("getsockname");
	osi_Msg(("%sunable to bind UDP socket\n", name));
	goto error;
    }
    if (retport)
	*retport = taddr.sin_port;

d203 1
d205 2
a206 1
	osi_Msg(("%s*WARNING* Unable to increase buffering on socket\n",name));
d214 1
a214 1
	osi_Msg(("%sunable to set non-blocking mode on socket\n", name));
d220 2
a221 5
    num_rx_sockets--;
    rx_sockets[num_rx_sockets] = OSI_NULLSOCKET;

    close(socketFd);

d230 6
a235 8
 * Note interaction of nextPollTime and lastPollWorked.  The idea is
 * that if rx is not keeping up with the incoming stream of packets
 * (because there are threads that are interfering with its running
 * sufficiently often), rx does a polling select using IOMGR_Select
 * (setting tv_sec = tv_usec = 0). Old code is a system select, but
 * this was bad since we didn't know what calling conversion the
 * system select() was using (on win32 hosts it was PASCAL, and you
 * lost your $sp)
d240 1
a240 1
 * then we try the polling select.  If we eventually
d253 3
a255 3
    uint32_t host;
    uint16_t port;
    struct rx_packet *p = NULL;
d273 2
a274 2
	if (p == NULL) {
	    if ((p = rxi_AllocPacket(RX_PACKET_CLASS_RECEIVE)) == NULL)
d284 1
a284 1
	    tvp = NULL;
d302 1
d309 1
d315 1
a315 1
	fds = IOMGR_Select (rx_maxSocketNumber + 1, &rfds, 0, 0, tvp);
d320 2
a321 3

	    for (socket = 0; socket < num_rx_sockets; socket++) {
		if (p == NULL)
d323 3
a325 4
		if (FD_ISSET(rx_sockets[socket], &rfds) &&
		    rxi_ReadPacket(rx_sockets[socket], p, &host, &port))
		{
		    p = rxi_ReceivePacket(p, rx_sockets[socket], host, port);
d347 16
d364 1
a364 1
osi_vMsg(const char *fmt, ...)
d366 3
a368 5
    va_list ap;
    va_start(ap, fmt);
    vfprintf(stderr, fmt, ap);
    va_end(ap);
    fflush(stderr);
d371 2
a432 2
	      if (ifr->ifr_flags & IFF_LOOPBACK)
		  continue;
a457 6

		  if (rx_MyMaxSendSize < myNetMTUs[numMyNetAddrs]
					   - RX_IPUDP_SIZE)
		      rx_MyMaxSendSize = myNetMTUs[numMyNetAddrs]
			  - RX_IPUDP_SIZE;
	      
d509 1
a509 1
rxi_InitPeerParams(struct rx_peer * pp)
d511 2
a512 2
    uint32_t ppaddr, msk, net;
    int rxmtu;
@


