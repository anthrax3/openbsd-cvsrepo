head	1.27;
access;
symbols
	OPENBSD_6_1:1.26.0.4
	OPENBSD_6_1_BASE:1.26
	OPENBSD_6_0:1.24.0.2
	OPENBSD_6_0_BASE:1.24
	OPENBSD_5_9:1.23.0.2
	OPENBSD_5_9_BASE:1.23
	NSD_4_1_6:1.1.1.17
	OPENBSD_5_8:1.20.0.4
	OPENBSD_5_8_BASE:1.20
	NSD_4_1_3:1.1.1.16
	OPENBSD_5_7:1.19.0.2
	OPENBSD_5_7_BASE:1.19
	NSD_4_1_1:1.1.1.15
	NSD_4_1_0:1.1.1.14
	OPENBSD_5_6:1.15.0.4
	OPENBSD_5_6_BASE:1.15
	NSD_4_0_3:1.1.1.13
	NSD_4_0_2:1.1.1.12
	OPENBSD_5_5:1.12.0.4
	OPENBSD_5_5_BASE:1.12
	NSD_4_0_1:1.1.1.11
	NSD_4_0_0:1.1.1.10
	NSD_3_2_16:1.1.1.9
	OPENBSD_5_4:1.8.0.4
	OPENBSD_5_4_BASE:1.8
	OPENBSD_5_3:1.8.0.2
	OPENBSD_5_3_BASE:1.8
	NSD_3_2_15:1.1.1.8
	NSD_3_2_14:1.1.1.7
	NSD_3_2_13:1.1.1.6
	OPENBSD_5_2:1.5.0.2
	OPENBSD_5_2_BASE:1.5
	NSD_3_2_11:1.1.1.5
	NSD_3_2_10:1.1.1.4
	OPENBSD_5_1_BASE:1.4
	OPENBSD_5_1:1.4.0.2
	NSD_3_2_9:1.1.1.4
	OPENBSD_5_0:1.3.0.2
	OPENBSD_5_0_BASE:1.3
	NSD_3_2_8:1.1.1.3
	OPENBSD_4_9:1.2.0.2
	OPENBSD_4_9_BASE:1.2
	NSD_3_2_6:1.1.1.2
	OPENBSD_4_8:1.1.1.2.0.2
	OPENBSD_4_8_BASE:1.1.1.2
	NSD_3_2_5:1.1.1.2
	OPENBSD_4_7:1.1.1.1.0.2
	OPENBSD_4_7_BASE:1.1.1.1
	NSD_3_2_4:1.1.1.1
	NLNETLABS:1.1.1;
locks; strict;
comment	@ * @;


1.27
date	2017.04.15.09.15.45;	author florian;	state Exp;
branches;
next	1.26;
commitid	fXBqT4bSlfjTKAVE;

1.26
date	2017.02.17.20.04.45;	author florian;	state Exp;
branches;
next	1.25;
commitid	WmSuN5M3Jbe54113;

1.25
date	2016.08.31.07.31.20;	author florian;	state Exp;
branches;
next	1.24;
commitid	1gMwFuybgH2l617m;

1.24
date	2016.06.24.08.34.03;	author florian;	state Exp;
branches;
next	1.23;
commitid	Z9jVKJJMPmC3zw2t;

1.23
date	2015.12.11.12.28.49;	author sthen;	state Exp;
branches;
next	1.22;
commitid	e67Hx722TeF4XpDl;

1.22
date	2015.11.21.21.12.46;	author florian;	state Exp;
branches;
next	1.21;
commitid	bnZGtyogtzopELoz;

1.21
date	2015.11.05.21.21.59;	author sthen;	state Exp;
branches;
next	1.20;
commitid	EVDZ1z8bzmTorwSZ;

1.20
date	2015.07.17.17.36.33;	author sthen;	state Exp;
branches;
next	1.19;
commitid	2WtkENMkCz3GhSbE;

1.19
date	2015.02.03.10.40.02;	author brad;	state Exp;
branches;
next	1.18;
commitid	MI9j0d0LR8Dk9lnd;

1.18
date	2014.12.18.23.26.13;	author brad;	state Exp;
branches;
next	1.17;
commitid	rBaz3dZHAQwIDT3j;

1.17
date	2014.11.18.20.54.28;	author krw;	state Exp;
branches;
next	1.16;
commitid	yCis8OrOsfixbKpI;

1.16
date	2014.09.16.17.01.38;	author brad;	state Exp;
branches;
next	1.15;
commitid	5uD1zN2z8VskC3BN;

1.15
date	2014.05.04.12.24.26;	author kettenis;	state Exp;
branches;
next	1.14;

1.14
date	2014.03.14.16.03.54;	author sthen;	state Exp;
branches;
next	1.13;

1.13
date	2014.03.13.02.09.34;	author brad;	state Exp;
branches;
next	1.12;

1.12
date	2014.02.04.03.07.25;	author brad;	state Exp;
branches;
next	1.11;

1.11
date	2013.11.26.12.53.58;	author sthen;	state Exp;
branches;
next	1.10;

1.10
date	2013.09.04.12.24.35;	author sthen;	state Exp;
branches;
next	1.9;

1.9
date	2013.09.03.09.26.56;	author sthen;	state Exp;
branches;
next	1.8;

1.8
date	2013.02.18.10.18.39;	author sthen;	state Exp;
branches;
next	1.7;

1.7
date	2012.11.23.20.30.26;	author sthen;	state Exp;
branches;
next	1.6;

1.6
date	2012.08.28.15.15.02;	author sthen;	state Exp;
branches;
next	1.5;

1.5
date	2012.07.09.21.56.41;	author sthen;	state Exp;
branches;
next	1.4;

1.4
date	2012.01.29.11.23.24;	author jakob;	state Exp;
branches;
next	1.3;

1.3
date	2011.05.21.18.29.56;	author jakob;	state Exp;
branches;
next	1.2;

1.2
date	2011.01.27.12.29.14;	author jakob;	state Exp;
branches;
next	1.1;

1.1
date	2010.01.15.19.24.59;	author jakob;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2010.01.15.19.24.59;	author jakob;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2010.04.15.20.57.12;	author jakob;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2011.05.21.18.17.24;	author jakob;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2012.01.29.11.15.40;	author jakob;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2012.07.09.21.55.07;	author sthen;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2012.08.28.15.11.42;	author sthen;	state Exp;
branches;
next	1.1.1.7;

1.1.1.7
date	2012.11.23.20.29.13;	author sthen;	state Exp;
branches;
next	1.1.1.8;

1.1.1.8
date	2013.02.18.10.17.41;	author sthen;	state Exp;
branches;
next	1.1.1.9;

1.1.1.9
date	2013.09.03.09.21.34;	author sthen;	state Exp;
branches;
next	1.1.1.10;

1.1.1.10
date	2013.11.26.12.50.11;	author sthen;	state Exp;
branches;
next	1.1.1.11;

1.1.1.11
date	2014.02.04.01.54.01;	author brad;	state Exp;
branches;
next	1.1.1.12;

1.1.1.12
date	2014.03.13.02.00.23;	author brad;	state Exp;
branches;
next	1.1.1.13;

1.1.1.13
date	2014.03.14.16.01.44;	author sthen;	state Exp;
branches;
next	1.1.1.14;

1.1.1.14
date	2014.09.16.16.53.59;	author brad;	state Exp;
branches;
next	1.1.1.15;
commitid	BWSdZeElrpYSRdME;

1.1.1.15
date	2015.02.03.10.24.28;	author brad;	state Exp;
branches;
next	1.1.1.16;
commitid	yn8l9RVkmdMVYIfl;

1.1.1.16
date	2015.07.17.17.36.01;	author sthen;	state Exp;
branches;
next	1.1.1.17;
commitid	Jeq5uGP63ff7nS9K;

1.1.1.17
date	2015.11.05.21.21.03;	author sthen;	state Exp;
branches;
next	;
commitid	RXfGmPAh2IZTjnIf;


desc
@@


1.27
log
@update to 4.1.16rc1
tests & OK sthen

(if there are more changes coming for 4.1.16 release we will just
commit them on top)
@
text
@/*
 * server.c -- nsd(8) network input/output
 *
 * Copyright (c) 2001-2006, NLnet Labs. All rights reserved.
 *
 * See LICENSE for the license.
 *
 */

#include "config.h"

#include <sys/types.h>
#include <sys/param.h>
#include <sys/socket.h>
#include <sys/uio.h>
#include <sys/wait.h>

#include <netinet/in.h>
#include <arpa/inet.h>

#include <assert.h>
#include <ctype.h>
#include <errno.h>
#include <fcntl.h>
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include <unistd.h>
#include <signal.h>
#include <netdb.h>
#include <poll.h>
#ifndef SHUT_WR
#define SHUT_WR 1
#endif
#ifdef HAVE_MMAP
#include <sys/mman.h>
#endif /* HAVE_MMAP */
#ifdef HAVE_OPENSSL_RAND_H
#include <openssl/rand.h>
#endif
#ifndef USE_MINI_EVENT
#  ifdef HAVE_EVENT_H
#    include <event.h>
#  else
#    include <event2/event.h>
#    include "event2/event_struct.h"
#    include "event2/event_compat.h"
#  endif
#else
#  include "mini_event.h"
#endif

#include "axfr.h"
#include "namedb.h"
#include "netio.h"
#include "xfrd.h"
#include "xfrd-tcp.h"
#include "xfrd-disk.h"
#include "difffile.h"
#include "nsec3.h"
#include "ipc.h"
#include "udb.h"
#include "remote.h"
#include "lookup3.h"
#include "rrl.h"

#define RELOAD_SYNC_TIMEOUT 25 /* seconds */

/*
 * Data for the UDP handlers.
 */
struct udp_handler_data
{
	struct nsd        *nsd;
	struct nsd_socket *socket;
	query_type        *query;
};

struct tcp_accept_handler_data {
	struct nsd         *nsd;
	struct nsd_socket  *socket;
	int event_added;
	struct event       event;
};

/*
 * These globals are used to enable the TCP accept handlers
 * when the number of TCP connection drops below the maximum
 * number of TCP connections.
 */
static size_t		tcp_accept_handler_count;
static struct tcp_accept_handler_data*	tcp_accept_handlers;

static struct event slowaccept_event;
static int slowaccept;

#ifndef NONBLOCKING_IS_BROKEN
#  define NUM_RECV_PER_SELECT 100
#endif

#if (!defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG))
struct mmsghdr msgs[NUM_RECV_PER_SELECT];
struct iovec iovecs[NUM_RECV_PER_SELECT];
struct query *queries[NUM_RECV_PER_SELECT];
#endif

/*
 * Data for the TCP connection handlers.
 *
 * The TCP handlers use non-blocking I/O.  This is necessary to avoid
 * blocking the entire server on a slow TCP connection, but does make
 * reading from and writing to the socket more complicated.
 *
 * Basically, whenever a read/write would block (indicated by the
 * EAGAIN errno variable) we remember the position we were reading
 * from/writing to and return from the TCP reading/writing event
 * handler.  When the socket becomes readable/writable again we
 * continue from the same position.
 */
struct tcp_handler_data
{
	/*
	 * The region used to allocate all TCP connection related
	 * data, including this structure.  This region is destroyed
	 * when the connection is closed.
	 */
	region_type*		region;

	/*
	 * The global nsd structure.
	 */
	struct nsd*			nsd;

	/*
	 * The current query data for this TCP connection.
	 */
	query_type*			query;

	/*
	 * The query_state is used to remember if we are performing an
	 * AXFR, if we're done processing, or if we should discard the
	 * query and connection.
	 */
	query_state_type	query_state;

	/*
	 * The event for the file descriptor and tcp timeout
	 */
	struct event event;

	/*
	 * The bytes_transmitted field is used to remember the number
	 * of bytes transmitted when receiving or sending a DNS
	 * packet.  The count includes the two additional bytes used
	 * to specify the packet length on a TCP connection.
	 */
	size_t				bytes_transmitted;

	/*
	 * The number of queries handled by this specific TCP connection.
	 */
	int					query_count;
	
	/*
	 * The timeout in msec for this tcp connection
	 */
	int	tcp_timeout;
};

/*
 * Handle incoming queries on the UDP server sockets.
 */
static void handle_udp(int fd, short event, void* arg);

/*
 * Handle incoming connections on the TCP sockets.  These handlers
 * usually wait for the NETIO_EVENT_READ event (indicating an incoming
 * connection) but are disabled when the number of current TCP
 * connections is equal to the maximum number of TCP connections.
 * Disabling is done by changing the handler to wait for the
 * NETIO_EVENT_NONE type.  This is done using the function
 * configure_tcp_accept_handlers.
 */
static void handle_tcp_accept(int fd, short event, void* arg);

/*
 * Handle incoming queries on a TCP connection.  The TCP connections
 * are configured to be non-blocking and the handler may be called
 * multiple times before a complete query is received.
 */
static void handle_tcp_reading(int fd, short event, void* arg);

/*
 * Handle outgoing responses on a TCP connection.  The TCP connections
 * are configured to be non-blocking and the handler may be called
 * multiple times before a complete response is sent.
 */
static void handle_tcp_writing(int fd, short event, void* arg);

/*
 * Send all children the quit nonblocking, then close pipe.
 */
static void send_children_quit(struct nsd* nsd);
/* same, for shutdown time, waits for child to exit to avoid restart issues */
static void send_children_quit_and_wait(struct nsd* nsd);

/* set childrens flags to send NSD_STATS to them */
#ifdef BIND8_STATS
static void set_children_stats(struct nsd* nsd);
#endif /* BIND8_STATS */

/*
 * Change the event types the HANDLERS are interested in to EVENT_TYPES.
 */
static void configure_handler_event_types(short event_types);

static uint16_t *compressed_dname_offsets = 0;
static uint32_t compression_table_capacity = 0;
static uint32_t compression_table_size = 0;

/*
 * Remove the specified pid from the list of child pids.  Returns -1 if
 * the pid is not in the list, child_num otherwise.  The field is set to 0.
 */
static int
delete_child_pid(struct nsd *nsd, pid_t pid)
{
	size_t i;
	for (i = 0; i < nsd->child_count; ++i) {
		if (nsd->children[i].pid == pid) {
			nsd->children[i].pid = 0;
			if(!nsd->children[i].need_to_exit) {
				if(nsd->children[i].child_fd != -1)
					close(nsd->children[i].child_fd);
				nsd->children[i].child_fd = -1;
				if(nsd->children[i].handler)
					nsd->children[i].handler->fd = -1;
			}
			return i;
		}
	}
	return -1;
}

/*
 * Restart child servers if necessary.
 */
static int
restart_child_servers(struct nsd *nsd, region_type* region, netio_type* netio,
	int* xfrd_sock_p)
{
	struct main_ipc_handler_data *ipc_data;
	size_t i;
	int sv[2];

	/* Fork the child processes... */
	for (i = 0; i < nsd->child_count; ++i) {
		if (nsd->children[i].pid <= 0) {
			if (nsd->children[i].child_fd != -1)
				close(nsd->children[i].child_fd);
			if (socketpair(AF_UNIX, SOCK_STREAM, 0, sv) == -1) {
				log_msg(LOG_ERR, "socketpair: %s",
					strerror(errno));
				return -1;
			}
			nsd->children[i].child_fd = sv[0];
			nsd->children[i].parent_fd = sv[1];
			nsd->children[i].pid = fork();
			switch (nsd->children[i].pid) {
			default: /* SERVER MAIN */
				close(nsd->children[i].parent_fd);
				nsd->children[i].parent_fd = -1;
				if (fcntl(nsd->children[i].child_fd, F_SETFL, O_NONBLOCK) == -1) {
					log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
				}
				if(!nsd->children[i].handler)
				{
					ipc_data = (struct main_ipc_handler_data*) region_alloc(
						region, sizeof(struct main_ipc_handler_data));
					ipc_data->nsd = nsd;
					ipc_data->child = &nsd->children[i];
					ipc_data->child_num = i;
					ipc_data->xfrd_sock = xfrd_sock_p;
					ipc_data->packet = buffer_create(region, QIOBUFSZ);
					ipc_data->forward_mode = 0;
					ipc_data->got_bytes = 0;
					ipc_data->total_bytes = 0;
					ipc_data->acl_num = 0;
					nsd->children[i].handler = (struct netio_handler*) region_alloc(
						region, sizeof(struct netio_handler));
					nsd->children[i].handler->fd = nsd->children[i].child_fd;
					nsd->children[i].handler->timeout = NULL;
					nsd->children[i].handler->user_data = ipc_data;
					nsd->children[i].handler->event_types = NETIO_EVENT_READ;
					nsd->children[i].handler->event_handler = parent_handle_child_command;
					netio_add_handler(netio, nsd->children[i].handler);
				}
				/* clear any ongoing ipc */
				ipc_data = (struct main_ipc_handler_data*)
					nsd->children[i].handler->user_data;
				ipc_data->forward_mode = 0;
				/* restart - update fd */
				nsd->children[i].handler->fd = nsd->children[i].child_fd;
				break;
			case 0: /* CHILD */
				/* the child need not be able to access the
				 * nsd.db file */
				namedb_close_udb(nsd->db);

				if (pledge("stdio rpath inet", NULL) == -1) {
					log_msg(LOG_ERR, "pledge");
					exit(1);
				}

				nsd->pid = 0;
				nsd->child_count = 0;
				nsd->server_kind = nsd->children[i].kind;
				nsd->this_child = &nsd->children[i];
				nsd->this_child->child_num = i;
				/* remove signal flags inherited from parent
				   the parent will handle them. */
				nsd->signal_hint_reload_hup = 0;
				nsd->signal_hint_reload = 0;
				nsd->signal_hint_child = 0;
				nsd->signal_hint_quit = 0;
				nsd->signal_hint_shutdown = 0;
				nsd->signal_hint_stats = 0;
				nsd->signal_hint_statsusr = 0;
				close(*xfrd_sock_p);
				close(nsd->this_child->child_fd);
				nsd->this_child->child_fd = -1;
				if (fcntl(nsd->this_child->parent_fd, F_SETFL, O_NONBLOCK) == -1) {
					log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
				}
				server_child(nsd);
				/* NOTREACH */
				exit(0);
			case -1:
				log_msg(LOG_ERR, "fork failed: %s",
					strerror(errno));
				return -1;
			}
		}
	}
	return 0;
}

#ifdef BIND8_STATS
static void set_bind8_alarm(struct nsd* nsd)
{
	/* resync so that the next alarm is on the next whole minute */
	if(nsd->st.period > 0) /* % by 0 gives divbyzero error */
		alarm(nsd->st.period - (time(NULL) % nsd->st.period));
}
#endif

/* set zone stat ids for zones initially read in */
static void
zonestatid_tree_set(struct nsd* nsd)
{
	struct radnode* n;
	for(n=radix_first(nsd->db->zonetree); n; n=radix_next(n)) {
		zone_type* zone = (zone_type*)n->elem;
		zone->zonestatid = getzonestatid(nsd->options, zone->opts);
	}
}

#ifdef USE_ZONE_STATS
void
server_zonestat_alloc(struct nsd* nsd)
{
	size_t num = (nsd->options->zonestatnames->count==0?1:
			nsd->options->zonestatnames->count);
	size_t sz = sizeof(struct nsdst)*num;
	char tmpfile[256];
	uint8_t z = 0;

	/* file names */
	nsd->zonestatfname[0] = 0;
	nsd->zonestatfname[1] = 0;
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.zstat.0",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
	nsd->zonestatfname[0] = region_strdup(nsd->region, tmpfile);
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.zstat.1",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
	nsd->zonestatfname[1] = region_strdup(nsd->region, tmpfile);

	/* file descriptors */
	nsd->zonestatfd[0] = open(nsd->zonestatfname[0], O_CREAT|O_RDWR, 0600);
	if(nsd->zonestatfd[0] == -1) {
		log_msg(LOG_ERR, "cannot create %s: %s", nsd->zonestatfname[0],
			strerror(errno));
		exit(1);
	}
	nsd->zonestatfd[1] = open(nsd->zonestatfname[1], O_CREAT|O_RDWR, 0600);
	if(nsd->zonestatfd[0] == -1) {
		log_msg(LOG_ERR, "cannot create %s: %s", nsd->zonestatfname[1],
			strerror(errno));
		close(nsd->zonestatfd[0]);
		unlink(nsd->zonestatfname[0]);
		exit(1);
	}

#ifdef HAVE_MMAP
	if(lseek(nsd->zonestatfd[0], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[0],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[0], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[0], strerror(errno));
		exit(1);
	}
	if(lseek(nsd->zonestatfd[1], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[1],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[1], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[1], strerror(errno));
		exit(1);
	}
	nsd->zonestat[0] = (struct nsdst*)mmap(NULL, sz, PROT_READ|PROT_WRITE,
		MAP_SHARED, nsd->zonestatfd[0], 0);
	if(nsd->zonestat[0] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
		exit(1);
	}
	nsd->zonestat[1] = (struct nsdst*)mmap(NULL, sz, PROT_READ|PROT_WRITE,
		MAP_SHARED, nsd->zonestatfd[1], 0);
	if(nsd->zonestat[1] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
		exit(1);
	}
	memset(nsd->zonestat[0], 0, sz);
	memset(nsd->zonestat[1], 0, sz);
	nsd->zonestatsize[0] = num;
	nsd->zonestatsize[1] = num;
	nsd->zonestatdesired = num;
	nsd->zonestatsizenow = num;
	nsd->zonestatnow = nsd->zonestat[0];
#endif /* HAVE_MMAP */
}

void
zonestat_remap(struct nsd* nsd, int idx, size_t sz)
{
#ifdef HAVE_MMAP
#ifdef MREMAP_MAYMOVE
	nsd->zonestat[idx] = (struct nsdst*)mremap(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx], sz,
		MREMAP_MAYMOVE);
	if(nsd->zonestat[idx] == MAP_FAILED) {
		log_msg(LOG_ERR, "mremap failed: %s", strerror(errno));
		exit(1);
	}
#else /* !HAVE MREMAP */
	if(msync(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx], MS_ASYNC) != 0)
		log_msg(LOG_ERR, "msync failed: %s", strerror(errno));
	if(munmap(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx]) != 0)
		log_msg(LOG_ERR, "munmap failed: %s", strerror(errno));
	nsd->zonestat[idx] = (struct nsdst*)mmap(NULL, sz,
		PROT_READ|PROT_WRITE, MAP_SHARED, nsd->zonestatfd[idx], 0);
	if(nsd->zonestat[idx] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		exit(1);
	}
#endif /* MREMAP */
#endif /* HAVE_MMAP */
}

/* realloc the zonestat array for the one that is not currently in use,
 * to match the desired new size of the array (if applicable) */
void
server_zonestat_realloc(struct nsd* nsd)
{
#ifdef HAVE_MMAP
	uint8_t z = 0;
	size_t sz;
	int idx = 0; /* index of the zonestat array that is not in use */
	if(nsd->zonestatnow == nsd->zonestat[0])
		idx = 1;
	if(nsd->zonestatsize[idx] == nsd->zonestatdesired)
		return;
	sz = sizeof(struct nsdst)*nsd->zonestatdesired;
	if(lseek(nsd->zonestatfd[idx], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[idx],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[idx], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[idx], strerror(errno));
		exit(1);
	}
	zonestat_remap(nsd, idx, sz);
	/* zero the newly allocated region */
	if(nsd->zonestatdesired > nsd->zonestatsize[idx]) {
		memset(((char*)nsd->zonestat[idx])+sizeof(struct nsdst) *
			nsd->zonestatsize[idx], 0, sizeof(struct nsdst) *
			(nsd->zonestatdesired - nsd->zonestatsize[idx]));
	}
	nsd->zonestatsize[idx] = nsd->zonestatdesired;
#endif /* HAVE_MMAP */
}

/* switchover to use the other array for the new children, that
 * briefly coexist with the old children.  And we want to avoid them
 * both writing to the same statistics arrays. */
void
server_zonestat_switch(struct nsd* nsd)
{
	if(nsd->zonestatnow == nsd->zonestat[0]) {
		nsd->zonestatnow = nsd->zonestat[1];
		nsd->zonestatsizenow = nsd->zonestatsize[1];
	} else {
		nsd->zonestatnow = nsd->zonestat[0];
		nsd->zonestatsizenow = nsd->zonestatsize[0];
	}
}
#endif /* USE_ZONE_STATS */

static void
cleanup_dname_compression_tables(void *ptr)
{
	free(ptr);
	compressed_dname_offsets = NULL;
	compression_table_capacity = 0;
}

static void
initialize_dname_compression_tables(struct nsd *nsd)
{
	size_t needed = domain_table_count(nsd->db->domains) + 1;
	needed += EXTRA_DOMAIN_NUMBERS;
	if(compression_table_capacity < needed) {
		if(compressed_dname_offsets) {
			region_remove_cleanup(nsd->db->region,
				cleanup_dname_compression_tables,
				compressed_dname_offsets);
			free(compressed_dname_offsets);
		}
		compressed_dname_offsets = (uint16_t *) xmallocarray(
			needed, sizeof(uint16_t));
		region_add_cleanup(nsd->db->region, cleanup_dname_compression_tables,
			compressed_dname_offsets);
		compression_table_capacity = needed;
		compression_table_size=domain_table_count(nsd->db->domains)+1;
	}
	memset(compressed_dname_offsets, 0, needed * sizeof(uint16_t));
	compressed_dname_offsets[0] = QHEADERSZ; /* The original query name */
}

/* create and bind sockets.  */
static int
server_init_ifs(struct nsd *nsd, size_t from, size_t to, int* reuseport_works)
{
	struct addrinfo* addr;
	size_t i;
#if defined(SO_REUSEPORT) || defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU) || defined(IP_TRANSPARENT)) || defined(IP_FREEBIND))
	int on = 1;
#endif

	/* UDP */

	/* Make a socket... */
	for (i = from; i < to; i++) {
		/* for reuseports copy socket specs of first entries */
		addr = nsd->udp[i%nsd->ifs].addr;
		if (!addr) {
			nsd->udp[i].s = -1;
			continue;
		}
		nsd->udp[i].fam = (int)addr->ai_family;
		if ((nsd->udp[i].s = socket(addr->ai_family, addr->ai_socktype, 0)) == -1) {
#if defined(INET6)
			if (addr->ai_family == AF_INET6 &&
				errno == EAFNOSUPPORT && nsd->grab_ip6_optional) {
				log_msg(LOG_WARNING, "fallback to UDP4, no IPv6: not supported");
				continue;
			}
#endif /* INET6 */
			log_msg(LOG_ERR, "can't create a socket: %s", strerror(errno));
			return -1;
		}

#ifdef SO_REUSEPORT
		if(nsd->reuseport && *reuseport_works &&
			setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_REUSEPORT,
			(void*)&on, (socklen_t)sizeof(on)) < 0) {
			if(verbosity >= 3
#ifdef ENOPROTOOPT
				|| errno != ENOPROTOOPT
#endif
				)
			    log_msg(LOG_ERR, "setsockopt(..., SO_REUSEPORT, "
				"...) failed: %s", strerror(errno));
			*reuseport_works = 0;
		}
#else
		(void)reuseport_works;
#endif /* SO_REUSEPORT */
#if defined(SO_RCVBUF) || defined(SO_SNDBUF)
	if(1) {
	int rcv = 1*1024*1024;
	int snd = 1*1024*1024;

#ifdef SO_RCVBUF
#  ifdef SO_RCVBUFFORCE
	if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_RCVBUFFORCE, (void*)&rcv,
		(socklen_t)sizeof(rcv)) < 0) {
		if(errno != EPERM && errno != ENOBUFS) {
			log_msg(LOG_ERR, "setsockopt(..., SO_RCVBUFFORCE, "
                                        "...) failed: %s", strerror(errno));
			return -1;
		} 
#  else
	if(1) {
#  endif /* SO_RCVBUFFORCE */
		if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_RCVBUF, (void*)&rcv,
			 (socklen_t)sizeof(rcv)) < 0) {
			if(errno != ENOBUFS && errno != ENOSYS) {
				log_msg(LOG_ERR, "setsockopt(..., SO_RCVBUF, "
                                        "...) failed: %s", strerror(errno));
				return -1;
			}
		}
	}
#endif /* SO_RCVBUF */

#ifdef SO_SNDBUF
#  ifdef SO_SNDBUFFORCE
	if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_SNDBUFFORCE, (void*)&snd,
		(socklen_t)sizeof(snd)) < 0) {
		if(errno != EPERM && errno != ENOBUFS) {
			log_msg(LOG_ERR, "setsockopt(..., SO_SNDBUFFORCE, "
                                        "...) failed: %s", strerror(errno));
			return -1;
		} 
#  else
	if(1) {
#  endif /* SO_SNDBUFFORCE */
		if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_SNDBUF, (void*)&snd,
			 (socklen_t)sizeof(snd)) < 0) {
			if(errno != ENOBUFS && errno != ENOSYS) {
				log_msg(LOG_ERR, "setsockopt(..., SO_SNDBUF, "
                                        "...) failed: %s", strerror(errno));
				return -1;
			}
		}
	}
#endif /* SO_SNDBUF */

	}
#endif /* defined(SO_RCVBUF) || defined(SO_SNDBUF) */

#if defined(INET6)
		if (addr->ai_family == AF_INET6) {
# if defined(IPV6_V6ONLY)
			if (setsockopt(nsd->udp[i].s,
				       IPPROTO_IPV6, IPV6_V6ONLY,
				       &on, sizeof(on)) < 0)
			{
				log_msg(LOG_ERR, "setsockopt(..., IPV6_V6ONLY, ...) failed: %s",
					strerror(errno));
				return -1;
			}
# endif
# if defined(IPV6_USE_MIN_MTU)
			/*
			 * There is no fragmentation of IPv6 datagrams
			 * during forwarding in the network. Therefore
			 * we do not send UDP datagrams larger than
			 * the minimum IPv6 MTU of 1280 octets. The
			 * EDNS0 message length can be larger if the
			 * network stack supports IPV6_USE_MIN_MTU.
			 */
			if (setsockopt(nsd->udp[i].s,
				       IPPROTO_IPV6, IPV6_USE_MIN_MTU,
				       &on, sizeof(on)) < 0)
			{
				log_msg(LOG_ERR, "setsockopt(..., IPV6_USE_MIN_MTU, ...) failed: %s",
					strerror(errno));
				return -1;
			}
# elif defined(IPV6_MTU)
			/*
			 * On Linux, PMTUD is disabled by default for datagrams
			 * so set the MTU equal to the MIN MTU to get the same.
			 */
			on = IPV6_MIN_MTU;
			if (setsockopt(nsd->udp[i].s, IPPROTO_IPV6, IPV6_MTU, 
				&on, sizeof(on)) < 0)
			{
				log_msg(LOG_ERR, "setsockopt(..., IPV6_MTU, ...) failed: %s",
					strerror(errno));
				return -1;
			}
			on = 1;
# endif
		}
#endif
#if defined(AF_INET)
		if (addr->ai_family == AF_INET) {
#  if defined(IP_MTU_DISCOVER) && defined(IP_PMTUDISC_DONT)
			int action = IP_PMTUDISC_DONT;
			if (setsockopt(nsd->udp[i].s, IPPROTO_IP, 
				IP_MTU_DISCOVER, &action, sizeof(action)) < 0)
			{
				log_msg(LOG_ERR, "setsockopt(..., IP_MTU_DISCOVER, IP_PMTUDISC_DONT...) failed: %s",
					strerror(errno));
				return -1;
			}
#  elif defined(IP_DONTFRAG)
			int off = 0;
			if (setsockopt(nsd->udp[i].s, IPPROTO_IP, IP_DONTFRAG,
				&off, sizeof(off)) < 0)
			{
				log_msg(LOG_ERR, "setsockopt(..., IP_DONTFRAG, ...) failed: %s",
					strerror(errno));
				return -1;
			}
#  endif
		}
#endif
		/* set it nonblocking */
		/* otherwise, on OSes with thundering herd problems, the
		   UDP recv could block NSD after select returns readable. */
		if (fcntl(nsd->udp[i].s, F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl udp: %s", strerror(errno));
		}

		/* Bind it... */
		if (nsd->options->ip_freebind) {
#ifdef IP_FREEBIND
			if (setsockopt(nsd->udp[i].s, IPPROTO_IP, IP_FREEBIND, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_FREEBIND, ...) failed for udp: %s",
					strerror(errno));
			}
#endif /* IP_FREEBIND */
		}

		if (nsd->options->ip_transparent) {
#ifdef IP_TRANSPARENT
			if (setsockopt(nsd->udp[i].s, IPPROTO_IP, IP_TRANSPARENT, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_TRANSPARENT, ...) failed for udp: %s",
					strerror(errno));
			}
#endif /* IP_TRANSPARENT */
		}

		if (bind(nsd->udp[i].s, (struct sockaddr *) addr->ai_addr, addr->ai_addrlen) != 0) {
			log_msg(LOG_ERR, "can't bind udp socket: %s", strerror(errno));
			return -1;
		}
	}

	/* TCP */

	/* Make a socket... */
	for (i = from; i < to; i++) {
		/* for reuseports copy socket specs of first entries */
		addr = nsd->tcp[i%nsd->ifs].addr;
		if (!addr) {
			nsd->tcp[i].s = -1;
			continue;
		}
		nsd->tcp[i].fam = (int)addr->ai_family;
		/* turn off REUSEPORT for TCP by copying the socket fd */
		if(i >= nsd->ifs) {
			nsd->tcp[i].s = nsd->tcp[i%nsd->ifs].s;
			continue;
		}
		if ((nsd->tcp[i].s = socket(addr->ai_family, addr->ai_socktype, 0)) == -1) {
#if defined(INET6)
			if (addr->ai_family == AF_INET6 &&
				errno == EAFNOSUPPORT && nsd->grab_ip6_optional) {
				log_msg(LOG_WARNING, "fallback to TCP4, no IPv6: not supported");
				continue;
			}
#endif /* INET6 */
			log_msg(LOG_ERR, "can't create a socket: %s", strerror(errno));
			return -1;
		}

#ifdef SO_REUSEPORT
		if(nsd->reuseport && *reuseport_works &&
			setsockopt(nsd->tcp[i].s, SOL_SOCKET, SO_REUSEPORT,
			(void*)&on, (socklen_t)sizeof(on)) < 0) {
			if(verbosity >= 3
#ifdef ENOPROTOOPT
				|| errno != ENOPROTOOPT
#endif
				)
			    log_msg(LOG_ERR, "setsockopt(..., SO_REUSEPORT, "
				"...) failed: %s", strerror(errno));
			*reuseport_works = 0;
		}
#endif /* SO_REUSEPORT */
#ifdef	SO_REUSEADDR
		if (setsockopt(nsd->tcp[i].s, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on)) < 0) {
			log_msg(LOG_ERR, "setsockopt(..., SO_REUSEADDR, ...) failed: %s", strerror(errno));
		}
#endif /* SO_REUSEADDR */

#if defined(INET6)
		if (addr->ai_family == AF_INET6) {
# if defined(IPV6_V6ONLY)
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IPV6, IPV6_V6ONLY,
				&on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_V6ONLY, ...) failed: %s", strerror(errno));
				return -1;
			}
# endif
# if defined(IPV6_USE_MIN_MTU)
			/*
			 * Use minimum MTU to minimize delays learning working
			 * PMTU when communicating through a tunnel.
			 */
			if (setsockopt(nsd->tcp[i].s,
				       IPPROTO_IPV6, IPV6_USE_MIN_MTU,
				       &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_USE_MIN_MTU, ...) failed: %s", strerror(errno));
				return -1;
			}
# elif defined(IPV6_MTU)
			/*
			 * On Linux, PMTUD is disabled by default for datagrams
			 * so set the MTU equal to the MIN MTU to get the same.
			 */
			on = IPV6_MIN_MTU;
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IPV6, IPV6_MTU,
				&on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_MTU, ...) failed: %s", strerror(errno));
				return -1;
			}
			on = 1;
# endif
		}
#endif
		/* set maximum segment size to tcp socket */
		if(nsd->tcp_mss > 0) {
#if defined(IPPROTO_TCP) && defined(TCP_MAXSEG)
			if(setsockopt(nsd->tcp[i].s, IPPROTO_TCP, TCP_MAXSEG,
					(void*)&nsd->tcp_mss,
					sizeof(nsd->tcp_mss)) < 0) {
				log_msg(LOG_ERR,
					"setsockopt(...,TCP_MAXSEG,...)"
					" failed for tcp: %s", strerror(errno));
			}
#else
			log_msg(LOG_ERR, "setsockopt(TCP_MAXSEG) unsupported");
#endif /* defined(IPPROTO_TCP) && defined(TCP_MAXSEG) */
		}

		/* set it nonblocking */
		/* (StevensUNP p463), if tcp listening socket is blocking, then
		   it may block in accept, even if select() says readable. */
		if (fcntl(nsd->tcp[i].s, F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl tcp: %s", strerror(errno));
		}

		/* Bind it... */
		if (nsd->options->ip_freebind) {
#ifdef IP_FREEBIND
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IP, IP_FREEBIND, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_FREEBIND, ...) failed for tcp: %s",
					strerror(errno));
			}
#endif /* IP_FREEBIND */
		}

		if (nsd->options->ip_transparent) {
#ifdef IP_TRANSPARENT
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IP, IP_TRANSPARENT, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_TRANSPARENT, ...) failed for tcp: %s",
					strerror(errno));
			}
#endif /* IP_TRANSPARENT */
		}

		if (bind(nsd->tcp[i].s, (struct sockaddr *) addr->ai_addr, addr->ai_addrlen) != 0) {
			log_msg(LOG_ERR, "can't bind tcp socket: %s", strerror(errno));
			return -1;
		}

		/* Listen to it... */
		if (listen(nsd->tcp[i].s, TCP_BACKLOG) == -1) {
			log_msg(LOG_ERR, "can't listen: %s", strerror(errno));
			return -1;
		}
	}

	return 0;
}

/*
 * Initialize the server, reuseport, create and bind the sockets.
 */
int
server_init(struct nsd *nsd)
{
	int reuseport_successful = 1; /* see if reuseport works in OS */
	if(nsd->reuseport) {
		/* increase the size of the udp and tcp interface arrays,
		 * there are going to be separate interface file descriptors
		 * for every server instance */
		nsd->udp = xrealloc(nsd->udp, (nsd->ifs*nsd->reuseport)*
			sizeof(*nsd->udp));
		nsd->tcp = xrealloc(nsd->tcp, (nsd->ifs*nsd->reuseport)*
			sizeof(*nsd->tcp));
		memset(&nsd->udp[nsd->ifs], 0, sizeof(*nsd->udp)*
			(nsd->ifs*(nsd->reuseport-1)));
		memset(&nsd->tcp[nsd->ifs], 0, sizeof(*nsd->tcp)*
			(nsd->ifs*(nsd->reuseport-1)));
	}

	/* open the server interface ports */
	if(server_init_ifs(nsd, 0, nsd->ifs, &reuseport_successful) == -1)
		return -1;

	/* continue to open the remaining reuseport ports */
	if(nsd->reuseport && reuseport_successful) {
		if(server_init_ifs(nsd, nsd->ifs, nsd->ifs*nsd->reuseport,
			&reuseport_successful) == -1)
			return -1;
		nsd->ifs *= nsd->reuseport;
	} else {
		nsd->reuseport = 0;
	}
	return 0;
}

/*
 * Prepare the server for take off.
 *
 */
int
server_prepare(struct nsd *nsd)
{
#ifdef RATELIMIT
	/* set secret modifier for hashing (udb ptr buckets and rate limits) */
#ifdef HAVE_ARC4RANDOM
	hash_set_raninit(arc4random());
#else
	uint32_t v = getpid() ^ time(NULL);
	srandom((unsigned long)v);
	if(RAND_status() && RAND_bytes((unsigned char*)&v, sizeof(v)) > 0)
		hash_set_raninit(v);
	else	hash_set_raninit(random());
#endif
	rrl_mmap_init(nsd->child_count, nsd->options->rrl_size,
		nsd->options->rrl_ratelimit,
		nsd->options->rrl_whitelist_ratelimit,
		nsd->options->rrl_slip,
		nsd->options->rrl_ipv4_prefix_length,
		nsd->options->rrl_ipv6_prefix_length);
#endif /* RATELIMIT */

	/* Open the database... */
	if ((nsd->db = namedb_open(nsd->dbfile, nsd->options)) == NULL) {
		log_msg(LOG_ERR, "unable to open the database %s: %s",
			nsd->dbfile, strerror(errno));
		unlink(nsd->task[0]->fname);
		unlink(nsd->task[1]->fname);
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
		xfrd_del_tempdir(nsd);
		return -1;
	}
	/* check if zone files have been modified */
	/* NULL for taskudb because we send soainfo in a moment, batched up,
	 * for all zones */
	if(nsd->options->zonefiles_check || (nsd->options->database == NULL ||
		nsd->options->database[0] == 0))
		namedb_check_zonefiles(nsd, nsd->options, NULL, NULL);
	zonestatid_tree_set(nsd);

	compression_table_capacity = 0;
	initialize_dname_compression_tables(nsd);

#ifdef	BIND8_STATS
	/* Initialize times... */
	time(&nsd->st.boot);
	set_bind8_alarm(nsd);
#endif /* BIND8_STATS */

	return 0;
}

/*
 * Fork the required number of servers.
 */
static int
server_start_children(struct nsd *nsd, region_type* region, netio_type* netio,
	int* xfrd_sock_p)
{
	size_t i;

	/* Start all child servers initially.  */
	for (i = 0; i < nsd->child_count; ++i) {
		nsd->children[i].pid = 0;
	}

	return restart_child_servers(nsd, region, netio, xfrd_sock_p);
}

void
server_close_all_sockets(struct nsd_socket sockets[], size_t n)
{
	size_t i;

	/* Close all the sockets... */
	for (i = 0; i < n; ++i) {
		if (sockets[i].s != -1) {
			close(sockets[i].s);
			if(sockets[i].addr)
				freeaddrinfo(sockets[i].addr);
			sockets[i].s = -1;
		}
	}
}

/*
 * Close the sockets, shutdown the server and exit.
 * Does not return.
 *
 */
void
server_shutdown(struct nsd *nsd)
{
	size_t i;

	server_close_all_sockets(nsd->udp, nsd->ifs);
	server_close_all_sockets(nsd->tcp, nsd->ifs);
	/* CHILD: close command channel to parent */
	if(nsd->this_child && nsd->this_child->parent_fd != -1)
	{
		close(nsd->this_child->parent_fd);
		nsd->this_child->parent_fd = -1;
	}
	/* SERVER: close command channels to children */
	if(!nsd->this_child)
	{
		for(i=0; i < nsd->child_count; ++i)
			if(nsd->children[i].child_fd != -1)
			{
				close(nsd->children[i].child_fd);
				nsd->children[i].child_fd = -1;
			}
	}

	tsig_finalize();
#ifdef HAVE_SSL
	daemon_remote_delete(nsd->rc); /* ssl-delete secret keys */
#endif

#if 0 /* OS collects memory pages */
	nsd_options_destroy(nsd->options);
	region_destroy(nsd->region);
#endif
	log_finalize();
	exit(0);
}

void
server_prepare_xfrd(struct nsd* nsd)
{
	char tmpfile[256];
	/* create task mmaps */
	nsd->mytask = 0;
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.task.0",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
	nsd->task[0] = task_file_create(tmpfile);
	if(!nsd->task[0]) {
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
		xfrd_del_tempdir(nsd);
		exit(1);
	}
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.task.1",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
	nsd->task[1] = task_file_create(tmpfile);
	if(!nsd->task[1]) {
		unlink(nsd->task[0]->fname);
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
		xfrd_del_tempdir(nsd);
		exit(1);
	}
	assert(udb_base_get_userdata(nsd->task[0])->data == 0);
	assert(udb_base_get_userdata(nsd->task[1])->data == 0);
	/* create xfrd listener structure */
	nsd->xfrd_listener = region_alloc(nsd->region,
		sizeof(netio_handler_type));
	nsd->xfrd_listener->user_data = (struct ipc_handler_conn_data*)
		region_alloc(nsd->region, sizeof(struct ipc_handler_conn_data));
	nsd->xfrd_listener->fd = -1;
	((struct ipc_handler_conn_data*)nsd->xfrd_listener->user_data)->nsd =
		nsd;
	((struct ipc_handler_conn_data*)nsd->xfrd_listener->user_data)->conn =
		xfrd_tcp_create(nsd->region, QIOBUFSZ);
}


void
server_start_xfrd(struct nsd *nsd, int del_db, int reload_active)
{
	pid_t pid;
	int sockets[2] = {0,0};
	struct ipc_handler_conn_data *data;

	if(nsd->xfrd_listener->fd != -1)
		close(nsd->xfrd_listener->fd);
	if(del_db) {
		/* recreate taskdb that xfrd was using, it may be corrupt */
		/* we (or reload) use nsd->mytask, and xfrd uses the other */
		char* tmpfile = nsd->task[1-nsd->mytask]->fname;
		nsd->task[1-nsd->mytask]->fname = NULL;
		/* free alloc already, so udb does not shrink itself */
		udb_alloc_delete(nsd->task[1-nsd->mytask]->alloc);
		nsd->task[1-nsd->mytask]->alloc = NULL;
		udb_base_free(nsd->task[1-nsd->mytask]);
		/* create new file, overwrite the old one */
		nsd->task[1-nsd->mytask] = task_file_create(tmpfile);
		free(tmpfile);
	}
	if (socketpair(AF_UNIX, SOCK_STREAM, 0, sockets) == -1) {
		log_msg(LOG_ERR, "startxfrd failed on socketpair: %s", strerror(errno));
		return;
	}
	pid = fork();
	switch (pid) {
	case -1:
		log_msg(LOG_ERR, "fork xfrd failed: %s", strerror(errno));
		break;
	default:
		/* PARENT: close first socket, use second one */
		close(sockets[0]);
		if (fcntl(sockets[1], F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
		}
		if(del_db) xfrd_free_namedb(nsd);
		/* use other task than I am using, since if xfrd died and is
		 * restarted, the reload is using nsd->mytask */
		nsd->mytask = 1 - nsd->mytask;
		xfrd_init(sockets[1], nsd, del_db, reload_active, pid);
		/* ENOTREACH */
		break;
	case 0:
		/* CHILD: close second socket, use first one */
		close(sockets[1]);
		if (fcntl(sockets[0], F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
		}
		nsd->xfrd_listener->fd = sockets[0];
		break;
	}
	/* server-parent only */
	nsd->xfrd_listener->timeout = NULL;
	nsd->xfrd_listener->event_types = NETIO_EVENT_READ;
	nsd->xfrd_listener->event_handler = parent_handle_xfrd_command;
	/* clear ongoing ipc reads */
	data = (struct ipc_handler_conn_data *) nsd->xfrd_listener->user_data;
	data->conn->is_reading = 0;
}

/** add all soainfo to taskdb */
static void
add_all_soa_to_task(struct nsd* nsd, struct udb_base* taskudb)
{
	struct radnode* n;
	udb_ptr task_last; /* last task, mytask is empty so NULL */
	/* add all SOA INFO to mytask */
	udb_ptr_init(&task_last, taskudb);
	for(n=radix_first(nsd->db->zonetree); n; n=radix_next(n)) {
		task_new_soainfo(taskudb, &task_last, (zone_type*)n->elem, 0);
	}
	udb_ptr_unlink(&task_last, taskudb);
}

void
server_send_soa_xfrd(struct nsd* nsd, int shortsoa)
{
	/* normally this exchanges the SOA from nsd->xfrd and the expire back.
	 *   parent fills one taskdb with soas, xfrd fills other with expires.
	 *   then they exchange and process.
	 * shortsoa: xfrd crashes and needs to be restarted and one taskdb
	 *   may be in use by reload.  Fill SOA in taskdb and give to xfrd.
	 *   expire notifications can be sent back via a normal reload later
	 *   (xfrd will wait for current running reload to finish if any).
	 */
	sig_atomic_t cmd = 0;
	pid_t mypid;
	int xfrd_sock = nsd->xfrd_listener->fd;
	struct udb_base* taskudb = nsd->task[nsd->mytask];
	udb_ptr t;
	if(!shortsoa) {
		if(nsd->signal_hint_shutdown) {
		shutdown:
			log_msg(LOG_WARNING, "signal received, shutting down...");
			server_close_all_sockets(nsd->udp, nsd->ifs);
			server_close_all_sockets(nsd->tcp, nsd->ifs);
#ifdef HAVE_SSL
			daemon_remote_close(nsd->rc);
#endif
			/* Unlink it if possible... */
			unlinkpid(nsd->pidfile);
			unlink(nsd->task[0]->fname);
			unlink(nsd->task[1]->fname);
#ifdef USE_ZONE_STATS
			unlink(nsd->zonestatfname[0]);
			unlink(nsd->zonestatfname[1]);
#endif
			/* write the nsd.db to disk, wait for it to complete */
			udb_base_sync(nsd->db->udb, 1);
			udb_base_close(nsd->db->udb);
			server_shutdown(nsd);
			exit(0);
		}
	}
	if(shortsoa) {
		/* put SOA in xfrd task because mytask may be in use */
		taskudb = nsd->task[1-nsd->mytask];
	}

	add_all_soa_to_task(nsd, taskudb);
	if(!shortsoa) {
		/* wait for xfrd to signal task is ready, RELOAD signal */
		if(block_read(nsd, xfrd_sock, &cmd, sizeof(cmd), -1) != sizeof(cmd) ||
			cmd != NSD_RELOAD) {
			log_msg(LOG_ERR, "did not get start signal from xfrd");
			exit(1);
		} 
		if(nsd->signal_hint_shutdown) {
			goto shutdown;
		}
	}
	/* give xfrd our task, signal it with RELOAD_DONE */
	task_process_sync(taskudb);
	cmd = NSD_RELOAD_DONE;
	if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending soa end from reload %d to xfrd: %s",
			(int)nsd->pid, strerror(errno));
	}
	mypid = getpid();
	if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
		log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
			strerror(errno));
	}

	if(!shortsoa) {
		/* process the xfrd task works (expiry data) */
		nsd->mytask = 1 - nsd->mytask;
		taskudb = nsd->task[nsd->mytask];
		task_remap(taskudb);
		udb_ptr_new(&t, taskudb, udb_base_get_userdata(taskudb));
		while(!udb_ptr_is_null(&t)) {
			task_process_expire(nsd->db, TASKLIST(&t));
			udb_ptr_set_rptr(&t, taskudb, &TASKLIST(&t)->next);
		}
		udb_ptr_unlink(&t, taskudb);
		task_clear(taskudb);

		/* tell xfrd that the task is emptied, signal with RELOAD_DONE */
		cmd = NSD_RELOAD_DONE;
		if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
			log_msg(LOG_ERR, "problems sending soa end from reload %d to xfrd: %s",
				(int)nsd->pid, strerror(errno));
		}
	}
}

/* pass timeout=-1 for blocking. Returns size, 0, -1(err), or -2(timeout) */
ssize_t
block_read(struct nsd* nsd, int s, void* p, ssize_t sz, int timeout)
{
	uint8_t* buf = (uint8_t*) p;
	ssize_t total = 0;
	struct pollfd fd;
	memset(&fd, 0, sizeof(fd));
	fd.fd = s;
	fd.events = POLLIN;
	
	while( total < sz) {
		ssize_t ret;
		ret = poll(&fd, 1, (timeout==-1)?-1:timeout*1000);
		if(ret == -1) {
			if(errno == EAGAIN)
				/* blocking read */
				continue;
			if(errno == EINTR) {
				if(nsd && (nsd->signal_hint_quit || nsd->signal_hint_shutdown))
					return -1;
				/* other signals can be handled later */
				continue;
			}
			/* some error */
			return -1;
		}
		if(ret == 0) {
			/* operation timed out */
			return -2;
		}
		ret = read(s, buf+total, sz-total);
		if(ret == -1) {
			if(errno == EAGAIN)
				/* blocking read */
				continue;
			if(errno == EINTR) {
				if(nsd && (nsd->signal_hint_quit || nsd->signal_hint_shutdown))
					return -1;
				/* other signals can be handled later */
				continue;
			}
			/* some error */
			return -1;
		}
		if(ret == 0) {
			/* closed connection! */
			return 0;
		}
		total += ret;
	}
	return total;
}

static void
reload_process_tasks(struct nsd* nsd, udb_ptr* last_task, int cmdsocket)
{
	sig_atomic_t cmd = NSD_QUIT_SYNC;
	udb_ptr t, next;
	udb_base* u = nsd->task[nsd->mytask];
	udb_ptr_init(&next, u);
	udb_ptr_new(&t, u, udb_base_get_userdata(u));
	udb_base_set_userdata(u, 0);
	while(!udb_ptr_is_null(&t)) {
		/* store next in list so this one can be deleted or reused */
		udb_ptr_set_rptr(&next, u, &TASKLIST(&t)->next);
		udb_rptr_zero(&TASKLIST(&t)->next, u);

		/* process task t */
		/* append results for task t and update last_task */
		task_process_in_reload(nsd, u, last_task, &t);

		/* go to next */
		udb_ptr_set_ptr(&t, u, &next);

		/* if the parent has quit, we must quit too, poll the fd for cmds */
		if(block_read(nsd, cmdsocket, &cmd, sizeof(cmd), 0) == sizeof(cmd)) {
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", (int)cmd));
			if(cmd == NSD_QUIT) {
				DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: quit to follow nsd"));
				/* sync to disk (if needed) */
				udb_base_sync(nsd->db->udb, 0);
				/* unlink files of remainder of tasks */
				while(!udb_ptr_is_null(&t)) {
					if(TASKLIST(&t)->task_type == task_apply_xfr) {
						xfrd_unlink_xfrfile(nsd, TASKLIST(&t)->yesno);
					}
					udb_ptr_set_rptr(&t, u, &TASKLIST(&t)->next);
				}
				udb_ptr_unlink(&t, u);
				udb_ptr_unlink(&next, u);
				exit(0);
			}
		}

	}
	udb_ptr_unlink(&t, u);
	udb_ptr_unlink(&next, u);
}

#ifdef BIND8_STATS
static void
parent_send_stats(struct nsd* nsd, int cmdfd)
{
	size_t i;
	if(!write_socket(cmdfd, &nsd->st, sizeof(nsd->st))) {
		log_msg(LOG_ERR, "could not write stats to reload");
		return;
	}
	for(i=0; i<nsd->child_count; i++)
		if(!write_socket(cmdfd, &nsd->children[i].query_count,
			sizeof(stc_type))) {
			log_msg(LOG_ERR, "could not write stats to reload");
			return;
		}
}

static void
reload_do_stats(int cmdfd, struct nsd* nsd, udb_ptr* last)
{
	struct nsdst s;
	stc_type* p;
	size_t i;
	if(block_read(nsd, cmdfd, &s, sizeof(s),
		RELOAD_SYNC_TIMEOUT) != sizeof(s)) {
		log_msg(LOG_ERR, "could not read stats from oldpar");
		return;
	}
	s.db_disk = (nsd->db->udb?nsd->db->udb->base_size:0);
	s.db_mem = region_get_mem(nsd->db->region);
	p = (stc_type*)task_new_stat_info(nsd->task[nsd->mytask], last, &s,
		nsd->child_count);
	if(!p) return;
	for(i=0; i<nsd->child_count; i++) {
		if(block_read(nsd, cmdfd, p++, sizeof(stc_type), 1)!=
			sizeof(stc_type))
			return;
	}
}
#endif /* BIND8_STATS */

/*
 * Reload the database, stop parent, re-fork children and continue.
 * as server_main.
 */
static void
server_reload(struct nsd *nsd, region_type* server_region, netio_type* netio,
	int cmdsocket)
{
	pid_t mypid;
	sig_atomic_t cmd = NSD_QUIT_SYNC;
	int ret;
	udb_ptr last_task;
	struct sigaction old_sigchld, ign_sigchld;
	/* ignore SIGCHLD from the previous server_main that used this pid */
	memset(&ign_sigchld, 0, sizeof(ign_sigchld));
	ign_sigchld.sa_handler = SIG_IGN;
	sigaction(SIGCHLD, &ign_sigchld, &old_sigchld);

	/* see what tasks we got from xfrd */
	task_remap(nsd->task[nsd->mytask]);
	udb_ptr_init(&last_task, nsd->task[nsd->mytask]);
	udb_compact_inhibited(nsd->db->udb, 1);
	reload_process_tasks(nsd, &last_task, cmdsocket);
	udb_compact_inhibited(nsd->db->udb, 0);
	udb_compact(nsd->db->udb);

#ifndef NDEBUG
	if(nsd_debug_level >= 1)
		region_log_stats(nsd->db->region);
#endif /* NDEBUG */
	/* sync to disk (if needed) */
	udb_base_sync(nsd->db->udb, 0);

	initialize_dname_compression_tables(nsd);

#ifdef BIND8_STATS
	/* Restart dumping stats if required.  */
	time(&nsd->st.boot);
	set_bind8_alarm(nsd);
#endif
#ifdef USE_ZONE_STATS
	server_zonestat_realloc(nsd); /* realloc for new children */
	server_zonestat_switch(nsd);
#endif

	/* listen for the signals of failed children again */
	sigaction(SIGCHLD, &old_sigchld, NULL);
	/* Start new child processes */
	if (server_start_children(nsd, server_region, netio, &nsd->
		xfrd_listener->fd) != 0) {
		send_children_quit(nsd);
		exit(1);
	}

	/* if the parent has quit, we must quit too, poll the fd for cmds */
	if(block_read(nsd, cmdsocket, &cmd, sizeof(cmd), 0) == sizeof(cmd)) {
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", (int)cmd));
		if(cmd == NSD_QUIT) {
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: quit to follow nsd"));
			send_children_quit(nsd);
			exit(0);
		}
	}

	/* Send quit command to parent: blocking, wait for receipt. */
	do {
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc send quit to main"));
		if (!write_socket(cmdsocket, &cmd, sizeof(cmd)))
		{
			log_msg(LOG_ERR, "problems sending command from reload to oldnsd: %s",
				strerror(errno));
		}
		/* blocking: wait for parent to really quit. (it sends RELOAD as ack) */
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc wait for ack main"));
		ret = block_read(nsd, cmdsocket, &cmd, sizeof(cmd),
			RELOAD_SYNC_TIMEOUT);
		if(ret == -2) {
			DEBUG(DEBUG_IPC, 1, (LOG_ERR, "reload timeout QUITSYNC. retry"));
		}
	} while (ret == -2);
	if(ret == -1) {
		log_msg(LOG_ERR, "reload: could not wait for parent to quit: %s",
			strerror(errno));
	}
	DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc reply main %d %d", ret, (int)cmd));
	if(cmd == NSD_QUIT) {
		/* small race condition possible here, parent got quit cmd. */
		send_children_quit(nsd);
		exit(1);
	}
	assert(ret==-1 || ret == 0 || cmd == NSD_RELOAD);
#ifdef BIND8_STATS
	reload_do_stats(cmdsocket, nsd, &last_task);
#endif
	udb_ptr_unlink(&last_task, nsd->task[nsd->mytask]);
	task_process_sync(nsd->task[nsd->mytask]);
#ifdef USE_ZONE_STATS
	server_zonestat_realloc(nsd); /* realloc for next children */
#endif

	/* send soainfo to the xfrd process, signal it that reload is done,
	 * it picks up the taskudb */
	cmd = NSD_RELOAD_DONE;
	if(!write_socket(nsd->xfrd_listener->fd, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending reload_done xfrd: %s",
			strerror(errno));
	}
	mypid = getpid();
	if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
		log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
			strerror(errno));
	}

	/* try to reopen file */
	if (nsd->file_rotation_ok)
		log_reopen(nsd->log_filename, 1);
	/* exit reload, continue as new server_main */
}

/*
 * Get the mode depending on the signal hints that have been received.
 * Multiple signal hints can be received and will be handled in turn.
 */
static sig_atomic_t
server_signal_mode(struct nsd *nsd)
{
	if(nsd->signal_hint_quit) {
		nsd->signal_hint_quit = 0;
		return NSD_QUIT;
	}
	else if(nsd->signal_hint_shutdown) {
		nsd->signal_hint_shutdown = 0;
		return NSD_SHUTDOWN;
	}
	else if(nsd->signal_hint_child) {
		nsd->signal_hint_child = 0;
		return NSD_REAP_CHILDREN;
	}
	else if(nsd->signal_hint_reload) {
		nsd->signal_hint_reload = 0;
		return NSD_RELOAD;
	}
	else if(nsd->signal_hint_reload_hup) {
		nsd->signal_hint_reload_hup = 0;
		return NSD_RELOAD_REQ;
	}
	else if(nsd->signal_hint_stats) {
		nsd->signal_hint_stats = 0;
#ifdef BIND8_STATS
		set_bind8_alarm(nsd);
#endif
		return NSD_STATS;
	}
	else if(nsd->signal_hint_statsusr) {
		nsd->signal_hint_statsusr = 0;
		return NSD_STATS;
	}
	return NSD_RUN;
}

/*
 * The main server simply waits for signals and child processes to
 * terminate.  Child processes are restarted as necessary.
 */
void
server_main(struct nsd *nsd)
{
	region_type *server_region = region_create(xalloc, free);
	netio_type *netio = netio_create(server_region);
	netio_handler_type reload_listener;
	int reload_sockets[2] = {-1, -1};
	struct timespec timeout_spec;
	int status;
	pid_t child_pid;
	pid_t reload_pid = -1;
	sig_atomic_t mode;

	/* Ensure we are the main process */
	assert(nsd->server_kind == NSD_SERVER_MAIN);

	/* Add listener for the XFRD process */
	netio_add_handler(netio, nsd->xfrd_listener);

	/* Start the child processes that handle incoming queries */
	if (server_start_children(nsd, server_region, netio,
		&nsd->xfrd_listener->fd) != 0) {
		send_children_quit(nsd);
		exit(1);
	}
	reload_listener.fd = -1;

	/* This_child MUST be 0, because this is the parent process */
	assert(nsd->this_child == 0);

	/* Run the server until we get a shutdown signal */
	while ((mode = nsd->mode) != NSD_SHUTDOWN) {
		/* Did we receive a signal that changes our mode? */
		if(mode == NSD_RUN) {
			nsd->mode = mode = server_signal_mode(nsd);
		}

		switch (mode) {
		case NSD_RUN:
			/* see if any child processes terminated */
			while((child_pid = waitpid(-1, &status, WNOHANG)) != -1 && child_pid != 0) {
				int is_child = delete_child_pid(nsd, child_pid);
				if (is_child != -1 && nsd->children[is_child].need_to_exit) {
					if(nsd->children[is_child].child_fd == -1)
						nsd->children[is_child].has_exited = 1;
					parent_check_all_children_exited(nsd);
				} else if(is_child != -1) {
					log_msg(LOG_WARNING,
					       "server %d died unexpectedly with status %d, restarting",
					       (int) child_pid, status);
					restart_child_servers(nsd, server_region, netio,
						&nsd->xfrd_listener->fd);
				} else if (child_pid == reload_pid) {
					sig_atomic_t cmd = NSD_RELOAD_DONE;
					pid_t mypid;
					log_msg(LOG_WARNING,
					       "Reload process %d failed with status %d, continuing with old database",
					       (int) child_pid, status);
					reload_pid = -1;
					if(reload_listener.fd != -1) close(reload_listener.fd);
					reload_listener.fd = -1;
					reload_listener.event_types = NETIO_EVENT_NONE;
					task_process_sync(nsd->task[nsd->mytask]);
					/* inform xfrd reload attempt ended */
					if(!write_socket(nsd->xfrd_listener->fd,
						&cmd, sizeof(cmd))) {
						log_msg(LOG_ERR, "problems "
						  "sending SOAEND to xfrd: %s",
						  strerror(errno));
					}
					mypid = getpid();
					if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
						log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
							strerror(errno));
					}
				} else if(status != 0) {
					/* check for status, because we get
					 * the old-servermain because reload
					 * is the process-parent of old-main,
					 * and we get older server-processes
					 * that are exiting after a reload */
					log_msg(LOG_WARNING,
					       "process %d terminated with status %d",
					       (int) child_pid, status);
				}
			}
			if (child_pid == -1) {
				if (errno == EINTR) {
					continue;
				}
				if (errno != ECHILD)
					log_msg(LOG_WARNING, "wait failed: %s", strerror(errno));
			}
			if (nsd->mode != NSD_RUN)
				break;

			/* timeout to collect processes. In case no sigchild happens. */
			timeout_spec.tv_sec = 60;
			timeout_spec.tv_nsec = 0;

			/* listen on ports, timeout for collecting terminated children */
			if(netio_dispatch(netio, &timeout_spec, 0) == -1) {
				if (errno != EINTR) {
					log_msg(LOG_ERR, "netio_dispatch failed: %s", strerror(errno));
				}
			}
			if(nsd->restart_children) {
				restart_child_servers(nsd, server_region, netio,
					&nsd->xfrd_listener->fd);
				nsd->restart_children = 0;
			}
			if(nsd->reload_failed) {
				sig_atomic_t cmd = NSD_RELOAD_DONE;
				pid_t mypid;
				nsd->reload_failed = 0;
				log_msg(LOG_WARNING,
				       "Reload process %d failed, continuing with old database",
				       (int) reload_pid);
				reload_pid = -1;
				if(reload_listener.fd != -1) close(reload_listener.fd);
				reload_listener.fd = -1;
				reload_listener.event_types = NETIO_EVENT_NONE;
				task_process_sync(nsd->task[nsd->mytask]);
				/* inform xfrd reload attempt ended */
				if(!write_socket(nsd->xfrd_listener->fd,
					&cmd, sizeof(cmd))) {
					log_msg(LOG_ERR, "problems "
					  "sending SOAEND to xfrd: %s",
					  strerror(errno));
				}
				mypid = getpid();
				if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
					log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
						strerror(errno));
				}
			}

			break;
		case NSD_RELOAD_REQ: {
			sig_atomic_t cmd = NSD_RELOAD_REQ;
			log_msg(LOG_WARNING, "SIGHUP received, reloading...");
			DEBUG(DEBUG_IPC,1, (LOG_INFO,
				"main: ipc send reload_req to xfrd"));
			if(!write_socket(nsd->xfrd_listener->fd,
				&cmd, sizeof(cmd))) {
				log_msg(LOG_ERR, "server_main: could not send "
				"reload_req to xfrd: %s", strerror(errno));
			}
			nsd->mode = NSD_RUN;
			} break;
		case NSD_RELOAD:
			/* Continue to run nsd after reload */
			nsd->mode = NSD_RUN;
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reloading..."));
			if (reload_pid != -1) {
				log_msg(LOG_WARNING, "Reload already in progress (pid = %d)",
				       (int) reload_pid);
				break;
			}

			/* switch the mytask to keep track of who owns task*/
			nsd->mytask = 1 - nsd->mytask;
			if (socketpair(AF_UNIX, SOCK_STREAM, 0, reload_sockets) == -1) {
				log_msg(LOG_ERR, "reload failed on socketpair: %s", strerror(errno));
				reload_pid = -1;
				break;
			}

			/* Do actual reload */
			reload_pid = fork();
			switch (reload_pid) {
			case -1:
				log_msg(LOG_ERR, "fork failed: %s", strerror(errno));
				break;
			default:
				/* PARENT */
				close(reload_sockets[0]);
				server_reload(nsd, server_region, netio,
					reload_sockets[1]);
				DEBUG(DEBUG_IPC,2, (LOG_INFO, "Reload exited to become new main"));
				close(reload_sockets[1]);
				DEBUG(DEBUG_IPC,2, (LOG_INFO, "Reload closed"));
				/* drop stale xfrd ipc data */
				((struct ipc_handler_conn_data*)nsd->
					xfrd_listener->user_data)
					->conn->is_reading = 0;
				reload_pid = -1;
				reload_listener.fd = -1;
				reload_listener.event_types = NETIO_EVENT_NONE;
				DEBUG(DEBUG_IPC,2, (LOG_INFO, "Reload resetup; run"));
				break;
			case 0:
				/* CHILD */
				/* server_main keep running until NSD_QUIT_SYNC
				 * received from reload. */
				close(reload_sockets[1]);
				reload_listener.fd = reload_sockets[0];
				reload_listener.timeout = NULL;
				reload_listener.user_data = nsd;
				reload_listener.event_types = NETIO_EVENT_READ;
				reload_listener.event_handler = parent_handle_reload_command; /* listens to Quit */
				netio_add_handler(netio, &reload_listener);
				reload_pid = getppid();
				break;
			}
			break;
		case NSD_QUIT_SYNC:
			/* synchronisation of xfrd, parent and reload */
			if(!nsd->quit_sync_done && reload_listener.fd != -1) {
				sig_atomic_t cmd = NSD_RELOAD;
				/* stop xfrd ipc writes in progress */
				DEBUG(DEBUG_IPC,1, (LOG_INFO,
					"main: ipc send indication reload"));
				if(!write_socket(nsd->xfrd_listener->fd,
					&cmd, sizeof(cmd))) {
					log_msg(LOG_ERR, "server_main: could not send reload "
					"indication to xfrd: %s", strerror(errno));
				}
				/* wait for ACK from xfrd */
				DEBUG(DEBUG_IPC,1, (LOG_INFO, "main: wait ipc reply xfrd"));
				nsd->quit_sync_done = 1;
			}
			nsd->mode = NSD_RUN;
			break;
		case NSD_QUIT:
			/* silent shutdown during reload */
			if(reload_listener.fd != -1) {
				/* acknowledge the quit, to sync reload that we will really quit now */
				sig_atomic_t cmd = NSD_RELOAD;
				DEBUG(DEBUG_IPC,1, (LOG_INFO, "main: ipc ack reload"));
				if(!write_socket(reload_listener.fd, &cmd, sizeof(cmd))) {
					log_msg(LOG_ERR, "server_main: "
						"could not ack quit: %s", strerror(errno));
				}
#ifdef BIND8_STATS
				parent_send_stats(nsd, reload_listener.fd);
#endif /* BIND8_STATS */
				close(reload_listener.fd);
			}
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "server_main: shutdown sequence"));
			/* only quit children after xfrd has acked */
			send_children_quit(nsd);

#if 0 /* OS collects memory pages */
			region_destroy(server_region);
#endif
			server_shutdown(nsd);

			/* ENOTREACH */
			break;
		case NSD_SHUTDOWN:
			break;
		case NSD_REAP_CHILDREN:
			/* continue; wait for child in run loop */
			nsd->mode = NSD_RUN;
			break;
		case NSD_STATS:
#ifdef BIND8_STATS
			set_children_stats(nsd);
#endif
			nsd->mode = NSD_RUN;
			break;
		default:
			log_msg(LOG_WARNING, "NSD main server mode invalid: %d", (int)nsd->mode);
			nsd->mode = NSD_RUN;
			break;
		}
	}
	log_msg(LOG_WARNING, "signal received, shutting down...");

	/* close opened ports to avoid race with restart of nsd */
	server_close_all_sockets(nsd->udp, nsd->ifs);
	server_close_all_sockets(nsd->tcp, nsd->ifs);
#ifdef HAVE_SSL
	daemon_remote_close(nsd->rc);
#endif
	send_children_quit_and_wait(nsd);

	/* Unlink it if possible... */
	unlinkpid(nsd->pidfile);
	unlink(nsd->task[0]->fname);
	unlink(nsd->task[1]->fname);
#ifdef USE_ZONE_STATS
	unlink(nsd->zonestatfname[0]);
	unlink(nsd->zonestatfname[1]);
#endif

	if(reload_listener.fd != -1) {
		sig_atomic_t cmd = NSD_QUIT;
		DEBUG(DEBUG_IPC,1, (LOG_INFO,
			"main: ipc send quit to reload-process"));
		if(!write_socket(reload_listener.fd, &cmd, sizeof(cmd))) {
			log_msg(LOG_ERR, "server_main: could not send quit to reload: %s",
				strerror(errno));
		}
		fsync(reload_listener.fd);
		close(reload_listener.fd);
		/* wait for reload to finish processing */
		while(1) {
			if(waitpid(reload_pid, NULL, 0) == -1) {
				if(errno == EINTR) continue;
				if(errno == ECHILD) break;
				log_msg(LOG_ERR, "waitpid(reload %d): %s",
					(int)reload_pid, strerror(errno));
			}
			break;
		}
	}
	if(nsd->xfrd_listener->fd != -1) {
		/* complete quit, stop xfrd */
		sig_atomic_t cmd = NSD_QUIT;
		DEBUG(DEBUG_IPC,1, (LOG_INFO,
			"main: ipc send quit to xfrd"));
		if(!write_socket(nsd->xfrd_listener->fd, &cmd, sizeof(cmd))) {
			log_msg(LOG_ERR, "server_main: could not send quit to xfrd: %s",
				strerror(errno));
		}
		fsync(nsd->xfrd_listener->fd);
		close(nsd->xfrd_listener->fd);
		(void)kill(nsd->pid, SIGTERM);
	}

#if 0 /* OS collects memory pages */
	region_destroy(server_region);
#endif
	/* write the nsd.db to disk, wait for it to complete */
	udb_base_sync(nsd->db->udb, 1);
	udb_base_close(nsd->db->udb);
	server_shutdown(nsd);
}

static query_state_type
server_process_query(struct nsd *nsd, struct query *query)
{
	return query_process(query, nsd);
}

static query_state_type
server_process_query_udp(struct nsd *nsd, struct query *query)
{
#ifdef RATELIMIT
	if(query_process(query, nsd) != QUERY_DISCARDED) {
		if(rrl_process_query(query))
			return rrl_slip(query);
		else	return QUERY_PROCESSED;
	}
	return QUERY_DISCARDED;
#else
	return query_process(query, nsd);
#endif
}

struct event_base*
nsd_child_event_base(void)
{
	struct event_base* base;
#ifdef USE_MINI_EVENT
	static time_t secs;
	static struct timeval now;
	base = event_init(&secs, &now);
#else
#  if defined(HAVE_EV_LOOP) || defined(HAVE_EV_DEFAULT_LOOP)
	/* libev */
	base = (struct event_base *)ev_default_loop(EVFLAG_AUTO);
#  else
	/* libevent */
#    ifdef HAVE_EVENT_BASE_NEW
	base = event_base_new();
#    else
	base = event_init();
#    endif
#  endif
#endif
	return base;
}

/*
 * Serve DNS requests.
 */
void
server_child(struct nsd *nsd)
{
	size_t i, from, numifs;
	region_type *server_region = region_create(xalloc, free);
	struct event_base* event_base = nsd_child_event_base();
	query_type *udp_query;
	sig_atomic_t mode;

	if(!event_base) {
		log_msg(LOG_ERR, "nsd server could not create event base");
		exit(1);
	}

#ifdef RATELIMIT
	rrl_init(nsd->this_child->child_num);
#endif

	assert(nsd->server_kind != NSD_SERVER_MAIN);
	DEBUG(DEBUG_IPC, 2, (LOG_INFO, "child process started"));

	if (!(nsd->server_kind & NSD_SERVER_TCP)) {
		server_close_all_sockets(nsd->tcp, nsd->ifs);
	}
	if (!(nsd->server_kind & NSD_SERVER_UDP)) {
		server_close_all_sockets(nsd->udp, nsd->ifs);
	}

	if (nsd->this_child && nsd->this_child->parent_fd != -1) {
		struct event *handler;
		struct ipc_handler_conn_data* user_data =
			(struct ipc_handler_conn_data*)region_alloc(
			server_region, sizeof(struct ipc_handler_conn_data));
		user_data->nsd = nsd;
		user_data->conn = xfrd_tcp_create(server_region, QIOBUFSZ);

		handler = (struct event*) region_alloc(
			server_region, sizeof(*handler));
		event_set(handler, nsd->this_child->parent_fd, EV_PERSIST|
			EV_READ, child_handle_parent_command, user_data);
		if(event_base_set(event_base, handler) != 0)
			log_msg(LOG_ERR, "nsd ipcchild: event_base_set failed");
		if(event_add(handler, NULL) != 0)
			log_msg(LOG_ERR, "nsd ipcchild: event_add failed");
	}

	if(nsd->reuseport) {
		numifs = nsd->ifs / nsd->reuseport;
		from = numifs * nsd->this_child->child_num;
		if(from+numifs > nsd->ifs) { /* should not happen */
			from = 0;
			numifs = nsd->ifs;
		}
	} else {
		from = 0;
		numifs = nsd->ifs;
	}

	if (nsd->server_kind & NSD_SERVER_UDP) {
#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
		udp_query = query_create(server_region,
			compressed_dname_offsets, compression_table_size);
#else
		udp_query = NULL;
		memset(msgs, 0, sizeof(msgs));
		for (i = 0; i < NUM_RECV_PER_SELECT; i++) {
			queries[i] = query_create(server_region,
				compressed_dname_offsets, compression_table_size);
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_base          = buffer_begin(queries[i]->packet);
			iovecs[i].iov_len           = buffer_remaining(queries[i]->packet);;
			msgs[i].msg_hdr.msg_iov     = &iovecs[i];
			msgs[i].msg_hdr.msg_iovlen  = 1;
			msgs[i].msg_hdr.msg_name    = &queries[i]->addr;
			msgs[i].msg_hdr.msg_namelen = queries[i]->addrlen;
		}
#endif
		for (i = from; i < from+numifs; ++i) {
			struct udp_handler_data *data;
			struct event *handler;

			data = (struct udp_handler_data *) region_alloc(
				server_region,
				sizeof(struct udp_handler_data));
			data->query = udp_query;
			data->nsd = nsd;
			data->socket = &nsd->udp[i];

			handler = (struct event*) region_alloc(
				server_region, sizeof(*handler));
			event_set(handler, nsd->udp[i].s, EV_PERSIST|EV_READ,
				handle_udp, data);
			if(event_base_set(event_base, handler) != 0)
				log_msg(LOG_ERR, "nsd udp: event_base_set failed");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "nsd udp: event_add failed");
		}
	}

	/*
	 * Keep track of all the TCP accept handlers so we can enable
	 * and disable them based on the current number of active TCP
	 * connections.
	 */
	tcp_accept_handler_count = numifs;
	tcp_accept_handlers = (struct tcp_accept_handler_data*)
		region_alloc_array(server_region,
		numifs, sizeof(*tcp_accept_handlers));
	if (nsd->server_kind & NSD_SERVER_TCP) {
		for (i = from; i < numifs; ++i) {
			struct event *handler = &tcp_accept_handlers[i-from].event;
			struct tcp_accept_handler_data* data =
				&tcp_accept_handlers[i-from];
			data->nsd = nsd;
			data->socket = &nsd->tcp[i];
			event_set(handler, nsd->tcp[i].s, EV_PERSIST|EV_READ,
				handle_tcp_accept, data);
			if(event_base_set(event_base, handler) != 0)
				log_msg(LOG_ERR, "nsd tcp: event_base_set failed");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "nsd tcp: event_add failed");
			data->event_added = 1;
		}
	} else tcp_accept_handler_count = 0;

	/* The main loop... */
	while ((mode = nsd->mode) != NSD_QUIT) {
		if(mode == NSD_RUN) nsd->mode = mode = server_signal_mode(nsd);

		/* Do we need to do the statistics... */
		if (mode == NSD_STATS) {
#ifdef BIND8_STATS
			int p = nsd->st.period;
			nsd->st.period = 1; /* force stats printout */
			/* Dump the statistics */
			bind8_stats(nsd);
			nsd->st.period = p;
#else /* !BIND8_STATS */
			log_msg(LOG_NOTICE, "Statistics support not enabled at compile time.");
#endif /* BIND8_STATS */

			nsd->mode = NSD_RUN;
		}
		else if (mode == NSD_REAP_CHILDREN) {
			/* got signal, notify parent. parent reaps terminated children. */
			if (nsd->this_child->parent_fd != -1) {
				sig_atomic_t parent_notify = NSD_REAP_CHILDREN;
				if (write(nsd->this_child->parent_fd,
				    &parent_notify,
				    sizeof(parent_notify)) == -1)
				{
					log_msg(LOG_ERR, "problems sending command from %d to parent: %s",
						(int) nsd->this_child->pid, strerror(errno));
				}
			} else /* no parent, so reap 'em */
				while (waitpid(-1, NULL, WNOHANG) > 0) ;
			nsd->mode = NSD_RUN;
		}
		else if(mode == NSD_RUN) {
			/* Wait for a query... */
			if(event_base_loop(event_base, EVLOOP_ONCE) == -1) {
				if (errno != EINTR) {
					log_msg(LOG_ERR, "dispatch failed: %s", strerror(errno));
					break;
				}
			}
		} else if(mode == NSD_QUIT) {
			/* ignore here, quit */
		} else {
			log_msg(LOG_ERR, "mode bad value %d, back to service.",
				(int)mode);
			nsd->mode = NSD_RUN;
		}
	}

#ifdef	BIND8_STATS
	bind8_stats(nsd);
#endif /* BIND8_STATS */

#if 0 /* OS collects memory pages */
	event_base_free(event_base);
	region_destroy(server_region);
#endif
	server_shutdown(nsd);
}

#if defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG)
static void
handle_udp(int fd, short event, void* arg)
{
	struct udp_handler_data *data = (struct udp_handler_data *) arg;
	int received, sent, recvcount, i;
	struct query *q;

	if (!(event & EV_READ)) {
		return;
	}
	recvcount = recvmmsg(fd, msgs, NUM_RECV_PER_SELECT, 0, NULL);
	/* this printf strangely gave a performance increase on Linux */
	/* printf("recvcount %d \n", recvcount); */
	if (recvcount == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			log_msg(LOG_ERR, "recvmmsg failed: %s", strerror(errno));
			STATUP(data->nsd, rxerr);
			/* No zone statup */
		}
		/* Simply no data available */
		return;
	}
	for (i = 0; i < recvcount; i++) {
	loopstart:
		received = msgs[i].msg_len;
		q = queries[i];
		if (received == -1) {
			log_msg(LOG_ERR, "recvmmsg %d failed %s", i, strerror(
				msgs[i].msg_hdr.msg_flags));
			STATUP(data->nsd, rxerr);
			/* No zone statup */
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_len = buffer_remaining(q->packet);
			goto swap_drop;
		}

		/* Account... */
#ifdef BIND8_STATS
		if (data->socket->fam == AF_INET) {
			STATUP(data->nsd, qudp);
		} else if (data->socket->fam == AF_INET6) {
			STATUP(data->nsd, qudp6);
		}
#endif

		buffer_skip(q->packet, received);
		buffer_flip(q->packet);

		/* Process and answer the query... */
		if (server_process_query_udp(data->nsd, q) != QUERY_DISCARDED) {
			if (RCODE(q->packet) == RCODE_OK && !AA(q->packet)) {
				STATUP(data->nsd, nona);
				ZTATUP(data->nsd, q->zone, nona);
			}

#ifdef USE_ZONE_STATS
			if (data->socket->fam == AF_INET) {
				ZTATUP(data->nsd, q->zone, qudp);
			} else if (data->socket->fam == AF_INET6) {
				ZTATUP(data->nsd, q->zone, qudp6);
			}
#endif

			/* Add EDNS0 and TSIG info if necessary.  */
			query_add_optional(q, data->nsd);

			buffer_flip(q->packet);
			iovecs[i].iov_len = buffer_remaining(q->packet);
#ifdef BIND8_STATS
			/* Account the rcode & TC... */
			STATUP2(data->nsd, rcode, RCODE(q->packet));
			ZTATUP2(data->nsd, q->zone, rcode, RCODE(q->packet));
			if (TC(q->packet)) {
				STATUP(data->nsd, truncated);
				ZTATUP(data->nsd, q->zone, truncated);
			}
#endif /* BIND8_STATS */
		} else {
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_len = buffer_remaining(q->packet);
		swap_drop:
			STATUP(data->nsd, dropped);
			ZTATUP(data->nsd, q->zone, dropped);
			if(i != recvcount-1) {
				/* swap with last and decrease recvcount */
				struct mmsghdr mtmp = msgs[i];
				struct iovec iotmp = iovecs[i];
				recvcount--;
				msgs[i] = msgs[recvcount];
				iovecs[i] = iovecs[recvcount];
				queries[i] = queries[recvcount];
				msgs[recvcount] = mtmp;
				iovecs[recvcount] = iotmp;
				queries[recvcount] = q;
				msgs[i].msg_hdr.msg_iov = &iovecs[i];
				msgs[recvcount].msg_hdr.msg_iov = &iovecs[recvcount];
				goto loopstart;
			} else { recvcount --; }
		}
	}

	/* send until all are sent */
	i = 0;
	while(i<recvcount) {
		sent = sendmmsg(fd, &msgs[i], recvcount-i, 0);
		if(sent == -1) {
			const char* es = strerror(errno);
			char a[48];
			addr2str(&queries[i]->addr, a, sizeof(a));
			log_msg(LOG_ERR, "sendmmsg [0]=%s count=%d failed: %s", a, (int)(recvcount-i), es);
#ifdef BIND8_STATS
			data->nsd->st.txerr += recvcount-i;
#endif /* BIND8_STATS */
			break;
		}
		i += sent;
	}
	for(i=0; i<recvcount; i++) {
		query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
		iovecs[i].iov_len = buffer_remaining(queries[i]->packet);
	}
}

#else /* defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG) */

static void
handle_udp(int fd, short event, void* arg)
{
	struct udp_handler_data *data = (struct udp_handler_data *) arg;
	int received, sent;
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
	int recvcount;
#endif /* HAVE_RECVMMSG */
	int i;
#endif /* NONBLOCKING_IS_BROKEN */
	struct query *q;
#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
	q = data->query;
#endif

	if (!(event & EV_READ)) {
		return;
	}
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
	recvcount = recvmmsg(fd, msgs, NUM_RECV_PER_SELECT, 0, NULL);
	/* this printf strangely gave a performance increase on Linux */
	/* printf("recvcount %d \n", recvcount); */
	if (recvcount == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			log_msg(LOG_ERR, "recvmmsg failed: %s", strerror(errno));
			STATUP(data->nsd, rxerr);
			/* No zone statup */
		}
		/* Simply no data available */
		return;
	}
	for (i = 0; i < recvcount; i++) {
		received = msgs[i].msg_len;
		msgs[i].msg_hdr.msg_namelen = queries[i]->addrlen;
		if (received == -1) {
			log_msg(LOG_ERR, "recvmmsg failed");
			STATUP(data->nsd, rxerr);
			/* No zone statup */
			/* the error can be found in msgs[i].msg_hdr.msg_flags */
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			continue;
		}
		q = queries[i];
#else
	for(i=0; i<NUM_RECV_PER_SELECT; i++) {
#endif /* HAVE_RECVMMSG */
#endif /* NONBLOCKING_IS_BROKEN */

#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
		/* Initialize the query... */
		query_reset(q, UDP_MAX_MESSAGE_LEN, 0);

		received = recvfrom(fd,
				    buffer_begin(q->packet),
				    buffer_remaining(q->packet),
				    0,
				    (struct sockaddr *)&q->addr,
				    &q->addrlen);
		if (received == -1) {
			if (errno != EAGAIN && errno != EINTR) {
				log_msg(LOG_ERR, "recvfrom failed: %s", strerror(errno));
				STATUP(data->nsd, rxerr);
				/* No zone statup */
			}
			return;
		}
#endif /* NONBLOCKING_IS_BROKEN || !HAVE_RECVMMSG */

		/* Account... */
		if (data->socket->fam == AF_INET) {
			STATUP(data->nsd, qudp);
		} else if (data->socket->fam == AF_INET6) {
			STATUP(data->nsd, qudp6);
		}

		buffer_skip(q->packet, received);
		buffer_flip(q->packet);

		/* Process and answer the query... */
		if (server_process_query_udp(data->nsd, q) != QUERY_DISCARDED) {
			if (RCODE(q->packet) == RCODE_OK && !AA(q->packet)) {
				STATUP(data->nsd, nona);
				ZTATUP(data->nsd, q->zone, nona);
			}

#ifdef USE_ZONE_STATS
			if (data->socket->fam == AF_INET) {
				ZTATUP(data->nsd, q->zone, qudp);
			} else if (data->socket->fam == AF_INET6) {
				ZTATUP(data->nsd, q->zone, qudp6);
			}
#endif

			/* Add EDNS0 and TSIG info if necessary.  */
			query_add_optional(q, data->nsd);

			buffer_flip(q->packet);

			sent = sendto(fd,
				      buffer_begin(q->packet),
				      buffer_remaining(q->packet),
				      0,
				      (struct sockaddr *) &q->addr,
				      q->addrlen);
			if (sent == -1) {
				const char* es = strerror(errno);
				char a[48];
				addr2str(&q->addr, a, sizeof(a));
				log_msg(LOG_ERR, "sendto %s failed: %s", a, es);
				STATUP(data->nsd, txerr);
				ZTATUP(data->nsd, q->zone, txerr);
			} else if ((size_t) sent != buffer_remaining(q->packet)) {
				log_msg(LOG_ERR, "sent %d in place of %d bytes", sent, (int) buffer_remaining(q->packet));
			} else {
#ifdef BIND8_STATS
				/* Account the rcode & TC... */
				STATUP2(data->nsd, rcode, RCODE(q->packet));
				ZTATUP2(data->nsd, q->zone, rcode, RCODE(q->packet));
				if (TC(q->packet)) {
					STATUP(data->nsd, truncated);
					ZTATUP(data->nsd, q->zone, truncated);
				}
#endif /* BIND8_STATS */
			}
		} else {
			STATUP(data->nsd, dropped);
			ZTATUP(data->nsd, q->zone, dropped);
		}
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
		query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
#endif
	}
#endif
}
#endif /* defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG) */


static void
cleanup_tcp_handler(struct tcp_handler_data* data)
{
	event_del(&data->event);
	close(data->event.ev_fd);

	/*
	 * Enable the TCP accept handlers when the current number of
	 * TCP connections is about to drop below the maximum number
	 * of TCP connections.
	 */
	if (slowaccept || data->nsd->current_tcp_count == data->nsd->maximum_tcp_count) {
		configure_handler_event_types(EV_READ|EV_PERSIST);
		if(slowaccept) {
			event_del(&slowaccept_event);
			slowaccept = 0;
		}
	}
	--data->nsd->current_tcp_count;
	assert(data->nsd->current_tcp_count >= 0);

	region_destroy(data->region);
}

static void
handle_tcp_reading(int fd, short event, void* arg)
{
	struct tcp_handler_data *data = (struct tcp_handler_data *) arg;
	ssize_t received;
	struct event_base* ev_base;
	struct timeval timeout;

	if ((event & EV_TIMEOUT)) {
		/* Connection timed out.  */
		cleanup_tcp_handler(data);
		return;
	}

	if (data->nsd->tcp_query_count > 0 &&
		data->query_count >= data->nsd->tcp_query_count) {
		/* No more queries allowed on this tcp connection.  */
		cleanup_tcp_handler(data);
		return;
	}

	assert((event & EV_READ));

	if (data->bytes_transmitted == 0) {
		query_reset(data->query, TCP_MAX_MESSAGE_LEN, 1);
	}

	/*
	 * Check if we received the leading packet length bytes yet.
	 */
	if (data->bytes_transmitted < sizeof(uint16_t)) {
		received = read(fd,
				(char *) &data->query->tcplen
				+ data->bytes_transmitted,
				sizeof(uint16_t) - data->bytes_transmitted);
		if (received == -1) {
			if (errno == EAGAIN || errno == EINTR) {
				/*
				 * Read would block, wait until more
				 * data is available.
				 */
				return;
			} else {
				char buf[48];
				addr2str(&data->query->addr, buf, sizeof(buf));
#ifdef ECONNRESET
				if (verbosity >= 2 || errno != ECONNRESET)
#endif /* ECONNRESET */
				log_msg(LOG_ERR, "failed reading from %s tcp: %s", buf, strerror(errno));
				cleanup_tcp_handler(data);
				return;
			}
		} else if (received == 0) {
			/* EOF */
			cleanup_tcp_handler(data);
			return;
		}

		data->bytes_transmitted += received;
		if (data->bytes_transmitted < sizeof(uint16_t)) {
			/*
			 * Not done with the tcplen yet, wait for more
			 * data to become available.
			 */
			return;
		}

		assert(data->bytes_transmitted == sizeof(uint16_t));

		data->query->tcplen = ntohs(data->query->tcplen);

		/*
		 * Minimum query size is:
		 *
		 *     Size of the header (12)
		 *   + Root domain name   (1)
		 *   + Query class        (2)
		 *   + Query type         (2)
		 */
		if (data->query->tcplen < QHEADERSZ + 1 + sizeof(uint16_t) + sizeof(uint16_t)) {
			VERBOSITY(2, (LOG_WARNING, "packet too small, dropping tcp connection"));
			cleanup_tcp_handler(data);
			return;
		}

		if (data->query->tcplen > data->query->maxlen) {
			VERBOSITY(2, (LOG_WARNING, "insufficient tcp buffer, dropping connection"));
			cleanup_tcp_handler(data);
			return;
		}

		buffer_set_limit(data->query->packet, data->query->tcplen);
	}

	assert(buffer_remaining(data->query->packet) > 0);

	/* Read the (remaining) query data.  */
	received = read(fd,
			buffer_current(data->query->packet),
			buffer_remaining(data->query->packet));
	if (received == -1) {
		if (errno == EAGAIN || errno == EINTR) {
			/*
			 * Read would block, wait until more data is
			 * available.
			 */
			return;
		} else {
			char buf[48];
			addr2str(&data->query->addr, buf, sizeof(buf));
#ifdef ECONNRESET
			if (verbosity >= 2 || errno != ECONNRESET)
#endif /* ECONNRESET */
			log_msg(LOG_ERR, "failed reading from %s tcp: %s", buf, strerror(errno));
			cleanup_tcp_handler(data);
			return;
		}
	} else if (received == 0) {
		/* EOF */
		cleanup_tcp_handler(data);
		return;
	}

	data->bytes_transmitted += received;
	buffer_skip(data->query->packet, received);
	if (buffer_remaining(data->query->packet) > 0) {
		/*
		 * Message not yet complete, wait for more data to
		 * become available.
		 */
		return;
	}

	assert(buffer_position(data->query->packet) == data->query->tcplen);

	/* Account... */
#ifdef BIND8_STATS
#ifndef INET6
	STATUP(data->nsd, ctcp);
#else
	if (data->query->addr.ss_family == AF_INET) {
		STATUP(data->nsd, ctcp);
	} else if (data->query->addr.ss_family == AF_INET6) {
		STATUP(data->nsd, ctcp6);
	}
#endif
#endif /* BIND8_STATS */

	/* We have a complete query, process it.  */

	/* tcp-query-count: handle query counter ++ */
	data->query_count++;

	buffer_flip(data->query->packet);
	data->query_state = server_process_query(data->nsd, data->query);
	if (data->query_state == QUERY_DISCARDED) {
		/* Drop the packet and the entire connection... */
		STATUP(data->nsd, dropped);
		ZTATUP(data->nsd, data->query->zone, dropped);
		cleanup_tcp_handler(data);
		return;
	}

#ifdef BIND8_STATS
	if (RCODE(data->query->packet) == RCODE_OK
	    && !AA(data->query->packet))
	{
		STATUP(data->nsd, nona);
		ZTATUP(data->nsd, data->query->zone, nona);
	}
#endif /* BIND8_STATS */

#ifdef USE_ZONE_STATS
#ifndef INET6
	ZTATUP(data->nsd, data->query->zone, ctcp);
#else
	if (data->query->addr.ss_family == AF_INET) {
		ZTATUP(data->nsd, data->query->zone, ctcp);
	} else if (data->query->addr.ss_family == AF_INET6) {
		ZTATUP(data->nsd, data->query->zone, ctcp6);
	}
#endif
#endif /* USE_ZONE_STATS */

	query_add_optional(data->query, data->nsd);

	/* Switch to the tcp write handler.  */
	buffer_flip(data->query->packet);
	data->query->tcplen = buffer_remaining(data->query->packet);
	data->bytes_transmitted = 0;

	timeout.tv_sec = data->tcp_timeout / 1000;
	timeout.tv_usec = (data->tcp_timeout % 1000)*1000;

	ev_base = data->event.ev_base;
	event_del(&data->event);
	event_set(&data->event, fd, EV_PERSIST | EV_WRITE | EV_TIMEOUT,
		handle_tcp_writing, data);
	if(event_base_set(ev_base, &data->event) != 0)
		log_msg(LOG_ERR, "event base set tcpr failed");
	if(event_add(&data->event, &timeout) != 0)
		log_msg(LOG_ERR, "event add tcpr failed");
	/* see if we can write the answer right away(usually so,EAGAIN ifnot)*/
	handle_tcp_writing(fd, EV_WRITE, data);
}

static void
handle_tcp_writing(int fd, short event, void* arg)
{
	struct tcp_handler_data *data = (struct tcp_handler_data *) arg;
	ssize_t sent;
	struct query *q = data->query;
	struct timeval timeout;
	struct event_base* ev_base;

	if ((event & EV_TIMEOUT)) {
		/* Connection timed out.  */
		cleanup_tcp_handler(data);
		return;
	}

	assert((event & EV_WRITE));

	if (data->bytes_transmitted < sizeof(q->tcplen)) {
		/* Writing the response packet length.  */
		uint16_t n_tcplen = htons(q->tcplen);
#ifdef HAVE_WRITEV
		struct iovec iov[2];
		iov[0].iov_base = (uint8_t*)&n_tcplen + data->bytes_transmitted;
		iov[0].iov_len = sizeof(n_tcplen) - data->bytes_transmitted; 
		iov[1].iov_base = buffer_begin(q->packet);
		iov[1].iov_len = buffer_limit(q->packet);
		sent = writev(fd, iov, 2);
#else /* HAVE_WRITEV */
		sent = write(fd,
			     (const char *) &n_tcplen + data->bytes_transmitted,
			     sizeof(n_tcplen) - data->bytes_transmitted);
#endif /* HAVE_WRITEV */
		if (sent == -1) {
			if (errno == EAGAIN || errno == EINTR) {
				/*
				 * Write would block, wait until
				 * socket becomes writable again.
				 */
				return;
			} else {
#ifdef ECONNRESET
				if(verbosity >= 2 || errno != ECONNRESET)
#endif /* ECONNRESET */
#ifdef EPIPE
				  if(verbosity >= 2 || errno != EPIPE)
#endif /* EPIPE 'broken pipe' */
				    log_msg(LOG_ERR, "failed writing to tcp: %s", strerror(errno));
				cleanup_tcp_handler(data);
				return;
			}
		}

		data->bytes_transmitted += sent;
		if (data->bytes_transmitted < sizeof(q->tcplen)) {
			/*
			 * Writing not complete, wait until socket
			 * becomes writable again.
			 */
			return;
		}

#ifdef HAVE_WRITEV
		sent -= sizeof(n_tcplen);
		/* handle potential 'packet done' code */
		goto packet_could_be_done;
#endif
 	}
 
	sent = write(fd,
		     buffer_current(q->packet),
		     buffer_remaining(q->packet));
	if (sent == -1) {
		if (errno == EAGAIN || errno == EINTR) {
			/*
			 * Write would block, wait until
			 * socket becomes writable again.
			 */
			return;
		} else {
#ifdef ECONNRESET
			if(verbosity >= 2 || errno != ECONNRESET)
#endif /* ECONNRESET */
#ifdef EPIPE
				  if(verbosity >= 2 || errno != EPIPE)
#endif /* EPIPE 'broken pipe' */
			log_msg(LOG_ERR, "failed writing to tcp: %s", strerror(errno));
			cleanup_tcp_handler(data);
			return;
		}
	}

	data->bytes_transmitted += sent;
#ifdef HAVE_WRITEV
  packet_could_be_done:
#endif
	buffer_skip(q->packet, sent);
	if (data->bytes_transmitted < q->tcplen + sizeof(q->tcplen)) {
		/*
		 * Still more data to write when socket becomes
		 * writable again.
		 */
		return;
	}

	assert(data->bytes_transmitted == q->tcplen + sizeof(q->tcplen));

	if (data->query_state == QUERY_IN_AXFR) {
		/* Continue processing AXFR and writing back results.  */
		buffer_clear(q->packet);
		data->query_state = query_axfr(data->nsd, q);
		if (data->query_state != QUERY_PROCESSED) {
			query_add_optional(data->query, data->nsd);

			/* Reset data. */
			buffer_flip(q->packet);
			q->tcplen = buffer_remaining(q->packet);
			data->bytes_transmitted = 0;
			/* Reset timeout.  */
			timeout.tv_sec = data->tcp_timeout / 1000;
			timeout.tv_usec = (data->tcp_timeout % 1000)*1000;
			ev_base = data->event.ev_base;
			event_del(&data->event);
			event_set(&data->event, fd, EV_PERSIST | EV_WRITE | EV_TIMEOUT,
				handle_tcp_writing, data);
			if(event_base_set(ev_base, &data->event) != 0)
				log_msg(LOG_ERR, "event base set tcpw failed");
			if(event_add(&data->event, &timeout) != 0)
				log_msg(LOG_ERR, "event add tcpw failed");

			/*
			 * Write data if/when the socket is writable
			 * again.
			 */
			return;
		}
	}

	/*
	 * Done sending, wait for the next request to arrive on the
	 * TCP socket by installing the TCP read handler.
	 */
	if (data->nsd->tcp_query_count > 0 &&
		data->query_count >= data->nsd->tcp_query_count) {

		(void) shutdown(fd, SHUT_WR);
	}

	data->bytes_transmitted = 0;

	timeout.tv_sec = data->tcp_timeout / 1000;
	timeout.tv_usec = (data->tcp_timeout % 1000)*1000;
	ev_base = data->event.ev_base;
	event_del(&data->event);
	event_set(&data->event, fd, EV_PERSIST | EV_READ | EV_TIMEOUT,
		handle_tcp_reading, data);
	if(event_base_set(ev_base, &data->event) != 0)
		log_msg(LOG_ERR, "event base set tcpw failed");
	if(event_add(&data->event, &timeout) != 0)
		log_msg(LOG_ERR, "event add tcpw failed");
}


static void
handle_slowaccept_timeout(int ATTR_UNUSED(fd), short ATTR_UNUSED(event),
	void* ATTR_UNUSED(arg))
{
	if(slowaccept) {
		configure_handler_event_types(EV_PERSIST | EV_READ);
		slowaccept = 0;
	}
}

/*
 * Handle an incoming TCP connection.  The connection is accepted and
 * a new TCP reader event handler is added.  The TCP handler
 * is responsible for cleanup when the connection is closed.
 */
static void
handle_tcp_accept(int fd, short event, void* arg)
{
	struct tcp_accept_handler_data *data
		= (struct tcp_accept_handler_data *) arg;
	int s;
	struct tcp_handler_data *tcp_data;
	region_type *tcp_region;
#ifdef INET6
	struct sockaddr_storage addr;
#else
	struct sockaddr_in addr;
#endif
	socklen_t addrlen;
	struct timeval timeout;

	if (!(event & EV_READ)) {
		return;
	}

	if (data->nsd->current_tcp_count >= data->nsd->maximum_tcp_count) {
		return;
	}

	/* Accept it... */
	addrlen = sizeof(addr);
	s = accept(fd, (struct sockaddr *) &addr, &addrlen);
	if (s == -1) {
		/**
		 * EMFILE and ENFILE is a signal that the limit of open
		 * file descriptors has been reached. Pause accept().
		 * EINTR is a signal interrupt. The others are various OS ways
		 * of saying that the client has closed the connection.
		 */
		if (errno == EMFILE || errno == ENFILE) {
			if (!slowaccept) {
				/* disable accept events */
				struct timeval tv;
				configure_handler_event_types(0);
				tv.tv_sec = SLOW_ACCEPT_TIMEOUT;
				tv.tv_usec = 0L;
				event_set(&slowaccept_event, -1, EV_TIMEOUT,
					handle_slowaccept_timeout, NULL);
				(void)event_base_set(data->event.ev_base,
					&slowaccept_event);
				(void)event_add(&slowaccept_event, &tv);
				slowaccept = 1;
				/* We don't want to spam the logs here */
			}
		} else if (errno != EINTR
			&& errno != EWOULDBLOCK
#ifdef ECONNABORTED
			&& errno != ECONNABORTED
#endif /* ECONNABORTED */
#ifdef EPROTO
			&& errno != EPROTO
#endif /* EPROTO */
			) {
			log_msg(LOG_ERR, "accept failed: %s", strerror(errno));
		}
		return;
	}

	if (fcntl(s, F_SETFL, O_NONBLOCK) == -1) {
		log_msg(LOG_ERR, "fcntl failed: %s", strerror(errno));
		close(s);
		return;
	}

	/*
	 * This region is deallocated when the TCP connection is
	 * closed by the TCP handler.
	 */
	tcp_region = region_create(xalloc, free);
	tcp_data = (struct tcp_handler_data *) region_alloc(
		tcp_region, sizeof(struct tcp_handler_data));
	tcp_data->region = tcp_region;
	tcp_data->query = query_create(tcp_region, compressed_dname_offsets,
		compression_table_size);
	tcp_data->nsd = data->nsd;
	tcp_data->query_count = 0;

	tcp_data->query_state = QUERY_PROCESSED;
	tcp_data->bytes_transmitted = 0;
	memcpy(&tcp_data->query->addr, &addr, addrlen);
	tcp_data->query->addrlen = addrlen;

	tcp_data->tcp_timeout = data->nsd->tcp_timeout * 1000;
	if (data->nsd->current_tcp_count > data->nsd->maximum_tcp_count/2) {
		/* very busy, give smaller timeout */
		tcp_data->tcp_timeout = 200;
	}
	timeout.tv_sec = tcp_data->tcp_timeout / 1000;
	timeout.tv_usec = (tcp_data->tcp_timeout % 1000)*1000;

	event_set(&tcp_data->event, s, EV_PERSIST | EV_READ | EV_TIMEOUT,
		handle_tcp_reading, tcp_data);
	if(event_base_set(data->event.ev_base, &tcp_data->event) != 0) {
		log_msg(LOG_ERR, "cannot set tcp event base");
		close(s);
		region_destroy(tcp_region);
		return;
	}
	if(event_add(&tcp_data->event, &timeout) != 0) {
		log_msg(LOG_ERR, "cannot add tcp to event base");
		close(s);
		region_destroy(tcp_region);
		return;
	}

	/*
	 * Keep track of the total number of TCP handlers installed so
	 * we can stop accepting connections when the maximum number
	 * of simultaneous TCP connections is reached.
	 */
	++data->nsd->current_tcp_count;
	if (data->nsd->current_tcp_count == data->nsd->maximum_tcp_count) {
		configure_handler_event_types(0);
	}
}

static void
send_children_command(struct nsd* nsd, sig_atomic_t command, int timeout)
{
	size_t i;
	assert(nsd->server_kind == NSD_SERVER_MAIN && nsd->this_child == 0);
	for (i = 0; i < nsd->child_count; ++i) {
		if (nsd->children[i].pid > 0 && nsd->children[i].child_fd != -1) {
			if (write(nsd->children[i].child_fd,
				&command,
				sizeof(command)) == -1)
			{
				if(errno != EAGAIN && errno != EINTR)
					log_msg(LOG_ERR, "problems sending command %d to server %d: %s",
					(int) command,
					(int) nsd->children[i].pid,
					strerror(errno));
			} else if (timeout > 0) {
				(void)block_read(NULL,
					nsd->children[i].child_fd,
					&command, sizeof(command), timeout);
			}
			fsync(nsd->children[i].child_fd);
			close(nsd->children[i].child_fd);
			nsd->children[i].child_fd = -1;
		}
	}
}

static void
send_children_quit(struct nsd* nsd)
{
	DEBUG(DEBUG_IPC, 1, (LOG_INFO, "send children quit"));
	send_children_command(nsd, NSD_QUIT, 0);
}

static void
send_children_quit_and_wait(struct nsd* nsd)
{
	DEBUG(DEBUG_IPC, 1, (LOG_INFO, "send children quit and wait"));
	send_children_command(nsd, NSD_QUIT_CHILD, 3);
}

#ifdef BIND8_STATS
static void
set_children_stats(struct nsd* nsd)
{
	size_t i;
	assert(nsd->server_kind == NSD_SERVER_MAIN && nsd->this_child == 0);
	DEBUG(DEBUG_IPC, 1, (LOG_INFO, "parent set stats to send to children"));
	for (i = 0; i < nsd->child_count; ++i) {
		nsd->children[i].need_to_send_STATS = 1;
		nsd->children[i].handler->event_types |= NETIO_EVENT_WRITE;
	}
}
#endif /* BIND8_STATS */

static void
configure_handler_event_types(short event_types)
{
	size_t i;

	for (i = 0; i < tcp_accept_handler_count; ++i) {
		struct event* handler = &tcp_accept_handlers[i].event;
		if(event_types) {
			/* reassign */
			int fd = handler->ev_fd;
			struct event_base* base = handler->ev_base;
			if(tcp_accept_handlers[i].event_added)
				event_del(handler);
			event_set(handler, fd, event_types,
				handle_tcp_accept, &tcp_accept_handlers[i]);
			if(event_base_set(base, handler) != 0)
				log_msg(LOG_ERR, "conhand: cannot event_base");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "conhand: cannot event_add");
			tcp_accept_handlers[i].event_added = 1;
		} else {
			/* remove */
			if(tcp_accept_handlers[i].event_added) {
				event_del(handler);
				tcp_accept_handlers[i].event_added = 0;
			}
		}
	}
}
@


1.26
log
@Update to 4.1.15.
This contains a local patch to query.c (missed _t conversion) that has
been submitted upstream.
OK sthen
@
text
@d40 1
d42 1
@


1.25
log
@update to 4.1.11
"Working fine here." millert@@
OK dlg, sthen
@
text
@d1399 1
a1399 1
			sizeof(stc_t))) {
d1409 1
a1409 1
	stc_t* p;
d1418 1
a1418 1
	p = (stc_t*)task_new_stat_info(nsd->task[nsd->mytask], last, &s,
d1422 2
a1423 1
		if(block_read(nsd, cmdfd, p++, sizeof(stc_t), 1)!=sizeof(stc_t))
@


1.24
log
@Update to 4.1.10
Testing by millert@@, sthen@@ and me.
came up with the same diff & OK sthen@@
@
text
@d163 5
d2638 2
a2639 2
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0L;
d2771 2
a2772 2
			timeout.tv_sec = data->nsd->tcp_timeout;
			timeout.tv_usec = 0L;
d2802 2
a2803 2
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0L;
d2917 7
a2923 2
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0;
@


1.23
log
@update to NSD 4.1.7, ok florian@@
@
text
@d563 1
a563 1
#if defined(SO_REUSEPORT) || defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU) || defined(IP_TRANSPARENT)))
d737 9
d844 15
d867 9
d2431 4
a2434 1
		slowaccept = 0;
@


1.22
log
@pledge for nsd.

The main process handles zone transfers (inet, dns), writing and
creating of zone files (wpath, cpath) and starting of child
processes (proc).

Child processes answer queries from the internet.

"Looks good" to deraadt@@
@
text
@d763 5
@


1.21
log
@merge
@
text
@d304 6
@


1.20
log
@merge conflicts
@
text
@d33 1
d308 1
d551 3
a553 6
/*
 * Initialize the server, create and bind the sockets.
 *
 */
int
server_init(struct nsd *nsd)
d555 1
d557 1
a557 1
#if defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU) || defined(IP_TRANSPARENT)))
d564 4
a567 2
	for (i = 0; i < nsd->ifs; i++) {
		if (!nsd->udp[i].addr) {
d571 2
a572 1
		if ((nsd->udp[i].s = socket(nsd->udp[i].addr->ai_family, nsd->udp[i].addr->ai_socktype, 0)) == -1) {
d574 1
a574 1
			if (nsd->udp[i].addr->ai_family == AF_INET6 &&
d584 16
d655 1
a655 1
		if (nsd->udp[i].addr->ai_family == AF_INET6) {
d701 1
a701 1
		if (nsd->udp[i].addr->ai_family == AF_INET) {
d740 1
a740 1
		if (bind(nsd->udp[i].s, (struct sockaddr *) nsd->udp[i].addr->ai_addr, nsd->udp[i].addr->ai_addrlen) != 0) {
d749 4
a752 2
	for (i = 0; i < nsd->ifs; i++) {
		if (!nsd->tcp[i].addr) {
d756 2
a757 1
		if ((nsd->tcp[i].s = socket(nsd->tcp[i].addr->ai_family, nsd->tcp[i].addr->ai_socktype, 0)) == -1) {
d759 1
a759 1
			if (nsd->tcp[i].addr->ai_family == AF_INET6 &&
d769 14
d790 1
a790 1
		if (nsd->tcp[i].addr->ai_family == AF_INET6) {
d841 1
a841 1
		if (bind(nsd->tcp[i].s, (struct sockaddr *) nsd->tcp[i].addr->ai_addr, nsd->tcp[i].addr->ai_addrlen) != 0) {
d857 37
d978 2
a979 1
			freeaddrinfo(sockets[i].addr);
d1245 5
a1249 4
	fd_set rfds;
	struct timeval tv;
	FD_ZERO(&rfds);

d1252 1
a1252 4
		FD_SET(s, &rfds);
		tv.tv_sec = timeout;
		tv.tv_usec = 0;
		ret = select(s+1, &rfds, NULL, NULL, timeout==-1?NULL:&tv);
d1925 1
a1925 1
	size_t i;
d1937 1
a1937 1
	rrl_init((nsd->this_child - nsd->children)/sizeof(nsd->children[0]));
d1968 12
d1999 1
a1999 1
		for (i = 0; i < nsd->ifs; ++i) {
d2026 1
a2026 1
	tcp_accept_handler_count = nsd->ifs;
d2029 1
a2029 1
		nsd->ifs, sizeof(*tcp_accept_handlers));
d2031 2
a2032 2
		for (i = 0; i < nsd->ifs; ++i) {
			struct event *handler = &tcp_accept_handlers[i].event;
d2034 1
a2034 1
				&tcp_accept_handlers[i];
d2147 1
a2147 1
		if (data->socket->addr->ai_family == AF_INET) {
d2149 1
a2149 1
		} else if (data->socket->addr->ai_family == AF_INET6) {
d2165 1
a2165 1
			if (data->socket->addr->ai_family == AF_INET) {
d2167 1
a2167 1
			} else if (data->socket->addr->ai_family == AF_INET6) {
d2305 1
a2305 1
		if (data->socket->addr->ai_family == AF_INET) {
d2307 1
a2307 1
		} else if (data->socket->addr->ai_family == AF_INET6) {
d2322 1
a2322 1
			if (data->socket->addr->ai_family == AF_INET) {
d2324 1
a2324 1
			} else if (data->socket->addr->ai_family == AF_INET6) {
@


1.19
log
@merge conflicts
@
text
@a31 1
#include <fcntl.h>
d368 2
a369 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.zstat.0",
		nsd->options->xfrdir, (unsigned)getpid());
d371 2
a372 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.zstat.1",
		nsd->options->xfrdir, (unsigned)getpid());
d538 2
a539 2
		compressed_dname_offsets = (uint16_t *) xalloc(
			needed * sizeof(uint16_t));
d959 2
a960 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.task.0",
		nsd->options->xfrdir, (unsigned)getpid());
d962 6
a967 1
	if(!nsd->task[0])
d969 3
a971 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.task.1",
		nsd->options->xfrdir, (unsigned)getpid());
d975 5
d1943 3
a1945 2
	tcp_accept_handlers = (struct tcp_accept_handler_data*) region_alloc(
		server_region, nsd->ifs * sizeof(*tcp_accept_handlers));
@


1.18
log
@Merge in some commits from upstream..

- Fix that failure to add tcp to tcp base does not leak the socket.
- Fixes for wildcard addition and deletion, speedup for some cases.
- Fix that queries for noname CH TXT are REFUSED instead of nodata.
- Fix #616: retry xfer for zones with no content after command.
- Fix that expired zones stay expired after a server restart.
- RFC 7344: CDS and CDNSKEY (read in).

ok sthen@@
@
text
@d32 1
d37 3
d345 174
d853 4
d866 1
d1092 4
d1337 4
d1393 3
d1743 4
d2031 1
d2044 1
d2051 1
d2057 1
d2066 8
d2075 1
d2085 2
a2086 1
			if (TC(q->packet))
d2088 2
d2096 1
d2167 1
d2178 1
d2203 1
d2223 1
d2226 8
d2251 1
d2258 2
a2259 1
				if (TC(q->packet))
d2261 2
d2267 1
d2436 1
d2438 1
a2438 1
        STATUP(data->nsd, ctcp);
d2446 1
d2458 1
d2463 1
d2468 12
d2481 2
@


1.17
log
@Nuke more obvious #include duplications.

ok deraadt@@ millert@@ tedu@@
@
text
@d2525 1
a2525 3
	if(event_base_set(data->event.ev_base, &tcp_data->event) != 0)
		log_msg(LOG_ERR, "cannot set tcp event base");
	if(event_add(&tcp_data->event, &timeout) != 0)
d2527 10
@


1.16
log
@merge conflicts
@
text
@a31 1
#include <fcntl.h>
@


1.15
log
@Fix bogus error check; spotted by clang.
Submitted upstream by brad@@ (and subsequently committed there).

ok jca@@, mikeb@@, brad@@
@
text
@d31 1
d424 1
a424 1
			if(errno != ENOBUFS) {
d447 1
a447 1
			if(errno != ENOBUFS) {
d682 2
a683 1
	if(nsd->options->zonefiles_check)
d846 1
a846 1
		xfrd_init(sockets[1], nsd, del_db, reload_active);
a892 1
#ifdef BIND8_STATS
a893 1
#endif
d897 20
d930 3
a940 1
#ifdef BIND8_STATS
a945 1
#endif
d1099 1
a1099 1
	s.db_disk = nsd->db->udb->base_size;
a1118 1
#ifdef BIND8_STATS
a1119 1
#endif
d1123 5
d1132 1
d1134 2
d1152 2
a1210 1
#ifdef BIND8_STATS
a1215 1
#endif
d1308 1
a1308 1
			while((child_pid = waitpid(0, &status, WNOHANG)) != -1 && child_pid != 0) {
a1321 1
#ifdef BIND8_STATS
a1322 1
#endif
a1337 1
#ifdef BIND8_STATS
d1343 6
a1348 2
#endif
				} else {
d1350 1
a1350 1
					       "Unknown child %d terminated with status %d",
d1358 2
a1359 1
				log_msg(LOG_WARNING, "wait failed: %s", strerror(errno));
d1374 30
d1442 2
a1443 2
			case 0:
				/* CHILD */
d1459 4
a1462 4
			default:
				/* PARENT, keep running until NSD_QUIT_SYNC
				 * received from CHILD.
				 */
d1470 1
a1518 1
			log_msg(LOG_WARNING, "signal received, shutting down...");
d1536 1
d1785 1
a1785 1
				while (waitpid(0, NULL, WNOHANG) > 0) ;
@


1.14
log
@merge conflicts
@
text
@d1308 1
a1308 1
						&cmd, sizeof(cmd)) == -1) {
@


1.13
log
@merge conflicts
@
text
@d1501 10
a1524 1
	xfrd_del_tempdir(nsd);
d1529 3
@


1.12
log
@merge conflicts
@
text
@d36 1
d38 7
a44 1
#include <event.h>
d46 1
a46 1
#include "mini_event.h"
a47 2

#include <openssl/rand.h>
@


1.11
log
@merge conflicts
@
text
@d677 1
a677 1
		namedb_check_zonefiles(nsd->db, nsd->options, NULL, NULL);
d869 1
a869 1
		task_new_soainfo(taskudb, &task_last, (zone_type*)n->elem);
d1829 4
a1832 1
			log_msg(LOG_ERR, "sendmmsg failed: %s", strerror(errno));
d1943 4
a1946 1
				log_msg(LOG_ERR, "sendto failed: %s", strerror(errno));
@


1.10
log
@time_t and random fixes from NSD upstream, ok deraadt@@
@
text
@d4 1
a4 1
 * Copyright (c) 2001-2011, NLnet Labs. All rights reserved.
d36 5
d49 1
d53 2
d58 2
a69 4
/*
 * Data for the TCP accept handlers.  Most data is simply passed along
 * to the TCP connection handler.
 */
d73 2
a74 2
	size_t              tcp_accept_handler_count;
	netio_handler_type *tcp_accept_handlers;
d77 20
a96 2
int slowaccept;
struct timespec slowaccept_timeout;
a130 8
	 * These fields are used to enable the TCP accept handlers
	 * when the number of TCP connection drops below the maximum
	 * number of TCP connections.
	 */
	size_t              tcp_accept_handler_count;
	netio_handler_type *tcp_accept_handlers;

	/*
d138 5
d159 1
a159 3
static void handle_udp(netio_type *netio,
		       netio_handler_type *handler,
		       netio_event_types_type event_types);
d170 1
a170 3
static void handle_tcp_accept(netio_type *netio,
			      netio_handler_type *handler,
			      netio_event_types_type event_types);
d177 1
a177 3
static void handle_tcp_reading(netio_type *netio,
			       netio_handler_type *handler,
			       netio_event_types_type event_types);
d184 1
a184 3
static void handle_tcp_writing(netio_type *netio,
			       netio_handler_type *handler,
			       netio_event_types_type event_types);
d199 1
a199 2
 * Change the event types the HANDLERS are interested in to
 * EVENT_TYPES.
d201 1
a201 9
static void configure_handler_event_types(size_t count,
					  netio_handler_type *handlers,
					  netio_event_types_type event_types);

/*
 * start xfrdaemon (again).
 */
static pid_t
server_start_xfrd(struct nsd *nsd, netio_handler_type* handler);
d259 3
a274 2
					ipc_data->busy_writing_zone_state = 0;
					ipc_data->write_conn = xfrd_tcp_create(region);
a287 1
				ipc_data->busy_writing_zone_state = 0;
d292 3
d301 1
d308 1
d311 3
d399 54
d665 1
a665 1
	if ((nsd->db = namedb_open(nsd->dbfile, nsd->options, nsd->child_count)) == NULL) {
d668 3
d673 5
a677 10

	/* Read diff file */
	if(!diff_read_file(nsd->db, nsd->options, NULL, nsd->child_count)) {
		log_msg(LOG_ERR, "The diff file contains errors. Will continue "
						 "without it");
	}

#ifdef NSEC3
	prehash(nsd->db, 0);
#endif
a751 1
	log_finalize();
d753 3
d757 1
d760 4
d765 30
a794 1
	exit(0);
d797 3
a799 2
static pid_t
server_start_xfrd(struct nsd *nsd, netio_handler_type* handler)
a802 1
	zone_type* zone;
d804 15
a818 3
	/* no need to send updates for zones, because xfrd will read from fork-memory */
	for(zone = nsd->db->zones; zone; zone=zone->next) {
		zone->updated = 0;
a819 3

	if(handler->fd != -1)
		close(handler->fd);
d822 1
a822 1
		return -1;
d829 2
a830 2
	case 0:
		/* CHILD: close first socket, use second one */
d832 8
a839 1
		xfrd_init(sockets[1], nsd);
d842 2
a843 2
	default:
		/* PARENT: close second socket, use first one */
d845 4
a848 1
		handler->fd = sockets[0];
d851 4
a854 4
	/* PARENT only */
	handler->timeout = NULL;
	handler->event_types = NETIO_EVENT_READ;
	handler->event_handler = parent_handle_xfrd_command;
d856 1
a856 1
	data = (struct ipc_handler_conn_data *) handler->user_data;
d858 83
a940 1
	return pid;
d944 1
a944 1
static ssize_t
d999 86
d1091 1
a1091 1
	int cmdsocket, int* xfrd_sock_p)
d1093 3
a1095 1
	pid_t old_pid;
a1096 2
	zone_type* zone;
	int xfrd_sock = *xfrd_sock_p;
d1098 6
a1104 22
	if(db_crc_different(nsd->db) == 0) {
		DEBUG(DEBUG_XFRD,1, (LOG_INFO,
			"CRC the same. skipping %s.", nsd->db->filename));
	} else {
		DEBUG(DEBUG_XFRD,1, (LOG_INFO,
			"CRC different. reread of %s.", nsd->db->filename));
		namedb_close(nsd->db);
		if ((nsd->db = namedb_open(nsd->dbfile, nsd->options,
			nsd->child_count)) == NULL) {
			log_msg(LOG_ERR, "unable to reload the database: %s", strerror(errno));
			exit(1);
		}
#if defined(NSEC3) && !defined(FULL_PREHASH)
		prehash(nsd->db, 0);
#endif
	}
	if(!diff_read_file(nsd->db, nsd->options, NULL, nsd->child_count)) {
		log_msg(LOG_ERR, "unable to load the diff file: %s", nsd->options->difffile);
		exit(1);
	}
	log_msg(LOG_INFO, "memory recyclebin holds %lu bytes", (unsigned long)
		region_get_recycle_size(nsd->db->region));
d1109 2
a1110 5
#ifdef NSEC3
#ifdef FULL_PREHASH
	prehash(nsd->db, 1);
#endif /* FULL_PREHASH */
#endif /* NSEC3 */
a1113 4
	/* Get our new process id */
	old_pid = nsd->pid;
	nsd->pid = getpid();

d1121 3
a1123 2
	if (server_start_children(nsd, server_region, netio, xfrd_sock_p) != 0) {
		send_children_quit(nsd); /* no wait required */
d1129 1
a1129 1
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", cmd));
d1132 1
a1132 1
			send_children_quit(nsd); /* no wait required */
a1136 10
	/* Overwrite pid before closing old parent, to avoid race condition:
	 * - parent process already closed
	 * - pidfile still contains old_pid
	 * - control script contacts parent process, using contents of pidfile
	 */
	if (writepid(nsd) == -1) {
		log_msg(LOG_ERR, "cannot overwrite the pidfile %s: %s", nsd->pidfile, strerror(errno));
	}

#define RELOAD_SYNC_TIMEOUT 25 /* seconds */
d1142 2
a1143 2
			log_msg(LOG_ERR, "problems sending command from reload %d to oldnsd %d: %s",
				(int)nsd->pid, (int)old_pid, strerror(errno));
a1160 1
		unlinkpid(nsd->pidfile);
d1164 5
d1170 6
a1175 5
	/* inform xfrd of new SOAs */
	cmd = NSD_SOA_BEGIN;
	if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending soa begin from reload %d to xfrd: %s",
			(int)nsd->pid, strerror(errno));
d1177 5
a1181 59
	for(zone= nsd->db->zones; zone; zone = zone->next) {
		uint16_t sz;
		const dname_type *dname_ns=0, *dname_em=0;
		if(zone->updated == 0)
			continue;
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "nsd: sending soa info for zone %s",
			dname_to_string(domain_dname(zone->apex),0)));
		cmd = NSD_SOA_INFO;
		sz = dname_total_size(domain_dname(zone->apex));
		if(zone->soa_rrset) {
			dname_ns = domain_dname(
				rdata_atom_domain(zone->soa_rrset->rrs[0].rdatas[0]));
			dname_em = domain_dname(
				rdata_atom_domain(zone->soa_rrset->rrs[0].rdatas[1]));
			sz += sizeof(uint32_t)*6 + sizeof(uint8_t)*2
				+ dname_ns->name_size + dname_em->name_size;
		}
		sz = htons(sz);
		/* use blocking writes */
		if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd)) ||
			!write_socket(xfrd_sock, &sz, sizeof(sz)) ||
			!write_socket(xfrd_sock, domain_dname(zone->apex),
				dname_total_size(domain_dname(zone->apex))))
		{
			log_msg(LOG_ERR, "problems sending soa info from reload %d to xfrd: %s",
				(int)nsd->pid, strerror(errno));
		}
		if(zone->soa_rrset) {
			uint32_t ttl = htonl(zone->soa_rrset->rrs[0].ttl);
			assert(dname_ns && dname_em);
			assert(zone->soa_rrset->rr_count > 0);
			assert(rrset_rrtype(zone->soa_rrset) == TYPE_SOA);
			assert(zone->soa_rrset->rrs[0].rdata_count == 7);
			if(!write_socket(xfrd_sock, &ttl, sizeof(uint32_t))
			   || !write_socket(xfrd_sock, &dname_ns->name_size, sizeof(uint8_t))
			   || !write_socket(xfrd_sock, dname_name(dname_ns), dname_ns->name_size)
			   || !write_socket(xfrd_sock, &dname_em->name_size, sizeof(uint8_t))
			   || !write_socket(xfrd_sock, dname_name(dname_em), dname_em->name_size)
			   || !write_socket(xfrd_sock, rdata_atom_data(
				zone->soa_rrset->rrs[0].rdatas[2]), sizeof(uint32_t))
			   || !write_socket(xfrd_sock, rdata_atom_data(
				zone->soa_rrset->rrs[0].rdatas[3]), sizeof(uint32_t))
			   || !write_socket(xfrd_sock, rdata_atom_data(
				zone->soa_rrset->rrs[0].rdatas[4]), sizeof(uint32_t))
			   || !write_socket(xfrd_sock, rdata_atom_data(
				zone->soa_rrset->rrs[0].rdatas[5]), sizeof(uint32_t))
			   || !write_socket(xfrd_sock, rdata_atom_data(
				zone->soa_rrset->rrs[0].rdatas[6]), sizeof(uint32_t)))
			{
				log_msg(LOG_ERR, "problems sending soa info from reload %d to xfrd: %s",
				(int)nsd->pid, strerror(errno));
			}
		}
		zone->updated = 0;
	}
	cmd = NSD_SOA_END;
	if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending soa end from reload %d to xfrd: %s",
			(int)nsd->pid, strerror(errno));
d1183 1
d1214 4
a1241 1
	netio_handler_type xfrd_listener;
a1243 1
	int fd;
a1246 1
	pid_t xfrd_pid = -1;
a1248 4
#ifdef RATELIMIT
	rrl_init((nsd->this_child - nsd->children)/sizeof(nsd->children[0]));
#endif

d1252 2
a1253 10
	xfrd_listener.user_data = (struct ipc_handler_conn_data*)region_alloc(
		server_region, sizeof(struct ipc_handler_conn_data));
	xfrd_listener.fd = -1;
	((struct ipc_handler_conn_data*)xfrd_listener.user_data)->nsd = nsd;
	((struct ipc_handler_conn_data*)xfrd_listener.user_data)->conn =
		xfrd_tcp_create(server_region);

	/* Start the XFRD process */
	xfrd_pid = server_start_xfrd(nsd, &xfrd_listener);
	netio_add_handler(netio, &xfrd_listener);
d1256 3
a1258 2
	if (server_start_children(nsd, server_region, netio, &xfrd_listener.fd) != 0) {
		send_children_quit(nsd); /* no wait required */
d1287 1
a1287 1
						&xfrd_listener.fd);
d1289 4
a1292 1
					sig_atomic_t cmd = NSD_SOA_END;
d1297 1
a1297 1
					if(reload_listener.fd > 0) close(reload_listener.fd);
d1300 1
d1302 2
a1303 1
					if(!write_socket(xfrd_listener.fd, &cmd, sizeof(cmd))) {
d1308 7
a1314 5
				} else if (child_pid == xfrd_pid) {
					log_msg(LOG_WARNING,
					       "xfrd process %d failed with status %d, restarting ",
					       (int) child_pid, status);
					xfrd_pid = server_start_xfrd(nsd, &xfrd_listener);
d1342 12
d1357 1
a1357 1

d1364 2
a1365 2
			log_msg(LOG_WARNING, "signal received, reloading...");

d1382 1
a1382 1
					reload_sockets[1], &xfrd_listener.fd);
d1387 2
a1388 1
				((struct ipc_handler_conn_data*)xfrd_listener.user_data)
d1416 2
a1417 1
				if(!write_socket(xfrd_listener.fd, &cmd, sizeof(cmd))) {
d1437 3
d1442 1
d1444 1
a1444 1
			send_children_quit(nsd); /* no wait required */
d1446 1
a1446 1
			namedb_fd_close(nsd->db);
d1448 1
a1453 1
			send_children_quit_and_wait(nsd);
d1467 1
a1467 1
			log_msg(LOG_WARNING, "NSD main server mode invalid: %d", nsd->mode);
d1473 8
a1480 6
	/* Truncate the pid file.  */
	if ((fd = open(nsd->pidfile, O_WRONLY | O_TRUNC, 0644)) == -1) {
		log_msg(LOG_ERR, "can not truncate the pid file %s: %s", nsd->pidfile, strerror(errno));
	} else {
		close(fd);
	}
d1483 2
d1497 1
a1497 1
	if(xfrd_listener.fd != -1) {
d1502 1
a1502 1
		if(!write_socket(xfrd_listener.fd, &cmd, sizeof(cmd))) {
d1506 3
a1508 3
		fsync(xfrd_listener.fd);
		close(xfrd_listener.fd);
		(void)kill(xfrd_pid, SIGTERM);
d1510 1
d1512 1
a1512 1
	namedb_fd_close(nsd->db);
d1514 1
d1524 38
d1571 1
a1571 2
	netio_type *netio = netio_create(server_region);
	netio_handler_type *tcp_accept_handlers;
d1575 9
d1595 6
a1600 1
		netio_handler_type *handler;
d1602 8
a1609 12
		handler = (netio_handler_type *) region_alloc(
			server_region, sizeof(netio_handler_type));
		handler->fd = nsd->this_child->parent_fd;
		handler->timeout = NULL;
		handler->user_data = (struct ipc_handler_conn_data*)region_alloc(
			server_region, sizeof(struct ipc_handler_conn_data));
		((struct ipc_handler_conn_data*)handler->user_data)->nsd = nsd;
		((struct ipc_handler_conn_data*)handler->user_data)->conn =
			xfrd_tcp_create(server_region);
		handler->event_types = NETIO_EVENT_READ;
		handler->event_handler = child_handle_parent_command;
		netio_add_handler(netio, handler);
d1613 1
d1616 15
a1630 1

d1633 1
a1633 1
			netio_handler_type *handler;
d1642 8
a1649 8
			handler = (netio_handler_type *) region_alloc(
				server_region, sizeof(netio_handler_type));
			handler->fd = nsd->udp[i].s;
			handler->timeout = NULL;
			handler->user_data = data;
			handler->event_types = NETIO_EVENT_READ;
			handler->event_handler = handle_udp;
			netio_add_handler(netio, handler);
d1658 3
a1660 2
	tcp_accept_handlers = (netio_handler_type *) region_alloc(
		server_region, nsd->ifs * sizeof(netio_handler_type));
d1663 3
a1665 6
			struct tcp_accept_handler_data *data;
			netio_handler_type *handler;

			data = (struct tcp_accept_handler_data *) region_alloc(
				server_region,
				sizeof(struct tcp_accept_handler_data));
d1668 7
a1674 10
			data->tcp_accept_handler_count = nsd->ifs;
			data->tcp_accept_handlers = tcp_accept_handlers;

			handler = &tcp_accept_handlers[i];
			handler->fd = nsd->tcp[i].s;
			handler->timeout = NULL;
			handler->user_data = data;
			handler->event_types = NETIO_EVENT_READ | NETIO_EVENT_ACCEPT;
			handler->event_handler = handle_tcp_accept;
			netio_add_handler(netio, handler);
d1676 1
a1676 1
	}
d1713 1
a1713 1
			if (netio_dispatch(netio, NULL, NULL) == -1) {
d1715 1
a1715 1
					log_msg(LOG_ERR, "netio_dispatch failed: %s", strerror(errno));
d1723 1
a1723 1
				mode);
d1732 2
a1733 1
	namedb_fd_close(nsd->db);
d1735 1
d1739 3
a1741 2
static query_state_type
server_process_query_udp(struct nsd *nsd, struct query *query)
d1743 97
a1839 5
#ifdef RATELIMIT
	if(query_process(query, nsd) != QUERY_DISCARDED) {
		if(rrl_process_query(query))
			return rrl_slip(query);
		else	return QUERY_PROCESSED;
a1840 4
	return QUERY_DISCARDED;
#else
	return query_process(query, nsd);
#endif
d1843 2
d1846 1
a1846 3
handle_udp(netio_type *ATTR_UNUSED(netio),
	   netio_handler_type *handler,
	   netio_event_types_type event_types)
d1848 1
a1848 2
	struct udp_handler_data *data
		= (struct udp_handler_data *) handler->user_data;
d1850 10
a1859 1
	struct query *q = data->query;
d1861 1
a1861 1
	if (!(event_types & NETIO_EVENT_READ)) {
d1864 12
a1875 7

	/* Account... */
#ifdef BIND8_STATS
	if (data->socket->addr->ai_family == AF_INET) {
		STATUP(data->nsd, qudp);
	} else if (data->socket->addr->ai_family == AF_INET6) {
		STATUP(data->nsd, qudp6);
d1877 34
a1910 1
#endif
d1912 5
a1916 14
	/* Initialize the query... */
	query_reset(q, UDP_MAX_MESSAGE_LEN, 0);

	received = recvfrom(handler->fd,
			    buffer_begin(q->packet),
			    buffer_remaining(q->packet),
			    0,
			    (struct sockaddr *)&q->addr,
			    &q->addrlen);
	if (received == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			log_msg(LOG_ERR, "recvfrom failed: %s", strerror(errno));
			STATUP(data->nsd, rxerr);
			/* No zone statup */
d1918 1
a1918 1
	} else {
a1923 1
#ifdef BIND8_STATS
a1925 4
# ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP(q->zone, nona);
# endif
a1927 11
# ifdef USE_ZONE_STATS
			if (q->zone) {
			  if (data->socket->addr->ai_family == AF_INET) {
				ZTATUP(q->zone, qudp);
			  } else if (data->socket->addr->ai_family == AF_INET6) {
				ZTATUP(q->zone, qudp6);
			  }
			}
# endif
#endif

d1933 1
a1933 1
			sent = sendto(handler->fd,
a1941 5

#ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP(q->zone, txerr);
#endif
d1944 1
a1945 1
			} else {
d1948 1
a1948 5
# ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP2(q->zone, rcode, RCODE(q->packet));
# endif
				if (TC(q->packet)) {
a1949 5
# ifdef USE_ZONE_STATS
					if (q->zone)
						ZTATUP(q->zone, truncated);
# endif
				}
a1951 1
#ifdef BIND8_STATS
d1954 4
a1957 5
# ifdef USE_ZONE_STATS
			if (q->zone) {
				ZTATUP(q->zone, dropped);
			}
# endif
a1958 1
		}
d1960 1
d1962 1
d1966 1
a1966 1
cleanup_tcp_handler(netio_type *netio, netio_handler_type *handler)
d1968 2
a1969 5
	struct tcp_handler_data *data
		= (struct tcp_handler_data *) handler->user_data;
	netio_remove_handler(netio, handler);
	close(handler->fd);
	slowaccept = 0;
d1976 3
a1978 4
	if (data->nsd->current_tcp_count == data->nsd->maximum_tcp_count) {
		configure_handler_event_types(data->tcp_accept_handler_count,
					      data->tcp_accept_handlers,
					      NETIO_EVENT_READ);
d1987 1
a1987 3
handle_tcp_reading(netio_type *netio,
		   netio_handler_type *handler,
		   netio_event_types_type event_types)
d1989 1
a1989 2
	struct tcp_handler_data *data
		= (struct tcp_handler_data *) handler->user_data;
d1991 2
d1994 1
a1994 1
	if (event_types & NETIO_EVENT_TIMEOUT) {
d1996 1
a1996 1
		cleanup_tcp_handler(netio, handler);
d2003 1
a2003 1
		cleanup_tcp_handler(netio, handler);
d2007 1
a2007 1
	assert(event_types & NETIO_EVENT_READ);
d2017 1
a2017 1
		received = read(handler->fd,
d2029 2
d2034 2
a2035 2
				log_msg(LOG_ERR, "failed reading from tcp: %s", strerror(errno));
				cleanup_tcp_handler(netio, handler);
d2040 1
a2040 1
			cleanup_tcp_handler(netio, handler);
d2067 1
a2067 1
			cleanup_tcp_handler(netio, handler);
d2073 1
a2073 1
			cleanup_tcp_handler(netio, handler);
d2083 1
a2083 1
	received = read(handler->fd,
d2094 2
d2099 2
a2100 2
			log_msg(LOG_ERR, "failed reading from tcp: %s", strerror(errno));
			cleanup_tcp_handler(netio, handler);
d2105 1
a2105 1
		cleanup_tcp_handler(netio, handler);
d2122 3
a2124 4
#ifdef BIND8_STATS
# ifndef INET6
	STATUP(data->nsd, ctcp);
# else
d2130 1
a2130 2
# endif
#endif /* BIND8_STATS */
d2142 1
a2142 6
#if defined(BIND8_STATS) && defined(USE_ZONE_STATS)
		if (data->query->zone) {
			ZTATUP(data->query->zone, dropped);
		}
#endif
		cleanup_tcp_handler(netio, handler);
a2145 1
#ifdef BIND8_STATS
a2149 4
# ifdef USE_ZONE_STATS
		if (data->query->zone)
			ZTATUP(data->query->zone, nona);
# endif
a2151 16
# ifdef USE_ZONE_STATS
	if (data->query->zone) {
#  ifndef INET6
		ZTATUP(data->query->zone, ctcp);
#  else
	if (data->query->addr.ss_family == AF_INET) {
		ZTATUP(data->query->zone, ctcp);
	} else if (data->query->addr.ss_family == AF_INET6) {
		ZTATUP(data->query->zone, ctcp6);
	}
#  endif
	}
# endif /* USE_ZONE_STATS */

#endif /* BIND8_STATS */

d2159 2
a2160 3
	handler->timeout->tv_sec = data->nsd->tcp_timeout;
	handler->timeout->tv_nsec = 0L;
	timespec_add(handler->timeout, netio_current_time(netio));
d2162 10
a2171 2
	handler->event_types = NETIO_EVENT_WRITE | NETIO_EVENT_TIMEOUT;
	handler->event_handler = handle_tcp_writing;
d2175 1
a2175 3
handle_tcp_writing(netio_type *netio,
		   netio_handler_type *handler,
		   netio_event_types_type event_types)
d2177 1
a2177 2
	struct tcp_handler_data *data
		= (struct tcp_handler_data *) handler->user_data;
d2180 2
d2183 1
a2183 1
	if (event_types & NETIO_EVENT_TIMEOUT) {
d2185 1
a2185 1
		cleanup_tcp_handler(netio, handler);
d2189 1
a2189 1
	assert(event_types & NETIO_EVENT_WRITE);
d2200 1
a2200 1
		sent = writev(handler->fd, iov, 2);
d2202 1
a2202 1
		sent = write(handler->fd,
d2218 1
a2218 1
					if(verbosity >= 2 || errno != EPIPE)
d2220 2
a2221 2
				log_msg(LOG_ERR, "failed writing to tcp: %s", strerror(errno));
				cleanup_tcp_handler(netio, handler);
d2240 3
a2242 3
	}

	sent = write(handler->fd,
d2257 1
a2257 1
					if(verbosity >= 2 || errno != EPIPE)
d2260 1
a2260 1
			cleanup_tcp_handler(netio, handler);
d2292 10
a2301 3
			handler->timeout->tv_sec = data->nsd->tcp_timeout;
			handler->timeout->tv_nsec = 0;
			timespec_add(handler->timeout, netio_current_time(netio));
d2318 1
a2318 1
		(void) shutdown(handler->fd, SHUT_WR);
d2323 12
a2334 3
	handler->timeout->tv_sec = data->nsd->tcp_timeout;
	handler->timeout->tv_nsec = 0;
	timespec_add(handler->timeout, netio_current_time(netio));
d2336 8
a2343 2
	handler->event_types = NETIO_EVENT_READ | NETIO_EVENT_TIMEOUT;
	handler->event_handler = handle_tcp_reading;
a2345 1

d2348 1
a2348 1
 * a new TCP reader event handler is added to NETIO.  The TCP handler
d2352 1
a2352 3
handle_tcp_accept(netio_type *netio,
		  netio_handler_type *handler,
		  netio_event_types_type event_types)
d2355 1
a2355 1
		= (struct tcp_accept_handler_data *) handler->user_data;
a2358 1
	netio_handler_type *tcp_handler;
d2365 1
d2367 1
a2367 1
	if (!(event_types & NETIO_EVENT_READ)) {
d2377 1
a2377 1
	s = accept(handler->fd, (struct sockaddr *) &addr, &addrlen);
d2387 10
a2396 3
				slowaccept_timeout.tv_sec = NETIO_SLOW_ACCEPT_TIMEOUT;
				slowaccept_timeout.tv_nsec = 0L;
				timespec_add(&slowaccept_timeout, netio_current_time(netio));
a2432 3
	tcp_data->tcp_accept_handler_count = data->tcp_accept_handler_count;
	tcp_data->tcp_accept_handlers = data->tcp_accept_handlers;

d2438 2
a2439 12
	tcp_handler = (netio_handler_type *) region_alloc(
		tcp_region, sizeof(netio_handler_type));
	tcp_handler->fd = s;
	tcp_handler->timeout = (struct timespec *) region_alloc(
		tcp_region, sizeof(struct timespec));
	tcp_handler->timeout->tv_sec = data->nsd->tcp_timeout;
	tcp_handler->timeout->tv_nsec = 0L;
	timespec_add(tcp_handler->timeout, netio_current_time(netio));

	tcp_handler->user_data = tcp_data;
	tcp_handler->event_types = NETIO_EVENT_READ | NETIO_EVENT_TIMEOUT;
	tcp_handler->event_handler = handle_tcp_reading;
d2441 6
a2446 1
	netio_add_handler(netio, tcp_handler);
d2455 1
a2455 3
		configure_handler_event_types(data->tcp_accept_handler_count,
					      data->tcp_accept_handlers,
					      NETIO_EVENT_NONE);
a2475 1
				/* wait for reply */
d2490 1
d2516 1
a2516 3
configure_handler_event_types(size_t count,
			      netio_handler_type *handlers,
			      netio_event_types_type event_types)
d2520 22
a2541 4
	assert(handlers);

	for (i = 0; i < count; ++i) {
		handlers[i].event_types = event_types;
@


1.9
log
@merge NSD 3.2.16
@
text
@a581 1
	srandom(arc4random());
@


1.8
log
@resolve conflicts
@
text
@d177 2
d363 1
a363 1
#if defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU)))
d464 9
d505 33
a537 6
#if defined(INET6) && defined(IPV6_V6ONLY)
		if (nsd->tcp[i].addr->ai_family == AF_INET6 &&
		    setsockopt(nsd->tcp[i].s, IPPROTO_IPV6, IPV6_V6ONLY, &on, sizeof(on)) < 0)
		{
			log_msg(LOG_ERR, "setsockopt(..., IPV6_V6ONLY, ...) failed: %s", strerror(errno));
			return -1;
d548 9
d593 4
a596 1
		nsd->options->rrl_whitelist_ratelimit);
d645 2
a646 2
static void
close_all_sockets(struct nsd_socket sockets[], size_t n)
d670 2
a671 2
	close_all_sockets(nsd->udp, nsd->ifs);
	close_all_sockets(nsd->tcp, nsd->ifs);
d859 1
a859 1
		send_children_quit(nsd);
d868 1
a868 1
			send_children_quit(nsd);
d903 1
a903 1
	DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc reply main %d %d", ret, cmd));
d1062 1
a1062 1
		send_children_quit(nsd);
d1223 1
a1223 1
			send_children_quit(nsd);
d1232 1
a1232 1
			send_children_quit(nsd);
d1315 1
a1315 1
		close_all_sockets(nsd->tcp, nsd->ifs);
d1318 1
a1318 1
		close_all_sockets(nsd->udp, nsd->ifs);
d1401 2
d1405 1
d2086 1
a2086 1
send_children_quit(struct nsd* nsd)
a2087 1
	sig_atomic_t command = NSD_QUIT;
d2101 5
d2112 13
@


1.7
log
@resolve conflicts
@
text
@d37 2
d47 2
d94 1
a94 1
	region_type     *region;
d99 1
a99 1
	struct nsd      *nsd;
d104 1
a104 1
	query_type      *query;
d119 1
a119 1
	query_state_type query_state;
d127 1
a127 1
	size_t           bytes_transmitted;
d532 17
d615 1
a615 1
static void
d775 1
a775 1
#ifndef FULL_PREHASH
d992 4
d1400 14
d1458 1
a1458 1
		if (server_process_query(data->nsd, q) != QUERY_DISCARDED) {
@


1.6
log
@resolve conflicts
@
text
@d15 1
d754 3
d815 1
a815 1
		if (write_socket(cmdsocket, &cmd, sizeof(cmd)) == -1)
d1027 1
a1027 2
					if(!write_socket(xfrd_listener.fd,
						&cmd, sizeof(cmd)) == -1) {
d1180 2
a1182 2
	close(fd);

d1744 8
d1755 1
d1785 5
a1789 1
		assert(data->bytes_transmitted == sizeof(q->tcplen));
a1791 2
	assert(data->bytes_transmitted < q->tcplen + sizeof(q->tcplen));

d1815 4
a1819 1
	data->bytes_transmitted += sent;
@


1.5
log
@resolve conflicts
@
text
@d1420 4
a1423 1
				ZTATUP(q->zone, nona);
d1427 2
a1428 1
			if (data->socket->addr->ai_family == AF_INET) {
d1430 1
a1430 1
			} else if (data->socket->addr->ai_family == AF_INET6) {
d1432 1
d1451 5
a1455 1
				ZTATUP(q->zone, txerr);
d1462 4
a1465 1
				ZTATUP2(q->zone, rcode, RCODE(q->packet));
d1468 4
a1471 1
					ZTATUP(q->zone, truncated);
d1683 4
a1686 1
		ZTATUP(data->query->zone, nona);
d1690 1
d1692 1
a1692 1
	ZTATUP(data->query->zone, ctcp);
d1700 1
@


1.4
log
@resolve conflicts
@
text
@d10 1
a10 1
#include <config.h>
d66 3
d208 1
a208 1
				if(nsd->children[i].child_fd > 0)
d234 1
a234 1
			if (nsd->children[i].child_fd > 0)
d601 1
a601 1
	if(nsd->this_child && nsd->this_child->parent_fd > 0)
d610 1
a610 1
			if(nsd->children[i].child_fd > 0)
d692 1
a692 1
				if(nsd->signal_hint_quit || nsd->signal_hint_shutdown)
d710 1
a710 1
				if(nsd->signal_hint_quit || nsd->signal_hint_shutdown)
d1117 1
a1117 1
			if(!nsd->quit_sync_done && reload_listener.fd > 0) {
d1134 1
a1134 1
			if(reload_listener.fd > 0) {
d1183 1
a1183 1
	if(reload_listener.fd > 0) {
d1194 1
a1194 1
	if(xfrd_listener.fd > 0) {
d1310 1
a1310 1
			handler->event_types = NETIO_EVENT_READ;
d1333 1
a1333 1
			if (nsd->this_child->parent_fd > 0) {
d1388 1
d1394 1
d1409 1
d1417 1
d1420 1
d1423 9
d1446 1
d1449 1
a1450 1
#ifdef BIND8_STATS
d1453 2
a1454 1
				if (TC(q->packet))
d1456 2
d1460 1
d1463 6
d1481 1
d1632 4
a1635 3
#ifndef INET6
        STATUP(data->nsd, ctcp);
#else
d1641 2
a1642 1
#endif
d1654 5
d1663 1
d1668 11
d1680 4
d1873 15
a1887 3
		/* EINTR is a signal interrupt. The others are various OS ways
		   of saying that the client has closed the connection. */
		if (	errno != EINTR
d1963 1
a1963 1
		if (nsd->children[i].pid > 0 && nsd->children[i].child_fd > 0) {
@


1.3
log
@resolve conflicts and regen configure using autoconf-2.65
@
text
@d86 1
a86 1
	region_type*		region;
d91 1
a91 1
	struct nsd*			nsd;
d96 1
a96 1
	query_type*			query;
d103 2
a104 2
	size_t				tcp_accept_handler_count;
	netio_handler_type*	tcp_accept_handlers;
d111 1
a111 1
	query_state_type	query_state;
d119 1
a119 1
	size_t				bytes_transmitted;
d762 1
d764 1
d1683 3
d1720 3
@


1.2
log
@upgrade to NSD 3.2.7; ok sthen@@, tested by multiple people
@
text
@d4 1
a4 1
 * Copyright (c) 2001-2006, NLnet Labs. All rights reserved.
d328 6
d1200 1
@


1.1
log
@Initial revision
@
text
@d32 3
d573 1
a573 1
			free(sockets[i].addr);
d777 10
d818 6
d1136 1
a1137 1
			namedb_close(nsd->db);
d1172 9
a1180 1
	if(reload_listener.fd > 0)
d1182 1
d1196 1
a1196 1
	namedb_close(nsd->db);
d1355 1
a1355 1
	namedb_close(nsd->db);
@


1.1.1.1
log
@NSD v3.2.4
@
text
@@


1.1.1.2
log
@NSD v3.2.5
@
text
@a1116 1
			namedb_fd_close(nsd->db);
d1118 1
d1168 1
a1168 1
	namedb_fd_close(nsd->db);
d1327 1
a1327 1
	namedb_fd_close(nsd->db);
@


1.1.1.3
log
@NSD v3.2.8
@
text
@d4 1
a4 1
 * Copyright (c) 2001-2011, NLnet Labs. All rights reserved.
a31 3
#ifndef SHUT_WR
#define SHUT_WR 1
#endif
a324 6
		if(compressed_dname_offsets) {
			region_remove_cleanup(nsd->db->region,
				cleanup_dname_compression_tables,
				compressed_dname_offsets);
			free(compressed_dname_offsets);
		}
d570 1
a570 1
			freeaddrinfo(sockets[i].addr);
a773 10
	/* if the parent has quit, we must quit too, poll the fd for cmds */
	if(block_read(nsd, cmdsocket, &cmd, sizeof(cmd), 0) == sizeof(cmd)) {
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", cmd));
		if(cmd == NSD_QUIT) {
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: quit to follow nsd"));
			send_children_quit(nsd);
			exit(0);
		}
	}

a804 6
	if(cmd == NSD_QUIT) {
		/* small race condition possible here, parent got quit cmd. */
		send_children_quit(nsd);
		unlinkpid(nsd->pidfile);
		exit(1);
	}
d1153 1
a1153 9
	if(reload_listener.fd > 0) {
		sig_atomic_t cmd = NSD_QUIT;
		DEBUG(DEBUG_IPC,1, (LOG_INFO,
			"main: ipc send quit to reload-process"));
		if(!write_socket(reload_listener.fd, &cmd, sizeof(cmd))) {
			log_msg(LOG_ERR, "server_main: could not send quit to reload: %s",
				strerror(errno));
		}
		fsync(reload_listener.fd);
a1154 1
	}
a1165 1
		(void)kill(xfrd_pid, SIGTERM);
@


1.1.1.4
log
@NSD v3.2.9
@
text
@d86 1
a86 1
	region_type     *region;
d91 1
a91 1
	struct nsd      *nsd;
d96 1
a96 1
	query_type      *query;
d103 2
a104 2
	size_t              tcp_accept_handler_count;
	netio_handler_type *tcp_accept_handlers;
d111 1
a111 1
	query_state_type query_state;
d119 1
a119 1
	size_t           bytes_transmitted;
a761 1
#ifdef FULL_PREHASH
a762 1
#endif /* FULL_PREHASH */
a1680 3
#ifdef EPIPE
					if(verbosity >= 2 || errno != EPIPE)
#endif /* EPIPE 'broken pipe' */
a1714 3
#ifdef EPIPE
					if(verbosity >= 2 || errno != EPIPE)
#endif /* EPIPE 'broken pipe' */
@


1.1.1.5
log
@NSD v3.2.11, ok phessler@@
@
text
@d10 1
a10 1
#include "config.h"
a65 3
int slowaccept;
struct timespec slowaccept_timeout;

d205 1
a205 1
				if(nsd->children[i].child_fd != -1)
d231 1
a231 1
			if (nsd->children[i].child_fd != -1)
d598 1
a598 1
	if(nsd->this_child && nsd->this_child->parent_fd != -1)
d607 1
a607 1
			if(nsd->children[i].child_fd != -1)
d689 1
a689 1
				if(nsd && (nsd->signal_hint_quit || nsd->signal_hint_shutdown))
d707 1
a707 1
				if(nsd && (nsd->signal_hint_quit || nsd->signal_hint_shutdown))
d1114 1
a1114 1
			if(!nsd->quit_sync_done && reload_listener.fd != -1) {
d1131 1
a1131 1
			if(reload_listener.fd != -1) {
d1180 1
a1180 1
	if(reload_listener.fd != -1) {
d1191 1
a1191 1
	if(xfrd_listener.fd != -1) {
d1307 1
a1307 1
			handler->event_types = NETIO_EVENT_READ | NETIO_EVENT_ACCEPT;
d1330 1
a1330 1
			if (nsd->this_child->parent_fd != -1) {
a1384 1
#ifdef BIND8_STATS
a1389 1
#endif
a1403 1
			/* No zone statup */
a1410 1
#ifdef BIND8_STATS
a1412 1
				ZTATUP(q->zone, nona);
a1414 9
# ifdef USE_ZONE_STATS
			if (data->socket->addr->ai_family == AF_INET) {
				ZTATUP(q->zone, qudp);
			} else if (data->socket->addr->ai_family == AF_INET6) {
				ZTATUP(q->zone, qudp6);
			}
# endif
#endif

a1428 1
				ZTATUP(q->zone, txerr);
d1431 1
a1432 1
			} else {
d1435 1
a1435 2
				ZTATUP2(q->zone, rcode, RCODE(q->packet));
				if (TC(q->packet)) {
a1436 2
					ZTATUP(q->zone, truncated);
				}
a1438 1
#ifdef BIND8_STATS
a1440 6
# ifdef USE_ZONE_STATS
			if (q->zone) {
				ZTATUP(q->zone, dropped);
			}
# endif
#endif
a1452 1
	slowaccept = 0;
d1603 3
a1605 4
#ifdef BIND8_STATS
# ifndef INET6
	STATUP(data->nsd, ctcp);
# else
d1611 1
a1611 2
# endif
#endif /* BIND8_STATS */
a1622 5
#if defined(BIND8_STATS) && defined(USE_ZONE_STATS)
		if (data->query->zone) {
			ZTATUP(data->query->zone, dropped);
		}
#endif
a1626 1
#ifdef BIND8_STATS
a1630 11
		ZTATUP(data->query->zone, nona);
	}

# ifdef USE_ZONE_STATS
#  ifndef INET6
	ZTATUP(data->query->zone, ctcp);
#  else
	if (data->query->addr.ss_family == AF_INET) {
		ZTATUP(data->query->zone, ctcp);
	} else if (data->query->addr.ss_family == AF_INET6) {
		ZTATUP(data->query->zone, ctcp6);
a1631 4
#  endif
# endif /* USE_ZONE_STATS */

#endif /* BIND8_STATS */
d1821 3
a1823 15
		/**
		 * EMFILE and ENFILE is a signal that the limit of open
		 * file descriptors has been reached. Pause accept().
		 * EINTR is a signal interrupt. The others are various OS ways
		 * of saying that the client has closed the connection.
		 */
		if (errno == EMFILE || errno == ENFILE) {
			if (!slowaccept) {
				slowaccept_timeout.tv_sec = NETIO_SLOW_ACCEPT_TIMEOUT;
				slowaccept_timeout.tv_nsec = 0L;
				timespec_add(&slowaccept_timeout, netio_current_time(netio));
				slowaccept = 1;
				/* We don't want to spam the logs here */
			}
		} else if (errno != EINTR
d1899 1
a1899 1
		if (nsd->children[i].pid > 0 && nsd->children[i].child_fd != -1) {
@


1.1.1.6
log
@update to NSD 3.2.13

- fix crash in nsd-patch if a zone has been removed from nsd.conf. (difffile.c)

- CVE-2012-2979 DOS fix, this is in optional code which is *not* used with
a standard OpenBSD build. (query.c, server.c)
@
text
@d1420 1
a1420 4
# ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP(q->zone, nona);
# endif
d1424 1
a1424 2
			if (q->zone) {
			  if (data->socket->addr->ai_family == AF_INET) {
d1426 1
a1426 1
			  } else if (data->socket->addr->ai_family == AF_INET6) {
a1427 1
			  }
d1446 1
a1446 5

#ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP(q->zone, txerr);
#endif
d1453 1
a1453 4
# ifdef USE_ZONE_STATS
				if (q->zone)
					ZTATUP2(q->zone, rcode, RCODE(q->packet));
# endif
d1456 1
a1456 4
# ifdef USE_ZONE_STATS
					if (q->zone)
						ZTATUP(q->zone, truncated);
# endif
d1668 1
a1668 4
# ifdef USE_ZONE_STATS
		if (data->query->zone)
			ZTATUP(data->query->zone, nona);
# endif
a1671 1
	if (data->query->zone) {
d1673 1
a1673 1
		ZTATUP(data->query->zone, ctcp);
a1680 1
	}
@


1.1.1.7
log
@update to NSD 3.2.14, requested by/ok brad@@
@
text
@a14 1
#include <sys/uio.h>
a752 3
#ifndef FULL_PREHASH
		prehash(nsd->db, 0);
#endif
d811 1
a811 1
		if (!write_socket(cmdsocket, &cmd, sizeof(cmd)))
d1023 2
a1024 1
					if(!write_socket(xfrd_listener.fd, &cmd, sizeof(cmd))) {
a1176 2
	} else {
		close(fd);
d1178 2
a1740 8
#ifdef HAVE_WRITEV
		struct iovec iov[2];
		iov[0].iov_base = (uint8_t*)&n_tcplen + data->bytes_transmitted;
		iov[0].iov_len = sizeof(n_tcplen) - data->bytes_transmitted; 
		iov[1].iov_base = buffer_begin(q->packet);
		iov[1].iov_len = buffer_limit(q->packet);
		sent = writev(handler->fd, iov, 2);
#else /* HAVE_WRITEV */
a1743 1
#endif /* HAVE_WRITEV */
d1773 1
a1773 5
#ifdef HAVE_WRITEV
		sent -= sizeof(n_tcplen);
		/* handle potential 'packet done' code */
		goto packet_could_be_done;
#endif
d1776 2
d1801 1
a1802 4
#ifdef HAVE_WRITEV
  packet_could_be_done:
#endif
	buffer_skip(q->packet, sent);
@


1.1.1.8
log
@Update to NSD 3.2.15, ok brad@@ phessler@@ deraadt@@ also tested by okan@@
@
text
@a36 2
#include <openssl/rand.h>

a44 2
#include "lookup3.h"
#include "rrl.h"
d90 1
a90 1
	region_type*		region;
d95 1
a95 1
	struct nsd*			nsd;
d100 1
a100 1
	query_type*			query;
d115 1
a115 1
	query_state_type	query_state;
d123 1
a123 1
	size_t				bytes_transmitted;
a527 17
#ifdef RATELIMIT
	/* set secret modifier for hashing (udb ptr buckets and rate limits) */
#ifdef HAVE_ARC4RANDOM
	srandom(arc4random());
	hash_set_raninit(arc4random());
#else
	uint32_t v = getpid() ^ time(NULL);
	srandom((unsigned long)v);
	if(RAND_status() && RAND_bytes((unsigned char*)&v, sizeof(v)) > 0)
		hash_set_raninit(v);
	else	hash_set_raninit(random());
#endif
	rrl_mmap_init(nsd->child_count, nsd->options->rrl_size,
		nsd->options->rrl_ratelimit,
		nsd->options->rrl_whitelist_ratelimit);
#endif /* RATELIMIT */

d594 1
a594 1
void
d754 1
a754 1
#if defined(NSEC3) && !defined(FULL_PREHASH)
a970 4
#ifdef RATELIMIT
	rrl_init((nsd->this_child - nsd->children)/sizeof(nsd->children[0]));
#endif

a1374 14
static query_state_type
server_process_query_udp(struct nsd *nsd, struct query *query)
{
#ifdef RATELIMIT
	if(query_process(query, nsd) != QUERY_DISCARDED) {
		if(rrl_process_query(query))
			return rrl_slip(query);
		else	return QUERY_PROCESSED;
	}
	return QUERY_DISCARDED;
#else
	return query_process(query, nsd);
#endif
}
d1419 1
a1419 1
		if (server_process_query_udp(data->nsd, q) != QUERY_DISCARDED) {
@


1.1.1.9
log
@update to NSD 3.2.16, ok deraadt@@ brad@@
@
text
@a176 2
/* same, for shutdown time, waits for child to exit to avoid restart issues */
static void send_children_quit_and_wait(struct nsd* nsd);
d361 1
a361 1
#if defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU) || defined(IP_TRANSPARENT)))
a461 9
		if (nsd->options->ip_transparent) {
#ifdef IP_TRANSPARENT
			if (setsockopt(nsd->udp[i].s, IPPROTO_IP, IP_TRANSPARENT, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_TRANSPARENT, ...) failed for udp: %s",
					strerror(errno));
			}
#endif /* IP_TRANSPARENT */
		}

d494 6
a499 33
#if defined(INET6)
		if (nsd->tcp[i].addr->ai_family == AF_INET6) {
# if defined(IPV6_V6ONLY)
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IPV6, IPV6_V6ONLY,
				&on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_V6ONLY, ...) failed: %s", strerror(errno));
				return -1;
			}
# endif
# if defined(IPV6_USE_MIN_MTU)
			/*
			 * Use minimum MTU to minimize delays learning working
			 * PMTU when communicating through a tunnel.
			 */
			if (setsockopt(nsd->tcp[i].s,
				       IPPROTO_IPV6, IPV6_USE_MIN_MTU,
				       &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_USE_MIN_MTU, ...) failed: %s", strerror(errno));
				return -1;
			}
# elif defined(IPV6_MTU)
			/*
			 * On Linux, PMTUD is disabled by default for datagrams
			 * so set the MTU equal to the MIN MTU to get the same.
			 */
			on = IPV6_MIN_MTU;
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IPV6, IPV6_MTU,
				&on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(..., IPV6_MTU, ...) failed: %s", strerror(errno));
				return -1;
			}
			on = 1;
# endif
a509 9
		if (nsd->options->ip_transparent) {
#ifdef IP_TRANSPARENT
			if (setsockopt(nsd->tcp[i].s, IPPROTO_IP, IP_TRANSPARENT, &on, sizeof(on)) < 0) {
				log_msg(LOG_ERR, "setsockopt(...,IP_TRANSPARENT, ...) failed for tcp: %s",
					strerror(errno));
			}
#endif /* IP_TRANSPARENT */
		}

d546 1
a546 4
		nsd->options->rrl_whitelist_ratelimit,
		nsd->options->rrl_slip,
		nsd->options->rrl_ipv4_prefix_length,
		nsd->options->rrl_ipv6_prefix_length);
d595 2
a596 2
void
server_close_all_sockets(struct nsd_socket sockets[], size_t n)
d620 2
a621 2
	server_close_all_sockets(nsd->udp, nsd->ifs);
	server_close_all_sockets(nsd->tcp, nsd->ifs);
d809 1
a809 1
		send_children_quit(nsd); /* no wait required */
d818 1
a818 1
			send_children_quit(nsd); /* no wait required */
d853 1
a853 1
	DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc reply main %d %d", ret, (int)cmd));
d1012 1
a1012 1
		send_children_quit(nsd); /* no wait required */
d1173 1
a1173 1
			send_children_quit(nsd); /* no wait required */
d1182 1
a1182 1
			send_children_quit_and_wait(nsd);
d1265 1
a1265 1
		server_close_all_sockets(nsd->tcp, nsd->ifs);
d1268 1
a1268 1
		server_close_all_sockets(nsd->udp, nsd->ifs);
a1350 2
			int p = nsd->st.period;
			nsd->st.period = 1; /* force stats printout */
a1352 1
			nsd->st.period = p;
d2033 1
a2033 1
send_children_command(struct nsd* nsd, sig_atomic_t command, int timeout)
d2035 1
a2048 5
			} else if (timeout > 0) {
				/* wait for reply */
				(void)block_read(NULL,
					nsd->children[i].child_fd,
					&command, sizeof(command), timeout);
a2054 13
}

static void
send_children_quit(struct nsd* nsd)
{
	send_children_command(nsd, NSD_QUIT, 0);
}

static void
send_children_quit_and_wait(struct nsd* nsd)
{
	DEBUG(DEBUG_IPC, 1, (LOG_INFO, "send children quit and wait"));
	send_children_command(nsd, NSD_QUIT_CHILD, 3);
@


1.1.1.10
log
@import NSD 4.0.0, tests from Dorian Büttner, Patrik Lundin, requested by brad@@
@
text
@d4 1
a4 1
 * Copyright (c) 2001-2006, NLnet Labs. All rights reserved.
a35 5
#ifndef USE_MINI_EVENT
#include <event.h>
#else
#include "mini_event.h"
#endif
a43 1
#include "xfrd-disk.h"
a46 2
#include "udb.h"
#include "remote.h"
a49 2
#define RELOAD_SYNC_TIMEOUT 25 /* seconds */

d60 4
d67 2
a68 2
	int event_added;
	struct event       event;
d71 2
a72 20
/*
 * These globals are used to enable the TCP accept handlers
 * when the number of TCP connection drops below the maximum
 * number of TCP connections.
 */
static size_t		tcp_accept_handler_count;
static struct tcp_accept_handler_data*	tcp_accept_handlers;

static struct event slowaccept_event;
static int slowaccept;

#ifndef NONBLOCKING_IS_BROKEN
#  define NUM_RECV_PER_SELECT 100
#endif

#if (!defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG))
struct mmsghdr msgs[NUM_RECV_PER_SELECT];
struct iovec iovecs[NUM_RECV_PER_SELECT];
struct query *queries[NUM_RECV_PER_SELECT];
#endif
d107 8
a121 5
	 * The event for the file descriptor and tcp timeout
	 */
	struct event event;

	/*
d138 3
a140 1
static void handle_udp(int fd, short event, void* arg);
d151 3
a153 1
static void handle_tcp_accept(int fd, short event, void* arg);
d160 3
a162 1
static void handle_tcp_reading(int fd, short event, void* arg);
d169 3
a171 1
static void handle_tcp_writing(int fd, short event, void* arg);
d186 2
a187 1
 * Change the event types the HANDLERS are interested in to EVENT_TYPES.
d189 9
a197 1
static void configure_handler_event_types(short event_types);
a254 3
				if (fcntl(nsd->children[i].child_fd, F_SETFL, O_NONBLOCK) == -1) {
					log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
				}
d268 2
d283 1
a287 3
				/* the child need not be able to access the
				 * nsd.db file */
				namedb_close_udb(nsd->db);
a293 1
				nsd->signal_hint_reload_hup = 0;
a299 1
				close(*xfrd_sock_p);
a301 3
				if (fcntl(nsd->this_child->parent_fd, F_SETFL, O_NONBLOCK) == -1) {
					log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
				}
a386 54
#if defined(SO_RCVBUF) || defined(SO_SNDBUF)
	if(1) {
	int rcv = 1*1024*1024;
	int snd = 1*1024*1024;

#ifdef SO_RCVBUF
#  ifdef SO_RCVBUFFORCE
	if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_RCVBUFFORCE, (void*)&rcv,
		(socklen_t)sizeof(rcv)) < 0) {
		if(errno != EPERM && errno != ENOBUFS) {
			log_msg(LOG_ERR, "setsockopt(..., SO_RCVBUFFORCE, "
                                        "...) failed: %s", strerror(errno));
			return -1;
		} 
#  else
	if(1) {
#  endif /* SO_RCVBUFFORCE */
		if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_RCVBUF, (void*)&rcv,
			 (socklen_t)sizeof(rcv)) < 0) {
			if(errno != ENOBUFS) {
				log_msg(LOG_ERR, "setsockopt(..., SO_RCVBUF, "
                                        "...) failed: %s", strerror(errno));
				return -1;
			}
		}
	}
#endif /* SO_RCVBUF */

#ifdef SO_SNDBUF
#  ifdef SO_SNDBUFFORCE
	if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_SNDBUFFORCE, (void*)&snd,
		(socklen_t)sizeof(snd)) < 0) {
		if(errno != EPERM && errno != ENOBUFS) {
			log_msg(LOG_ERR, "setsockopt(..., SO_SNDBUFFORCE, "
                                        "...) failed: %s", strerror(errno));
			return -1;
		} 
#  else
	if(1) {
#  endif /* SO_SNDBUFFORCE */
		if(setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_SNDBUF, (void*)&snd,
			 (socklen_t)sizeof(snd)) < 0) {
			if(errno != ENOBUFS) {
				log_msg(LOG_ERR, "setsockopt(..., SO_SNDBUF, "
                                        "...) failed: %s", strerror(errno));
				return -1;
			}
		}
	}
#endif /* SO_SNDBUF */

	}
#endif /* defined(SO_RCVBUF) || defined(SO_SNDBUF) */

d582 1
d600 1
a600 1
	if ((nsd->db = namedb_open(nsd->dbfile, nsd->options)) == NULL) {
a602 3
		unlink(nsd->task[0]->fname);
		unlink(nsd->task[1]->fname);
		xfrd_del_tempdir(nsd);
d605 10
a614 5
	/* check if zone files have been modified */
	/* NULL for taskudb because we send soainfo in a moment, batched up,
	 * for all zones */
	if(nsd->options->zonefiles_check)
		namedb_check_zonefiles(nsd->db, nsd->options, NULL, NULL);
d689 1
a690 3
#ifdef HAVE_SSL
	daemon_remote_delete(nsd->rc); /* ssl-delete secret keys */
#endif
a691 1
#if 0 /* OS collects memory pages */
d694 1
a694 2
#endif
	log_finalize();
d698 2
a699 35
void
server_prepare_xfrd(struct nsd* nsd)
{
	char tmpfile[256];
	/* create task mmaps */
	nsd->mytask = 0;
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.task.0",
		nsd->options->xfrdir, (unsigned)getpid());
	nsd->task[0] = task_file_create(tmpfile);
	if(!nsd->task[0])
		exit(1);
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.task.1",
		nsd->options->xfrdir, (unsigned)getpid());
	nsd->task[1] = task_file_create(tmpfile);
	if(!nsd->task[1]) {
		unlink(nsd->task[0]->fname);
		exit(1);
	}
	assert(udb_base_get_userdata(nsd->task[0])->data == 0);
	assert(udb_base_get_userdata(nsd->task[1])->data == 0);
	/* create xfrd listener structure */
	nsd->xfrd_listener = region_alloc(nsd->region,
		sizeof(netio_handler_type));
	nsd->xfrd_listener->user_data = (struct ipc_handler_conn_data*)
		region_alloc(nsd->region, sizeof(struct ipc_handler_conn_data));
	nsd->xfrd_listener->fd = -1;
	((struct ipc_handler_conn_data*)nsd->xfrd_listener->user_data)->nsd =
		nsd;
	((struct ipc_handler_conn_data*)nsd->xfrd_listener->user_data)->conn =
		xfrd_tcp_create(nsd->region, QIOBUFSZ);
}


void
server_start_xfrd(struct nsd *nsd, int del_db, int reload_active)
d703 1
d705 4
d710 2
a711 15
	if(nsd->xfrd_listener->fd != -1)
		close(nsd->xfrd_listener->fd);
	if(del_db) {
		/* recreate taskdb that xfrd was using, it may be corrupt */
		/* we (or reload) use nsd->mytask, and xfrd uses the other */
		char* tmpfile = nsd->task[1-nsd->mytask]->fname;
		nsd->task[1-nsd->mytask]->fname = NULL;
		/* free alloc already, so udb does not shrink itself */
		udb_alloc_delete(nsd->task[1-nsd->mytask]->alloc);
		nsd->task[1-nsd->mytask]->alloc = NULL;
		udb_base_free(nsd->task[1-nsd->mytask]);
		/* create new file, overwrite the old one */
		nsd->task[1-nsd->mytask] = task_file_create(tmpfile);
		free(tmpfile);
	}
d714 1
a714 1
		return;
d721 2
a722 2
	default:
		/* PARENT: close first socket, use second one */
d724 1
a724 8
		if (fcntl(sockets[1], F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
		}
		if(del_db) xfrd_free_namedb(nsd);
		/* use other task than I am using, since if xfrd died and is
		 * restarted, the reload is using nsd->mytask */
		nsd->mytask = 1 - nsd->mytask;
		xfrd_init(sockets[1], nsd, del_db, reload_active);
d727 2
a728 2
	case 0:
		/* CHILD: close second socket, use first one */
d730 1
a730 4
		if (fcntl(sockets[0], F_SETFL, O_NONBLOCK) == -1) {
			log_msg(LOG_ERR, "cannot fcntl pipe: %s", strerror(errno));
		}
		nsd->xfrd_listener->fd = sockets[0];
d733 4
a736 4
	/* server-parent only */
	nsd->xfrd_listener->timeout = NULL;
	nsd->xfrd_listener->event_types = NETIO_EVENT_READ;
	nsd->xfrd_listener->event_handler = parent_handle_xfrd_command;
d738 1
a738 1
	data = (struct ipc_handler_conn_data *) nsd->xfrd_listener->user_data;
d740 1
a740 83
}

/** add all soainfo to taskdb */
static void
add_all_soa_to_task(struct nsd* nsd, struct udb_base* taskudb)
{
	struct radnode* n;
	udb_ptr task_last; /* last task, mytask is empty so NULL */
	/* add all SOA INFO to mytask */
	udb_ptr_init(&task_last, taskudb);
	for(n=radix_first(nsd->db->zonetree); n; n=radix_next(n)) {
		task_new_soainfo(taskudb, &task_last, (zone_type*)n->elem);
	}
	udb_ptr_unlink(&task_last, taskudb);
}

void
server_send_soa_xfrd(struct nsd* nsd, int shortsoa)
{
	/* normally this exchanges the SOA from nsd->xfrd and the expire back.
	 *   parent fills one taskdb with soas, xfrd fills other with expires.
	 *   then they exchange and process.
	 * shortsoa: xfrd crashes and needs to be restarted and one taskdb
	 *   may be in use by reload.  Fill SOA in taskdb and give to xfrd.
	 *   expire notifications can be sent back via a normal reload later
	 *   (xfrd will wait for current running reload to finish if any).
	 */
	sig_atomic_t cmd = 0;
#ifdef BIND8_STATS
	pid_t mypid;
#endif
	int xfrd_sock = nsd->xfrd_listener->fd;
	struct udb_base* taskudb = nsd->task[nsd->mytask];
	udb_ptr t;
	if(shortsoa) {
		/* put SOA in xfrd task because mytask may be in use */
		taskudb = nsd->task[1-nsd->mytask];
	}

	add_all_soa_to_task(nsd, taskudb);
	if(!shortsoa) {
		/* wait for xfrd to signal task is ready, RELOAD signal */
		if(block_read(nsd, xfrd_sock, &cmd, sizeof(cmd), -1) != sizeof(cmd) ||
			cmd != NSD_RELOAD) {
			log_msg(LOG_ERR, "did not get start signal from xfrd");
			exit(1);
		} 
	}
	/* give xfrd our task, signal it with RELOAD_DONE */
	task_process_sync(taskudb);
	cmd = NSD_RELOAD_DONE;
	if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending soa end from reload %d to xfrd: %s",
			(int)nsd->pid, strerror(errno));
	}
#ifdef BIND8_STATS
	mypid = getpid();
	if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
		log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
			strerror(errno));
	}
#endif

	if(!shortsoa) {
		/* process the xfrd task works (expiry data) */
		nsd->mytask = 1 - nsd->mytask;
		taskudb = nsd->task[nsd->mytask];
		task_remap(taskudb);
		udb_ptr_new(&t, taskudb, udb_base_get_userdata(taskudb));
		while(!udb_ptr_is_null(&t)) {
			task_process_expire(nsd->db, TASKLIST(&t));
			udb_ptr_set_rptr(&t, taskudb, &TASKLIST(&t)->next);
		}
		udb_ptr_unlink(&t, taskudb);
		task_clear(taskudb);

		/* tell xfrd that the task is emptied, signal with RELOAD_DONE */
		cmd = NSD_RELOAD_DONE;
		if(!write_socket(xfrd_sock, &cmd,  sizeof(cmd))) {
			log_msg(LOG_ERR, "problems sending soa end from reload %d to xfrd: %s",
				(int)nsd->pid, strerror(errno));
		}
	}
d744 1
a744 1
ssize_t
a798 86
static void
reload_process_tasks(struct nsd* nsd, udb_ptr* last_task, int cmdsocket)
{
	sig_atomic_t cmd = NSD_QUIT_SYNC;
	udb_ptr t, next;
	udb_base* u = nsd->task[nsd->mytask];
	udb_ptr_init(&next, u);
	udb_ptr_new(&t, u, udb_base_get_userdata(u));
	udb_base_set_userdata(u, 0);
	while(!udb_ptr_is_null(&t)) {
		/* store next in list so this one can be deleted or reused */
		udb_ptr_set_rptr(&next, u, &TASKLIST(&t)->next);
		udb_rptr_zero(&TASKLIST(&t)->next, u);

		/* process task t */
		/* append results for task t and update last_task */
		task_process_in_reload(nsd, u, last_task, &t);

		/* go to next */
		udb_ptr_set_ptr(&t, u, &next);

		/* if the parent has quit, we must quit too, poll the fd for cmds */
		if(block_read(nsd, cmdsocket, &cmd, sizeof(cmd), 0) == sizeof(cmd)) {
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", (int)cmd));
			if(cmd == NSD_QUIT) {
				DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: quit to follow nsd"));
				/* sync to disk (if needed) */
				udb_base_sync(nsd->db->udb, 0);
				/* unlink files of remainder of tasks */
				while(!udb_ptr_is_null(&t)) {
					if(TASKLIST(&t)->task_type == task_apply_xfr) {
						xfrd_unlink_xfrfile(nsd, TASKLIST(&t)->yesno);
					}
					udb_ptr_set_rptr(&t, u, &TASKLIST(&t)->next);
				}
				udb_ptr_unlink(&t, u);
				udb_ptr_unlink(&next, u);
				exit(0);
			}
		}

	}
	udb_ptr_unlink(&t, u);
	udb_ptr_unlink(&next, u);
}

#ifdef BIND8_STATS
static void
parent_send_stats(struct nsd* nsd, int cmdfd)
{
	size_t i;
	if(!write_socket(cmdfd, &nsd->st, sizeof(nsd->st))) {
		log_msg(LOG_ERR, "could not write stats to reload");
		return;
	}
	for(i=0; i<nsd->child_count; i++)
		if(!write_socket(cmdfd, &nsd->children[i].query_count,
			sizeof(stc_t))) {
			log_msg(LOG_ERR, "could not write stats to reload");
			return;
		}
}

static void
reload_do_stats(int cmdfd, struct nsd* nsd, udb_ptr* last)
{
	struct nsdst s;
	stc_t* p;
	size_t i;
	if(block_read(nsd, cmdfd, &s, sizeof(s),
		RELOAD_SYNC_TIMEOUT) != sizeof(s)) {
		log_msg(LOG_ERR, "could not read stats from oldpar");
		return;
	}
	s.db_disk = nsd->db->udb->base_size;
	s.db_mem = region_get_mem(nsd->db->region);
	p = (stc_t*)task_new_stat_info(nsd->task[nsd->mytask], last, &s,
		nsd->child_count);
	if(!p) return;
	for(i=0; i<nsd->child_count; i++) {
		if(block_read(nsd, cmdfd, p++, sizeof(stc_t), 1)!=sizeof(stc_t))
			return;
	}
}
#endif /* BIND8_STATS */

d805 1
a805 1
	int cmdsocket)
d807 1
a807 3
#ifdef BIND8_STATS
	pid_t mypid;
#endif
d809 2
a811 6
	udb_ptr last_task;

	/* see what tasks we got from xfrd */
	task_remap(nsd->task[nsd->mytask]);
	udb_ptr_init(&last_task, nsd->task[nsd->mytask]);
	reload_process_tasks(nsd, &last_task, cmdsocket);
d813 22
d839 5
a843 2
	/* sync to disk (if needed) */
	udb_base_sync(nsd->db->udb, 0);
d847 4
d858 2
a859 3
	if (server_start_children(nsd, server_region, netio, &nsd->
		xfrd_listener->fd) != 0) {
		send_children_quit(nsd);
d865 1
a865 1
		DEBUG(DEBUG_IPC,1, (LOG_INFO, "reload: ipc command from main %d", (int)cmd));
d868 1
a868 1
			send_children_quit(nsd);
d873 10
d888 2
a889 2
			log_msg(LOG_ERR, "problems sending command from reload to oldnsd: %s",
				strerror(errno));
d907 1
a910 5
#ifdef BIND8_STATS
	reload_do_stats(cmdsocket, nsd, &last_task);
#endif
	udb_ptr_unlink(&last_task, nsd->task[nsd->mytask]);
	task_process_sync(nsd->task[nsd->mytask]);
d912 60
a971 6
	/* send soainfo to the xfrd process, signal it that reload is done,
	 * it picks up the taskudb */
	cmd = NSD_RELOAD_DONE;
	if(!write_socket(nsd->xfrd_listener->fd, &cmd,  sizeof(cmd))) {
		log_msg(LOG_ERR, "problems sending reload_done xfrd: %s",
			strerror(errno));
d973 4
a976 5
#ifdef BIND8_STATS
	mypid = getpid();
	if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
		log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
			strerror(errno));
a977 1
#endif
a1007 4
	else if(nsd->signal_hint_reload_hup) {
		nsd->signal_hint_reload_hup = 0;
		return NSD_RELOAD_REQ;
	}
d1032 1
d1035 1
d1039 1
d1042 4
d1049 10
a1058 2
	/* Add listener for the XFRD process */
	netio_add_handler(netio, nsd->xfrd_listener);
d1061 2
a1062 3
	if (server_start_children(nsd, server_region, netio,
		&nsd->xfrd_listener->fd) != 0) {
		send_children_quit(nsd);
d1091 1
a1091 1
						&nsd->xfrd_listener->fd);
d1093 1
a1093 4
					sig_atomic_t cmd = NSD_RELOAD_DONE;
#ifdef BIND8_STATS
					pid_t mypid;
#endif
d1098 1
a1098 1
					if(reload_listener.fd != -1) close(reload_listener.fd);
a1100 1
					task_process_sync(nsd->task[nsd->mytask]);
d1102 1
a1102 2
					if(!write_socket(nsd->xfrd_listener->fd,
						&cmd, sizeof(cmd)) == -1) {
d1107 5
a1111 7
#ifdef BIND8_STATS
					mypid = getpid();
					if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
						log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
							strerror(errno));
					}
#endif
a1138 12
		case NSD_RELOAD_REQ: {
			sig_atomic_t cmd = NSD_RELOAD_REQ;
			log_msg(LOG_WARNING, "SIGHUP received, reloading...");
			DEBUG(DEBUG_IPC,1, (LOG_INFO,
				"main: ipc send reload_req to xfrd"));
			if(!write_socket(nsd->xfrd_listener->fd,
				&cmd, sizeof(cmd))) {
				log_msg(LOG_ERR, "server_main: could not send "
				"reload_req to xfrd: %s", strerror(errno));
			}
			nsd->mode = NSD_RUN;
			} break;
d1142 1
a1142 1
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "reloading..."));
d1149 2
a1150 2
			/* switch the mytask to keep track of who owns task*/
			nsd->mytask = 1 - nsd->mytask;
d1167 1
a1167 1
					reload_sockets[1]);
d1172 1
a1172 2
				((struct ipc_handler_conn_data*)nsd->
					xfrd_listener->user_data)
d1200 1
a1200 2
				if(!write_socket(nsd->xfrd_listener->fd,
					&cmd, sizeof(cmd))) {
a1219 3
#ifdef BIND8_STATS
				parent_send_stats(nsd, reload_listener.fd);
#endif /* BIND8_STATS */
a1221 1
			DEBUG(DEBUG_IPC,1, (LOG_INFO, "server_main: shutdown sequence"));
d1223 1
a1223 1
			send_children_quit(nsd);
d1225 1
a1225 1
#if 0 /* OS collects memory pages */
a1226 1
#endif
d1232 1
d1246 1
a1246 1
			log_msg(LOG_WARNING, "NSD main server mode invalid: %d", (int)nsd->mode);
d1252 6
a1257 8
	/* close opened ports to avoid race with restart of nsd */
	server_close_all_sockets(nsd->udp, nsd->ifs);
	server_close_all_sockets(nsd->tcp, nsd->ifs);
#ifdef HAVE_SSL
	daemon_remote_close(nsd->rc);
#endif
	send_children_quit_and_wait(nsd);

a1259 2
	unlink(nsd->task[0]->fname);
	unlink(nsd->task[1]->fname);
d1272 1
a1272 1
	if(nsd->xfrd_listener->fd != -1) {
d1277 1
a1277 1
		if(!write_socket(nsd->xfrd_listener->fd, &cmd, sizeof(cmd))) {
d1281 3
a1283 3
		fsync(nsd->xfrd_listener->fd);
		close(nsd->xfrd_listener->fd);
		(void)kill(nsd->pid, SIGTERM);
a1284 1
	xfrd_del_tempdir(nsd);
d1286 1
a1286 1
#if 0 /* OS collects memory pages */
a1287 1
#endif
a1296 38
static query_state_type
server_process_query_udp(struct nsd *nsd, struct query *query)
{
#ifdef RATELIMIT
	if(query_process(query, nsd) != QUERY_DISCARDED) {
		if(rrl_process_query(query))
			return rrl_slip(query);
		else	return QUERY_PROCESSED;
	}
	return QUERY_DISCARDED;
#else
	return query_process(query, nsd);
#endif
}

struct event_base*
nsd_child_event_base(void)
{
	struct event_base* base;
#ifdef USE_MINI_EVENT
	static time_t secs;
	static struct timeval now;
	base = event_init(&secs, &now);
#else
#  if defined(HAVE_EV_LOOP) || defined(HAVE_EV_DEFAULT_LOOP)
	/* libev */
	base = (struct event_base *)ev_default_loop(EVFLAG_AUTO);
#  else
	/* libevent */
#    ifdef HAVE_EVENT_BASE_NEW
	base = event_base_new();
#    else
	base = event_init();
#    endif
#  endif
#endif
	return base;
}
d1306 2
a1307 1
	struct event_base* event_base = nsd_child_event_base();
a1310 9
	if(!event_base) {
		log_msg(LOG_ERR, "nsd server could not create event base");
		exit(1);
	}

#ifdef RATELIMIT
	rrl_init((nsd->this_child - nsd->children)/sizeof(nsd->children[0]));
#endif

d1322 7
a1328 3
		struct event *handler;
		struct ipc_handler_conn_data* user_data =
			(struct ipc_handler_conn_data*)region_alloc(
d1330 6
a1335 11
		user_data->nsd = nsd;
		user_data->conn = xfrd_tcp_create(server_region, QIOBUFSZ);

		handler = (struct event*) region_alloc(
			server_region, sizeof(*handler));
		event_set(handler, nsd->this_child->parent_fd, EV_PERSIST|
			EV_READ, child_handle_parent_command, user_data);
		if(event_base_set(event_base, handler) != 0)
			log_msg(LOG_ERR, "nsd ipcchild: event_base_set failed");
		if(event_add(handler, NULL) != 0)
			log_msg(LOG_ERR, "nsd ipcchild: event_add failed");
a1338 1
#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
d1341 1
a1341 15
#else
		udp_query = NULL;
		memset(msgs, 0, sizeof(msgs));
		for (i = 0; i < NUM_RECV_PER_SELECT; i++) {
			queries[i] = query_create(server_region,
				compressed_dname_offsets, compression_table_size);
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_base          = buffer_begin(queries[i]->packet);
			iovecs[i].iov_len           = buffer_remaining(queries[i]->packet);;
			msgs[i].msg_hdr.msg_iov     = &iovecs[i];
			msgs[i].msg_hdr.msg_iovlen  = 1;
			msgs[i].msg_hdr.msg_name    = &queries[i]->addr;
			msgs[i].msg_hdr.msg_namelen = queries[i]->addrlen;
		}
#endif
d1344 1
a1344 1
			struct event *handler;
d1353 8
a1360 8
			handler = (struct event*) region_alloc(
				server_region, sizeof(*handler));
			event_set(handler, nsd->udp[i].s, EV_PERSIST|EV_READ,
				handle_udp, data);
			if(event_base_set(event_base, handler) != 0)
				log_msg(LOG_ERR, "nsd udp: event_base_set failed");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "nsd udp: event_add failed");
d1369 2
a1370 3
	tcp_accept_handler_count = nsd->ifs;
	tcp_accept_handlers = (struct tcp_accept_handler_data*) region_alloc(
		server_region, nsd->ifs * sizeof(*tcp_accept_handlers));
d1373 6
a1378 3
			struct event *handler = &tcp_accept_handlers[i].event;
			struct tcp_accept_handler_data* data =
				&tcp_accept_handlers[i];
d1381 10
a1390 7
			event_set(handler, nsd->tcp[i].s, EV_PERSIST|EV_READ,
				handle_tcp_accept, data);
			if(event_base_set(event_base, handler) != 0)
				log_msg(LOG_ERR, "nsd tcp: event_base_set failed");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "nsd tcp: event_add failed");
			data->event_added = 1;
d1392 1
a1392 1
	} else tcp_accept_handler_count = 0;
d1429 1
a1429 1
			if(event_base_loop(event_base, EVLOOP_ONCE) == -1) {
d1431 1
a1431 1
					log_msg(LOG_ERR, "dispatch failed: %s", strerror(errno));
d1439 1
a1439 1
				(int)mode);
d1448 1
a1448 2
#if 0 /* OS collects memory pages */
	event_base_free(event_base);
d1450 15
a1465 1
	server_shutdown(nsd);
a1467 1
#if defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG)
d1469 3
a1471 1
handle_udp(int fd, short event, void* arg)
d1473 4
a1476 3
	struct udp_handler_data *data = (struct udp_handler_data *) arg;
	int received, sent, recvcount, i;
	struct query *q;
d1478 1
a1478 12
	if (!(event & EV_READ)) {
		return;
	}
	recvcount = recvmmsg(fd, msgs, NUM_RECV_PER_SELECT, 0, NULL);
	/* this printf strangely gave a performance increase on Linux */
	/* printf("recvcount %d \n", recvcount); */
	if (recvcount == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			log_msg(LOG_ERR, "recvmmsg failed: %s", strerror(errno));
			STATUP(data->nsd, rxerr);
		}
		/* Simply no data available */
a1480 12
	for (i = 0; i < recvcount; i++) {
	loopstart:
		received = msgs[i].msg_len;
		q = queries[i];
		if (received == -1) {
			log_msg(LOG_ERR, "recvmmsg %d failed %s", i, strerror(
				msgs[i].msg_hdr.msg_flags));
			STATUP(data->nsd, rxerr);
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_len = buffer_remaining(q->packet);
			goto swap_drop;
		}
d1482 1
a1482 21
		/* Account... */
		if (data->socket->addr->ai_family == AF_INET) {
			STATUP(data->nsd, qudp);
		} else if (data->socket->addr->ai_family == AF_INET6) {
			STATUP(data->nsd, qudp6);
		}

		buffer_skip(q->packet, received);
		buffer_flip(q->packet);

		/* Process and answer the query... */
		if (server_process_query_udp(data->nsd, q) != QUERY_DISCARDED) {
			if (RCODE(q->packet) == RCODE_OK && !AA(q->packet)) {
				STATUP(data->nsd, nona);
			}

			/* Add EDNS0 and TSIG info if necessary.  */
			query_add_optional(q, data->nsd);

			buffer_flip(q->packet);
			iovecs[i].iov_len = buffer_remaining(q->packet);
d1484 4
a1487 44
			/* Account the rcode & TC... */
			STATUP2(data->nsd, rcode, RCODE(q->packet));
			if (TC(q->packet))
				STATUP(data->nsd, truncated);
#endif /* BIND8_STATS */
		} else {
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			iovecs[i].iov_len = buffer_remaining(q->packet);
		swap_drop:
			STATUP(data->nsd, dropped);
			if(i != recvcount-1) {
				/* swap with last and decrease recvcount */
				struct mmsghdr mtmp = msgs[i];
				struct iovec iotmp = iovecs[i];
				recvcount--;
				msgs[i] = msgs[recvcount];
				iovecs[i] = iovecs[recvcount];
				queries[i] = queries[recvcount];
				msgs[recvcount] = mtmp;
				iovecs[recvcount] = iotmp;
				queries[recvcount] = q;
				msgs[i].msg_hdr.msg_iov = &iovecs[i];
				msgs[recvcount].msg_hdr.msg_iov = &iovecs[recvcount];
				goto loopstart;
			} else { recvcount --; }
		}
	}

	/* send until all are sent */
	i = 0;
	while(i<recvcount) {
		sent = sendmmsg(fd, &msgs[i], recvcount-i, 0);
		if(sent == -1) {
			log_msg(LOG_ERR, "sendmmsg failed: %s", strerror(errno));
#ifdef BIND8_STATS
			data->nsd->st.txerr += recvcount-i;
#endif /* BIND8_STATS */
			break;
		}
		i += sent;
	}
	for(i=0; i<recvcount; i++) {
		query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
		iovecs[i].iov_len = buffer_remaining(queries[i]->packet);
a1488 18
}

#else /* defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG) */

static void
handle_udp(int fd, short event, void* arg)
{
	struct udp_handler_data *data = (struct udp_handler_data *) arg;
	int received, sent;
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
	int recvcount;
#endif /* HAVE_RECVMMSG */
	int i;
#endif /* NONBLOCKING_IS_BROKEN */
	struct query *q;
#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
	q = data->query;
d1491 10
a1500 9
	if (!(event & EV_READ)) {
		return;
	}
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
	recvcount = recvmmsg(fd, msgs, NUM_RECV_PER_SELECT, 0, NULL);
	/* this printf strangely gave a performance increase on Linux */
	/* printf("recvcount %d \n", recvcount); */
	if (recvcount == -1) {
d1502 1
a1502 11
			log_msg(LOG_ERR, "recvmmsg failed: %s", strerror(errno));
			STATUP(data->nsd, rxerr);
		}
		/* Simply no data available */
		return;
	}
	for (i = 0; i < recvcount; i++) {
		received = msgs[i].msg_len;
		msgs[i].msg_hdr.msg_namelen = queries[i]->addrlen;
		if (received == -1) {
			log_msg(LOG_ERR, "recvmmsg failed");
d1504 1
a1504 34
			/* the error can be found in msgs[i].msg_hdr.msg_flags */
			query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
			continue;
		}
		q = queries[i];
#else
	for(i=0; i<NUM_RECV_PER_SELECT; i++) {
#endif /* HAVE_RECVMMSG */
#endif /* NONBLOCKING_IS_BROKEN */

#if (defined(NONBLOCKING_IS_BROKEN) || !defined(HAVE_RECVMMSG))
		/* Initialize the query... */
		query_reset(q, UDP_MAX_MESSAGE_LEN, 0);

		received = recvfrom(fd,
				    buffer_begin(q->packet),
				    buffer_remaining(q->packet),
				    0,
				    (struct sockaddr *)&q->addr,
				    &q->addrlen);
		if (received == -1) {
			if (errno != EAGAIN && errno != EINTR) {
				log_msg(LOG_ERR, "recvfrom failed: %s", strerror(errno));
				STATUP(data->nsd, rxerr);
			}
			return;
		}
#endif /* NONBLOCKING_IS_BROKEN || !HAVE_RECVMMSG */

		/* Account... */
		if (data->socket->addr->ai_family == AF_INET) {
			STATUP(data->nsd, qudp);
		} else if (data->socket->addr->ai_family == AF_INET6) {
			STATUP(data->nsd, qudp6);
d1506 1
a1506 1

d1512 1
d1515 4
d1521 11
d1537 1
a1537 1
			sent = sendto(fd,
d1546 5
d1553 1
a1554 1
#ifdef BIND8_STATS
d1557 5
a1561 1
				if (TC(q->packet))
d1563 5
d1570 1
d1573 6
a1579 4
#ifndef NONBLOCKING_IS_BROKEN
#ifdef HAVE_RECVMMSG
		query_reset(queries[i], UDP_MAX_MESSAGE_LEN, 0);
#endif
a1580 1
#endif
a1581 1
#endif /* defined(HAVE_SENDMMSG) && !defined(NONBLOCKING_IS_BROKEN) && defined(HAVE_RECVMMSG) */
d1585 1
a1585 1
cleanup_tcp_handler(struct tcp_handler_data* data)
d1587 5
a1591 2
	event_del(&data->event);
	close(data->event.ev_fd);
d1598 4
a1601 3
	if (slowaccept || data->nsd->current_tcp_count == data->nsd->maximum_tcp_count) {
		configure_handler_event_types(EV_READ|EV_PERSIST);
		slowaccept = 0;
d1610 3
a1612 1
handle_tcp_reading(int fd, short event, void* arg)
d1614 2
a1615 1
	struct tcp_handler_data *data = (struct tcp_handler_data *) arg;
a1616 2
	struct event_base* ev_base;
	struct timeval timeout;
d1618 1
a1618 1
	if ((event & EV_TIMEOUT)) {
d1620 1
a1620 1
		cleanup_tcp_handler(data);
d1627 1
a1627 1
		cleanup_tcp_handler(data);
d1631 1
a1631 1
	assert((event & EV_READ));
d1641 1
a1641 1
		received = read(fd,
a1652 2
				char buf[48];
				addr2str(&data->query->addr, buf, sizeof(buf));
d1656 2
a1657 2
				log_msg(LOG_ERR, "failed reading from %s tcp: %s", buf, strerror(errno));
				cleanup_tcp_handler(data);
d1662 1
a1662 1
			cleanup_tcp_handler(data);
d1689 1
a1689 1
			cleanup_tcp_handler(data);
d1695 1
a1695 1
			cleanup_tcp_handler(data);
d1705 1
a1705 1
	received = read(fd,
a1715 2
			char buf[48];
			addr2str(&data->query->addr, buf, sizeof(buf));
d1719 2
a1720 2
			log_msg(LOG_ERR, "failed reading from %s tcp: %s", buf, strerror(errno));
			cleanup_tcp_handler(data);
d1725 1
a1725 1
		cleanup_tcp_handler(data);
d1742 4
a1745 3
#ifndef INET6
        STATUP(data->nsd, ctcp);
#else
d1751 2
a1752 1
#endif
d1764 6
a1769 1
		cleanup_tcp_handler(data);
d1773 1
d1778 4
d1784 16
d1807 3
a1809 2
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0L;
d1811 2
a1812 10
	ev_base = data->event.ev_base;
	event_del(&data->event);
	event_set(&data->event, fd, EV_PERSIST | EV_WRITE | EV_TIMEOUT,
		handle_tcp_writing, data);
	if(event_base_set(ev_base, &data->event) != 0)
		log_msg(LOG_ERR, "event base set tcpr failed");
	if(event_add(&data->event, &timeout) != 0)
		log_msg(LOG_ERR, "event add tcpr failed");
	/* see if we can write the answer right away(usually so,EAGAIN ifnot)*/
	handle_tcp_writing(fd, EV_WRITE, data);
d1816 3
a1818 1
handle_tcp_writing(int fd, short event, void* arg)
d1820 2
a1821 1
	struct tcp_handler_data *data = (struct tcp_handler_data *) arg;
a1823 2
	struct timeval timeout;
	struct event_base* ev_base;
d1825 1
a1825 1
	if ((event & EV_TIMEOUT)) {
d1827 1
a1827 1
		cleanup_tcp_handler(data);
d1831 1
a1831 1
	assert((event & EV_WRITE));
d1842 1
a1842 1
		sent = writev(fd, iov, 2);
d1844 1
a1844 1
		sent = write(fd,
d1860 1
a1860 1
				  if(verbosity >= 2 || errno != EPIPE)
d1862 2
a1863 2
				    log_msg(LOG_ERR, "failed writing to tcp: %s", strerror(errno));
				cleanup_tcp_handler(data);
d1882 3
a1884 3
 	}
 
	sent = write(fd,
d1899 1
a1899 1
				  if(verbosity >= 2 || errno != EPIPE)
d1902 1
a1902 1
			cleanup_tcp_handler(data);
d1934 3
a1936 10
			timeout.tv_sec = data->nsd->tcp_timeout;
			timeout.tv_usec = 0L;
			ev_base = data->event.ev_base;
			event_del(&data->event);
			event_set(&data->event, fd, EV_PERSIST | EV_WRITE | EV_TIMEOUT,
				handle_tcp_writing, data);
			if(event_base_set(ev_base, &data->event) != 0)
				log_msg(LOG_ERR, "event base set tcpw failed");
			if(event_add(&data->event, &timeout) != 0)
				log_msg(LOG_ERR, "event add tcpw failed");
d1953 1
a1953 1
		(void) shutdown(fd, SHUT_WR);
d1958 6
a1963 10
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0L;
	ev_base = data->event.ev_base;
	event_del(&data->event);
	event_set(&data->event, fd, EV_PERSIST | EV_READ | EV_TIMEOUT,
		handle_tcp_reading, data);
	if(event_base_set(ev_base, &data->event) != 0)
		log_msg(LOG_ERR, "event base set tcpw failed");
	if(event_add(&data->event, &timeout) != 0)
		log_msg(LOG_ERR, "event add tcpw failed");
a1966 10
static void
handle_slowaccept_timeout(int ATTR_UNUSED(fd), short ATTR_UNUSED(event),
	void* ATTR_UNUSED(arg))
{
	if(slowaccept) {
		configure_handler_event_types(EV_PERSIST | EV_READ);
		slowaccept = 0;
	}
}

d1969 1
a1969 1
 * a new TCP reader event handler is added.  The TCP handler
d1973 3
a1975 1
handle_tcp_accept(int fd, short event, void* arg)
d1978 1
a1978 1
		= (struct tcp_accept_handler_data *) arg;
d1982 1
a1988 1
	struct timeval timeout;
d1990 1
a1990 1
	if (!(event & EV_READ)) {
d2000 1
a2000 1
	s = accept(fd, (struct sockaddr *) &addr, &addrlen);
d2010 3
a2012 10
				/* disable accept events */
				struct timeval tv;
				configure_handler_event_types(0);
				tv.tv_sec = SLOW_ACCEPT_TIMEOUT;
				tv.tv_usec = 0L;
				event_set(&slowaccept_event, -1, EV_TIMEOUT,
					handle_slowaccept_timeout, NULL);
				(void)event_base_set(data->event.ev_base,
					&slowaccept_event);
				(void)event_add(&slowaccept_event, &tv);
d2049 3
d2057 12
a2068 2
	timeout.tv_sec = data->nsd->tcp_timeout;
	timeout.tv_usec = 0;
d2070 1
a2070 6
	event_set(&tcp_data->event, s, EV_PERSIST | EV_READ | EV_TIMEOUT,
		handle_tcp_reading, tcp_data);
	if(event_base_set(data->event.ev_base, &tcp_data->event) != 0)
		log_msg(LOG_ERR, "cannot set tcp event base");
	if(event_add(&tcp_data->event, &timeout) != 0)
		log_msg(LOG_ERR, "cannot set tcp event base");
d2079 3
a2081 1
		configure_handler_event_types(0);
d2102 1
a2116 1
	DEBUG(DEBUG_IPC, 1, (LOG_INFO, "send children quit"));
d2142 3
a2144 1
configure_handler_event_types(short event_types)
d2148 4
a2151 22
	for (i = 0; i < tcp_accept_handler_count; ++i) {
		struct event* handler = &tcp_accept_handlers[i].event;
		if(event_types) {
			/* reassign */
			int fd = handler->ev_fd;
			struct event_base* base = handler->ev_base;
			if(tcp_accept_handlers[i].event_added)
				event_del(handler);
			event_set(handler, fd, event_types,
				handle_tcp_accept, &tcp_accept_handlers[i]);
			if(event_base_set(base, handler) != 0)
				log_msg(LOG_ERR, "conhand: cannot event_base");
			if(event_add(handler, NULL) != 0)
				log_msg(LOG_ERR, "conhand: cannot event_add");
			tcp_accept_handlers[i].event_added = 1;
		} else {
			/* remove */
			if(tcp_accept_handlers[i].event_added) {
				event_del(handler);
				tcp_accept_handlers[i].event_added = 0;
			}
		}
@


1.1.1.11
log
@update to NSD 4.0.1, ok sthen@@
@
text
@d677 1
a677 1
		namedb_check_zonefiles(nsd, nsd->options, NULL, NULL);
d869 1
a869 1
		task_new_soainfo(taskudb, &task_last, (zone_type*)n->elem, 0);
d1829 1
a1829 4
			const char* es = strerror(errno);
			char a[48];
			addr2str(&queries[i]->addr, a, sizeof(a));
			log_msg(LOG_ERR, "sendmmsg [0]=%s count=%d failed: %s", a, (int)(recvcount-i), es);
d1940 1
a1940 4
				const char* es = strerror(errno);
				char a[48];
				addr2str(&q->addr, a, sizeof(a));
				log_msg(LOG_ERR, "sendto %s failed: %s", a, es);
@


1.1.1.12
log
@update to NSD 4.0.2, ok sthen@@
@
text
@a35 1
#include <openssl/rand.h>
d37 1
a37 7
#  ifdef HAVE_EVENT_H
#    include <event.h>
#  else
#    include <event2/event.h>
#    include "event2/event_struct.h"
#    include "event2/event_compat.h"
#  endif
d39 1
a39 1
#  include "mini_event.h"
d41 2
@


1.1.1.13
log
@Update to NSD 4.0.3, main change is to improve/fix nsd.db database
corruption checks and avoid some causes of corruption. More details at
http://article.gmane.org/gmane.network.dns.nsd.general/1687
@
text
@a1500 10
		/* wait for reload to finish processing */
		while(1) {
			if(waitpid(reload_pid, NULL, 0) == -1) {
				if(errno == EINTR) continue;
				if(errno == ECHILD) break;
				log_msg(LOG_ERR, "waitpid(reload %d): %s",
					(int)reload_pid, strerror(errno));
			}
			break;
		}
d1515 1
a1519 3
	/* write the nsd.db to disk, wait for it to complete */
	udb_base_sync(nsd->db->udb, 1);
	udb_base_close(nsd->db->udb);
@


1.1.1.14
log
@update to NSD 4.1.0, ok sthen@@
@
text
@a30 1
#include <signal.h>
d423 1
a423 1
			if(errno != ENOBUFS && errno != ENOSYS) {
d446 1
a446 1
			if(errno != ENOBUFS && errno != ENOSYS) {
d681 1
a681 2
	if(nsd->options->zonefiles_check || (nsd->options->database == NULL ||
		nsd->options->database[0] == 0))
d844 1
a844 1
		xfrd_init(sockets[1], nsd, del_db, reload_active, pid);
d891 1
d893 1
a896 20
	if(!shortsoa) {
		if(nsd->signal_hint_shutdown) {
		shutdown:
			log_msg(LOG_WARNING, "signal received, shutting down...");
			server_close_all_sockets(nsd->udp, nsd->ifs);
			server_close_all_sockets(nsd->tcp, nsd->ifs);
#ifdef HAVE_SSL
			daemon_remote_close(nsd->rc);
#endif
			/* Unlink it if possible... */
			unlinkpid(nsd->pidfile);
			unlink(nsd->task[0]->fname);
			unlink(nsd->task[1]->fname);
			/* write the nsd.db to disk, wait for it to complete */
			udb_base_sync(nsd->db->udb, 1);
			udb_base_close(nsd->db->udb);
			server_shutdown(nsd);
			exit(0);
		}
	}
a909 3
		if(nsd->signal_hint_shutdown) {
			goto shutdown;
		}
d918 1
d924 1
d1078 1
a1078 1
	s.db_disk = (nsd->db->udb?nsd->db->udb->base_size:0);
d1098 1
d1100 1
a1103 5
	struct sigaction old_sigchld, ign_sigchld;
	/* ignore SIGCHLD from the previous server_main that used this pid */
	memset(&ign_sigchld, 0, sizeof(ign_sigchld));
	ign_sigchld.sa_handler = SIG_IGN;
	sigaction(SIGCHLD, &ign_sigchld, &old_sigchld);
a1107 1
	udb_compact_inhibited(nsd->db->udb, 1);
a1108 2
	udb_compact_inhibited(nsd->db->udb, 0);
	udb_compact(nsd->db->udb);
a1124 2
	/* listen for the signals of failed children again */
	sigaction(SIGCHLD, &old_sigchld, NULL);
d1182 1
d1188 1
d1281 1
a1281 1
			while((child_pid = waitpid(-1, &status, WNOHANG)) != -1 && child_pid != 0) {
d1295 1
d1297 1
d1308 1
a1308 1
						&cmd, sizeof(cmd))) {
d1313 1
d1319 2
a1320 6
				} else if(status != 0) {
					/* check for status, because we get
					 * the old-servermain because reload
					 * is the process-parent of old-main,
					 * and we get older server-processes
					 * that are exiting after a reload */
d1322 1
a1322 1
					       "process %d terminated with status %d",
d1330 1
a1330 2
				if (errno != ECHILD)
					log_msg(LOG_WARNING, "wait failed: %s", strerror(errno));
a1344 30
			if(nsd->restart_children) {
				restart_child_servers(nsd, server_region, netio,
					&nsd->xfrd_listener->fd);
				nsd->restart_children = 0;
			}
			if(nsd->reload_failed) {
				sig_atomic_t cmd = NSD_RELOAD_DONE;
				pid_t mypid;
				nsd->reload_failed = 0;
				log_msg(LOG_WARNING,
				       "Reload process %d failed, continuing with old database",
				       (int) reload_pid);
				reload_pid = -1;
				if(reload_listener.fd != -1) close(reload_listener.fd);
				reload_listener.fd = -1;
				reload_listener.event_types = NETIO_EVENT_NONE;
				task_process_sync(nsd->task[nsd->mytask]);
				/* inform xfrd reload attempt ended */
				if(!write_socket(nsd->xfrd_listener->fd,
					&cmd, sizeof(cmd))) {
					log_msg(LOG_ERR, "problems "
					  "sending SOAEND to xfrd: %s",
					  strerror(errno));
				}
				mypid = getpid();
				if(!write_socket(nsd->xfrd_listener->fd, &mypid,  sizeof(mypid))) {
					log_msg(LOG_ERR, "problems sending reloadpid to xfrd: %s",
						strerror(errno));
				}
			}
d1383 2
a1384 2
			default:
				/* PARENT */
d1400 4
a1403 4
			case 0:
				/* CHILD */
				/* server_main keep running until NSD_QUIT_SYNC
				 * received from reload. */
a1410 1
				reload_pid = getppid();
d1459 1
a1476 1
	log_msg(LOG_WARNING, "signal received, shutting down...");
d1725 1
a1725 1
				while (waitpid(-1, NULL, WNOHANG) > 0) ;
@


1.1.1.15
log
@update to NSD 4.1.1, ok sthen@@
@
text
@a36 3
#ifdef HAVE_MMAP
#include <sys/mman.h>
#endif /* HAVE_MMAP */
a341 174
/* set zone stat ids for zones initially read in */
static void
zonestatid_tree_set(struct nsd* nsd)
{
	struct radnode* n;
	for(n=radix_first(nsd->db->zonetree); n; n=radix_next(n)) {
		zone_type* zone = (zone_type*)n->elem;
		zone->zonestatid = getzonestatid(nsd->options, zone->opts);
	}
}

#ifdef USE_ZONE_STATS
void
server_zonestat_alloc(struct nsd* nsd)
{
	size_t num = (nsd->options->zonestatnames->count==0?1:
			nsd->options->zonestatnames->count);
	size_t sz = sizeof(struct nsdst)*num;
	char tmpfile[256];
	uint8_t z = 0;

	/* file names */
	nsd->zonestatfname[0] = 0;
	nsd->zonestatfname[1] = 0;
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.zstat.0",
		nsd->options->xfrdir, (unsigned)getpid());
	nsd->zonestatfname[0] = region_strdup(nsd->region, tmpfile);
	snprintf(tmpfile, sizeof(tmpfile), "%snsd.%u.zstat.1",
		nsd->options->xfrdir, (unsigned)getpid());
	nsd->zonestatfname[1] = region_strdup(nsd->region, tmpfile);

	/* file descriptors */
	nsd->zonestatfd[0] = open(nsd->zonestatfname[0], O_CREAT|O_RDWR, 0600);
	if(nsd->zonestatfd[0] == -1) {
		log_msg(LOG_ERR, "cannot create %s: %s", nsd->zonestatfname[0],
			strerror(errno));
		exit(1);
	}
	nsd->zonestatfd[1] = open(nsd->zonestatfname[1], O_CREAT|O_RDWR, 0600);
	if(nsd->zonestatfd[0] == -1) {
		log_msg(LOG_ERR, "cannot create %s: %s", nsd->zonestatfname[1],
			strerror(errno));
		close(nsd->zonestatfd[0]);
		unlink(nsd->zonestatfname[0]);
		exit(1);
	}

#ifdef HAVE_MMAP
	if(lseek(nsd->zonestatfd[0], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[0],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[0], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[0], strerror(errno));
		exit(1);
	}
	if(lseek(nsd->zonestatfd[1], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[1],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[1], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[1], strerror(errno));
		exit(1);
	}
	nsd->zonestat[0] = (struct nsdst*)mmap(NULL, sz, PROT_READ|PROT_WRITE,
		MAP_SHARED, nsd->zonestatfd[0], 0);
	if(nsd->zonestat[0] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
		exit(1);
	}
	nsd->zonestat[1] = (struct nsdst*)mmap(NULL, sz, PROT_READ|PROT_WRITE,
		MAP_SHARED, nsd->zonestatfd[1], 0);
	if(nsd->zonestat[1] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
		exit(1);
	}
	memset(nsd->zonestat[0], 0, sz);
	memset(nsd->zonestat[1], 0, sz);
	nsd->zonestatsize[0] = num;
	nsd->zonestatsize[1] = num;
	nsd->zonestatdesired = num;
	nsd->zonestatsizenow = num;
	nsd->zonestatnow = nsd->zonestat[0];
#endif /* HAVE_MMAP */
}

void
zonestat_remap(struct nsd* nsd, int idx, size_t sz)
{
#ifdef HAVE_MMAP
#ifdef MREMAP_MAYMOVE
	nsd->zonestat[idx] = (struct nsdst*)mremap(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx], sz,
		MREMAP_MAYMOVE);
	if(nsd->zonestat[idx] == MAP_FAILED) {
		log_msg(LOG_ERR, "mremap failed: %s", strerror(errno));
		exit(1);
	}
#else /* !HAVE MREMAP */
	if(msync(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx], MS_ASYNC) != 0)
		log_msg(LOG_ERR, "msync failed: %s", strerror(errno));
	if(munmap(nsd->zonestat[idx],
		sizeof(struct nsdst)*nsd->zonestatsize[idx]) != 0)
		log_msg(LOG_ERR, "munmap failed: %s", strerror(errno));
	nsd->zonestat[idx] = (struct nsdst*)mmap(NULL, sz,
		PROT_READ|PROT_WRITE, MAP_SHARED, nsd->zonestatfd[idx], 0);
	if(nsd->zonestat[idx] == MAP_FAILED) {
		log_msg(LOG_ERR, "mmap failed: %s", strerror(errno));
		exit(1);
	}
#endif /* MREMAP */
#endif /* HAVE_MMAP */
}

/* realloc the zonestat array for the one that is not currently in use,
 * to match the desired new size of the array (if applicable) */
void
server_zonestat_realloc(struct nsd* nsd)
{
#ifdef HAVE_MMAP
	uint8_t z = 0;
	size_t sz;
	int idx = 0; /* index of the zonestat array that is not in use */
	if(nsd->zonestatnow == nsd->zonestat[0])
		idx = 1;
	if(nsd->zonestatsize[idx] == nsd->zonestatdesired)
		return;
	sz = sizeof(struct nsdst)*nsd->zonestatdesired;
	if(lseek(nsd->zonestatfd[idx], (off_t)sz-1, SEEK_SET) == -1) {
		log_msg(LOG_ERR, "lseek %s: %s", nsd->zonestatfname[idx],
			strerror(errno));
		exit(1);
	}
	if(write(nsd->zonestatfd[idx], &z, 1) == -1) {
		log_msg(LOG_ERR, "cannot extend stat file %s (%s)",
			nsd->zonestatfname[idx], strerror(errno));
		exit(1);
	}
	zonestat_remap(nsd, idx, sz);
	/* zero the newly allocated region */
	if(nsd->zonestatdesired > nsd->zonestatsize[idx]) {
		memset(((char*)nsd->zonestat[idx])+sizeof(struct nsdst) *
			nsd->zonestatsize[idx], 0, sizeof(struct nsdst) *
			(nsd->zonestatdesired - nsd->zonestatsize[idx]));
	}
	nsd->zonestatsize[idx] = nsd->zonestatdesired;
#endif /* HAVE_MMAP */
}

/* switchover to use the other array for the new children, that
 * briefly coexist with the old children.  And we want to avoid them
 * both writing to the same statistics arrays. */
void
server_zonestat_switch(struct nsd* nsd)
{
	if(nsd->zonestatnow == nsd->zonestat[0]) {
		nsd->zonestatnow = nsd->zonestat[1];
		nsd->zonestatsizenow = nsd->zonestatsize[1];
	} else {
		nsd->zonestatnow = nsd->zonestat[0];
		nsd->zonestatsizenow = nsd->zonestatsize[0];
	}
}
#endif /* USE_ZONE_STATS */

a675 4
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
a684 1
	zonestatid_tree_set(nsd);
a909 4
#ifdef USE_ZONE_STATS
			unlink(nsd->zonestatfname[0]);
			unlink(nsd->zonestatfname[1]);
#endif
a1150 4
#ifdef USE_ZONE_STATS
	server_zonestat_realloc(nsd); /* realloc for new children */
	server_zonestat_switch(nsd);
#endif
a1202 3
#ifdef USE_ZONE_STATS
	server_zonestat_realloc(nsd); /* realloc for next children */
#endif
a1549 4
#ifdef USE_ZONE_STATS
	unlink(nsd->zonestatfname[0]);
	unlink(nsd->zonestatfname[1]);
#endif
a1833 1
			/* No zone statup */
a1845 1
			/* No zone statup */
a1851 1
#ifdef BIND8_STATS
a1856 1
#endif
a1864 1
				ZTATUP(data->nsd, q->zone, nona);
a1866 8
#ifdef USE_ZONE_STATS
			if (data->socket->addr->ai_family == AF_INET) {
				ZTATUP(data->nsd, q->zone, qudp);
			} else if (data->socket->addr->ai_family == AF_INET6) {
				ZTATUP(data->nsd, q->zone, qudp6);
			}
#endif

d1875 1
a1875 2
			ZTATUP2(data->nsd, q->zone, rcode, RCODE(q->packet));
			if (TC(q->packet)) {
a1876 2
				ZTATUP(data->nsd, q->zone, truncated);
			}
a1882 1
			ZTATUP(data->nsd, q->zone, dropped);
a1952 1
			/* No zone statup */
a1962 1
			/* No zone statup */
a1986 1
				/* No zone statup */
a2005 8
				ZTATUP(data->nsd, q->zone, nona);
			}

#ifdef USE_ZONE_STATS
			if (data->socket->addr->ai_family == AF_INET) {
				ZTATUP(data->nsd, q->zone, qudp);
			} else if (data->socket->addr->ai_family == AF_INET6) {
				ZTATUP(data->nsd, q->zone, qudp6);
a2006 1
#endif
a2024 1
				ZTATUP(data->nsd, q->zone, txerr);
d2031 1
a2031 2
				ZTATUP2(data->nsd, q->zone, rcode, RCODE(q->packet));
				if (TC(q->packet)) {
a2032 2
					ZTATUP(data->nsd, q->zone, truncated);
				}
a2036 1
			ZTATUP(data->nsd, q->zone, dropped);
a2204 1
#ifdef BIND8_STATS
d2206 1
a2206 1
	STATUP(data->nsd, ctcp);
a2213 1
#endif /* BIND8_STATS */
a2224 1
		ZTATUP(data->nsd, data->query->zone, dropped);
a2228 1
#ifdef BIND8_STATS
a2232 1
		ZTATUP(data->nsd, data->query->zone, nona);
a2233 13
#endif /* BIND8_STATS */

#ifdef USE_ZONE_STATS
#ifndef INET6
	ZTATUP(data->nsd, data->query->zone, ctcp);
#else
	if (data->query->addr.ss_family == AF_INET) {
		ZTATUP(data->nsd, data->query->zone, ctcp);
	} else if (data->query->addr.ss_family == AF_INET6) {
		ZTATUP(data->nsd, data->query->zone, ctcp6);
	}
#endif
#endif /* USE_ZONE_STATS */
d2526 3
a2528 1
	if(event_base_set(data->event.ev_base, &tcp_data->event) != 0) {
a2529 10
		close(s);
		region_destroy(tcp_region);
		return;
	}
	if(event_add(&tcp_data->event, &timeout) != 0) {
		log_msg(LOG_ERR, "cannot add tcp to event base");
		close(s);
		region_destroy(tcp_region);
		return;
	}
@


1.1.1.16
log
@update to NSD 4.1.3, ok florian@@, also tested by brad
@
text
@d32 1
d369 2
a370 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.zstat.0",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
d372 2
a373 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.zstat.1",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
d539 2
a540 2
		compressed_dname_offsets = (uint16_t *) xmallocarray(
			needed, sizeof(uint16_t));
d960 2
a961 2
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.task.0",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
d963 1
a963 6
	if(!nsd->task[0]) {
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
		xfrd_del_tempdir(nsd);
d965 2
a966 3
	}
	snprintf(tmpfile, sizeof(tmpfile), "%snsd-xfr-%d/nsd.%u.task.1",
		nsd->options->xfrdir, (int)getpid(), (unsigned)getpid());
a969 5
#ifdef USE_ZONE_STATS
		unlink(nsd->zonestatfname[0]);
		unlink(nsd->zonestatfname[1]);
#endif
		xfrd_del_tempdir(nsd);
d1933 2
a1934 3
	tcp_accept_handlers = (struct tcp_accept_handler_data*)
		region_alloc_array(server_region,
		nsd->ifs, sizeof(*tcp_accept_handlers));
@


1.1.1.17
log
@update to NSD 4.1.6, ok millert@@ florian@@
@
text
@a32 1
#include <poll.h>
a306 1
				nsd->this_child->child_num = i;
d549 6
a554 3
/* create and bind sockets.  */
static int
server_init_ifs(struct nsd *nsd, size_t from, size_t to, int* reuseport_works)
a555 1
	struct addrinfo* addr;
d557 1
a557 1
#if defined(SO_REUSEPORT) || defined(SO_REUSEADDR) || (defined(INET6) && (defined(IPV6_V6ONLY) || defined(IPV6_USE_MIN_MTU) || defined(IPV6_MTU) || defined(IP_TRANSPARENT)))
d564 2
a565 4
	for (i = from; i < to; i++) {
		/* for reuseports copy socket specs of first entries */
		addr = nsd->udp[i%nsd->ifs].addr;
		if (!addr) {
d569 1
a569 2
		nsd->udp[i].fam = (int)addr->ai_family;
		if ((nsd->udp[i].s = socket(addr->ai_family, addr->ai_socktype, 0)) == -1) {
d571 1
a571 1
			if (addr->ai_family == AF_INET6 &&
a580 16
#ifdef SO_REUSEPORT
		if(nsd->reuseport && *reuseport_works &&
			setsockopt(nsd->udp[i].s, SOL_SOCKET, SO_REUSEPORT,
			(void*)&on, (socklen_t)sizeof(on)) < 0) {
			if(verbosity >= 3
#ifdef ENOPROTOOPT
				|| errno != ENOPROTOOPT
#endif
				)
			    log_msg(LOG_ERR, "setsockopt(..., SO_REUSEPORT, "
				"...) failed: %s", strerror(errno));
			*reuseport_works = 0;
		}
#else
		(void)reuseport_works;
#endif /* SO_REUSEPORT */
d636 1
a636 1
		if (addr->ai_family == AF_INET6) {
d682 1
a682 1
		if (addr->ai_family == AF_INET) {
d721 1
a721 1
		if (bind(nsd->udp[i].s, (struct sockaddr *) addr->ai_addr, addr->ai_addrlen) != 0) {
d730 2
a731 4
	for (i = from; i < to; i++) {
		/* for reuseports copy socket specs of first entries */
		addr = nsd->tcp[i%nsd->ifs].addr;
		if (!addr) {
d735 1
a735 2
		nsd->tcp[i].fam = (int)addr->ai_family;
		if ((nsd->tcp[i].s = socket(addr->ai_family, addr->ai_socktype, 0)) == -1) {
d737 1
a737 1
			if (addr->ai_family == AF_INET6 &&
a746 14
#ifdef SO_REUSEPORT
		if(nsd->reuseport && *reuseport_works &&
			setsockopt(nsd->tcp[i].s, SOL_SOCKET, SO_REUSEPORT,
			(void*)&on, (socklen_t)sizeof(on)) < 0) {
			if(verbosity >= 3
#ifdef ENOPROTOOPT
				|| errno != ENOPROTOOPT
#endif
				)
			    log_msg(LOG_ERR, "setsockopt(..., SO_REUSEPORT, "
				"...) failed: %s", strerror(errno));
			*reuseport_works = 0;
		}
#endif /* SO_REUSEPORT */
d754 1
a754 1
		if (addr->ai_family == AF_INET6) {
d805 1
a805 1
		if (bind(nsd->tcp[i].s, (struct sockaddr *) addr->ai_addr, addr->ai_addrlen) != 0) {
a820 37
 * Initialize the server, reuseport, create and bind the sockets.
 */
int
server_init(struct nsd *nsd)
{
	int reuseport_successful = 1; /* see if reuseport works in OS */
	if(nsd->reuseport) {
		/* increase the size of the udp and tcp interface arrays,
		 * there are going to be separate interface file descriptors
		 * for every server instance */
		nsd->udp = xrealloc(nsd->udp, (nsd->ifs*nsd->reuseport)*
			sizeof(*nsd->udp));
		nsd->tcp = xrealloc(nsd->tcp, (nsd->ifs*nsd->reuseport)*
			sizeof(*nsd->tcp));
		memset(&nsd->udp[nsd->ifs], 0, sizeof(*nsd->udp)*
			(nsd->ifs*(nsd->reuseport-1)));
		memset(&nsd->tcp[nsd->ifs], 0, sizeof(*nsd->tcp)*
			(nsd->ifs*(nsd->reuseport-1)));
	}

	/* open the server interface ports */
	if(server_init_ifs(nsd, 0, nsd->ifs, &reuseport_successful) == -1)
		return -1;

	/* continue to open the remaining reuseport ports */
	if(nsd->reuseport && reuseport_successful) {
		if(server_init_ifs(nsd, nsd->ifs, nsd->ifs*nsd->reuseport,
			&reuseport_successful) == -1)
			return -1;
		nsd->ifs *= nsd->reuseport;
	} else {
		nsd->reuseport = 0;
	}
	return 0;
}

/*
d905 1
a905 2
			if(sockets[i].addr)
				freeaddrinfo(sockets[i].addr);
d1171 4
a1174 5
	struct pollfd fd;
	memset(&fd, 0, sizeof(fd));
	fd.fd = s;
	fd.events = POLLIN;
	
d1177 4
a1180 1
		ret = poll(&fd, 1, (timeout==-1)?-1:timeout*1000);
d1853 1
a1853 1
	size_t i, from, numifs;
d1865 1
a1865 1
	rrl_init(nsd->this_child->child_num);
a1895 12
	if(nsd->reuseport) {
		numifs = nsd->ifs / nsd->reuseport;
		from = numifs * nsd->this_child->child_num;
		if(from+numifs > nsd->ifs) { /* should not happen */
			from = 0;
			numifs = nsd->ifs;
		}
	} else {
		from = 0;
		numifs = nsd->ifs;
	}

d1915 1
a1915 1
		for (i = from; i < from+numifs; ++i) {
d1942 1
a1942 1
	tcp_accept_handler_count = numifs;
d1945 1
a1945 1
		numifs, sizeof(*tcp_accept_handlers));
d1947 2
a1948 2
		for (i = from; i < numifs; ++i) {
			struct event *handler = &tcp_accept_handlers[i-from].event;
d1950 1
a1950 1
				&tcp_accept_handlers[i-from];
d2063 1
a2063 1
		if (data->socket->fam == AF_INET) {
d2065 1
a2065 1
		} else if (data->socket->fam == AF_INET6) {
d2081 1
a2081 1
			if (data->socket->fam == AF_INET) {
d2083 1
a2083 1
			} else if (data->socket->fam == AF_INET6) {
d2221 1
a2221 1
		if (data->socket->fam == AF_INET) {
d2223 1
a2223 1
		} else if (data->socket->fam == AF_INET6) {
d2238 1
a2238 1
			if (data->socket->fam == AF_INET) {
d2240 1
a2240 1
			} else if (data->socket->fam == AF_INET6) {
@


