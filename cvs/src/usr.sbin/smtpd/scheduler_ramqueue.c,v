head	1.43;
access;
symbols
	OPENBSD_6_1:1.43.0.4
	OPENBSD_6_1_BASE:1.43
	OPENBSD_6_0:1.42.0.6
	OPENBSD_6_0_BASE:1.42
	OPENBSD_5_9:1.42.0.2
	OPENBSD_5_9_BASE:1.42
	OPENBSD_5_8:1.41.0.6
	OPENBSD_5_8_BASE:1.41
	OPENBSD_5_7:1.41.0.2
	OPENBSD_5_7_BASE:1.41
	OPENBSD_5_6:1.40.0.4
	OPENBSD_5_6_BASE:1.40
	OPENBSD_5_5:1.37.0.2
	OPENBSD_5_5_BASE:1.37
	OPENBSD_5_4:1.30.0.2
	OPENBSD_5_4_BASE:1.30
	OPENBSD_5_3:1.27.0.2
	OPENBSD_5_3_BASE:1.27
	OPENBSD_5_2:1.11.0.2
	OPENBSD_5_2_BASE:1.11
	OPENBSD_5_1_BASE:1.4
	OPENBSD_5_1:1.4.0.2;
locks; strict;
comment	@ * @;


1.43
date	2017.01.09.09.53.23;	author reyk;	state Exp;
branches;
next	1.42;
commitid	jM4eOMW1AJwdfKrr;

1.42
date	2015.10.29.10.25.36;	author sunil;	state Exp;
branches;
next	1.41;
commitid	TXZugvtal96cI6ec;

1.41
date	2015.01.20.17.37.54;	author deraadt;	state Exp;
branches;
next	1.40;
commitid	ZBTFreARDSMmzOIV;

1.40
date	2014.07.10.14.45.02;	author eric;	state Exp;
branches;
next	1.39;
commitid	dolePHRcgnf1dDVL;

1.39
date	2014.07.08.07.59.31;	author sobrado;	state Exp;
branches;
next	1.38;
commitid	QejPnWBk7nSpcYUN;

1.38
date	2014.04.19.13.51.24;	author gilles;	state Exp;
branches;
next	1.37;

1.37
date	2014.02.04.14.56.03;	author eric;	state Exp;
branches;
next	1.36;

1.36
date	2013.12.26.17.25.32;	author eric;	state Exp;
branches;
next	1.35;

1.35
date	2013.12.05.09.26.47;	author eric;	state Exp;
branches;
next	1.34;

1.34
date	2013.12.03.10.38.40;	author eric;	state Exp;
branches;
next	1.33;

1.33
date	2013.11.20.09.22.42;	author eric;	state Exp;
branches;
next	1.32;

1.32
date	2013.10.27.17.47.53;	author eric;	state Exp;
branches;
next	1.31;

1.31
date	2013.10.25.19.18.43;	author eric;	state Exp;
branches;
next	1.30;

1.30
date	2013.07.19.21.34.31;	author eric;	state Exp;
branches;
next	1.29;

1.29
date	2013.07.19.15.14.23;	author eric;	state Exp;
branches;
next	1.28;

1.28
date	2013.05.24.17.03.14;	author eric;	state Exp;
branches;
next	1.27;

1.27
date	2013.02.10.15.01.16;	author eric;	state Exp;
branches;
next	1.26;

1.26
date	2013.01.26.09.37.23;	author gilles;	state Exp;
branches;
next	1.25;

1.25
date	2012.11.20.09.47.45;	author eric;	state Exp;
branches;
next	1.24;

1.24
date	2012.11.12.14.58.53;	author eric;	state Exp;
branches;
next	1.23;

1.23
date	2012.11.02.14.46.43;	author eric;	state Exp;
branches;
next	1.22;

1.22
date	2012.09.27.19.50.07;	author eric;	state Exp;
branches;
next	1.21;

1.21
date	2012.09.11.08.37.52;	author eric;	state Exp;
branches;
next	1.20;

1.20
date	2012.08.25.15.47.47;	author eric;	state Exp;
branches;
next	1.19;

1.19
date	2012.08.25.10.23.12;	author gilles;	state Exp;
branches;
next	1.18;

1.18
date	2012.08.24.12.29.50;	author eric;	state Exp;
branches;
next	1.17;

1.17
date	2012.08.19.15.06.36;	author chl;	state Exp;
branches;
next	1.16;

1.16
date	2012.08.19.14.46.25;	author chl;	state Exp;
branches;
next	1.15;

1.15
date	2012.08.19.10.33.35;	author eric;	state Exp;
branches;
next	1.14;

1.14
date	2012.08.18.18.18.23;	author gilles;	state Exp;
branches;
next	1.13;

1.13
date	2012.08.11.19.19.19;	author chl;	state Exp;
branches;
next	1.12;

1.12
date	2012.08.08.08.50.42;	author eric;	state Exp;
branches;
next	1.11;

1.11
date	2012.07.10.11.13.40;	author gilles;	state Exp;
branches;
next	1.10;

1.10
date	2012.06.20.20.45.23;	author eric;	state Exp;
branches;
next	1.9;

1.9
date	2012.06.17.15.17.08;	author gilles;	state Exp;
branches;
next	1.8;

1.8
date	2012.05.25.13.51.42;	author chl;	state Exp;
branches;
next	1.7;

1.7
date	2012.04.15.12.12.35;	author chl;	state Exp;
branches;
next	1.6;

1.6
date	2012.03.13.23.07.58;	author gilles;	state Exp;
branches;
next	1.5;

1.5
date	2012.03.07.22.54.49;	author gilles;	state Exp;
branches;
next	1.4;

1.4
date	2012.01.31.21.05.26;	author gilles;	state Exp;
branches;
next	1.3;

1.3
date	2012.01.30.10.02.55;	author chl;	state Exp;
branches;
next	1.2;

1.2
date	2012.01.28.16.50.02;	author gilles;	state Exp;
branches;
next	1.1;

1.1
date	2012.01.28.11.33.07;	author gilles;	state Exp;
branches;
next	;


desc
@@


1.43
log
@smtpd joins the 7 other daemons that share the same log.c file.

The only major difference was the "log_trace" concept that is only
used by smtpd - move it from log.c into util.c and make it a local
concept.  This also needed to rename the global "verbose" variable to
"tracing" in a few places.

OK krw@@ gilles@@ eric@@
@
text
@/*	$OpenBSD: scheduler_ramqueue.c,v 1.42 2015/10/29 10:25:36 sunil Exp $	*/

/*
 * Copyright (c) 2012 Gilles Chehade <gilles@@poolp.org>
 * Copyright (c) 2012 Eric Faurot <eric@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/types.h>
#include <sys/queue.h>
#include <sys/tree.h>
#include <sys/socket.h>

#include <ctype.h>
#include <err.h>
#include <event.h>
#include <fcntl.h>
#include <imsg.h>
#include <inttypes.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <limits.h>
#include <time.h>

#include "smtpd.h"
#include "log.h"

TAILQ_HEAD(evplist, rq_envelope);

struct rq_message {
	uint32_t		 msgid;
	struct tree		 envelopes;
};

struct rq_envelope {
	TAILQ_ENTRY(rq_envelope) entry;
	SPLAY_ENTRY(rq_envelope) t_entry;

	uint64_t		 evpid;
	uint64_t		 holdq;
	enum delivery_type	 type;

#define	RQ_EVPSTATE_PENDING	 0
#define	RQ_EVPSTATE_SCHEDULED	 1
#define	RQ_EVPSTATE_INFLIGHT	 2
#define	RQ_EVPSTATE_HELD	 3
	uint8_t			 state;

#define	RQ_ENVELOPE_EXPIRED	 0x01
#define	RQ_ENVELOPE_REMOVED	 0x02
#define	RQ_ENVELOPE_SUSPEND	 0x04
#define	RQ_ENVELOPE_UPDATE	 0x08
#define	RQ_ENVELOPE_OVERFLOW	 0x10
	uint8_t			 flags;

	time_t			 ctime;
	time_t			 sched;
	time_t			 expire;

	struct rq_message	*message;

	time_t			 t_inflight;
	time_t			 t_scheduled;
};

struct rq_holdq {
	struct evplist		 q;
	size_t			 count;
};

struct rq_queue {
	size_t			 evpcount;
	struct tree		 messages;
	SPLAY_HEAD(prioqtree, rq_envelope)	q_priotree;

	struct evplist		 q_pending;
	struct evplist		 q_inflight;

	struct evplist		 q_mta;
	struct evplist		 q_mda;
	struct evplist		 q_bounce;
	struct evplist		 q_update;
	struct evplist		 q_expired;
	struct evplist		 q_removed;
};

static int rq_envelope_cmp(struct rq_envelope *, struct rq_envelope *);

SPLAY_PROTOTYPE(prioqtree, rq_envelope, t_entry, rq_envelope_cmp);
static int scheduler_ram_init(const char *);
static int scheduler_ram_insert(struct scheduler_info *);
static size_t scheduler_ram_commit(uint32_t);
static size_t scheduler_ram_rollback(uint32_t);
static int scheduler_ram_update(struct scheduler_info *);
static int scheduler_ram_delete(uint64_t);
static int scheduler_ram_hold(uint64_t, uint64_t);
static int scheduler_ram_release(int, uint64_t, int);
static int scheduler_ram_batch(int, int *, size_t *, uint64_t *, int *);
static size_t scheduler_ram_messages(uint32_t, uint32_t *, size_t);
static size_t scheduler_ram_envelopes(uint64_t, struct evpstate *, size_t);
static int scheduler_ram_schedule(uint64_t);
static int scheduler_ram_remove(uint64_t);
static int scheduler_ram_suspend(uint64_t);
static int scheduler_ram_resume(uint64_t);
static int scheduler_ram_query(uint64_t);

static void sorted_insert(struct rq_queue *, struct rq_envelope *);

static void rq_queue_init(struct rq_queue *);
static void rq_queue_merge(struct rq_queue *, struct rq_queue *);
static void rq_queue_dump(struct rq_queue *, const char *);
static void rq_queue_schedule(struct rq_queue *rq);
static struct evplist *rq_envelope_list(struct rq_queue *, struct rq_envelope *);
static void rq_envelope_schedule(struct rq_queue *, struct rq_envelope *);
static int rq_envelope_remove(struct rq_queue *, struct rq_envelope *);
static int rq_envelope_suspend(struct rq_queue *, struct rq_envelope *);
static int rq_envelope_resume(struct rq_queue *, struct rq_envelope *);
static void rq_envelope_delete(struct rq_queue *, struct rq_envelope *);
static const char *rq_envelope_to_text(struct rq_envelope *);

struct scheduler_backend scheduler_backend_ramqueue = {
	scheduler_ram_init,

	scheduler_ram_insert,
	scheduler_ram_commit,
	scheduler_ram_rollback,

	scheduler_ram_update,
	scheduler_ram_delete,
	scheduler_ram_hold,
	scheduler_ram_release,

	scheduler_ram_batch,

	scheduler_ram_messages,
	scheduler_ram_envelopes,
	scheduler_ram_schedule,
	scheduler_ram_remove,
	scheduler_ram_suspend,
	scheduler_ram_resume,
	scheduler_ram_query,
};

static struct rq_queue	ramqueue;
static struct tree	updates;
static struct tree	holdqs[3]; /* delivery type */

static time_t		currtime;

#define BACKOFF_TRANSFER	400
#define BACKOFF_DELIVERY	10
#define BACKOFF_OVERFLOW	3

static time_t
scheduler_backoff(time_t t0, time_t base, uint32_t step)
{
	return (t0 + base * step * step);
}

static time_t
scheduler_next(time_t t0, time_t base, uint32_t step)
{
	time_t t;

	/* XXX be more efficient */
	while ((t = scheduler_backoff(t0, base, step)) <= currtime)
		step++;

	return (t);
}

static int
scheduler_ram_init(const char *arg)
{
	rq_queue_init(&ramqueue);
	tree_init(&updates);
	tree_init(&holdqs[D_MDA]);
	tree_init(&holdqs[D_MTA]);
	tree_init(&holdqs[D_BOUNCE]);

	return (1);
}

static int
scheduler_ram_insert(struct scheduler_info *si)
{
	struct rq_queue		*update;
	struct rq_message	*message;
	struct rq_envelope	*envelope;
	uint32_t		 msgid;

	currtime = time(NULL);

	msgid = evpid_to_msgid(si->evpid);

	/* find/prepare a ramqueue update */
	if ((update = tree_get(&updates, msgid)) == NULL) {
		update = xcalloc(1, sizeof *update, "scheduler_insert");
		stat_increment("scheduler.ramqueue.update", 1);
		rq_queue_init(update);
		tree_xset(&updates, msgid, update);
	}

	/* find/prepare the msgtree message in ramqueue update */
	if ((message = tree_get(&update->messages, msgid)) == NULL) {
		message = xcalloc(1, sizeof *message, "scheduler_insert");
		message->msgid = msgid;
		tree_init(&message->envelopes);
		tree_xset(&update->messages, msgid, message);
		stat_increment("scheduler.ramqueue.message", 1);
	}

	/* create envelope in ramqueue message */
	envelope = xcalloc(1, sizeof *envelope, "scheduler_insert");
	envelope->evpid = si->evpid;
	envelope->type = si->type;
	envelope->message = message;
	envelope->ctime = si->creation;
	envelope->expire = si->creation + si->expire;
	envelope->sched = scheduler_backoff(si->creation,
	    (si->type == D_MTA) ? BACKOFF_TRANSFER : BACKOFF_DELIVERY, si->retry);
	tree_xset(&message->envelopes, envelope->evpid, envelope);

	update->evpcount++;
	stat_increment("scheduler.ramqueue.envelope", 1);

	envelope->state = RQ_EVPSTATE_PENDING;
	TAILQ_INSERT_TAIL(&update->q_pending, envelope, entry);

	si->nexttry = envelope->sched;

	return (1);
}

static size_t
scheduler_ram_commit(uint32_t msgid)
{
	struct rq_queue	*update;
	size_t		 r;

	currtime = time(NULL);

	update = tree_xpop(&updates, msgid);
	r = update->evpcount;

	if (tracing & TRACE_SCHEDULER)
		rq_queue_dump(update, "update to commit");

	rq_queue_merge(&ramqueue, update);

	if (tracing & TRACE_SCHEDULER)
		rq_queue_dump(&ramqueue, "resulting queue");

	rq_queue_schedule(&ramqueue);

	free(update);
	stat_decrement("scheduler.ramqueue.update", 1);

	return (r);
}

static size_t
scheduler_ram_rollback(uint32_t msgid)
{
	struct rq_queue		*update;
	struct rq_envelope	*evp;
	size_t			 r;

	currtime = time(NULL);

	if ((update = tree_pop(&updates, msgid)) == NULL)
		return (0);
	r = update->evpcount;

	while ((evp = TAILQ_FIRST(&update->q_pending))) {
		TAILQ_REMOVE(&update->q_pending, evp, entry);
		rq_envelope_delete(update, evp);
	}

	free(update);
	stat_decrement("scheduler.ramqueue.update", 1);

	return (r);
}

static int
scheduler_ram_update(struct scheduler_info *si)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;

	currtime = time(NULL);

	msgid = evpid_to_msgid(si->evpid);
	msg = tree_xget(&ramqueue.messages, msgid);
	evp = tree_xget(&msg->envelopes, si->evpid);

	/* it *must* be in-flight */
	if (evp->state != RQ_EVPSTATE_INFLIGHT)
		errx(1, "evp:%016" PRIx64 " not in-flight", si->evpid);

	TAILQ_REMOVE(&ramqueue.q_inflight, evp, entry);

	/*
	 * If the envelope was removed while inflight,  schedule it for
	 * removal immediately.
	 */
	if (evp->flags & RQ_ENVELOPE_REMOVED) {
		TAILQ_INSERT_TAIL(&ramqueue.q_removed, evp, entry);
		evp->state = RQ_EVPSTATE_SCHEDULED;
		evp->t_scheduled = currtime;
		return (1);
	}

	evp->sched = scheduler_next(evp->ctime,
	    (si->type == D_MTA) ? BACKOFF_TRANSFER : BACKOFF_DELIVERY, si->retry);

	evp->state = RQ_EVPSTATE_PENDING;
	if (!(evp->flags & RQ_ENVELOPE_SUSPEND))
		sorted_insert(&ramqueue, evp);

	si->nexttry = evp->sched;

	return (1);
}

static int
scheduler_ram_delete(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;

	currtime = time(NULL);

	msgid = evpid_to_msgid(evpid);
	msg = tree_xget(&ramqueue.messages, msgid);
	evp = tree_xget(&msg->envelopes, evpid);

	/* it *must* be in-flight */
	if (evp->state != RQ_EVPSTATE_INFLIGHT)
		errx(1, "evp:%016" PRIx64 " not in-flight", evpid);

	TAILQ_REMOVE(&ramqueue.q_inflight, evp, entry);

	rq_envelope_delete(&ramqueue, evp);

	return (1);
}

#define HOLDQ_MAXSIZE	1000

static int
scheduler_ram_hold(uint64_t evpid, uint64_t holdq)
{
	struct rq_holdq		*hq;
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;

	currtime = time(NULL);

	msgid = evpid_to_msgid(evpid);
	msg = tree_xget(&ramqueue.messages, msgid);
	evp = tree_xget(&msg->envelopes, evpid);

	/* it *must* be in-flight */
	if (evp->state != RQ_EVPSTATE_INFLIGHT)
		errx(1, "evp:%016" PRIx64 " not in-flight", evpid);

	TAILQ_REMOVE(&ramqueue.q_inflight, evp, entry);

	/* If the envelope is suspended, just mark it as pending */
	if (evp->flags & RQ_ENVELOPE_SUSPEND) {
		evp->state = RQ_EVPSTATE_PENDING;
		return (0);
	}

	hq = tree_get(&holdqs[evp->type], holdq);
	if (hq == NULL) {
		hq = xcalloc(1, sizeof(*hq), "scheduler_hold");
		TAILQ_INIT(&hq->q);
		tree_xset(&holdqs[evp->type], holdq, hq);
		stat_increment("scheduler.ramqueue.holdq", 1);
	}

	/* If the holdq is full, just "tempfail" the envelope */
	if (hq->count >= HOLDQ_MAXSIZE) {
		evp->state = RQ_EVPSTATE_PENDING;
		evp->flags |= RQ_ENVELOPE_UPDATE;
		evp->flags |= RQ_ENVELOPE_OVERFLOW;
		sorted_insert(&ramqueue, evp);
		stat_increment("scheduler.ramqueue.hold-overflow", 1);
		return (0);
	}

	evp->state = RQ_EVPSTATE_HELD;
	evp->holdq = holdq;
	/* This is an optimization: upon release, the envelopes will be
	 * inserted in the pending queue from the first element to the last.
	 * Since elements already in the queue were received first, they
	 * were scheduled first, so they will be reinserted before the
	 * current element.
	 */
	TAILQ_INSERT_HEAD(&hq->q, evp, entry);
	hq->count += 1;
	stat_increment("scheduler.ramqueue.hold", 1);

	return (1);
}

static int
scheduler_ram_release(int type, uint64_t holdq, int n)
{
	struct rq_holdq		*hq;
	struct rq_envelope	*evp;
	int			 i, update;

	currtime = time(NULL);

	hq = tree_get(&holdqs[type], holdq);
	if (hq == NULL)
		return (0);

	if (n == -1) {
		n = 0;
		update = 1;
	}
	else
		update = 0;

	for (i = 0; n == 0 || i < n; i++) {
		evp = TAILQ_FIRST(&hq->q);
		if (evp == NULL)
			break;

		TAILQ_REMOVE(&hq->q, evp, entry);
		hq->count -= 1;
		evp->holdq = 0;

		/* When released, all envelopes are put in the pending queue
		 * and will be rescheduled immediately.  As an optimization,
		 * we could just schedule them directly.
		 */
		evp->state = RQ_EVPSTATE_PENDING;
		if (update)
			evp->flags |= RQ_ENVELOPE_UPDATE;
		sorted_insert(&ramqueue, evp);
	}

	if (TAILQ_EMPTY(&hq->q)) {
		tree_xpop(&holdqs[type], holdq);
		free(hq);
		stat_decrement("scheduler.ramqueue.holdq", 1);
	}
	stat_decrement("scheduler.ramqueue.hold", i);

	return (i);
}

static int
scheduler_ram_batch(int mask, int *delay, size_t *count, uint64_t *evpids, int *types)
{
	struct rq_envelope	*evp;
	size_t			 i, n;
	time_t			 t;

	currtime = time(NULL);

	rq_queue_schedule(&ramqueue);
	if (tracing & TRACE_SCHEDULER)
		rq_queue_dump(&ramqueue, "scheduler_ram_batch()");

	i = 0;
	n = 0;

	for (;;) {

		if (mask & SCHED_REMOVE && (evp = TAILQ_FIRST(&ramqueue.q_removed))) {
			TAILQ_REMOVE(&ramqueue.q_removed, evp, entry);
			types[i] = SCHED_REMOVE;
			evpids[i] = evp->evpid;
			rq_envelope_delete(&ramqueue, evp);

			if (++i == *count)
				break;
		}

		if (mask & SCHED_EXPIRE && (evp = TAILQ_FIRST(&ramqueue.q_expired))) {
			TAILQ_REMOVE(&ramqueue.q_expired, evp, entry);
			types[i] = SCHED_EXPIRE;
			evpids[i] = evp->evpid;
			rq_envelope_delete(&ramqueue, evp);

			if (++i == *count)
				break;
		}

		if (mask & SCHED_UPDATE && (evp = TAILQ_FIRST(&ramqueue.q_update))) {
			TAILQ_REMOVE(&ramqueue.q_update, evp, entry);
			types[i] = SCHED_UPDATE;
			evpids[i] = evp->evpid;

			if (evp->flags & RQ_ENVELOPE_OVERFLOW)
				t = BACKOFF_OVERFLOW;
			else if (evp->type == D_MTA)
				t = BACKOFF_TRANSFER;
			else
				t = BACKOFF_DELIVERY;

			evp->sched = scheduler_next(evp->ctime, t, 0);
			evp->flags &= ~(RQ_ENVELOPE_UPDATE|RQ_ENVELOPE_OVERFLOW);
			evp->state = RQ_EVPSTATE_PENDING;
			if (!(evp->flags & RQ_ENVELOPE_SUSPEND))
				sorted_insert(&ramqueue, evp);

			if (++i == *count)
				break;
		}

		if (mask & SCHED_BOUNCE && (evp = TAILQ_FIRST(&ramqueue.q_bounce))) {
			TAILQ_REMOVE(&ramqueue.q_bounce, evp, entry);
			types[i] = SCHED_BOUNCE;
			evpids[i] = evp->evpid;

			TAILQ_INSERT_TAIL(&ramqueue.q_inflight, evp, entry);
			evp->state = RQ_EVPSTATE_INFLIGHT;
			evp->t_inflight = currtime;

			if (++i == *count)
				break;
		}

		if (mask & SCHED_MDA && (evp = TAILQ_FIRST(&ramqueue.q_mda))) {
			TAILQ_REMOVE(&ramqueue.q_mda, evp, entry);
			types[i] = SCHED_MDA;
			evpids[i] = evp->evpid;

			TAILQ_INSERT_TAIL(&ramqueue.q_inflight, evp, entry);
			evp->state = RQ_EVPSTATE_INFLIGHT;
			evp->t_inflight = currtime;

			if (++i == *count)
				break;
		}

		if (mask & SCHED_MTA && (evp = TAILQ_FIRST(&ramqueue.q_mta))) {
			TAILQ_REMOVE(&ramqueue.q_mta, evp, entry);
			types[i] = SCHED_MTA;
			evpids[i] = evp->evpid;

			TAILQ_INSERT_TAIL(&ramqueue.q_inflight, evp, entry);
			evp->state = RQ_EVPSTATE_INFLIGHT;
			evp->t_inflight = currtime;

			if (++i == *count)
				break;
		}

		/* nothing seen this round */
		if (i == n)
			break;

		n = i;
	}

	if (i) {
		*count = i;
		return (1);
	}

	if ((evp = TAILQ_FIRST(&ramqueue.q_pending))) {
		if (evp->sched < evp->expire)
			t = evp->sched;
		else
			t = evp->expire;
		*delay = (t < currtime) ? 0 : (t - currtime);
	}
	else
		*delay = -1;

	return (0);
}

static size_t
scheduler_ram_messages(uint32_t from, uint32_t *dst, size_t size)
{
	uint64_t id;
	size_t	 n;
	void	*i;

	for (n = 0, i = NULL; n < size; n++) {
		if (tree_iterfrom(&ramqueue.messages, &i, from, &id, NULL) == 0)
			break;
		dst[n] = id;
	}

	return (n);
}

static size_t
scheduler_ram_envelopes(uint64_t from, struct evpstate *dst, size_t size)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	void			*i;
	size_t			 n;

	if ((msg = tree_get(&ramqueue.messages, evpid_to_msgid(from))) == NULL)
		return (0);

	for (n = 0, i = NULL; n < size; ) {

		if (tree_iterfrom(&msg->envelopes, &i, from, NULL,
		    (void**)&evp) == 0)
			break;

		if (evp->flags & (RQ_ENVELOPE_REMOVED | RQ_ENVELOPE_EXPIRED))
			continue;

		dst[n].evpid = evp->evpid;
		dst[n].flags = 0;
		dst[n].retry = 0;
		dst[n].time = 0;

		if (evp->state == RQ_EVPSTATE_PENDING) {
			dst[n].time = evp->sched;
			dst[n].flags = EF_PENDING;
		}
		else if (evp->state == RQ_EVPSTATE_SCHEDULED) {
			dst[n].time = evp->t_scheduled;
			dst[n].flags = EF_PENDING;
		}
		else if (evp->state == RQ_EVPSTATE_INFLIGHT) {
			dst[n].time = evp->t_inflight;
			dst[n].flags = EF_INFLIGHT;
		}
		else if (evp->state == RQ_EVPSTATE_HELD) {
			/* same as scheduled */
			dst[n].time = evp->t_scheduled;
			dst[n].flags = EF_PENDING;
			dst[n].flags |= EF_HOLD;
		}
		if (evp->flags & RQ_ENVELOPE_SUSPEND)
			dst[n].flags |= EF_SUSPEND;

		n++;
	}

	return (n);
}

static int
scheduler_ram_schedule(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;
	int			 r;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return (0);
		if (evp->state == RQ_EVPSTATE_INFLIGHT)
			return (0);
		rq_envelope_schedule(&ramqueue, evp);
		return (1);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		i = NULL;
		r = 0;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp))) {
			if (evp->state == RQ_EVPSTATE_INFLIGHT)
				continue;
			rq_envelope_schedule(&ramqueue, evp);
			r++;
		}
		return (r);
	}
}

static int
scheduler_ram_remove(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;
	int			 r;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return (0);
		if (rq_envelope_remove(&ramqueue, evp))
			return (1);
		return (0);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		i = NULL;
		r = 0;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			if (rq_envelope_remove(&ramqueue, evp))
				r++;
		return (r);
	}
}

static int
scheduler_ram_suspend(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;
	int			 r;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return (0);
		if (rq_envelope_suspend(&ramqueue, evp))
			return (1);
		return (0);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		i = NULL;
		r = 0;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			if (rq_envelope_suspend(&ramqueue, evp))
				r++;
		return (r);
	}
}

static int
scheduler_ram_resume(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;
	int			 r;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return (0);
		if (rq_envelope_resume(&ramqueue, evp))
			return (1);
		return (0);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return (0);
		i = NULL;
		r = 0;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			if (rq_envelope_resume(&ramqueue, evp))
				r++;
		return (r);
	}
}

static int
scheduler_ram_query(uint64_t evpid)
{
	uint32_t msgid;

	if (evpid > 0xffffffff)
		msgid = evpid_to_msgid(evpid);
	else
		msgid = evpid;

	if (tree_get(&ramqueue.messages, msgid) == NULL)
		return (0);

	return (1);
}

static void
sorted_insert(struct rq_queue *rq, struct rq_envelope *evp)
{
	struct rq_envelope	*evp2;

	SPLAY_INSERT(prioqtree, &rq->q_priotree, evp);
	evp2 = SPLAY_NEXT(prioqtree, &rq->q_priotree, evp);
	if (evp2)
		TAILQ_INSERT_BEFORE(evp2, evp, entry);
	else
		TAILQ_INSERT_TAIL(&rq->q_pending, evp, entry);
}

static void
rq_queue_init(struct rq_queue *rq)
{
	memset(rq, 0, sizeof *rq);
	tree_init(&rq->messages);
	TAILQ_INIT(&rq->q_pending);
	TAILQ_INIT(&rq->q_inflight);
	TAILQ_INIT(&rq->q_mta);
	TAILQ_INIT(&rq->q_mda);
	TAILQ_INIT(&rq->q_bounce);
	TAILQ_INIT(&rq->q_update);
	TAILQ_INIT(&rq->q_expired);
	TAILQ_INIT(&rq->q_removed);
	SPLAY_INIT(&rq->q_priotree);
}

static void
rq_queue_merge(struct rq_queue *rq, struct rq_queue *update)
{
	struct rq_message	*message, *tomessage;
	struct rq_envelope	*envelope;
	uint64_t		 id;
	void			*i;

	while (tree_poproot(&update->messages, &id, (void*)&message)) {
		if ((tomessage = tree_get(&rq->messages, id)) == NULL) {
			/* message does not exist. re-use structure */
			tree_xset(&rq->messages, id, message);
			continue;
		}
		/* need to re-link all envelopes before merging them */
		i = NULL;
		while ((tree_iter(&message->envelopes, &i, &id,
		    (void*)&envelope)))
			envelope->message = tomessage;
		tree_merge(&tomessage->envelopes, &message->envelopes);
		free(message);
		stat_decrement("scheduler.ramqueue.message", 1);
	}

	/* Sorted insert in the pending queue */
	while ((envelope = TAILQ_FIRST(&update->q_pending))) {
		TAILQ_REMOVE(&update->q_pending, envelope, entry);
		sorted_insert(rq, envelope);
	}

	rq->evpcount += update->evpcount;
}

#define SCHEDULEMAX	1024

static void
rq_queue_schedule(struct rq_queue *rq)
{
	struct rq_envelope	*evp;
	size_t			 n;

	n = 0;
	while ((evp = TAILQ_FIRST(&rq->q_pending))) {
		if (evp->sched > currtime && evp->expire > currtime)
			break;

		if (n == SCHEDULEMAX)
			break;

		if (evp->state != RQ_EVPSTATE_PENDING)
			errx(1, "evp:%016" PRIx64 " flags=0x%x", evp->evpid,
			    evp->flags);

		if (evp->expire <= currtime) {
			TAILQ_REMOVE(&rq->q_pending, evp, entry);
			SPLAY_REMOVE(prioqtree, &rq->q_priotree, evp);
			TAILQ_INSERT_TAIL(&rq->q_expired, evp, entry);
			evp->state = RQ_EVPSTATE_SCHEDULED;
			evp->flags |= RQ_ENVELOPE_EXPIRED;
			evp->t_scheduled = currtime;
			continue;
		}
		rq_envelope_schedule(rq, evp);
		n += 1;
	}
}

static struct evplist *
rq_envelope_list(struct rq_queue *rq, struct rq_envelope *evp)
{
	switch (evp->state) {
	case RQ_EVPSTATE_PENDING:
		return &rq->q_pending;

	case RQ_EVPSTATE_SCHEDULED:
		if (evp->flags & RQ_ENVELOPE_EXPIRED)
			return &rq->q_expired;
		if (evp->flags & RQ_ENVELOPE_REMOVED)
			return &rq->q_removed;
		if (evp->flags & RQ_ENVELOPE_UPDATE)
			return &rq->q_update;
		if (evp->type == D_MTA)
			return &rq->q_mta;
		if (evp->type == D_MDA)
			return &rq->q_mda;
		if (evp->type == D_BOUNCE)
			return &rq->q_bounce;
		errx(1, "%016" PRIx64 " bad evp type %d", evp->evpid, evp->type);

	case RQ_EVPSTATE_INFLIGHT:
		return &rq->q_inflight;

	case RQ_EVPSTATE_HELD:
		return (NULL);
	}

	errx(1, "%016" PRIx64 " bad state %d", evp->evpid, evp->state);
	return (NULL);
}

static void
rq_envelope_schedule(struct rq_queue *rq, struct rq_envelope *evp)
{
	struct rq_holdq	*hq;
	struct evplist	*q = NULL;

	switch (evp->type) {
	case D_MTA:
		q = &rq->q_mta;
		break;
	case D_MDA:
		q = &rq->q_mda;
		break;
	case D_BOUNCE:
		q = &rq->q_bounce;
		break;
	}

	if (evp->flags & RQ_ENVELOPE_UPDATE)
		q = &rq->q_update;

	if (evp->state == RQ_EVPSTATE_HELD) {
		hq = tree_xget(&holdqs[evp->type], evp->holdq);
		TAILQ_REMOVE(&hq->q, evp, entry);
		hq->count -= 1;
		if (TAILQ_EMPTY(&hq->q)) {
			tree_xpop(&holdqs[evp->type], evp->holdq);
			free(hq);
		}
		evp->holdq = 0;
		stat_decrement("scheduler.ramqueue.hold", 1);
	}
	else if (!(evp->flags & RQ_ENVELOPE_SUSPEND)) {
		TAILQ_REMOVE(&rq->q_pending, evp, entry);
		SPLAY_REMOVE(prioqtree, &rq->q_priotree, evp);
	}

	TAILQ_INSERT_TAIL(q, evp, entry);
	evp->state = RQ_EVPSTATE_SCHEDULED;
	evp->t_scheduled = currtime;
}

static int
rq_envelope_remove(struct rq_queue *rq, struct rq_envelope *evp)
{
	struct rq_holdq	*hq;
	struct evplist	*evl;

	if (evp->flags & (RQ_ENVELOPE_REMOVED | RQ_ENVELOPE_EXPIRED))
		return (0);
	/*
	 * If envelope is inflight, mark it envelope for removal.
	 */
	if (evp->state == RQ_EVPSTATE_INFLIGHT) {
		evp->flags |= RQ_ENVELOPE_REMOVED;
		return (1);
	}

	if (evp->state == RQ_EVPSTATE_HELD) {
		hq = tree_xget(&holdqs[evp->type], evp->holdq);
		TAILQ_REMOVE(&hq->q, evp, entry);
		hq->count -= 1;
		if (TAILQ_EMPTY(&hq->q)) {
			tree_xpop(&holdqs[evp->type], evp->holdq);
			free(hq);
		}
		evp->holdq = 0;
		stat_decrement("scheduler.ramqueue.hold", 1);
	}
	else if (!(evp->flags & RQ_ENVELOPE_SUSPEND)) {
		evl = rq_envelope_list(rq, evp);
		TAILQ_REMOVE(evl, evp, entry);
		if (evl == &rq->q_pending)
			SPLAY_REMOVE(prioqtree, &rq->q_priotree, evp);
	}

	TAILQ_INSERT_TAIL(&rq->q_removed, evp, entry);
	evp->state = RQ_EVPSTATE_SCHEDULED;
	evp->flags |= RQ_ENVELOPE_REMOVED;
	evp->t_scheduled = currtime;

	return (1);
}

static int
rq_envelope_suspend(struct rq_queue *rq, struct rq_envelope *evp)
{
	struct rq_holdq	*hq;
	struct evplist	*evl;

	if (evp->flags & RQ_ENVELOPE_SUSPEND)
		return (0);

	if (evp->state == RQ_EVPSTATE_HELD) {
		hq = tree_xget(&holdqs[evp->type], evp->holdq);
		TAILQ_REMOVE(&hq->q, evp, entry);
		hq->count -= 1;
		if (TAILQ_EMPTY(&hq->q)) {
			tree_xpop(&holdqs[evp->type], evp->holdq);
			free(hq);
		}
		evp->holdq = 0;
		evp->state = RQ_EVPSTATE_PENDING;
		stat_decrement("scheduler.ramqueue.hold", 1);
	}
	else if (evp->state != RQ_EVPSTATE_INFLIGHT) {
		evl = rq_envelope_list(rq, evp);
		TAILQ_REMOVE(evl, evp, entry);
		if (evl == &rq->q_pending)
			SPLAY_REMOVE(prioqtree, &rq->q_priotree, evp);
	}

	evp->flags |= RQ_ENVELOPE_SUSPEND;

	return (1);
}

static int
rq_envelope_resume(struct rq_queue *rq, struct rq_envelope *evp)
{
	struct evplist	*evl;

	if (!(evp->flags & RQ_ENVELOPE_SUSPEND))
		return (0);

	if (evp->state != RQ_EVPSTATE_INFLIGHT) {
		evl = rq_envelope_list(rq, evp);
		if (evl == &rq->q_pending)
			sorted_insert(rq, evp);
		else
			TAILQ_INSERT_TAIL(evl, evp, entry);
	}

	evp->flags &= ~RQ_ENVELOPE_SUSPEND;

	return (1);
}

static void
rq_envelope_delete(struct rq_queue *rq, struct rq_envelope *evp)
{
	tree_xpop(&evp->message->envelopes, evp->evpid);
	if (tree_empty(&evp->message->envelopes)) {
		tree_xpop(&rq->messages, evp->message->msgid);
		free(evp->message);
		stat_decrement("scheduler.ramqueue.message", 1);
	}

	free(evp);
	rq->evpcount--;
	stat_decrement("scheduler.ramqueue.envelope", 1);
}

static const char *
rq_envelope_to_text(struct rq_envelope *e)
{
	static char	buf[256];
	char		t[64];

	(void)snprintf(buf, sizeof buf, "evp:%016" PRIx64 " [", e->evpid);

	if (e->type == D_BOUNCE)
		(void)strlcat(buf, "bounce", sizeof buf);
	else if (e->type == D_MDA)
		(void)strlcat(buf, "mda", sizeof buf);
	else if (e->type == D_MTA)
		(void)strlcat(buf, "mta", sizeof buf);

	(void)snprintf(t, sizeof t, ",expire=%s",
	    duration_to_text(e->expire - currtime));
	(void)strlcat(buf, t, sizeof buf);


	switch (e->state) {
	case RQ_EVPSTATE_PENDING:
		(void)snprintf(t, sizeof t, ",pending=%s",
		    duration_to_text(e->sched - currtime));
		(void)strlcat(buf, t, sizeof buf);
		break;

	case RQ_EVPSTATE_SCHEDULED:
		(void)snprintf(t, sizeof t, ",scheduled=%s",
		    duration_to_text(currtime - e->t_scheduled));
		(void)strlcat(buf, t, sizeof buf);
		break;

	case RQ_EVPSTATE_INFLIGHT:
		(void)snprintf(t, sizeof t, ",inflight=%s",
		    duration_to_text(currtime - e->t_inflight));
		(void)strlcat(buf, t, sizeof buf);
		break;

	case RQ_EVPSTATE_HELD:
		(void)snprintf(t, sizeof t, ",held=%s",
		    duration_to_text(currtime - e->t_inflight));
		(void)strlcat(buf, t, sizeof buf);
		break;
	default:
		errx(1, "%016" PRIx64 " bad state %d", e->evpid, e->state);
	}

	if (e->flags & RQ_ENVELOPE_REMOVED)
		(void)strlcat(buf, ",removed", sizeof buf);
	if (e->flags & RQ_ENVELOPE_EXPIRED)
		(void)strlcat(buf, ",expired", sizeof buf);
	if (e->flags & RQ_ENVELOPE_SUSPEND)
		(void)strlcat(buf, ",suspended", sizeof buf);

	(void)strlcat(buf, "]", sizeof buf);

	return (buf);
}

static void
rq_queue_dump(struct rq_queue *rq, const char * name)
{
	struct rq_message	*message;
	struct rq_envelope	*envelope;
	void			*i, *j;
	uint64_t		 id;

	log_debug("debug: /--- ramqueue: %s", name);

	i = NULL;
	while ((tree_iter(&rq->messages, &i, &id, (void*)&message))) {
		log_debug("debug: | msg:%08" PRIx32, message->msgid);
		j = NULL;
		while ((tree_iter(&message->envelopes, &j, &id,
		    (void*)&envelope)))
			log_debug("debug: |   %s",
			    rq_envelope_to_text(envelope));
	}
	log_debug("debug: \\---");
}

static int
rq_envelope_cmp(struct rq_envelope *e1, struct rq_envelope *e2)
{
	time_t	ref1, ref2;

	ref1 = (e1->sched < e1->expire) ? e1->sched : e1->expire;
	ref2 = (e2->sched < e2->expire) ? e2->sched : e2->expire;
	if (ref1 != ref2)
		return (ref1 < ref2) ? -1 : 1;

	if (e1->evpid != e2->evpid)
		return (e1->evpid < e2->evpid) ? -1 : 1;

	return 0;
}

SPLAY_GENERATE(prioqtree, rq_envelope, t_entry, rq_envelope_cmp);
@


1.42
log
@Implement smtpctl discover <evpid|msgid>.

discover subcommand schedules envelopes manually moved to the queue.
It triggers a queue walk searching for envelopes with the given id,
schedules them and informs the user number of envelopes scheduled.
Admins no longer would need to restart the daemon to discover
manually moved messages.

Ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.41 2015/01/20 17:37:54 deraadt Exp $	*/
d258 1
a258 1
	if (verbose & TRACE_SCHEDULER)
d263 1
a263 1
	if (verbose & TRACE_SCHEDULER)
d484 1
a484 1
	if (verbose & TRACE_SCHEDULER)
@


1.41
log
@use <limits.h> comprehensively.  For now try to push <> includes to
each .c file, and out of the .h files.  To avoid overinclude.
ok gilles, in principle.  If this has been done right, -portable should
become easier to maintain.
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.40 2014/07/10 14:45:02 eric Exp $	*/
d117 1
d153 1
d804 16
@


1.40
log
@Improve the scheduler, better and simpler.

- Get rid of the scheduler_batch structure. The scheduler can now return
  envelopes of different types in a single run, interlacing them to avoid
  batch effects.

- Ask for an acknowledgement from the queue when removing or expiring
  an envelope to benefit from the inflight envelope limitation mechanism.
  This ensures that the scheduler always keeps sending envelopes at a rate
  that the queue can sustain in all cases.

- Limit the number of envelopes in a holdq.  When a holdq is full,
  new envelopes are put back in the pending queue instead, with a
  shorter retry time.

- Plumbing for proc-ified schedulers.

imsg version bump. smtpctl stop before updating.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.38 2014/04/19 13:51:24 gilles Exp $	*/
d34 1
@


1.39
log
@fix typos.

ok jmc@@
@
text
@d64 1
d79 1
d101 1
a101 1
static int scheduler_ram_init(void);
d109 1
a109 1
static int scheduler_ram_batch(int, struct scheduler_batch *);
d159 22
d182 1
a182 1
scheduler_ram_init(void)
a195 1
	uint32_t		 msgid;
d199 1
d229 2
a230 1
	envelope->sched = scheduler_compute_schedule(si);
d325 2
a326 2
	while ((evp->sched = scheduler_compute_schedule(si)) <= currtime)
		si->retry += 1;
d361 2
d394 11
d416 1
d448 1
d464 1
d472 1
a472 1
scheduler_ram_batch(int typemask, struct scheduler_batch *ret)
a473 1
	struct evplist		*q;
d475 2
a476 2
	size_t			 n;
	int			 retry;
d484 2
a485 38
	if (typemask & SCHED_REMOVE && TAILQ_FIRST(&ramqueue.q_removed)) {
		q = &ramqueue.q_removed;
		ret->type = SCHED_REMOVE;
	}
	else if (typemask & SCHED_EXPIRE && TAILQ_FIRST(&ramqueue.q_expired)) {
		q = &ramqueue.q_expired;
		ret->type = SCHED_EXPIRE;
	}
	else if (typemask & SCHED_UPDATE && TAILQ_FIRST(&ramqueue.q_update)) {
		q = &ramqueue.q_update;
		ret->type = SCHED_UPDATE;
	}
	else if (typemask & SCHED_BOUNCE && TAILQ_FIRST(&ramqueue.q_bounce)) {
		q = &ramqueue.q_bounce;
		ret->type = SCHED_BOUNCE;
	}
	else if (typemask & SCHED_MDA && TAILQ_FIRST(&ramqueue.q_mda)) {
		q = &ramqueue.q_mda;
		ret->type = SCHED_MDA;
	}
	else if (typemask & SCHED_MTA && TAILQ_FIRST(&ramqueue.q_mta)) {
		q = &ramqueue.q_mta;
		ret->type = SCHED_MTA;
	}
	else if ((evp = TAILQ_FIRST(&ramqueue.q_pending))) {
		ret->type = SCHED_DELAY;
		ret->evpcount = 0;
		if (evp->sched < evp->expire)
			ret->delay = evp->sched - currtime;
		else
			ret->delay = evp->expire - currtime;
		goto done;
	}
	else {
		ret->type = SCHED_NONE;
		ret->evpcount = 0;
		goto done;
	}
d487 1
a487 1
	for (n = 0; (evp = TAILQ_FIRST(q)) && n < ret->evpcount; n++) {
d489 5
a493 1
		TAILQ_REMOVE(q, evp, entry);
d495 3
a497 3
		/* consistency check */
		if (evp->state != RQ_EVPSTATE_SCHEDULED)
			errx(1, "evp:%016" PRIx64 " not scheduled", evp->evpid);
d499 4
a502 3
		ret->evpids[n] = evp->evpid;

		if (ret->type == SCHED_REMOVE || ret->type == SCHED_EXPIRE)
a503 1
		else if (ret->type == SCHED_UPDATE) {
d505 3
a507 1
			evp->flags &= ~RQ_ENVELOPE_UPDATE;
d509 11
a519 4
			/* XXX we can't really use scheduler_compute_schedule */
			retry = 0;
			while ((evp->sched = evp->ctime + 800 * retry * retry / 2) <= currtime)
				retry += 1;
d521 2
d526 3
d530 6
a535 1
		else {
d539 29
d569 6
d577 4
a580 1
	ret->evpcount = n;
d582 9
a590 1
   done:
d592 1
a592 17
	ret->mask = 0;
	if (TAILQ_FIRST(&ramqueue.q_removed))
		ret->mask |= SCHED_REMOVE;
	if (TAILQ_FIRST(&ramqueue.q_expired))
		ret->mask |= SCHED_EXPIRE;
	if (TAILQ_FIRST(&ramqueue.q_update))
		ret->mask |= SCHED_UPDATE;
	if (TAILQ_FIRST(&ramqueue.q_bounce))
		ret->mask |= SCHED_BOUNCE;
	if (TAILQ_FIRST(&ramqueue.q_mda))
		ret->mask |= SCHED_MDA;
	if (TAILQ_FIRST(&ramqueue.q_mta))
		ret->mask |= SCHED_MTA;
	if (TAILQ_FIRST(&ramqueue.q_pending))
		ret->mask |= SCHED_DELAY;

	return ((ret->type == SCHED_NONE) ? 0 : 1);
d865 2
d871 1
d873 1
d878 3
d895 1
d956 1
d993 1
d1028 1
@


1.38
log
@(void) cast strlcpy/strlcat/snprintf calls that cannot truncate (and would
       be harmless in this case if they could)
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.37 2014/02/04 14:56:03 eric Exp $	*/
d291 1
a291 1
	 * removal immediatly.
d412 1
a412 1
		 * and will be rescheduled immediatly.  As an optimization,
@


1.37
log
@internal improvements and cleanups

- get rid of the whole penalty thing for failed envelopes in the mta and scheduler.
- do not disable routes on smtp errors
- try to schedule all types of envelopes on each scheduler frame.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1023 1
a1023 1
	snprintf(buf, sizeof buf, "evp:%016" PRIx64 " [", e->evpid);
d1026 1
a1026 1
		strlcat(buf, "bounce", sizeof buf);
d1028 1
a1028 1
		strlcat(buf, "mda", sizeof buf);
d1030 1
a1030 1
		strlcat(buf, "mta", sizeof buf);
d1032 1
a1032 1
	snprintf(t, sizeof t, ",expire=%s",
d1034 1
a1034 1
	strlcat(buf, t, sizeof buf);
d1039 1
a1039 1
		snprintf(t, sizeof t, ",pending=%s",
d1041 1
a1041 1
		strlcat(buf, t, sizeof buf);
d1045 1
a1045 1
		snprintf(t, sizeof t, ",scheduled=%s",
d1047 1
a1047 1
		strlcat(buf, t, sizeof buf);
d1051 1
a1051 1
		snprintf(t, sizeof t, ",inflight=%s",
d1053 1
a1053 1
		strlcat(buf, t, sizeof buf);
d1057 1
a1057 1
		snprintf(t, sizeof t, ",held=%s",
d1059 1
a1059 1
		strlcat(buf, t, sizeof buf);
d1066 1
a1066 1
		strlcat(buf, ",removed", sizeof buf);
d1068 1
a1068 1
		strlcat(buf, ",expired", sizeof buf);
d1070 1
a1070 1
		strlcat(buf, ",suspended", sizeof buf);
d1072 1
a1072 1
	strlcat(buf, "]", sizeof buf);
@


1.36
log
@bcopy -> memmove
bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.35 2013/12/05 09:26:47 eric Exp $	*/
d475 1
a475 1
		return (1);
d480 1
a480 1
		return (0);
d517 19
a535 1
	return (1);
@


1.35
log
@When a relay fails, let the scheduler update all envelopes in the
holdq as if they tempfailed.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d744 1
a744 1
	bzero(rq, sizeof *rq);
@


1.34
log
@schedule in O(log n)
@
text
@d63 1
d66 1
d91 1
d203 1
d388 1
a388 1
	int			 i;
d396 7
d416 2
d436 1
d452 4
d495 13
d751 1
d828 2
d866 3
@


1.33
log
@Rework the mda and scheduler to use the holdq mechanism instead of
tempfail for limiting the number of pending deliveries to the same
user.  This allows to reach optimal delivery time even in case of
burst, while keeping the number of inflight envelopes low.
@
text
@d48 1
d81 1
d93 3
d112 1
a112 2
static void sorted_insert(struct evplist *, struct rq_envelope *);
static void sorted_merge(struct evplist *, struct evplist *);
d208 1
a208 1
	sorted_insert(&update->q_pending, envelope);
d301 1
a301 1
		sorted_insert(&ramqueue.q_pending, evp);
d405 1
a405 1
		sorted_insert(&ramqueue.q_pending, evp);
d698 1
a698 17
sorted_insert(struct evplist *list, struct rq_envelope *evp)
{
	struct rq_envelope	*item;
	time_t			 ref;

	TAILQ_FOREACH(item, list, entry) {
		ref = (evp->sched < evp->expire) ? evp->sched : evp->expire;
		if (ref <= item->expire && ref <= item->sched) {
			TAILQ_INSERT_BEFORE(item, evp, entry);
			return;
		}
	}
	TAILQ_INSERT_TAIL(list, evp, entry);
}

static void
sorted_merge(struct evplist *list, struct evplist *from)
d700 1
a700 1
	struct rq_envelope	*e;
d702 6
a707 5
	/* XXX this is O(not good enough) */
	while ((e = TAILQ_LAST(from, evplist))) {
		TAILQ_REMOVE(from, e, entry);
		sorted_insert(list, e);
	}
d722 1
d749 6
a754 1
	sorted_merge(&rq->q_pending, &update->q_pending);
d773 1
d843 1
a843 1
	else if (!(evp->flags & RQ_ENVELOPE_SUSPEND))
d845 2
d856 2
a857 1
	struct rq_holdq		*hq;
d880 4
a883 1
		TAILQ_REMOVE(rq_envelope_list(rq, evp), evp, entry);
d898 1
d915 4
a918 1
		TAILQ_REMOVE(rq_envelope_list(rq, evp), evp, entry);
d929 2
d934 7
a940 2
	if (evp->state != RQ_EVPSTATE_INFLIGHT)
		sorted_insert(rq_envelope_list(rq, evp), evp);
d1043 18
@


1.32
log
@Implement a feedback mechanism which allows the mta to "hold" envelopes
in the scheduler when it has too many tasks for a given relay.  The
envelopes are put on a wait queue, and are not scheduled again until
the mta "releases" some envelopes from that queue.

It prevents from having too many inflight envelopes, which are out of reach
for the admin.
@
text
@d98 1
a98 1
static int scheduler_ram_release(uint64_t, int);
d146 1
a146 1
static struct tree	holdqs;
d155 3
a157 1
	tree_init(&holdqs);
d354 1
a354 1
	hq = tree_get(&holdqs, holdq);
d358 1
a358 1
		tree_xset(&holdqs, holdq, hq);
d376 1
a376 1
scheduler_ram_release(uint64_t holdq, int n)
d384 1
a384 1
	hq = tree_get(&holdqs, holdq);
d405 1
a405 1
		tree_xpop(&holdqs, holdq);
d838 1
a838 1
		hq = tree_xget(&holdqs, evp->holdq);
d841 1
a841 1
			tree_xpop(&holdqs, evp->holdq);
d871 1
a871 1
		hq = tree_xget(&holdqs, evp->holdq);
d874 1
a874 1
			tree_xpop(&holdqs, evp->holdq);
d901 1
a901 1
		hq = tree_xget(&holdqs, evp->holdq);
d904 1
a904 1
			tree_xpop(&holdqs, evp->holdq);
@


1.31
log
@If the admin issues a remove request for an envelope which is inflight, do not
ignore the request. Instead defer the removal until the envelope returns to the
scheduler. Simplify code by a great deal in the process.
@
text
@d5 1
a5 1
 * Copyright (c) 2012-2013 Eric Faurot <eric@@openbsd.org>
d50 1
d56 1
d73 4
d97 2
d131 2
d146 1
d155 1
d327 85
d536 6
d808 3
d820 1
d835 11
a845 1
	if (!(evp->flags & RQ_ENVELOPE_SUSPEND))
d856 2
d868 11
a878 1
	if (!(evp->flags & RQ_ENVELOPE_SUSPEND)) {
d893 2
d898 12
a909 1
	if (evp->state != RQ_EVPSTATE_INFLIGHT) {
d986 5
@


1.30
log
@Implement a scheduler_proc backend
@
text
@d5 1
a5 1
 * Copyright (c) 2012 Eric Faurot <eric@@openbsd.org>
a43 2
	struct rq_message	*q_next;
	struct evplist		 q_mta;
d50 1
a50 1
	int			 type;
d52 8
a59 6
#define	RQ_ENVELOPE_PENDING	 0x01
#define	RQ_ENVELOPE_SCHEDULED	 0x02
#define	RQ_ENVELOPE_EXPIRED	 0x04
#define	RQ_ENVELOPE_REMOVED	 0x08
#define	RQ_ENVELOPE_INFLIGHT	 0x10
#define	RQ_ENVELOPE_SUSPEND	 0x20
d78 1
a78 1
	struct rq_message	*q_mtabatch;
a171 1
		TAILQ_INIT(&message->q_mta);
d189 1
a189 1
	envelope->flags = RQ_ENVELOPE_PENDING;
d262 1
a262 1
	if (!(evp->flags & RQ_ENVELOPE_INFLIGHT))
d265 13
d281 1
a281 3
	TAILQ_REMOVE(&ramqueue.q_inflight, evp, entry);
	evp->flags &= ~RQ_ENVELOPE_INFLIGHT;
	evp->flags |= RQ_ENVELOPE_PENDING;
d304 1
a304 1
	if (!(evp->flags & RQ_ENVELOPE_INFLIGHT))
d308 1
a308 1
	evp->flags &= ~RQ_ENVELOPE_INFLIGHT;
a318 1
	struct rq_message	*msg;
d343 2
a344 5
	else if (typemask & SCHED_MTA && ramqueue.q_mtabatch) {
		msg = ramqueue.q_mtabatch;
		ramqueue.q_mtabatch = msg->q_next;
		msg->q_next = NULL;
		q = &msg->q_mta;
d366 2
a367 2
		/* consistency check */ 
		if (!(evp->flags & RQ_ENVELOPE_SCHEDULED))
d376 1
a376 2
			evp->flags &= ~RQ_ENVELOPE_SCHEDULED;
			evp->flags |= RQ_ENVELOPE_INFLIGHT;
d422 2
d425 3
a427 2
		dst[n].evpid = evp->evpid;
		if (evp->flags & RQ_ENVELOPE_PENDING) {
d431 1
a431 1
		else if (evp->flags & RQ_ENVELOPE_SCHEDULED) {
d435 1
a435 1
		else if (evp->flags & RQ_ENVELOPE_INFLIGHT) {
d441 1
d465 4
a468 5
		if (evp->flags & RQ_ENVELOPE_PENDING) {
			rq_envelope_schedule(&ramqueue, evp);
			return (1);
		}
		return (0);
d476 6
a481 5
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			if (evp->flags & RQ_ENVELOPE_PENDING) {
				rq_envelope_schedule(&ramqueue, evp);
				r++;
			}
d623 1
d667 1
a667 1
		if (evp->flags != RQ_ENVELOPE_PENDING)
d674 1
a674 1
			evp->flags &= ~RQ_ENVELOPE_PENDING;
a675 1
			evp->flags |= RQ_ENVELOPE_SCHEDULED;
d686 5
a690 1
	if (evp->flags & RQ_ENVELOPE_SCHEDULED) {
d696 1
a696 1
			return &evp->message->q_mta;
d701 1
a701 1
	}
d703 1
a703 4
	if (evp->flags & RQ_ENVELOPE_PENDING)
		return &rq->q_pending;

	if (evp->flags & RQ_ENVELOPE_INFLIGHT)
d705 1
d707 1
d716 5
a720 8
	if (evp->type == D_MTA) {
		if (TAILQ_EMPTY(&evp->message->q_mta)) {
			evp->message->q_next = rq->q_mtabatch;
			rq->q_mtabatch = evp->message;
		}
		q = &evp->message->q_mta;
	}
	else if (evp->type == D_MDA)
d722 2
a723 1
	else if (evp->type == D_BOUNCE)
d725 5
a730 1
	TAILQ_REMOVE(&rq->q_pending, evp, entry);
d732 1
a732 2
	evp->flags &= ~RQ_ENVELOPE_PENDING;
	evp->flags |= RQ_ENVELOPE_SCHEDULED;
a738 3
	struct rq_message	*m;
	struct evplist		*q = NULL;

d742 1
a742 2
	 * For now we just ignore it, but we could mark the envelope for
	 * removal and possibly send a cancellation to the agent.
d744 4
a747 2
	if (evp->flags & (RQ_ENVELOPE_INFLIGHT))
		return (0);
d749 3
a751 1
	q = rq_envelope_list(rq, evp);
a752 1
	TAILQ_REMOVE(q, evp, entry);
d754 1
a754 1
	evp->flags &= ~RQ_ENVELOPE_PENDING;
a755 1
	evp->flags |= RQ_ENVELOPE_SCHEDULED;
a757 17
	/*
	 * We might need to unschedule the message if it was the only
	 * scheduled envelope
	 */
	if (q == &evp->message->q_mta && TAILQ_EMPTY(q)) {
		if (rq->q_mtabatch == evp->message)
			rq->q_mtabatch = evp->message->q_next;
		else {
			for (m = rq->q_mtabatch; m->q_next; m = m->q_next)
				if (m->q_next == evp->message) {
					m->q_next = evp->message->q_next;
					break;
				}
		}
		evp->message->q_next = NULL;
	}

d767 1
a767 1
	if (!(evp->flags & RQ_ENVELOPE_INFLIGHT))
d769 1
d782 1
a782 1
	if (!(evp->flags & RQ_ENVELOPE_INFLIGHT))
d786 1
d824 3
a826 1
	if (e->flags & RQ_ENVELOPE_PENDING) {
d830 3
a832 2
	}
	if (e->flags & RQ_ENVELOPE_SCHEDULED) {
d836 3
a838 2
	}
	if (e->flags & RQ_ENVELOPE_INFLIGHT) {
d842 4
d847 1
@


1.29
log
@scheduler improvements:
- implement suspend/resume scheduling for individual envelopes or message,
  with the associated smtpctl commands.
- allow the mta to request immediate scheduling of an envelope.
- on temporary failures a penalty can be given to further delay the next try.
@
text
@@


1.28
log
@sync with OpenSMTPD 5.3.2

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.27 2013/02/10 15:01:16 eric Exp $	*/
d59 1
d96 2
d106 1
d109 2
d130 2
a269 1
	sorted_insert(&ramqueue.q_pending, evp);
d272 2
d431 2
d511 68
d674 25
d738 1
a738 9
	if (evp->flags & RQ_ENVELOPE_SCHEDULED) {
		if (evp->type == D_MTA)
			q = &evp->message->q_mta;
		else if (evp->type == D_MDA)
			q = &rq->q_mda;
		else if (evp->type == D_BOUNCE)
			q = &rq->q_bounce;
	} else
		q = &rq->q_pending;
d767 27
d847 2
@


1.27
log
@When getting the next batch of envelope to schedule, use an array to
store envelope ids, rather than a dynamic list.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.26 2013/01/26 09:37:23 gilles Exp $	*/
a22 1
#include <sys/param.h>
d84 11
a94 11
static void scheduler_ramqueue_init(void);
static void scheduler_ramqueue_insert(struct scheduler_info *);
static size_t scheduler_ramqueue_commit(uint32_t);
static size_t scheduler_ramqueue_rollback(uint32_t);
static void scheduler_ramqueue_update(struct scheduler_info *);
static void scheduler_ramqueue_delete(uint64_t);
static void scheduler_ramqueue_batch(int, struct scheduler_batch *);
static size_t scheduler_ramqueue_messages(uint32_t, uint32_t *, size_t);
static size_t scheduler_ramqueue_envelopes(uint64_t, struct evpstate *, size_t);
static void scheduler_ramqueue_schedule(uint64_t);
static void scheduler_ramqueue_remove(uint64_t);
d104 1
a104 1
static void rq_envelope_remove(struct rq_queue *, struct rq_envelope *);
d109 1
a109 1
	scheduler_ramqueue_init,
d111 3
a113 3
	scheduler_ramqueue_insert,
	scheduler_ramqueue_commit,
	scheduler_ramqueue_rollback,
d115 2
a116 2
	scheduler_ramqueue_update,
	scheduler_ramqueue_delete,
d118 1
a118 1
	scheduler_ramqueue_batch,
d120 4
a123 4
	scheduler_ramqueue_messages,
	scheduler_ramqueue_envelopes,
	scheduler_ramqueue_schedule,
	scheduler_ramqueue_remove,
d131 2
a132 2
static void
scheduler_ramqueue_init(void)
d136 2
d140 2
a141 2
static void
scheduler_ramqueue_insert(struct scheduler_info *si)
d186 2
d191 1
a191 1
scheduler_ramqueue_commit(uint32_t msgid)
d218 1
a218 1
scheduler_ramqueue_rollback(uint32_t msgid)
d241 2
a242 2
static void
scheduler_ramqueue_update(struct scheduler_info *si)
d267 2
d271 2
a272 2
static void
scheduler_ramqueue_delete(uint64_t evpid)
d291 2
d295 2
a296 2
static void
scheduler_ramqueue_batch(int typemask, struct scheduler_batch *ret)
d307 1
a307 1
		rq_queue_dump(&ramqueue, "scheduler_ramqueue_batch()");
d334 1
d339 1
a339 1
		return;
d343 2
a344 1
		return;
a367 1
}
d369 1
a369 56
static void
scheduler_ramqueue_schedule(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return;
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return;
		if (evp->flags & RQ_ENVELOPE_PENDING)
			rq_envelope_schedule(&ramqueue, evp);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return;
		i = NULL;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			if (evp->flags & RQ_ENVELOPE_PENDING)
				rq_envelope_schedule(&ramqueue, evp);
	}
}

static void
scheduler_ramqueue_remove(uint64_t evpid)
{
	struct rq_message	*msg;
	struct rq_envelope	*evp;
	uint32_t		 msgid;
	void			*i;

	currtime = time(NULL);

	if (evpid > 0xffffffff) {
		msgid = evpid_to_msgid(evpid);
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return;
		if ((evp = tree_get(&msg->envelopes, evpid)) == NULL)
			return;
		rq_envelope_remove(&ramqueue, evp);
	}
	else {
		msgid = evpid;
		if ((msg = tree_get(&ramqueue.messages, msgid)) == NULL)
			return;
		i = NULL;
		while (tree_iter(&msg->envelopes, &i, NULL, (void*)(&evp)))
			rq_envelope_remove(&ramqueue, evp);
	}
d373 1
a373 1
scheduler_ramqueue_messages(uint32_t from, uint32_t *dst, size_t size)
d389 1
a389 1
scheduler_ramqueue_envelopes(uint64_t from, struct evpstate *dst, size_t size)
d428 72
d619 1
a619 1
static void
d626 1
a626 1
		return;
d632 1
a632 1
		return;
d667 2
@


1.26
log
@Sync with our smtpd repo:

* first bricks of ldap and sqlite support (not finished but both working)
* new table API to replace map API, all lookups are done through tables
* improved handling of temporary errors throughout the daemon
* improved scheduler and mta logic: connection reuse, optimizes batches
* improved queue: more tolerant to admin errors, new layout, less disk-IO
* improved memory usage under high load
* SSL certs/keys isolated to lookup process to avoid facing network
* VIRTUAL support improved, fully virtual setups possible now
* runtime tracing of processes through smtpctl trace
* ssl_privsep.c sync-ed with relayd
* ssl.c no longer contains smtpd specific interfaces
* smtpd-specific ssl bits moved to ssl_smtpd.c
* update mail address in copyright

FLUSH YOUR QUEUE. FLUSH YOUR QUEUE. FLUSH YOUR QUEUE. FLUSH YOUR QUEUE.

smtpd.conf(5) simplified, it will require adaptations

ok eric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.24 2012/11/12 14:58:53 eric Exp $	*/
d294 1
a294 1
	struct id_list		*item;
d338 1
a338 4
	ret->evpids = NULL;
	ret->evpcount = 0;

	while ((evp = TAILQ_FIRST(q))) {
d346 1
a346 4
		item = xmalloc(sizeof *item, "schedule_batch");
		item->id = evp->evpid;
		item->next = ret->evpids;
		ret->evpids = item;
a355 1
		ret->evpcount++;
d357 2
@


1.25
log
@Allow "smtpctl show queue" to run in "online" mode if the smtpd server
is running.  The scheduler sends the runtime state of each envelope to
the queue process which loads the envelope, fills the runtime bits and
sends the envelope back to the client. Iteration over the envelope set
happens in small chunks to make the request interruptible and to allow
the server to keep doing its job in the meantime.

Adpat "smtpctl schedule-all" to schedule the messages one by one using
the same iteration mechanism.

Document "smtpctl monitor" and "smtpctl show queue".

ok gilles@@
@
text
@d4 1
a4 1
 * Copyright (c) 2012 Gilles Chehade <gilles@@openbsd.org>
a131 2
extern int verbose;

d183 2
d262 2
d464 1
a464 1
			dst[n].flags = DF_PENDING;
d468 1
a468 1
			dst[n].flags = DF_PENDING;
d472 1
a472 1
			dst[n].flags = DF_INFLIGHT;
@


1.24
log
@Cleanups and improvements:

* Log more events (especially client session) and use a better scheme
  for that: each messages is prefixed with a token to easily identify
  its class:
    - info/warn/debug: general server messages
    - smtp-in: smtp client connections
    - relay: status update for relayed messages
    - delivery: status update for local deliveries

* Implement "smtpctl monitor" to display updates of selected internal
  counters.

* When reloading the on-disk queue at startup do not commit a message
  if no envelope was submitted for that message.

* Remove unused stuff in the config parser.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.22 2012/09/27 19:50:07 eric Exp $	*/
d45 2
a46 2
	struct rq_message	*sched_next;
	struct rq_envelope	*sched_mta;
a66 1
	struct rq_envelope	*sched_next;
d75 2
a76 7
	struct evplist		 pending;

	struct rq_message	*sched_mta;
	struct rq_envelope	*sched_mda;
	struct rq_envelope	*sched_bounce;
	struct rq_envelope	*sched_expired;
	struct rq_envelope	*sched_removed;
d78 5
d92 2
d121 2
d130 1
a130 1
static time_t		currtime;	
d165 1
d184 1
a184 1
	sorted_insert(&update->pending, envelope);
d202 4
d227 2
a228 2
	while ((evp = TAILQ_FIRST(&update->pending))) {
		TAILQ_REMOVE(&update->pending, evp, entry);
d258 2
a261 1
	sorted_insert(&ramqueue.pending, evp);
d281 1
d289 3
a291 1
	struct rq_envelope	*evp, *tmp, **batch;
d300 2
a301 2
	if (typemask & SCHED_REMOVE && ramqueue.sched_removed) {
		batch = &ramqueue.sched_removed;
d304 2
a305 2
	else if (typemask & SCHED_EXPIRE && ramqueue.sched_expired) {
		batch = &ramqueue.sched_expired;
d308 2
a309 2
	else if (typemask & SCHED_BOUNCE && ramqueue.sched_bounce) {
		batch = &ramqueue.sched_bounce;
d312 2
a313 2
	else if (typemask & SCHED_MDA && ramqueue.sched_mda) {
		batch = &ramqueue.sched_mda;
d316 5
a320 3
	else if (typemask & SCHED_MTA && ramqueue.sched_mta) {
		batch = &ramqueue.sched_mta->sched_mta;
		ramqueue.sched_mta = ramqueue.sched_mta->sched_next;
d323 1
a323 1
	else if ((evp = TAILQ_FIRST(&ramqueue.pending))) {
d338 4
a341 2
	for(evp = *batch; evp; evp = tmp) {
		tmp = evp->sched_next;
d351 1
a351 1
		evp->sched_next = NULL;
d355 1
a361 2

	*batch = NULL;
d370 1
a370 1
	void			*i, *j;
d374 1
a374 10
	if (evpid == 0) {
		j = NULL;
		while (tree_iter(&ramqueue.messages, &j, NULL, (void*)(&msg))) {
			i = NULL;
			while (tree_iter(&msg->envelopes, &i, NULL,
			    (void*)(&evp)))
				rq_envelope_schedule(&ramqueue, evp);
		}
	}
	else if (evpid > 0xffffffff) {
d380 2
a381 1
		rq_envelope_schedule(&ramqueue, evp);
d389 2
a390 1
			rq_envelope_schedule(&ramqueue, evp);
d422 56
d511 6
a516 1
	TAILQ_INIT(&rq->pending);
d535 1
a535 1
		while((tree_iter(&message->envelopes, &i, &id,
d543 2
a544 1
	sorted_merge(&rq->pending, &update->pending);
d552 1
a552 1
	while ((evp = TAILQ_FIRST(&rq->pending))) {
a555 1
		/* it *must* be pending */
d561 2
a562 1
			TAILQ_REMOVE(&rq->pending, evp, entry);
a566 2
			evp->sched_next = rq->sched_expired;
			rq->sched_expired = evp;
d576 1
a576 5
	if (evp->flags & (RQ_ENVELOPE_SCHEDULED | RQ_ENVELOPE_INFLIGHT))
		return;

	if (evp->flags & RQ_ENVELOPE_PENDING)
		TAILQ_REMOVE(&rq->pending, evp, entry);
d579 3
a581 3
		if (evp->message->sched_mta == NULL) {
			evp->message->sched_next = rq->sched_mta;
			rq->sched_mta = evp->message;
d583 1
a583 10
		evp->sched_next = evp->message->sched_mta;
		evp->message->sched_mta = evp;
	}
	else if (evp->type == D_MDA) {
		evp->sched_next = rq->sched_mda;
		rq->sched_mda = evp;
	}
	else if (evp->type == D_BOUNCE) {
		evp->sched_next = rq->sched_bounce;
		rq->sched_bounce = evp;
d585 7
d600 10
a609 1
	if (!(evp->flags & (RQ_ENVELOPE_PENDING)))
d612 12
a623 3
	TAILQ_REMOVE(&rq->pending, evp, entry);
	evp->sched_next = rq->sched_removed;
	rq->sched_removed = evp;
d628 17
d658 1
d677 2
a678 1
	snprintf(t, sizeof t, ",expire=%s", duration_to_text(e->expire - currtime));
d717 1
a717 1
	while((tree_iter(&rq->messages, &i, &id, (void*)&message))) {
d720 1
a720 1
		while((tree_iter(&message->envelopes, &j, &id,
@


1.23
log
@Make counters more informative in the scheduler:

- Change the scheduler backend API a bit: commit() and rollback()
  API calls return the number of envelopes added or canceled; put
  the number of envelopes in the structure returned by batch().

- Properly report the number of incoming, registered, removed and
  expired envelopes, as well as the outcome of deliveries.

ok gilles@@
@
text
@d615 1
a615 1
	log_debug("/--- ramqueue: %s", name);
d619 1
a619 1
		log_debug("| msg:%08" PRIx32, message->msgid);
d623 1
a623 1
			log_debug("|   %s",
d626 1
a626 1
	log_debug("\\---");
@


1.22
log
@When merging messages from an update, decrement the message counter if the
message already exists in the main queue, otherwise it's counted twice.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.21 2012/09/11 08:37:52 eric Exp $	*/
d73 1
d88 2
a89 2
static void scheduler_ramqueue_commit(uint32_t);
static void scheduler_ramqueue_rollback(uint32_t);
d176 1
d183 1
a183 1
static void
d187 1
d192 1
d202 2
d206 1
a206 1
static void
d211 1
d216 2
a217 1
		return;
d226 2
d323 1
d343 1
@


1.21
log
@Rework the scheduler internals.  Fix some scheduling loop issues and
handle envelope scheduling/expiration better.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.20 2012/08/25 15:47:47 eric Exp $	*/
a196 1

d459 1
@


1.20
log
@It's ok to try to rollback an update we don't know about. It might
happen if the message is aborted before an envelope is sent to the
scheduler.  In this case, just ignore it.

ok gilles@@ chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.19 2012/08/25 10:23:12 gilles Exp $	*/
d45 2
d55 5
a59 2
#define	RQ_ENVELOPE_INFLIGHT	 0x01
#define	RQ_ENVELOPE_EXPIRED	 0x02
d66 4
a69 1
	struct evplist		*queue;
d74 9
a82 6
	struct evplist		 mda;
	struct evplist		 mta;
	struct evplist		 bounce;
	struct evplist		 inflight;
	struct tree		 expired;
	struct tree		 removed;
d91 1
a91 1
static void scheduler_ramqueue_batch(int, time_t, struct scheduler_batch *);
a94 1
static int  scheduler_ramqueue_next(int, uint64_t *, time_t *);
d100 4
a103 1
static void rq_queue_dump(struct rq_queue *, const char *, time_t);
d105 1
a105 1
static const char *rq_envelope_to_text(struct rq_envelope *, time_t);
d123 4
a126 2
static struct rq_queue		ramqueue;
static struct tree		updates;
d145 2
d171 1
d173 1
a173 1
	envelope->expire = si->creation + si->expire;
d177 2
a178 20
	if (envelope->expire < envelope->sched) {
		envelope->flags |= RQ_ENVELOPE_EXPIRED;
		tree_xset(&update->expired, envelope->evpid, envelope);
	}

	tree_xset(&message->envelopes, envelope->evpid, envelope);

	if (si->type == D_BOUNCE)
		envelope->queue = &update->bounce;
	else if (si->type == D_MDA)
		envelope->queue = &update->mda;
	else if (si->type == D_MTA)
		envelope->queue = &update->mta;
	else
		errx(1, "bad type");

	sorted_insert(envelope->queue, envelope);

	if (verbose & TRACE_SCHEDULER)
		rq_queue_dump(update, "inserted", time(NULL));
d185 2
a186 1
	time_t		 now;
d190 3
a192 5
	if (verbose & TRACE_SCHEDULER) {
		now = time(NULL);
		rq_queue_dump(update, "commit update", now);
		rq_queue_dump(&ramqueue, "before commit", now);
	}
d194 1
d196 1
a196 2
	if (verbose & TRACE_SCHEDULER)
		rq_queue_dump(&ramqueue, "after commit", time(NULL));
a197 1
	free(update);
d205 3
a207 1
	struct rq_envelope	*envelope;
d212 4
a215 6
	while ((envelope = TAILQ_FIRST(&update->bounce)))
		rq_envelope_delete(update, envelope);
	while ((envelope = TAILQ_FIRST(&update->mda)))
		rq_envelope_delete(update, envelope);
	while ((envelope = TAILQ_FIRST(&update->mta)))
		rq_envelope_delete(update, envelope);
d224 2
a225 2
	struct rq_message	*message;
	struct rq_envelope	*envelope;
d228 2
d231 2
a232 2
	message = tree_xget(&ramqueue.messages, msgid);
	envelope = tree_xget(&message->envelopes, si->evpid);
d234 3
a236 10
	/* it *should* be in-flight */
	if (!(envelope->flags & RQ_ENVELOPE_INFLIGHT))
		log_warnx("evp:%016" PRIx64 " not in-flight", si->evpid);

	envelope->flags &= ~RQ_ENVELOPE_INFLIGHT;
	envelope->sched = scheduler_compute_schedule(si);
	if (envelope->expire < envelope->sched) {
		envelope->flags |= RQ_ENVELOPE_EXPIRED;
		tree_xset(&ramqueue.expired, envelope->evpid, envelope);
	}
d238 2
a239 7
	TAILQ_REMOVE(envelope->queue, envelope, entry);
	if (si->type == D_BOUNCE)
		envelope->queue = &ramqueue.bounce;
	else if (si->type == D_MDA)
		envelope->queue = &ramqueue.mda;
	else if (si->type == D_MTA)
		envelope->queue = &ramqueue.mta;
d241 3
a243 1
	sorted_insert(envelope->queue, envelope);
d249 2
a250 2
	struct rq_message	*message;
	struct rq_envelope	*envelope;
d253 2
d256 2
a257 2
	message = tree_xget(&ramqueue.messages, msgid);
	envelope = tree_xget(&message->envelopes, evpid);
d260 1
a260 1
	if (!(envelope->flags & RQ_ENVELOPE_INFLIGHT))
d263 2
a264 51
	rq_envelope_delete(&ramqueue, envelope);
}

static int
scheduler_ramqueue_next(int typemask, uint64_t *evpid, time_t *sched)
{
	struct rq_envelope	*evp_mda = NULL;
	struct rq_envelope	*evp_mta = NULL;
	struct rq_envelope	*evp_bounce = NULL;
	struct rq_envelope	*envelope = NULL;

	if (verbose & TRACE_SCHEDULER)
		rq_queue_dump(&ramqueue, "next", time(NULL));

	*sched = 0;

	if (typemask & SCHED_REMOVE && tree_root(&ramqueue.removed, evpid, NULL))
		return (1);
	if (typemask & SCHED_EXPIRE && tree_root(&ramqueue.expired, evpid, NULL))
		return (1);

	/* fetch first envelope from each queue */
	if (typemask & SCHED_BOUNCE)
		evp_bounce = TAILQ_FIRST(&ramqueue.bounce);
	if (typemask & SCHED_MDA)
		evp_mda = TAILQ_FIRST(&ramqueue.mda);
	if (typemask & SCHED_MTA)
		evp_mta = TAILQ_FIRST(&ramqueue.mta);

	/* set current envelope to either one */
	if (evp_bounce)
		envelope = evp_bounce;
	else if (evp_mda)
		envelope = evp_mda;
	else if (evp_mta)
		envelope = evp_mta;
	else
		return (0);

	/* figure out which one should be scheduled first */
	if (evp_bounce && evp_bounce->sched < envelope->sched)
		envelope = evp_bounce;
	if (evp_mda && evp_mda->sched < envelope->sched)
		envelope = evp_mda;
	if (evp_mta && evp_mta->sched < envelope->sched)
		envelope = evp_mta;

	*evpid = envelope->evpid;
	*sched = envelope->sched;

	return (1);
d268 1
a268 1
scheduler_ramqueue_batch(int typemask, time_t curr, struct scheduler_batch *ret)
d270 1
a270 2
	struct rq_message	*message;
	struct rq_envelope	*envelope;
a271 4
	uint64_t		 evpid;
	void			*i;
	int			 type;
	time_t			 sched;
d273 1
a273 1
	ret->evpids = NULL;
d275 3
a277 4
	if (!scheduler_ramqueue_next(typemask, &evpid, &sched)) {
		ret->type = SCHED_NONE;
		return;
	}
d279 2
a280 1
	if (tree_get(&ramqueue.removed, evpid)) {
a281 7
		while (tree_poproot(&ramqueue.removed, &evpid, NULL)) {
			item = xmalloc(sizeof *item, "schedule_batch");
			item->id = evpid;
			item->next = ret->evpids;
			ret->evpids = item;
		}
		return;
d283 2
a284 6

	message = tree_xget(&ramqueue.messages, evpid_to_msgid(evpid));
	envelope = tree_xget(&message->envelopes, evpid);

	/* if the envelope has expired, return the expired list */
	if (envelope->flags & RQ_ENVELOPE_EXPIRED) {
d286 20
a305 9
		while (tree_poproot(&ramqueue.expired, &evpid, (void**)&envelope)) {
			TAILQ_REMOVE(envelope->queue, envelope, entry);
			TAILQ_INSERT_TAIL(&ramqueue.inflight, envelope, entry);
			envelope->flags |= RQ_ENVELOPE_INFLIGHT;
			item = xmalloc(sizeof *item, "schedule_batch");
			item->id = evpid;
			item->next = ret->evpids;
			ret->evpids = item;
		}
d308 2
a309 4

	if (sched > curr) {
		ret->type = SCHED_DELAY;
		ret->delay = sched - curr;
d313 7
a319 7
	type = envelope->type;
	if (type == D_BOUNCE)
		ret->type = SCHED_BOUNCE;
	else if (type == D_MDA)
		ret->type = SCHED_MDA;
	else if (type == D_MTA)
		ret->type = SCHED_MTA;
a320 14
	i = NULL;
	while((tree_iter(&message->envelopes, &i, &evpid, (void*)&envelope))) {
		if (envelope->type != type)
			continue;
		if (envelope->sched > curr)
			continue;
		if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
			continue;
		if (envelope->flags & RQ_ENVELOPE_EXPIRED)
			continue;
		TAILQ_REMOVE(envelope->queue, envelope, entry);
		TAILQ_INSERT_TAIL(&ramqueue.inflight, envelope, entry);
		envelope->queue = &ramqueue.inflight;
		envelope->flags |= RQ_ENVELOPE_INFLIGHT;
d322 1
a322 1
		item->id = evpid;
d325 8
d334 2
a337 1

d341 2
a342 2
	struct rq_message	*message;
	struct rq_envelope	*envelope;
d346 2
d350 1
a350 3
		while (tree_iter(&ramqueue.messages, &j, NULL,
		    (void*)(&message))) {

d352 3
a354 9
			while (tree_iter(&message->envelopes, &i, &evpid,
			    (void*)(&envelope))) {
				if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
					continue;

				envelope->sched = time(NULL);
				TAILQ_REMOVE(envelope->queue, envelope, entry);
				sorted_insert(envelope->queue, envelope);
			}
d359 1
a359 1
		if ((message = tree_get(&ramqueue.messages, msgid)) == NULL)
d361 1
a361 1
		if ((envelope = tree_get(&message->envelopes, evpid)) == NULL)
d363 1
a363 6
		if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
			return;
       
		envelope->sched = time(NULL);
		TAILQ_REMOVE(envelope->queue, envelope, entry);
		sorted_insert(envelope->queue, envelope);
d367 1
a367 1
		if ((message = tree_get(&ramqueue.messages, msgid)) == NULL)
a368 1

d370 2
a371 9
		while (tree_iter(&message->envelopes, &i, &evpid,
		    (void*)(&envelope))) {
			if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
				continue;

			envelope->sched = time(NULL);
			TAILQ_REMOVE(envelope->queue, envelope, entry);
			sorted_insert(envelope->queue, envelope);
		}
a372 1

d378 2
a379 2
	struct rq_message	*message;
	struct rq_envelope	*envelope;
a380 1
	struct evplist		 rmlist;
d383 2
d387 1
a387 3
		if ((message = tree_get(&ramqueue.messages, msgid)) == NULL)
			return;
		if ((envelope = tree_get(&message->envelopes, evpid)) == NULL)
d389 1
a389 1
		if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
d391 1
a391 2
		rq_envelope_delete(&ramqueue, envelope);
		tree_xset(&ramqueue.removed, evpid, &ramqueue);
d395 1
a395 1
		if ((message = tree_get(&ramqueue.messages, msgid)) == NULL)
a396 2

		TAILQ_INIT(&rmlist);
d398 2
a399 11
		while (tree_iter(&message->envelopes, &i, &evpid,
		    (void*)(&envelope))) {
			if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
				continue;
			tree_xset(&ramqueue.removed, evpid, &ramqueue);
			TAILQ_REMOVE(envelope->queue, envelope, entry);
			envelope->queue = &rmlist;
			TAILQ_INSERT_HEAD(&rmlist, envelope, entry);
		}
		while((envelope = TAILQ_FIRST(&rmlist)))
			rq_envelope_delete(&ramqueue, envelope);
d407 1
d410 2
a411 1
		if (evp->sched < item->sched) {
a427 1
		e->queue = list;
a434 1

d436 1
a436 6
	tree_init(&rq->expired);
	tree_init(&rq->removed);
	TAILQ_INIT(&rq->mda);
	TAILQ_INIT(&rq->mta);
	TAILQ_INIT(&rq->bounce);
	TAILQ_INIT(&rq->inflight);
d462 39
a500 3
	sorted_merge(&rq->bounce, &update->bounce);
	sorted_merge(&rq->mda, &update->mda);
	sorted_merge(&rq->mta, &update->mta);
d502 19
a520 2
	tree_merge(&rq->expired, &update->expired);
	tree_merge(&rq->removed, &update->removed);
d524 1
a524 1
rq_envelope_delete(struct rq_queue *rq, struct rq_envelope *envelope)
d526 2
a527 2
	struct rq_message	*message;
	uint32_t		 msgid;
d529 8
a536 2
	if (envelope->flags & RQ_ENVELOPE_EXPIRED)
		tree_pop(&rq->expired, envelope->evpid);
d538 7
a544 8
	TAILQ_REMOVE(envelope->queue, envelope, entry);
	message = envelope->message;
	msgid = message->msgid;

	tree_xpop(&message->envelopes, envelope->evpid);
	if (tree_empty(&message->envelopes)) {
		tree_xpop(&rq->messages, msgid);
		free(message);
d548 1
a548 1
	free(envelope);
d553 1
a553 1
rq_envelope_to_text(struct rq_envelope *e, time_t tref)
d567 1
a567 3
	snprintf(t, sizeof t, ",sched=%s", duration_to_text(e->sched - tref));
	strlcat(buf, t, sizeof buf);
	snprintf(t, sizeof t, ",exp=%s", duration_to_text(e->expire - tref));
d570 17
a588 2
	if (e->flags & RQ_ENVELOPE_INFLIGHT)
		strlcat(buf, ",in-flight", sizeof buf);
d596 1
a596 1
rq_queue_dump(struct rq_queue *rq, const char * name, time_t tref)
d611 2
a612 1
			log_debug("|   %s", rq_envelope_to_text(envelope, tref));
a613 13

	log_debug("| bounces:");
	TAILQ_FOREACH(envelope, &rq->bounce, entry)
		log_debug("|   %s", rq_envelope_to_text(envelope, tref));
	log_debug("| mda:");
	TAILQ_FOREACH(envelope, &rq->mda, entry)
		log_debug("|   %s", rq_envelope_to_text(envelope, tref));
	log_debug("| mta:");
	TAILQ_FOREACH(envelope, &rq->mta, entry)
		log_debug("|   %s", rq_envelope_to_text(envelope, tref));
	log_debug("| in-flight:");
	TAILQ_FOREACH(envelope, &rq->inflight, entry)
		log_debug("|   %s", rq_envelope_to_text(envelope, tref));
@


1.19
log
@- introduce struct stat_value
- statistics can now have a type (counter, timestamp, timeval, timespec and
  possibly others in the future)
- stat_increment() / stat_decrement() now take an increment/decrement value
  and are at the moment only of type counter
- stat_set() now takes a stat_value
- provide helpers to convert raw values to stat_value

ok eric@@, ok chl@@

while at it fix a rq_queue_dump() call using a bogus timestamp in scheduler
ramqueue.
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.18 2012/08/24 12:29:50 eric Exp $	*/
d209 2
a210 1
	update = tree_xpop(&updates, msgid);
@


1.18
log
@Remove the rq_host and rq_batch structures from the ramqueue scheduler.
The scheduler should only allow admin to schedule specific envelopes by
id, or msgid. More advanced scheduling (per host/route/whatever) should
be achieved using smtpctl schedule-id and proper filtering on the queue,
or using ad-hoc scheduler backend and tools.

ok gilles@@ chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.17 2012/08/19 15:06:36 chl Exp $	*/
d135 1
a135 1
		stat_increment("scheduler.ramqueue.update");
d146 1
a146 1
		stat_increment("scheduler.ramqueue.message");
d157 1
a157 1
	stat_increment("scheduler.ramqueue.envelope");
d197 1
a197 1
		rq_queue_dump(&ramqueue, "after commit", now);
d200 1
a200 1
	stat_decrement("scheduler.ramqueue.update");
d219 1
a219 1
	stat_decrement("scheduler.ramqueue.update");
d593 1
a593 1
		stat_decrement("scheduler.ramqueue.message");
d597 1
a597 1
	stat_decrement("scheduler.ramqueue.envelope");
@


1.17
log
@fix smtpctl schedule-all and schedule-id <msgid>

with help from eric@@

ok eric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.16 2012/08/19 14:46:25 chl Exp $	*/
a39 1
SPLAY_HEAD(hosttree, rq_host);
a41 6
struct rq_host {
	SPLAY_ENTRY(rq_host)	 hosttree_entry;
	char			 host[MAXHOSTNAMELEN];
	struct tree		 batches;
};

a46 6
struct rq_batch {
	uint32_t		 msgid;
	struct tree		 envelopes;
	struct rq_host		*host;
};

a60 1
	struct rq_batch		*batch;
a64 1
	struct hosttree		 hosts;
a87 1

a92 5
static struct rq_host *rq_host_lookup(struct hosttree *, char *);
static struct rq_host *rq_host_create(struct hosttree *, char *);
static int rq_host_cmp(struct rq_host *, struct rq_host *);

SPLAY_PROTOTYPE(hosttree, rq_host, hosttree_entry, rq_host_cmp);
a126 1
	struct rq_host		*host;
a127 1
	struct rq_batch		*batch;
d135 1
a139 15
	/* find/prepare the host in ramqueue update */
	if ((host = rq_host_lookup(&update->hosts, si->destination)) == NULL) {
		host = rq_host_create(&update->hosts, si->destination);
		stat_increment("scheduler.ramqueue.host");
	}

	/* find/prepare the hosttree message in ramqueue update */
	if ((batch = tree_get(&host->batches, msgid)) == NULL) {
		batch = xcalloc(1, sizeof *batch, "scheduler_insert");
		batch->msgid = msgid;
		tree_init(&batch->envelopes);
		tree_xset(&host->batches, msgid, batch);
		stat_increment("scheduler.ramqueue.batch");
	}

a153 1
	envelope->batch = batch;
a163 1
	tree_xset(&batch->envelopes, envelope->evpid, envelope);
d200 1
d218 2
a219 1
	free(update);	
a538 1
	SPLAY_INIT(&rq->hosts);
a548 2
	struct rq_batch		*batch, *tobatch;
	struct rq_host		*host, *tohost;
a552 26
	/* merge host tree */
	while ((host = SPLAY_ROOT(&update->hosts))) {
		SPLAY_REMOVE(hosttree, &update->hosts, host);
		tohost = rq_host_lookup(&rq->hosts, host->host);
		if (tohost == NULL)
			tohost = rq_host_create(&rq->hosts, host->host);
		/* merge batches for current host */
		while (tree_poproot(&host->batches, &id, (void*)&batch)) {
			tobatch = tree_get(&tohost->batches, batch->msgid);
			if (tobatch == NULL) {
				/* batch does not exist. re-use structure */
				batch->host = tohost;
				tree_xset(&tohost->batches, batch->msgid, batch);
				continue;
			}
			/* need to re-link all envelopes before merging them */
			i = NULL;
			while((tree_iter(&batch->envelopes, &i, &id,
			    (void*)&envelope)))
				envelope->batch = tobatch;
			tree_merge(&tobatch->envelopes, &batch->envelopes);
			free(batch);
		}
		free(host);
	}

a579 2
	struct rq_batch		*batch;
	struct rq_host		*host;
a585 1
	batch = envelope->batch;
a587 1
	host = batch->host;
a595 11
	tree_xpop(&batch->envelopes, envelope->evpid);
	if (tree_empty(&batch->envelopes)) {
		tree_xpop(&host->batches, msgid);
		if (tree_empty(&host->batches)) {
			SPLAY_REMOVE(hosttree, &rq->hosts, host);
			free(host);
			stat_decrement("scheduler.ramqueue.host");
		}
		free(batch);
		stat_decrement("scheduler.ramqueue.batch");
	}
a596 1

a632 2
	struct rq_host		*host;
	struct rq_batch		*batch;
a638 12
	SPLAY_FOREACH(host, hosttree, &rq->hosts) {
		log_debug("| host:%s", host->host);
		i = NULL;
		while((tree_iter(&host->batches, &i, &id, (void*)&batch))) {
			log_debug("|  batch:%08" PRIx32, batch->msgid);
			j = NULL;
			while((tree_iter(&batch->envelopes, &j, &id,
			    (void*)&envelope)))
				log_debug("|    %s",
				    rq_envelope_to_text(envelope, tref));
		}
	}
a662 29

static int
rq_host_cmp(struct rq_host *a, struct rq_host *b)
{
	return (strcmp(a->host, b->host));
}

static struct rq_host *
rq_host_lookup(struct hosttree *host_tree, char *host)
{
	struct rq_host	key;

	strlcpy(key.host, host, sizeof key.host);
	return (SPLAY_FIND(hosttree, host_tree, &key));
}

static struct rq_host *
rq_host_create(struct hosttree *host_tree, char *host)
{
	struct rq_host	*rq_host;

	rq_host = xcalloc(1, sizeof *rq_host, "rq_host_create");
	tree_init(&rq_host->batches);
	strlcpy(rq_host->host, host, sizeof rq_host->host);
	SPLAY_INSERT(hosttree, host_tree, rq_host);

	return (rq_host);
}
SPLAY_GENERATE(hosttree, rq_host, hosttree_entry, rq_host_cmp);
@


1.16
log
@avoid crash when removing non-existing envelope

from eric, ok eric@@

cvs: ----------------------------------------------------------------------
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.15 2012/08/19 10:33:35 eric Exp $	*/
d449 1
d451 46
a496 7
	msgid = evpid_to_msgid(evpid);
	if ((message = tree_get(&ramqueue.messages, msgid)) == NULL)
		return;
	if ((envelope = tree_xget(&message->envelopes, evpid)) == NULL)
		return;
	if (envelope->flags & RQ_ENVELOPE_INFLIGHT)
		return;
a497 3
	envelope->sched = time(NULL);
	TAILQ_REMOVE(envelope->queue, envelope, entry);
	sorted_insert(envelope->queue, envelope);
@


1.15
log
@Let the scheduler return all schedulable envelopes for the same message
as a single batch. The route for each envelope is sorted out by the mta
properly, so they are grouped as a single MAIL on each route.

ok gilles@@ chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.14 2012/08/18 18:18:23 gilles Exp $	*/
d476 1
a476 1
		if ((envelope = tree_xget(&message->envelopes, evpid)) == NULL)
@


1.14
log
@- introduce stat_backend, an API for pluggable statistic backends
  > statistics are no longer static structures in shared memory
  > statistics are only set, smtpd never uses them in its logic
  > each statistic is a key/value where key can be any (dynamic) string
- convert all uses of the former API to use the new one
- implement stat_ramstat that keeps non-persistent stats in ram structure

ok eric@@, ok chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.13 2012/08/11 19:19:19 chl Exp $	*/
a363 1
	struct rq_batch		*batch;
a412 1
	batch = envelope->batch;
d422 1
a422 1
	while((tree_iter(&batch->envelopes, &i, &evpid, (void*)&envelope))) {
@


1.13
log
@fix use after free

ok eric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.12 2012/08/08 08:50:42 eric Exp $	*/
d163 1
a163 1
	if ((host = rq_host_lookup(&update->hosts, si->destination)) == NULL)
d165 2
d174 1
d183 1
d195 2
d628 1
d637 1
d640 1
d643 2
@


1.12
log
@Improve the scheduler backend API.

New envelopes are pushed into the scheduler through the insert()
commit() rollback() transactional interface functions.

Worklists are pulled from the scheduler through a single batch()
interface function, which returns a list of envelope ids and the
type of processing. Envelopes returned in this batch are said to
be "in-flight", as opposed to "pending". They are supposed to be
processed in some way, and either updated() or deleted() at some
point.

The schedule()/remove() functions are used to alter the internal
state of "pending" envelopes to make them schedulable.  The enve-
lopes will be part of a worklist on the next call to batch().

Rewrite the scheduler_ramqueue backend.

The initial queue loading in now done by the queue.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d607 1
d615 1
d620 1
a620 1
		tree_xpop(&rq->messages, message->msgid);
d626 1
a626 1
		tree_xpop(&host->batches, message->msgid);
@


1.11
log
@- simplify the scheduler loop logic further, it is ridiculously simple now
  and I don't think we can do much better (at that level) :-p
- always break out of the handler after processing an envelope, this will
  avoid a busy scheduler from not getting a chance to handle SIGTERM/SIGINT
  YES we can now ctrl-c a maaaaad scheduler !

ok eric@@, ok chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.10 2012/06/20 20:45:23 eric Exp $	*/
d5 1
d40 2
d43 4
a46 4
struct ramqueue_host {
	RB_ENTRY(ramqueue_host)		hosttree_entry;
	TAILQ_HEAD(,ramqueue_batch)	batch_queue;
	char				hostname[MAXHOSTNAMELEN];
d48 4
a51 8
struct ramqueue_batch {
	enum delivery_type		type;
	TAILQ_ENTRY(ramqueue_batch)	batch_entry;
	TAILQ_HEAD(,ramqueue_envelope)	envelope_queue;
	struct ramqueue_host	       *rq_host;
	u_int64_t			b_id;
	u_int32_t      			msgid;
	u_int32_t			evpcnt;
d53 5
a57 21
struct ramqueue_envelope {
	TAILQ_ENTRY(ramqueue_envelope)	 queue_entry;
	TAILQ_ENTRY(ramqueue_envelope)	 batchqueue_entry;
	RB_ENTRY(ramqueue_envelope)	 evptree_entry;
	struct ramqueue_batch		*rq_batch;
	struct ramqueue_message		*rq_msg;
	struct ramqueue_host		*rq_host;
	u_int64_t      			 evpid;
	time_t				 sched;
};
struct ramqueue_message {
	RB_ENTRY(ramqueue_message)		msgtree_entry;
	RB_HEAD(evptree, ramqueue_envelope)	evptree;
	u_int32_t				msgid;
	u_int32_t				evpcnt;
};
struct ramqueue {
	RB_HEAD(hosttree, ramqueue_host)	hosttree;
	RB_HEAD(msgtree, ramqueue_message)	msgtree;
	RB_HEAD(offloadtree, ramqueue_envelope)	offloadtree;
	TAILQ_HEAD(,ramqueue_envelope)		queue;
d60 16
a75 10
RB_PROTOTYPE(hosttree,    ramqueue_host, hosttree_entry, ramqueue_host_cmp);
RB_PROTOTYPE(msgtree,     ramqueue_message, msg_entry, ramqueue_msg_cmp);
RB_PROTOTYPE(evptree,     ramqueue_envelope, evp_entry, ramqueue_evp_cmp);
RB_PROTOTYPE(offloadtree, ramqueue_envelope, evp_entry, ramqueue_evp_cmp);

enum ramqueue_iter_type {
	RAMQUEUE_ITER_HOST,
	RAMQUEUE_ITER_BATCH,
	RAMQUEUE_ITER_MESSAGE,
	RAMQUEUE_ITER_QUEUE
d78 9
a86 7
struct ramqueue_iter {
	enum ramqueue_iter_type		type;
	union {
		struct ramqueue_host		*host;
		struct ramqueue_batch		*batch;
		struct ramqueue_message		*message;
	} u;
d89 23
d113 1
a113 37
static int ramqueue_host_cmp(struct ramqueue_host *, struct ramqueue_host *);
static int ramqueue_msg_cmp(struct ramqueue_message *, struct ramqueue_message *);
static int ramqueue_evp_cmp(struct ramqueue_envelope *, struct ramqueue_envelope *);
static struct ramqueue_host *ramqueue_lookup_host(char *);
static struct ramqueue_host *ramqueue_insert_host(char *);
static void ramqueue_remove_host(struct ramqueue_host *);
static struct ramqueue_batch *ramqueue_lookup_batch(struct ramqueue_host *,
    u_int32_t);
static struct ramqueue_batch *ramqueue_insert_batch(struct ramqueue_host *,
    u_int32_t);
static void ramqueue_remove_batch(struct ramqueue_host *, struct ramqueue_batch *);
static struct ramqueue_message *ramqueue_lookup_message(u_int32_t);
static struct ramqueue_message *ramqueue_insert_message(u_int32_t);
static void ramqueue_remove_message(struct ramqueue_message *);

static struct ramqueue_envelope *ramqueue_lookup_envelope(u_int64_t);
static struct ramqueue_envelope *ramqueue_lookup_offload(u_int64_t);


/*NEEDSFIX*/
static int ramqueue_expire(struct envelope *);
static time_t ramqueue_next_schedule(struct scheduler_info *, time_t);

static void  scheduler_ramqueue_init(void);
static int   scheduler_ramqueue_setup(void);
static int   scheduler_ramqueue_next(u_int64_t *, time_t *);
static void  scheduler_ramqueue_insert(struct scheduler_info *);
static void  scheduler_ramqueue_schedule(u_int64_t);
static void  scheduler_ramqueue_remove(u_int64_t);
static void *scheduler_ramqueue_host(char *);
static void *scheduler_ramqueue_message(u_int32_t);
static void *scheduler_ramqueue_batch(u_int64_t);
static void *scheduler_ramqueue_queue(void);
static void  scheduler_ramqueue_close(void *);
static int   scheduler_ramqueue_fetch(void *, u_int64_t *);
static int   scheduler_ramqueue_force(u_int64_t);
static void  scheduler_ramqueue_display(void);
d117 1
a117 2
	scheduler_ramqueue_setup,
	scheduler_ramqueue_next,
d119 8
a128 8
	scheduler_ramqueue_host,
	scheduler_ramqueue_message,
	scheduler_ramqueue_batch,
	scheduler_ramqueue_queue,
	scheduler_ramqueue_close,
	scheduler_ramqueue_fetch,
	scheduler_ramqueue_force,
	scheduler_ramqueue_display
d130 5
a134 1
static struct ramqueue	ramqueue;
d137 1
a137 1
scheduler_ramqueue_display_hosttree(void)
d139 2
a140 17
	struct ramqueue_host		*rq_host;
	struct ramqueue_batch		*rq_batch;
	struct ramqueue_envelope	*rq_evp;

	log_debug("\tscheduler_ramqueue: hosttree display");
	RB_FOREACH(rq_host, hosttree, &ramqueue.hosttree) {
		log_debug("\t\thost: [%p] %s", rq_host, rq_host->hostname);
		TAILQ_FOREACH(rq_batch, &rq_host->batch_queue, batch_entry) {
			log_debug("\t\t\tbatch: [%p] %016x",
			    rq_batch, rq_batch->msgid);
			TAILQ_FOREACH(rq_evp, &rq_batch->envelope_queue,
			    batchqueue_entry) {
				log_debug("\t\t\t\tevpid: [%p] %016"PRIx64,
				    rq_evp, rq_evp->evpid);
			}
		}
	}
d144 1
a144 1
scheduler_ramqueue_display_msgtree(void)
d146 8
a153 2
	struct ramqueue_message		*rq_msg;
	struct ramqueue_envelope	*rq_evp;
d155 5
a159 7
	log_debug("\tscheduler_ramqueue: msgtree display");
	RB_FOREACH(rq_msg, msgtree, &ramqueue.msgtree) {
		log_debug("\t\tmsg: [%p] %016x", rq_msg, rq_msg->msgid);
		RB_FOREACH(rq_evp, evptree, &rq_msg->evptree) {
			log_debug("\t\t\tevp: [%p] %016"PRIx64,
			    rq_evp, rq_evp->evpid);
		}
a160 1
}
d162 3
a164 4
static void
scheduler_ramqueue_display_offloadtree(void)
{
	struct ramqueue_envelope	*rq_evp;
d166 6
a171 4
	log_debug("\tscheduler_ramqueue: offloadtree display");
	RB_FOREACH(rq_evp, offloadtree, &ramqueue.offloadtree) {
		log_debug("\t\t\tevp: [%p] %016"PRIx64,
		    rq_evp, rq_evp->evpid);
a172 1
}
d174 6
a179 9
static void
scheduler_ramqueue_display_queue(void)
{
	struct ramqueue_envelope *rq_evp;

	log_debug("\tscheduler_ramqueue: queue display");
	TAILQ_FOREACH(rq_evp, &ramqueue.queue, queue_entry) {
		log_debug("\t\tevpid: [%p] [batch: %p], %016"PRIx64,
		    rq_evp, rq_evp->rq_batch, rq_evp->evpid);
a180 1
}
d182 8
a189 9
static void
scheduler_ramqueue_display(void)
{
	log_debug("scheduler_ramqueue: display");
	scheduler_ramqueue_display_hosttree();
	scheduler_ramqueue_display_msgtree();
	scheduler_ramqueue_display_offloadtree();
	scheduler_ramqueue_display_queue();
}
d191 4
a194 10
static void
scheduler_ramqueue_init(void)
{
	log_debug("scheduler_ramqueue: init");
	bzero(&ramqueue, sizeof (ramqueue));
	TAILQ_INIT(&ramqueue.queue);
	RB_INIT(&ramqueue.hosttree);
	RB_INIT(&ramqueue.msgtree);
	RB_INIT(&ramqueue.offloadtree);
}
d196 2
a197 19
static int
scheduler_ramqueue_setup(void)
{
	struct envelope		envelope;
	static struct qwalk    *q = NULL;
	u_int64_t	evpid;
	struct scheduler_info	si;

	log_debug("scheduler_ramqueue: load");

	log_info("scheduler_ramqueue: queue loading in progress");
	if (q == NULL)
		q = qwalk_new(0);

	while (qwalk(q, &evpid)) {
		/* the envelope is already in ramqueue, skip */
		if (ramqueue_lookup_envelope(evpid) ||
		    ramqueue_lookup_offload(evpid))
			continue;
d199 8
a206 7
		if (! queue_envelope_load(evpid, &envelope)) {
			log_debug("scheduler_ramqueue: evp -> /corrupt");
			queue_message_corrupt(evpid_to_msgid(evpid));
			continue;
		}
		if (ramqueue_expire(&envelope))
			continue;
d208 1
a208 2
		scheduler_info(&si, &envelope);
		scheduler_ramqueue_insert(&si);
d210 2
a211 7
		log_debug("ramqueue: loading interrupted");
		return (0);
	}
	qwalk_close(q);
	q = NULL;
	log_debug("ramqueue: loading over");
	return (1);
d214 2
a215 2
static int
scheduler_ramqueue_next(u_int64_t *evpid, time_t *sched)
d217 4
a220 1
	struct ramqueue_envelope *rq_evp = NULL;
d222 4
a225 14
	log_debug("scheduler_ramqueue: next");
	TAILQ_FOREACH(rq_evp, &ramqueue.queue, queue_entry) {
		if (rq_evp->rq_batch->type == D_MDA)
			if (env->sc_flags & (SMTPD_MDA_PAUSED|SMTPD_MDA_BUSY))
				continue;
		if (rq_evp->rq_batch->type == D_MTA)
			if (env->sc_flags & (SMTPD_MTA_PAUSED|SMTPD_MTA_BUSY))
				continue;
		if (evpid)
			*evpid = rq_evp->evpid;
		if (sched)
			*sched = rq_evp->sched;
		log_debug("scheduler_ramqueue: next: found");
		return 1;
d227 1
d229 2
a230 62
	log_debug("scheduler_ramqueue: next: nothing schedulable");
	return 0;
}

static void
scheduler_ramqueue_insert(struct scheduler_info *si)
{
	struct ramqueue_host *rq_host;
	struct ramqueue_message *rq_msg;
	struct ramqueue_batch *rq_batch;
	struct ramqueue_envelope *rq_evp, *evp;
	u_int32_t msgid;
	time_t curtm = time(NULL);

	log_debug("scheduler_ramqueue: insert");

	rq_evp = ramqueue_lookup_offload(si->evpid);
	if (rq_evp) {
		rq_msg = rq_evp->rq_msg;
		rq_batch = rq_evp->rq_batch;
		rq_host = rq_evp->rq_host;
		RB_REMOVE(offloadtree, &ramqueue.offloadtree, rq_evp);
	}
	else {
		msgid = evpid_to_msgid(si->evpid);
		rq_msg = ramqueue_lookup_message(msgid);
		if (rq_msg == NULL)
			rq_msg = ramqueue_insert_message(msgid);

		rq_host = ramqueue_lookup_host(si->destination);
		if (rq_host == NULL)
			rq_host = ramqueue_insert_host(si->destination);

		rq_batch = ramqueue_lookup_batch(rq_host, msgid);
		if (rq_batch == NULL)
			rq_batch = ramqueue_insert_batch(rq_host, msgid);
		
		rq_evp = calloc(1, sizeof (*rq_evp));
		if (rq_evp == NULL)
			fatal("calloc");
		rq_evp->evpid = si->evpid;
		rq_batch->evpcnt++;
		rq_msg->evpcnt++;
	}

	rq_evp->sched = ramqueue_next_schedule(si, curtm);
	rq_evp->rq_host = rq_host;
	rq_evp->rq_batch = rq_batch;
	rq_evp->rq_msg = rq_msg;
	RB_INSERT(evptree, &rq_msg->evptree, rq_evp);
	TAILQ_INSERT_TAIL(&rq_batch->envelope_queue, rq_evp,
		    batchqueue_entry);

	/* sorted insert */
	TAILQ_FOREACH(evp, &ramqueue.queue, queue_entry) {
		if (evp->sched >= rq_evp->sched) {
			TAILQ_INSERT_BEFORE(evp, rq_evp, queue_entry);
			break;
		}
	}
	if (evp == NULL)
		TAILQ_INSERT_TAIL(&ramqueue.queue, rq_evp, queue_entry);
d232 1
a232 1
	stat_increment(STATS_RAMQUEUE_ENVELOPE);
d236 1
a236 1
scheduler_ramqueue_schedule(u_int64_t evpid)
d238 2
a239 5
	struct ramqueue_envelope *rq_evp;
	struct ramqueue_message	 *rq_msg;
	struct ramqueue_batch	 *rq_batch;

	log_debug("scheduler_ramqueue: schedule");
d241 1
a241 3
	rq_evp = ramqueue_lookup_envelope(evpid);
	rq_msg = rq_evp->rq_msg;
	rq_batch = rq_evp->rq_batch;
d243 6
a248 4
	/* remove from msg tree, batch queue and linear queue */
	RB_REMOVE(evptree, &rq_msg->evptree, rq_evp);
	TAILQ_REMOVE(&rq_batch->envelope_queue, rq_evp, batchqueue_entry);
	TAILQ_REMOVE(&ramqueue.queue, rq_evp, queue_entry);
d250 1
a250 5
	/* insert into offload tree*/
	RB_INSERT(offloadtree, &ramqueue.offloadtree, rq_evp);

	/* that's one less envelope to process in the ramqueue */
	stat_decrement(STATS_RAMQUEUE_ENVELOPE);
d254 1
a254 1
scheduler_ramqueue_remove(u_int64_t evpid)
d256 3
a258 6
	struct ramqueue_batch *rq_batch;
	struct ramqueue_message *rq_msg;
	struct ramqueue_envelope *rq_evp;
	struct ramqueue_host *rq_host;

	log_debug("scheduler_ramqueue: remove");
d260 3
a262 12
	rq_evp = ramqueue_lookup_offload(evpid);
	if (rq_evp) {
		RB_REMOVE(offloadtree, &ramqueue.offloadtree, rq_evp);
		rq_msg = rq_evp->rq_msg;
		rq_batch = rq_evp->rq_batch;
		rq_host = rq_evp->rq_host;
	}
	else {
		rq_evp = ramqueue_lookup_envelope(evpid);
		rq_msg = rq_evp->rq_msg;
		rq_batch = rq_evp->rq_batch;
		rq_host = rq_evp->rq_host;
d264 3
a266 5
		RB_REMOVE(evptree, &rq_msg->evptree, rq_evp);
		TAILQ_REMOVE(&rq_batch->envelope_queue, rq_evp, batchqueue_entry);
		TAILQ_REMOVE(&ramqueue.queue, rq_evp, queue_entry);
		stat_decrement(STATS_RAMQUEUE_ENVELOPE);
	}
d268 5
a272 6
	rq_batch->evpcnt--;
	rq_msg->evpcnt--;

	/* check if we are the last of a message */
	if (rq_msg->evpcnt == 0) {
		ramqueue_remove_message(rq_msg);
d275 7
a281 4
	/* check if we are the last of a batch */
	if (rq_batch->evpcnt == 0) {
		ramqueue_remove_batch(rq_host, rq_batch);
	}
d283 1
a283 6
	/* check if we are the last of a host */
	if (TAILQ_FIRST(&rq_host->batch_queue) == NULL) {
		ramqueue_remove_host(rq_host);
	}

	free(rq_evp);
d286 2
a287 2
static void *
scheduler_ramqueue_host(char *host)
d289 3
a291 2
	struct ramqueue_iter *iter;
	struct ramqueue_host *rq_host;
d293 3
a295 3
	rq_host = ramqueue_lookup_host(host);
	if (rq_host == NULL)
		return NULL;
d297 3
a299 3
	iter = calloc(1, sizeof *iter);
	if (iter == NULL)
		err(1, "calloc");
d301 1
a301 4
	iter->type = RAMQUEUE_ITER_HOST;
	iter->u.host = rq_host;

	return iter;
d304 2
a305 2
static void *
scheduler_ramqueue_batch(u_int64_t evpid)
d307 32
a338 2
	struct ramqueue_iter *iter;
	struct ramqueue_envelope *rq_evp;
d340 7
a346 3
	rq_evp = ramqueue_lookup_envelope(evpid);
	if (rq_evp == NULL)
		return NULL;
d348 2
a349 3
	iter = calloc(1, sizeof *iter);
	if (iter == NULL)
		err(1, "calloc");
d351 1
a351 4
	iter->type = RAMQUEUE_ITER_BATCH;
	iter->u.batch = rq_evp->rq_batch;

	return iter;
d354 2
a355 2
static void *
scheduler_ramqueue_message(u_int32_t msgid)
d357 26
a382 2
	struct ramqueue_iter *iter;
	struct ramqueue_message *rq_msg;
d384 2
a385 3
	rq_msg = ramqueue_lookup_message(msgid);
	if (rq_msg == NULL)
		return NULL;
d387 14
a400 3
	iter = calloc(1, sizeof *iter);
	if (iter == NULL)
		err(1, "calloc");
d402 35
a436 2
	iter->type = RAMQUEUE_ITER_MESSAGE;
	iter->u.message = rq_msg;
a437 1
	return iter;
d439 2
a440 4
}

static void *
scheduler_ramqueue_queue(void)
d442 3
a444 1
	struct ramqueue_iter *iter;
d446 7
a452 3
	iter = calloc(1, sizeof *iter);
	if (iter == NULL)
		err(1, "calloc");
d454 3
a456 3
	iter->type = RAMQUEUE_ITER_QUEUE;

	return iter;
d460 1
a460 1
scheduler_ramqueue_close(void *hdl)
d462 5
a466 27
	free(hdl);
}

int
scheduler_ramqueue_fetch(void *hdl, u_int64_t *evpid)
{
	struct ramqueue_iter		*iter = hdl;
	struct ramqueue_envelope	*rq_evp;
	struct ramqueue_batch		*rq_batch;

	switch (iter->type) {
	case RAMQUEUE_ITER_HOST:
		rq_batch = TAILQ_FIRST(&iter->u.host->batch_queue);
		if (rq_batch == NULL)
			break;
		rq_evp = TAILQ_FIRST(&rq_batch->envelope_queue);
		if (rq_evp == NULL)
			break;
		*evpid = rq_evp->evpid;
		return 1;

	case RAMQUEUE_ITER_BATCH:
		rq_evp = TAILQ_FIRST(&iter->u.batch->envelope_queue);
		if (rq_evp == NULL)
			break;
		*evpid = rq_evp->evpid;
		return 1;
d468 10
a477 13
	case RAMQUEUE_ITER_MESSAGE:
		rq_evp = RB_ROOT(&iter->u.message->evptree);
		if (rq_evp == NULL)
			break;
		*evpid = rq_evp->evpid;
		return 1;

	case RAMQUEUE_ITER_QUEUE:
		rq_evp = TAILQ_FIRST(&ramqueue.queue);
		if (rq_evp == NULL)
			break;
		*evpid = rq_evp->evpid;
		return 1;
d479 15
a493 17

	return 0;
}

static int
scheduler_ramqueue_force(u_int64_t id)
{
	struct ramqueue_envelope	*rq_evp;
	struct ramqueue_message		*rq_msg;
	int	ret;

	/* schedule *all* */
	if (id == 0) {
		ret = 0;
		TAILQ_FOREACH(rq_evp, &ramqueue.queue, queue_entry) {
			rq_evp->sched = 0;
			ret++;
d495 2
a496 1
		return ret;
a497 26

	/* scheduling by evpid */
	if (id > 0xffffffffL) {
		rq_evp = ramqueue_lookup_envelope(id);
		if (rq_evp == NULL)
			return 0;

		rq_evp->sched = 0;
		TAILQ_REMOVE(&ramqueue.queue, rq_evp, queue_entry);
		TAILQ_INSERT_HEAD(&ramqueue.queue, rq_evp, queue_entry);
		return 1;
	}

	rq_msg = ramqueue_lookup_message(id);
	if (rq_msg == NULL)
		return 0;

	/* scheduling by msgid */
	ret = 0;
	RB_FOREACH(rq_evp, evptree, &rq_msg->evptree) {
		rq_evp->sched = 0;
		TAILQ_REMOVE(&ramqueue.queue, rq_evp, queue_entry);
		TAILQ_INSERT_HEAD(&ramqueue.queue, rq_evp, queue_entry);
		ret++;
	}
	return ret;
d500 2
a501 2
static struct ramqueue_host *
ramqueue_lookup_host(char *host)
d503 1
a503 1
	struct ramqueue_host hostkey;
d505 7
a511 2
	strlcpy(hostkey.hostname, host, sizeof(hostkey.hostname));
	return RB_FIND(hosttree, &ramqueue.hosttree, &hostkey);
d514 2
a515 2
static struct ramqueue_message *
ramqueue_lookup_message(u_int32_t msgid)
d517 1
a517 1
	struct ramqueue_message msgkey;
d519 6
a524 2
	msgkey.msgid = msgid;
	return RB_FIND(msgtree, &ramqueue.msgtree, &msgkey);
d527 2
a528 2
static struct ramqueue_envelope *
ramqueue_lookup_offload(u_int64_t evpid)
d530 1
a530 1
	struct ramqueue_envelope evpkey;
d532 8
a539 2
	evpkey.evpid = evpid;
	return RB_FIND(offloadtree, &ramqueue.offloadtree, &evpkey);
d542 2
a543 2
static struct ramqueue_envelope *
ramqueue_lookup_envelope(u_int64_t evpid)
d545 6
a550 2
	struct ramqueue_message *rq_msg;
	struct ramqueue_envelope evpkey;
d552 24
a575 16
	rq_msg = ramqueue_lookup_message(evpid_to_msgid(evpid));
	if (rq_msg == NULL)
		return NULL;

	evpkey.evpid = evpid;
	return RB_FIND(evptree, &rq_msg->evptree, &evpkey);
}

static struct ramqueue_batch *
ramqueue_lookup_batch(struct ramqueue_host *rq_host, u_int32_t msgid)
{
	struct ramqueue_batch *rq_batch;

	TAILQ_FOREACH(rq_batch, &rq_host->batch_queue, batch_entry) {
		if (rq_batch->msgid == msgid)
			return rq_batch;
d578 5
a582 54
	return NULL;
}

static int
ramqueue_expire(struct envelope *envelope)
{
	struct envelope bounce;
	struct scheduler_info	si;
	time_t	curtm;

	curtm = time(NULL);
	if (curtm - envelope->creation >= envelope->expire) {
		envelope_set_errormsg(envelope,
		    "message expired after sitting in queue for %d days",
		    envelope->expire / 60 / 60 / 24);
		bounce_record_message(envelope, &bounce);

		scheduler_info(&si, &bounce);
		scheduler_ramqueue_insert(&si);

		log_debug("#### %s: queue_envelope_delete: %016" PRIx64,
		    __func__, envelope->id);
		queue_envelope_delete(envelope);
		return 1;
	}
	return 0;
}

static time_t
ramqueue_next_schedule(struct scheduler_info *si, time_t curtm)
{
	time_t delay;

	if (si->lasttry == 0)
		return curtm;

	delay = SMTPD_QUEUE_MAXINTERVAL;
	
	if (si->type == D_MDA ||
	    si->type == D_BOUNCE) {
		if (si->retry < 5)
			return curtm;
			
		if (si->retry < 15)
			delay = (si->retry * 60) + arc4random_uniform(60);
	}

	if (si->type == D_MTA) {
		if (si->retry < 3)
			delay = SMTPD_QUEUE_INTERVAL;
		else if (si->retry <= 7) {
			delay = SMTPD_QUEUE_INTERVAL * (1 << (si->retry - 3));
			if (delay > SMTPD_QUEUE_MAXINTERVAL)
				delay = SMTPD_QUEUE_MAXINTERVAL;
d584 7
d593 3
a595 2
	if (curtm >= si->lasttry + delay)
		return curtm;
d597 2
a598 1
	return curtm + delay;
d601 2
a602 2
static struct ramqueue_message *
ramqueue_insert_message(u_int32_t msgid)
d604 3
a606 1
	struct ramqueue_message *rq_msg;
d608 2
a609 7
	rq_msg = calloc(1, sizeof (*rq_msg));
	if (rq_msg == NULL)
		fatal("calloc");
	rq_msg->msgid = msgid;
	RB_INSERT(msgtree, &ramqueue.msgtree, rq_msg);
	RB_INIT(&rq_msg->evptree);
	stat_increment(STATS_RAMQUEUE_MESSAGE);
d611 4
a614 2
	return rq_msg;
}
d616 5
a620 4
static struct ramqueue_host *
ramqueue_insert_host(char *host)
{
	struct ramqueue_host *rq_host;
d622 10
a631 9
	rq_host = calloc(1, sizeof (*rq_host));
	if (rq_host == NULL)
		fatal("calloc");
	strlcpy(rq_host->hostname, host, sizeof(rq_host->hostname));
	TAILQ_INIT(&rq_host->batch_queue);
	RB_INSERT(hosttree, &ramqueue.hosttree, rq_host);
	stat_increment(STATS_RAMQUEUE_HOST);

	return rq_host;
d634 2
a635 2
static struct ramqueue_batch *
ramqueue_insert_batch(struct ramqueue_host *rq_host, u_int32_t msgid)
d637 2
a638 1
	struct ramqueue_batch *rq_batch;
d640 1
a640 6
	rq_batch = calloc(1, sizeof (*rq_batch));
	if (rq_batch == NULL)
		fatal("calloc");
	rq_batch->b_id = generate_uid();
	rq_batch->rq_host = rq_host;
	rq_batch->msgid = msgid;
d642 6
a647 2
	TAILQ_INIT(&rq_batch->envelope_queue);
	TAILQ_INSERT_TAIL(&rq_host->batch_queue, rq_batch, batch_entry);
d649 4
a652 1
	stat_increment(STATS_RAMQUEUE_BATCH);
d654 6
a659 2
	return rq_batch;
}
d661 1
a661 6
static void
ramqueue_remove_host(struct ramqueue_host *rq_host)
{
	RB_REMOVE(hosttree, &ramqueue.hosttree, rq_host);
	free(rq_host);
	stat_decrement(STATS_RAMQUEUE_HOST);
d665 1
a665 1
ramqueue_remove_message(struct ramqueue_message *rq_msg)
d667 6
a672 4
	RB_REMOVE(msgtree, &ramqueue.msgtree, rq_msg);
	free(rq_msg);
	stat_decrement(STATS_RAMQUEUE_MESSAGE);
}
d674 13
d688 22
a709 7
static void
ramqueue_remove_batch(struct ramqueue_host *rq_host,
    struct ramqueue_batch *rq_batch)
{
	TAILQ_REMOVE(&rq_host->batch_queue, rq_batch, batch_entry);
	free(rq_batch);
	stat_decrement(STATS_RAMQUEUE_BATCH);
d713 1
a713 1
ramqueue_host_cmp(struct ramqueue_host *h1, struct ramqueue_host *h2)
d715 1
a715 1
	return strcmp(h1->hostname, h2->hostname);
d718 4
d723 2
a724 4
static int
ramqueue_msg_cmp(struct ramqueue_message *m1, struct ramqueue_message *m2)
{
	return (m1->msgid < m2->msgid ? -1 : m1->msgid > m2->msgid);
d727 2
a728 2
static int
ramqueue_evp_cmp(struct ramqueue_envelope *e1, struct ramqueue_envelope *e2)
d730 8
a737 1
	return (e1->evpid < e2->evpid ? -1 : e1->evpid > e2->evpid);
d739 1
a739 5

RB_GENERATE(hosttree,    ramqueue_host, hosttree_entry, ramqueue_host_cmp);
RB_GENERATE(msgtree,     ramqueue_message, msgtree_entry, ramqueue_msg_cmp);
RB_GENERATE(evptree,     ramqueue_envelope, evptree_entry, ramqueue_evp_cmp);
RB_GENERATE(offloadtree, ramqueue_envelope, evptree_entry, ramqueue_evp_cmp);
@


1.10
log
@Finally get rid of the queue_kind enum in the queue API. Keep that
internally in fsqueue backend for now, and let the fsqueue_message()
and fsqueue_envelope() dispatchers do the right thing.

Based on a diff by chl@@

ok chl@@ gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.9 2012/06/17 15:17:08 gilles Exp $	*/
d119 1
a119 1
static int ramqueue_expire(struct envelope *, time_t);
d123 1
a123 1
static int   scheduler_ramqueue_setup(time_t, time_t);
d239 1
a239 1
scheduler_ramqueue_setup(time_t curtm, time_t nsched)
a243 1
	time_t		sched;
d263 1
a263 1
		if (ramqueue_expire(&envelope, curtm))
a267 3
		
		if (! scheduler_ramqueue_next(&evpid, &sched))
			continue;
d269 2
a270 7
		if (sched <= nsched)
			nsched = sched;

		if (nsched <= curtm) {
			log_debug("ramqueue: loading interrupted");
			return (0);
		}
d663 1
a663 1
ramqueue_expire(struct envelope *envelope, time_t curtm)
d667 1
d669 1
@


1.9
log
@- introduce struct scheduler_info and the scheduler_info() function to fill
  a struct scheduler_info given a struct envelope
- adapt the scheduler API and the scheduler_ramqueue backend to use the new
  struct scheduler_info instead of a struct envelope

idea discussed with eric@@ and chl@@, mechanical diff, no functional change
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.8 2012/05/25 13:51:42 chl Exp $	*/
d251 1
a251 1
		q = qwalk_new(Q_QUEUE, 0);
d259 1
a259 1
		if (! queue_envelope_load(Q_QUEUE, evpid, &envelope)) {
d261 1
a261 1
			queue_message_corrupt(Q_QUEUE, evpid_to_msgid(evpid));
d688 1
a688 1
		queue_envelope_delete(Q_QUEUE, envelope);
@


1.8
log
@add missing header needed by time()

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.7 2012/04/15 12:12:35 chl Exp $	*/
d120 1
a120 1
static time_t ramqueue_next_schedule(struct envelope *, time_t);
d125 1
a125 1
static void  scheduler_ramqueue_insert(struct envelope *);
d245 1
d266 3
a268 1
		scheduler_ramqueue_insert(&envelope);
d313 1
a313 1
scheduler_ramqueue_insert(struct envelope *envelope)
d324 1
a324 1
	rq_evp = ramqueue_lookup_offload(envelope->id);
d332 1
a332 1
		msgid = evpid_to_msgid(envelope->id);
d337 1
a337 1
		rq_host = ramqueue_lookup_host(envelope->dest.domain);
d339 1
a339 1
			rq_host = ramqueue_insert_host(envelope->dest.domain);
d348 1
a348 1
		rq_evp->evpid = envelope->id;
d353 1
a353 1
	rq_evp->sched = ramqueue_next_schedule(envelope, curtm);
d675 1
d682 4
a685 1
		scheduler_ramqueue_insert(&bounce);
d695 1
a695 1
ramqueue_next_schedule(struct envelope *envelope, time_t curtm)
d699 1
a699 1
	if (envelope->lasttry == 0)
d704 3
a706 3
	if (envelope->type == D_MDA ||
	    envelope->type == D_BOUNCE) {
		if (envelope->retry < 5)
d709 2
a710 2
		if (envelope->retry < 15)
			delay = (envelope->retry * 60) + arc4random_uniform(60);
d713 2
a714 2
	if (envelope->type == D_MTA) {
		if (envelope->retry < 3)
d716 2
a717 2
		else if (envelope->retry <= 7) {
			delay = SMTPD_QUEUE_INTERVAL * (1 << (envelope->retry - 3));
d723 1
a723 1
	if (curtm >= envelope->lasttry + delay)
@


1.7
log
@Remove dead assignments and newly created unused variables.

Found by LLVM/Clang Static Analyzer.

ok gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.6 2012/03/13 23:07:58 gilles Exp $	*/
d34 1
@


1.6
log
@When moving back envelope from offload tree to msg tree ... remove the
envelope from offload tree not msg tree, this corrupts the ramqueue in
ways that I couldn't imagine before wasting so many hours tracking it.

Fixes crash on my server under load, no crash after about 20K mails
processed from up to 150 concurrent sessions.
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.5 2012/03/07 22:54:49 gilles Exp $	*/
a375 1
	struct ramqueue_host	 *rq_host;
a381 1
	rq_host = rq_evp->rq_host;
@


1.5
log
@various reliability fixes:

- prevent queue_fsqueue from fatal() when it hits an ENOENT, it can happen
- change a bit the scheduler API to simplify it, fix runner accordingly

- we can't remove msg/batch from ramqueue while envelope is offloaded or
  it will cause a double, instead we add refcnt to both msg/batch and
  only free them when it hits 0
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.4 2012/01/31 21:05:26 gilles Exp $	*/
d319 1
d325 1
a325 1
		RB_REMOVE(evptree, &rq_msg->evptree, rq_evp);
@


1.4
log
@fix an issue observed this week-end while flooding ajacoutot@@ :

we keep track of available fd's to prevent scheduling of messages if we
know that we are going to fail. however, since the envelope is not
removed from the scheduler, it will be rescheduled right away leading to
a busy loop in the scheduler. we know flag the mda/mta processes as BUSY
and do not schedule envelopes that target a BUSY process.

also, fix a potential bug that could lead to a use after free when doing
a batch/message/host traversal of schedulable envelopes.

while at it fix misuse of env->sc_opts as env->sc_flags, was not really
causing any issue as the misuse was constant ...
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.3 2012/01/30 10:02:55 chl Exp $	*/
a41 1
	u_int64_t			h_id;
d48 1
a48 1
	u_int64_t			h_id;
d51 1
d67 1
d72 1
d76 4
a79 3
RB_PROTOTYPE(hosttree, ramqueue_host, hosttree_entry, ramqueue_host_cmp);
RB_PROTOTYPE(msgtree,  ramqueue_message, msg_entry, ramqueue_msg_cmp);
RB_PROTOTYPE(evptree,  ramqueue_envelope, evp_entry, ramqueue_evp_cmp);
d114 1
d125 2
a126 1
static void  scheduler_ramqueue_remove(void *, u_int64_t);
d133 1
a133 1
static int   scheduler_ramqueue_schedule(u_int64_t);
d141 1
d149 1
a149 1
	scheduler_ramqueue_schedule,
d193 12
d222 1
d234 1
d252 5
d319 16
a334 4
	msgid = evpid_to_msgid(envelope->id);
	rq_msg = ramqueue_lookup_message(msgid);
	if (rq_msg == NULL)
		rq_msg = ramqueue_insert_message(msgid);
d336 11
a346 3
	rq_host = ramqueue_lookup_host(envelope->dest.domain);
	if (rq_host == NULL)
		rq_host = ramqueue_insert_host(envelope->dest.domain);
a347 8
	rq_batch = ramqueue_lookup_batch(rq_host, msgid);
	if (rq_batch == NULL)
		rq_batch = ramqueue_insert_batch(rq_host, msgid);
		
	rq_evp = calloc(1, sizeof (*rq_evp));
	if (rq_evp == NULL)
		fatal("calloc");
	rq_evp->evpid = envelope->id;
a351 1

d354 1
a354 1
	    batchqueue_entry);
d370 1
a370 1
scheduler_ramqueue_remove(void *hdl, u_int64_t evpid)
a371 3
	struct ramqueue_iter *iter = hdl;
	struct ramqueue_batch *rq_batch;
	struct ramqueue_message *rq_msg;
d373 5
a377 1
	struct ramqueue_host *rq_host;
a378 1
	log_debug("scheduler_ramqueue: remove");
d384 1
d388 5
d394 1
d396 9
d406 17
a422 7
	/* check if we are the last of a batch */
	if (TAILQ_FIRST(&rq_batch->envelope_queue) == NULL) {
		ramqueue_remove_batch(rq_host, rq_batch);
		if (iter != NULL && iter->type == RAMQUEUE_ITER_BATCH) {
			log_debug("scheduler_ramqueue_remove: batch removed");
			iter->u.batch = NULL;
		}
d425 3
d429 1
a429 1
	if (RB_ROOT(&rq_msg->evptree) == NULL) {
d431 5
a435 4
		if (iter != NULL && iter->type == RAMQUEUE_ITER_MESSAGE) {
			log_debug("scheduler_ramqueue_remove: message removed");
			iter->u.message = NULL;
		}
a440 4
		if (iter != NULL && iter->type == RAMQUEUE_ITER_HOST) {
			log_debug("scheduler_ramqueue_remove: host removed");
			iter->u.host = NULL;
		}
d535 1
a535 3
	case RAMQUEUE_ITER_HOST: {
		if (iter->u.host == NULL)
			return 0;
a543 1
	}
a545 2
		if (iter->u.batch == NULL)
			return 0;
a552 2
		if (iter->u.message == NULL)
			return 0;
d571 1
a571 1
scheduler_ramqueue_schedule(u_int64_t id)
d633 9
a745 1
	rq_host->h_id = generate_uid();
d763 1
d819 4
a822 3
RB_GENERATE(hosttree, ramqueue_host, hosttree_entry, ramqueue_host_cmp);
RB_GENERATE(msgtree, ramqueue_message, msgtree_entry, ramqueue_msg_cmp);
RB_GENERATE(evptree, ramqueue_envelope, evptree_entry, ramqueue_evp_cmp);
@


1.3
log
@Add missing header needed by PRI format string

ok eric@@ gilles@@
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.2 2012/01/28 16:50:02 gilles Exp $	*/
d121 1
a121 1
static void  scheduler_ramqueue_remove(u_int64_t);
d266 1
a266 1
			if (env->sc_opts & SMTPD_MDA_PAUSED)
d269 1
a269 1
			if (env->sc_opts & SMTPD_MTA_PAUSED)
d334 1
a334 1
scheduler_ramqueue_remove(u_int64_t evpid)
d336 1
d353 1
d355 1
a355 1
	if (TAILQ_FIRST(&rq_batch->envelope_queue) == NULL)
d357 5
d364 1
a364 1
	if (RB_ROOT(&rq_msg->evptree) == NULL)
d366 5
d373 1
a373 1
	if (TAILQ_FIRST(&rq_host->batch_queue) == NULL)
d375 5
d442 1
d474 2
d487 2
d496 2
@


1.2
log
@add optional display handler to scheduler_backend, if not NULL the handler
will be called for each iteration of the runner

implement a display handler for scheduler_ramqueue to display the entire
ramqueue (hosttree, msgtree and linear queue) in log_debug
@
text
@d1 1
a1 1
/*	$OpenBSD: scheduler_ramqueue.c,v 1.1 2012/01/28 11:33:07 gilles Exp $	*/
d30 1
@


1.1
log
@- introduce the scheduler_backend API
- introduce the scheduler_ramqueue backend
- remove all occurences of ramqueue outside of the ramqueue backend
- teach runner how to use the new API

it is now possible to write custom schedulers !

ok eric@@, ok chl@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d128 1
d142 2
a143 1
	scheduler_ramqueue_schedule
d147 58
d316 1
a316 3
	rq_evp->rq_msg = rq_msg;

	TAILQ_INSERT_TAIL(&rq_evp->rq_batch->envelope_queue, rq_evp,
@

