head	1.7;
access;
symbols
	OPENBSD_5_5:1.6.0.46
	OPENBSD_5_5_BASE:1.6
	OPENBSD_5_4:1.6.0.42
	OPENBSD_5_4_BASE:1.6
	OPENBSD_5_3:1.6.0.40
	OPENBSD_5_3_BASE:1.6
	OPENBSD_5_2:1.6.0.38
	OPENBSD_5_2_BASE:1.6
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.36
	OPENBSD_5_0:1.6.0.34
	OPENBSD_5_0_BASE:1.6
	OPENBSD_4_9:1.6.0.32
	OPENBSD_4_9_BASE:1.6
	OPENBSD_4_8:1.6.0.30
	OPENBSD_4_8_BASE:1.6
	OPENBSD_4_7:1.6.0.26
	OPENBSD_4_7_BASE:1.6
	OPENBSD_4_6:1.6.0.28
	OPENBSD_4_6_BASE:1.6
	OPENBSD_4_5:1.6.0.24
	OPENBSD_4_5_BASE:1.6
	OPENBSD_4_4:1.6.0.22
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.6.0.20
	OPENBSD_4_3_BASE:1.6
	OPENBSD_4_2:1.6.0.18
	OPENBSD_4_2_BASE:1.6
	OPENBSD_4_1:1.6.0.16
	OPENBSD_4_1_BASE:1.6
	OPENBSD_4_0:1.6.0.14
	OPENBSD_4_0_BASE:1.6
	OPENBSD_3_9:1.6.0.12
	OPENBSD_3_9_BASE:1.6
	OPENBSD_3_8:1.6.0.10
	OPENBSD_3_8_BASE:1.6
	OPENBSD_3_7:1.6.0.8
	OPENBSD_3_7_BASE:1.6
	OPENBSD_3_6:1.6.0.6
	OPENBSD_3_6_BASE:1.6
	OPENBSD_3_5:1.6.0.4
	OPENBSD_3_5_BASE:1.6
	apache_1_3_29-mod_ssl_2_8_16:1.1.1.4
	OPENBSD_3_4:1.6.0.2
	OPENBSD_3_4_BASE:1.6
	apache_1_3_28-mod_ssl_2_8_15:1.1.1.4
	OPENBSD_3_3:1.5.0.6
	OPENBSD_3_3_BASE:1.5
	apache_1_3_27-mod_ssl_2_8_12:1.1.1.3
	apache_1_3_27:1.1.1.3
	OPENBSD_3_2:1.5.0.4
	OPENBSD_3_2_BASE:1.5
	apache_1_3_26:1.1.1.3
	OPENBSD_3_1:1.5.0.2
	OPENBSD_3_1_BASE:1.5
	OPENBSD_3_0:1.4.0.4
	OPENBSD_3_0_BASE:1.4
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_9:1.4.0.2
	OPENBSD_2_8:1.3.0.6
	OPENBSD_2_8_BASE:1.3
	OPENBSD_2_7:1.3.0.4
	OPENBSD_2_7_BASE:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2
	OPENBSD_2_4:1.1.1.2.0.2
	OPENBSD_2_4_BASE:1.1.1.2
	apache_1_3_2:1.1.1.2
	OPENBSD_2_3:1.1.1.1.0.2
	OPENBSD_2_3_BASE:1.1.1.1
	apache:1.1.1
	apache_1_2_6:1.1.1;
locks; strict;
comment	@# @;


1.7
date	2014.04.22.14.47.24;	author henning;	state dead;
branches;
next	1.6;

1.6
date	2003.08.21.13.11.32;	author henning;	state Exp;
branches;
next	1.5;

1.5
date	2002.02.12.07.56.47;	author beck;	state Exp;
branches;
next	1.4;

1.4
date	2001.03.29.10.21.30;	author beck;	state Exp;
branches;
next	1.3;

1.3
date	99.09.29.06.29.01;	author beck;	state Exp;
branches;
next	1.2;

1.2
date	99.03.01.01.05.12;	author beck;	state Exp;
branches;
next	1.1;

1.1
date	98.03.25.07.08.36;	author beck;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	98.03.25.07.08.36;	author beck;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	98.10.01.17.20.13;	author beck;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2002.07.19.21.28.07;	author henning;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2003.08.21.12.53.31;	author henning;	state Exp;
branches;
next	;


desc
@@


1.7
log
@this commit is really florian@@'s, since he's the one who made removal
of our forked apache possible by his work on nginx and slowcgi, but he
doesn't want it - so it is my pleasure to tedu it. I spent so much work
on chroot in it 10 years ago - and am very happy to see it go now, nginx
is a far better choice today.
Bye bye, Apache, won't miss you.
@
text
@<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="generator" content="HTML Tidy, see www.w3.org" />
    <meta name="description"
    content="Some 'how to' tips for the Apache httpd server" />
    <meta name="keywords"
    content="apache,redirect,robots,rotate,logfiles" />

    <title>Apache HOWTO documentation</title>
  </head>
  <!-- Background white, links blue (unvisited), navy (visited), red (active) -->

  <body bgcolor="#FFFFFF" text="#000000" link="#0000FF"
  vlink="#000080" alink="#FF0000">
        <div align="CENTER">
      <img src="../images/sub.gif" alt="[APACHE DOCUMENTATION]" /> 

      <h3>Apache HTTP Server Version 1.3</h3>
    </div>


    <h1 align="CENTER">Apache HOWTO documentation</h1>
    How to: 

    <ul>
      <li><a href="#redirect">redirect an entire server or
      directory to a single URL</a></li>

      <li><a href="#logreset">reset your log files</a></li>

      <li><a href="#stoprob">stop/restrict robots</a></li>

      <li><a href="#proxyssl">proxy SSL requests <em>through</em>
      your non-SSL server</a></li>
    </ul>
    <hr />

    <h2><a id="redirect" name="redirect">How to redirect an entire
    server or directory to a single URL</a></h2>

    <p>There are two chief ways to redirect all requests for an
    entire server to a single location: one which requires the use
    of <code>mod_rewrite</code>, and another which uses a CGI
    script.</p>

    <p>First: if all you need to do is migrate a server from one
    name to another, simply use the <code>Redirect</code>
    directive, as supplied by <code>mod_alias</code>:</p>

    <blockquote>
<pre>
  Redirect / http://www.apache.org/
</pre>
    </blockquote>

    <p>Since <code>Redirect</code> will forward along the complete
    path, however, it may not be appropriate - for example, when
    the directory structure has changed after the move, and you
    simply want to direct people to the home page.</p>

    <p>The best option is to use the standard Apache module
    <code>mod_rewrite</code>. If that module is compiled in, the
    following lines</p>

    <blockquote>
<pre>
RewriteEngine On
RewriteRule /.* http://www.apache.org/ [R]
</pre>
    </blockquote>
    will send an HTTP 302 Redirect back to the client, and no
    matter what they gave in the original URL, they'll be sent to
    "http://www.apache.org/". 

    <p>The second option is to set up a <code>ScriptAlias</code>
    pointing to a <strong>CGI script</strong> which outputs a 301
    or 302 status and the location of the other server.</p>

    <p>By using a <strong>CGI script</strong> you can intercept
    various requests and treat them specially, <em>e.g.</em>, you
    might want to intercept <strong>POST</strong> requests, so that
    the client isn't redirected to a script on the other server
    which expects POST information (a redirect will lose the POST
    information.) You might also want to use a CGI script if you
    don't want to compile mod_rewrite into your server.</p>

    <p>Here's how to redirect all requests to a script... In the
    server configuration file,</p>

    <blockquote>
<pre>
ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script/
</pre>
    </blockquote>
    and here's a simple perl script to redirect requests: 

    <blockquote>
<pre>
#!/usr/local/bin/perl

print "Status: 302 Moved Temporarily\r\n" .
      "Location: http://www.some.where.else.com/\r\n" .
      "\r\n";

</pre>
    </blockquote>
    <hr />

    <h2><a id="logreset" name="logreset">How to reset your log
    files</a></h2>

    <p>Sooner or later, you'll want to reset your log files
    (access_log and error_log) because they are too big, or full of
    old information you don't need.</p>

    <p><code>access.log</code> typically grows by 1Mb for each
    10,000 requests.</p>

    <p>Most people's first attempt at replacing the logfile is to
    just move the logfile or remove the logfile. This doesn't
    work.</p>

    <p>Apache will continue writing to the logfile at the same
    offset as before the logfile moved. This results in a new
    logfile being created which is just as big as the old one, but
    it now contains thousands (or millions) of null characters.</p>

    <p>The correct procedure is to move the logfile, then signal
    Apache to tell it to reopen the logfiles.</p>

    <p>Apache is signaled using the <strong>SIGHUP</strong> (-1)
    signal. <em>e.g.</em></p>

    <blockquote>
      <code>mv access_log access_log.old<br />
       kill -1 `cat httpd.pid`</code>
    </blockquote>

    <p>Note: <code>httpd.pid</code> is a file containing the
    <strong>p</strong>rocess <strong>id</strong> of the Apache
    httpd daemon, Apache saves this in the same directory as the
    log files.</p>

    <p>Many people use this method to replace (and backup) their
    logfiles on a nightly or weekly basis.</p>
    <hr />

    <h2><a id="stoprob" name="stoprob">How to stop or restrict
    robots</a></h2>

    <p>Ever wondered why so many clients are interested in a file
    called <code>robots.txt</code> which you don't have, and never
    did have?</p>

    <p>These clients are called <strong>robots</strong> (also known
    as crawlers, spiders and other cute names) - special automated
    clients which wander around the web looking for interesting
    resources.</p>

    <p>Most robots are used to generate some kind of <em>web
    index</em> which is then used by a <em>search engine</em> to
    help locate information.</p>

    <p><code>robots.txt</code> provides a means to request that
    robots limit their activities at the site, or more often than
    not, to leave the site alone.</p>

    <p>When the first robots were developed, they had a bad
    reputation for sending hundreds/thousands of requests to each
    site, often resulting in the site being overloaded. Things have
    improved dramatically since then, thanks to <a
    href="http://www.robotstxt.org/wc/guidelines.html">
    Guidelines for Robot Writers</a>, but even so, some robots may
    exhibit unfriendly behavior which the webmaster isn't willing
    to tolerate, and will want to stop.</p>

    <p>Another reason some webmasters want to block access to
    robots, is to stop them indexing dynamic information. Many
    search engines will use the data collected from your pages for
    months to come - not much use if you're serving stock quotes,
    news, weather reports or anything else that will be stale by
    the time people find it in a search engine.</p>

    <p>If you decide to exclude robots completely, or just limit
    the areas in which they can roam, create a
    <code>robots.txt</code> file; refer to the <a
    href="http://www.robotstxt.org/wc/robots.html">
    robot information pages</a> provided by Martijn Koster for the
    syntax.</p>
    <hr />

    <h2><a id="proxyssl" name="proxyssl">How to proxy SSL requests
    <em>through</em> your non-SSL Apache server</a><br />
     <small>(<em>submitted by David Sedlock</em>)</small></h2>

    <p>SSL uses port 443 for requests for secure pages. If your
    browser just sits there for a long time when you attempt to
    access a secure page over your Apache proxy, then the proxy may
    not be configured to handle SSL. You need to instruct Apache to
    listen on port 443 in addition to any of the ports on which it
    is already listening:</p>
<pre>
    Listen 80
    Listen 443
</pre>

    <p>Then set the security proxy in your browser to 443. That
    might be it!</p>

    <p>If your proxy is sending requests to another proxy, then you
    may have to set the directive ProxyRemote differently. Here are
    my settings:</p>
<pre>
    ProxyRemote http://nicklas:80/ http://proxy.mayn.franken.de:8080
    ProxyRemote http://nicklas:443/ http://proxy.mayn.franken.de:443
</pre>

    <p>Requests on port 80 of my proxy <samp>nicklas</samp> are
    forwarded to <samp>proxy.mayn.franken.de:8080</samp>, while
    requests on port 443 are forwarded to
    <samp>proxy.mayn.franken.de:443</samp>. If the remote proxy is
    not set up to handle port 443, then the last directive can be
    left out. SSL requests will only go over the first proxy.</p>

    <p>Note that your Apache does NOT have to be set up to serve
    secure pages with SSL. Proxying SSL is a different thing from
    using it.</p>
        <hr />

    <h3 align="CENTER">Apache HTTP Server Version 1.3</h3>
    <a href="./"><img src="../images/index.gif" alt="Index" /></a>
    <a href="../"><img src="../images/home.gif" alt="Home" /></a>

  </body>
</html>

@


1.6
log
@merge
@
text
@@


1.5
log
@Apache 1.3.23+mod_ssl-2.8.6-1.3.23 merge
@
text
@d175 1
a175 1
    href="http://info.webcrawler.com/mak/projects/robots/guidelines.html">
d190 1
a190 1
    href="http://info.webcrawler.com/mak/projects/robots/robots.html">
@


1.4
log
@Apache 1.3.19+mod_ssl 2.8.1 merge - also adds shared build of mod_headers
and mod_expire
@
text
@d1 2
a2 47
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
<META NAME="description"
 CONTENT="Some 'how to' tips for the Apache httpd server">
<META NAME="keywords" CONTENT="apache,redirect,robots,rotate,logfiles">
<TITLE>Apache HOWTO documentation</TITLE>
</HEAD>

<!-- Background white, links blue (unvisited), navy (visited), red (active) -->
<BODY
 BGCOLOR="#FFFFFF"
 TEXT="#000000"
 LINK="#0000FF"
 VLINK="#000080"
 ALINK="#FF0000"
>
<DIV ALIGN="CENTER">
 <IMG SRC="../images/sub.gif" ALT="[APACHE DOCUMENTATION]">
 <H3>
  Apache HTTP Server Version 1.3
 </H3>
</DIV>

<H1 ALIGN="CENTER">Apache HOWTO documentation</H1>

How to:
<UL>
<LI><A HREF="#redirect">redirect an entire server or directory to a single
    URL</A>
<LI><A HREF="#logreset">reset your log files</A>
<LI><A HREF="#stoprob">stop/restrict robots</A>
<LI><A HREF="#proxyssl">proxy SSL requests <EM>through</EM> your non-SSL
    server</A>
</UL>

<HR>
<H2><A NAME="redirect">How to redirect an entire server or directory to a
single URL</A></H2>

<P>There are two chief ways to redirect all requests for an entire
server to a single location: one which requires the use of
<CODE>mod_rewrite</CODE>, and another which uses a CGI script.

<P>First: if all you need to do is migrate a server from one name to
another, simply use the <CODE>Redirect</CODE> directive, as supplied
by <CODE>mod_alias</CODE>:
d4 39
a42 3
<BLOCKQUOTE><PRE>
  Redirect / http://www.apache.org/
</PRE></BLOCKQUOTE>
d44 4
a47 8
<P>Since <CODE>Redirect</CODE> will forward along the complete path,
however, it may not be appropriate - for example, when the directory
structure has changed after the move, and you simply want to direct people
to the home page.

<P>The best option is to use the standard Apache module
<CODE>mod_rewrite</CODE>.
If that module is compiled in, the following lines
d49 3
a51 3
<BLOCKQUOTE><PRE>RewriteEngine On
RewriteRule /.* http://www.apache.org/ [R]
</PRE></BLOCKQUOTE>
d53 5
a57 22
will send an HTTP 302 Redirect back to the client, and no matter
what they gave in the original URL, they'll be sent to
"http://www.apache.org/".

<p>The second option is to set up a <CODE>ScriptAlias</CODE> pointing to
a <STRONG>CGI script</STRONG> which outputs a 301 or 302 status and the
location
of the other server.</P>

<P>By using a <STRONG>CGI script</STRONG> you can intercept various requests
and
treat them specially, <EM>e.g.</EM>, you might want to intercept
<STRONG>POST</STRONG>
requests, so that the client isn't redirected to a script on the other
server which expects POST information (a redirect will lose the POST
information.) You might also want to use a CGI script if you don't
want to compile mod_rewrite into your server.

<P>Here's how to redirect all requests to a script... In the server
configuration file,
<BLOCKQUOTE><PRE>ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script/</PRE>
</BLOCKQUOTE>
d59 40
a98 1
and here's a simple perl script to redirect requests:
d100 2
a101 1
<BLOCKQUOTE><PRE>
d108 101
a208 3
</PRE></BLOCKQUOTE>

<HR>
d210 2
a211 1
<H2><A NAME="logreset">How to reset your log files</A></H2>
d213 4
a216 93
<P>Sooner or later, you'll want to reset your log files (access_log and
error_log) because they are too big, or full of old information you don't
need.</P>

<P><CODE>access.log</CODE> typically grows by 1Mb for each 10,000 requests.</P>

<P>Most people's first attempt at replacing the logfile is to just move the
logfile or remove the logfile. This doesn't work.</P>

<P>Apache will continue writing to the logfile at the same offset as before the
logfile moved. This results in a new logfile being created which is just
as big as the old one, but it now contains thousands (or millions) of null
characters.</P>

<P>The correct procedure is to move the logfile, then signal Apache to tell
it to reopen the logfiles.</P>

<P>Apache is signaled using the <STRONG>SIGHUP</STRONG> (-1) signal.
<EM>e.g.</EM>
<BLOCKQUOTE><CODE>
mv access_log access_log.old<BR>
kill -1 `cat httpd.pid`
</CODE></BLOCKQUOTE>

<P>Note: <CODE>httpd.pid</CODE> is a file containing the
<STRONG>p</STRONG>rocess <STRONG>id</STRONG>
of the Apache httpd daemon, Apache saves this in the same directory as the log
files.</P>

<P>Many people use this method to replace (and backup) their logfiles on a
nightly or weekly basis.</P>
<HR>

<H2><A NAME="stoprob">How to stop or restrict robots</A></H2>

<P>Ever wondered why so many clients are interested in a file called
<CODE>robots.txt</CODE> which you don't have, and never did have?</P>

<P>These clients are called <STRONG>robots</STRONG> (also known as crawlers,
spiders and other cute names) - special automated clients which
wander around the web looking for interesting resources.</P>

<P>Most robots are used to generate some kind of <EM>web index</EM> which
is then used by a <EM>search engine</EM> to help locate information.</P>

<P><CODE>robots.txt</CODE> provides a means to request that robots limit their
activities at the site, or more often than not, to leave the site alone.</P>

<P>When the first robots were developed, they had a bad reputation for
sending hundreds/thousands of requests to each site, often resulting
in the site being overloaded. Things have improved dramatically since
then, thanks to <A
HREF="http://info.webcrawler.com/mak/projects/robots/guidelines.html">
Guidelines for Robot Writers</A>, but even so, some robots may exhibit
unfriendly behavior which the webmaster isn't willing to tolerate, and
will want to stop.</P>

<P>Another reason some webmasters want to block access to robots, is to
stop them indexing dynamic information. Many search engines will use the
data collected from your pages for months to come - not much use if you're
serving stock quotes, news, weather reports or anything else that will be
stale by the time people find it in a search engine.</P>

<P>If you decide to exclude robots completely, or just limit the areas
in which they can roam, create a <CODE>robots.txt</CODE> file; refer
to the <A HREF="http://info.webcrawler.com/mak/projects/robots/robots.html"
>robot information pages</A> provided by Martijn Koster for the syntax.</P>

<HR>
<H2><A NAME="proxyssl">How to proxy SSL requests <EM>through</EM>
    your non-SSL Apache server</A>
    <BR>
    <SMALL>(<EM>submitted by David Sedlock</EM>)</SMALL>
</H2>
<P>
SSL uses port 443 for requests for secure pages. If your browser just
sits there for a long time when you attempt to access a secure page
over your Apache proxy, then the proxy may not be configured to handle
SSL. You need to instruct Apache to listen on port 443 in addition to
any of the ports on which it is already listening:
</P>
<PRE>
    Listen 80
    Listen 443
</PRE>
<P>
Then set the security proxy in your browser to 443. That might be it!
</P>
<P>
If your proxy is sending requests to another proxy, then you may have
to set the directive ProxyRemote differently. Here are my settings:
</P>
<PRE>
d219 17
a235 18
</PRE>
<P>
Requests on port 80 of my proxy <SAMP>nicklas</SAMP> are forwarded to
<SAMP>proxy.mayn.franken.de:8080</SAMP>, while requests on port 443 are
forwarded to <SAMP>proxy.mayn.franken.de:443</SAMP>.
If the remote proxy is not set up to
handle port 443, then the last directive can be left out. SSL requests
will only go over the first proxy.
</P>
<P>
Note that your Apache does NOT have to be set up to serve secure pages
with SSL. Proxying SSL is a different thing from using it.
</P>
<HR>

<H3 ALIGN="CENTER">
 Apache HTTP Server Version 1.3
</H3>
d237 2
a238 2
<A HREF="./"><IMG SRC="../images/index.gif" ALT="Index"></A>
<A HREF="../"><IMG SRC="../images/home.gif" ALT="Home"></A>
a239 2
</BODY>
</HTML>
@


1.3
log
@Apache 1.3.9 + Mod_ssl 2.4.2 - now builds with apaci nastiness.
@
text
@d66 1
a66 1
This will send an HTTP 302 Redirect back to the client, and no matter
d68 1
a68 1
"http://www.apache.org".
d70 1
a70 1
The second option is to set up a <CODE>ScriptAlias</CODE> pointing to
d98 1
a98 1
</PRE></BLOCKQUOTE></P>
a126 1
</P>
d143 1
a143 1
spiders and other cute name) - special automated clients which
d163 1
a163 1
data collected from your pages for months to come - not much use if your
d202 1
a202 1
proxy<SAMP>.mayn.franken.de:8080</SAMP>, while requests on port 443 are
@


1.2
log
@Apache 1.3.4 merge
@
text
@d86 1
a86 1
<BLOCKQUOTE><PRE>ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script</PRE>
d94 3
a96 2
print "Status: 302 Moved Temporarily\r
Location: http://www.some.where.else.com/\r\n\r\n";
@


1.1
log
@Initial revision
@
text
@d4 2
a5 1
<META NAME="description" CONTENT="Some 'how to' tips for the Apache httpd server">
d21 1
a21 1
  Apache HTTP Server Version 1.2
d28 8
a35 5
<ul>
<li><A HREF="#redirect">redirect an entire server or directory to a single URL</A>
<li><A HREF="#logreset">reset your log files</A>
<li><A HREF="#stoprob">stop/restrict robots</A>
</ul>
d38 2
a39 1
<H2><A name="redirect">How to redirect an entire server or directory to a single URL</A></H2>
d43 1
a43 1
<code>mod_rewrite</code>, and another which uses a CGI script.
d46 2
a47 2
another, simply use the <code>Redirect</code> directive, as supplied
by <code>mod_alias</code>:
d49 1
a49 1
<blockquote><pre>
d51 1
a51 1
</pre></blockquote>
d53 1
a53 1
<P>Since <code>Redirect</code> will forward along the complete path,
d58 3
a60 2
<P>The best option is to use the standard Apache module <code>mod_rewrite</code>.
If that module is compiled in, the following lines:
d62 1
a62 1
<blockquote><pre>RewriteEngine On
d64 1
a64 1
</pre></blockquote>
d70 3
a72 2
The second option is to set up a <CODE>ScriptAlias</Code> pointing to
a <B>cgi script</B> which outputs a 301 or 302 status and the location
d75 4
a78 2
<P>By using a <B>cgi-script</B> you can intercept various requests and
treat them specially, e.g. you might want to intercept <B>POST</B>
d86 2
a87 1
<blockquote><pre>ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script</pre></blockquote>
d91 1
a91 1
<blockquote><pre>
d97 1
a97 1
</pre></blockquote></P>
d101 1
a101 1
<H2><A name="logreset">How to reset your log files</A></H2>
d117 2
a118 1
<P>The correct procedure is to move the logfile, then signal Apache to tell it to reopen the logfiles.</P>
d120 3
a122 2
<P>Apache is signaled using the <B>SIGHUP</B> (-1) signal. e.g.
<blockquote><code>
d125 1
a125 1
</code></blockquote>
d128 2
a129 1
<P>Note: <code>httpd.pid</code> is a file containing the <B>p</B>rocess <B>id</B>
d137 1
a137 1
<H2><A name="stoprob">How to stop or restrict robots</A></H2>
d140 1
a140 1
<code>robots.txt</code> which you don't have, and never did have?</P>
d142 1
a142 1
<P>These clients are called <B>robots</B> (also known as crawlers,
d146 2
a147 2
<P>Most robots are used to generate some kind of <em>web index</em> which
is then used by a <em>search engine</em> to help locate information.</P>
d149 1
a149 1
<P><code>robots.txt</code> provides a means to request that robots limit their
d152 8
a159 1
<P>When the first robots were developed, they had a bad reputation for sending hundreds/thousands of requests to each site, often resulting in the site being overloaded. Things have improved dramatically since then, thanks to <A HREF="http://info.webcrawler.com/mak/projects/robots/guidelines.html"> Guidelines for Robot Writers</A>, but even so, some robots may <A HREF="http://www.zyzzyva.com/robots/alert/">exhibit unfriendly behavior</A> which the webmaster isn't willing to tolerate, and will want to stop.</P>
d169 2
a170 1
to the <A HREF="http://info.webcrawler.com/mak/projects/robots/robots.html">robot information pages</A> provided by Martijn Koster for the syntax.</P>
d173 41
d215 1
a215 1
 Apache HTTP Server Version 1.2
@


1.1.1.1
log
@Initial import from apache 1.2.6
@
text
@@


1.1.1.2
log
@Apache 1.3.2
@
text
@d4 1
a4 2
<META NAME="description"
 CONTENT="Some 'how to' tips for the Apache httpd server">
d20 1
a20 1
  Apache HTTP Server Version 1.3
d27 5
a31 6
<UL>
<LI><A HREF="#redirect">redirect an entire server or directory to a single
    URL</A>
<LI><A HREF="#logreset">reset your log files</A>
<LI><A HREF="#stoprob">stop/restrict robots</A>
</UL>
d34 1
a34 2
<H2><A NAME="redirect">How to redirect an entire server or directory to a
single URL</A></H2>
d38 1
a38 1
<CODE>mod_rewrite</CODE>, and another which uses a CGI script.
d41 2
a42 2
another, simply use the <CODE>Redirect</CODE> directive, as supplied
by <CODE>mod_alias</CODE>:
d44 1
a44 1
<BLOCKQUOTE><PRE>
d46 1
a46 1
</PRE></BLOCKQUOTE>
d48 1
a48 1
<P>Since <CODE>Redirect</CODE> will forward along the complete path,
d53 2
a54 3
<P>The best option is to use the standard Apache module
<CODE>mod_rewrite</CODE>.
If that module is compiled in, the following lines
d56 1
a56 1
<BLOCKQUOTE><PRE>RewriteEngine On
d58 1
a58 1
</PRE></BLOCKQUOTE>
d64 2
a65 3
The second option is to set up a <CODE>ScriptAlias</CODE> pointing to
a <STRONG>CGI script</STRONG> which outputs a 301 or 302 status and the
location
d68 2
a69 4
<P>By using a <STRONG>CGI script</STRONG> you can intercept various requests
and
treat them specially, <EM>e.g.</EM>, you might want to intercept
<STRONG>POST</STRONG>
d77 1
a77 2
<BLOCKQUOTE><PRE>ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script</PRE>
</BLOCKQUOTE>
d81 1
a81 1
<BLOCKQUOTE><PRE>
d87 1
a87 1
</PRE></BLOCKQUOTE></P>
d91 1
a91 1
<H2><A NAME="logreset">How to reset your log files</A></H2>
d107 1
a107 2
<P>The correct procedure is to move the logfile, then signal Apache to tell
it to reopen the logfiles.</P>
d109 2
a110 3
<P>Apache is signaled using the <STRONG>SIGHUP</STRONG> (-1) signal.
<EM>e.g.</EM>
<BLOCKQUOTE><CODE>
d113 1
a113 1
</CODE></BLOCKQUOTE>
d116 1
a116 2
<P>Note: <CODE>httpd.pid</CODE> is a file containing the
<STRONG>p</STRONG>rocess <STRONG>id</STRONG>
d124 1
a124 1
<H2><A NAME="stoprob">How to stop or restrict robots</A></H2>
d127 1
a127 1
<CODE>robots.txt</CODE> which you don't have, and never did have?</P>
d129 1
a129 1
<P>These clients are called <STRONG>robots</STRONG> (also known as crawlers,
d133 2
a134 2
<P>Most robots are used to generate some kind of <EM>web index</EM> which
is then used by a <EM>search engine</EM> to help locate information.</P>
d136 1
a136 1
<P><CODE>robots.txt</CODE> provides a means to request that robots limit their
d139 1
a139 8
<P>When the first robots were developed, they had a bad reputation for
sending hundreds/thousands of requests to each site, often resulting
in the site being overloaded. Things have improved dramatically since
then, thanks to <A
HREF="http://info.webcrawler.com/mak/projects/robots/guidelines.html">
Guidelines for Robot Writers</A>, but even so, some robots may exhibit
unfriendly behavior which the webmaster isn't willing to tolerate, and
will want to stop.</P>
d149 1
a149 2
to the <A HREF="http://info.webcrawler.com/mak/projects/robots/robots.html"
>robot information pages</A> provided by Martijn Koster for the syntax.</P>
a151 1

d153 1
a153 1
 Apache HTTP Server Version 1.3
@


1.1.1.3
log
@import apache 1.3.26 + mod_ssl 2.8.10
@
text
@d1 45
a45 2
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
d47 47
a93 7
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="generator" content="HTML Tidy, see www.w3.org" />
    <meta name="description"
    content="Some 'how to' tips for the Apache httpd server" />
    <meta name="keywords"
    content="apache,redirect,robots,rotate,logfiles" />
d95 1
a95 3
    <title>Apache HOWTO documentation</title>
  </head>
  <!-- Background white, links blue (unvisited), navy (visited), red (active) -->
d97 1
a97 4
  <body bgcolor="#FFFFFF" text="#000000" link="#0000FF"
  vlink="#000080" alink="#FF0000">
        <div align="CENTER">
      <img src="../images/sub.gif" alt="[APACHE DOCUMENTATION]" /> 
d99 1
a99 2
      <h3>Apache HTTP Server Version 1.3</h3>
    </div>
d101 3
d105 1
a105 2
    <h1 align="CENTER">Apache HOWTO documentation</h1>
    How to: 
d107 2
a108 3
    <ul>
      <li><a href="#redirect">redirect an entire server or
      directory to a single URL</a></li>
d110 4
a113 1
      <li><a href="#logreset">reset your log files</a></li>
d115 2
a116 1
      <li><a href="#stoprob">stop/restrict robots</a></li>
d118 7
a124 4
      <li><a href="#proxyssl">proxy SSL requests <em>through</em>
      your non-SSL server</a></li>
    </ul>
    <hr />
d126 4
a129 2
    <h2><a id="redirect" name="redirect">How to redirect an entire
    server or directory to a single URL</a></h2>
d131 3
a133 4
    <p>There are two chief ways to redirect all requests for an
    entire server to a single location: one which requires the use
    of <code>mod_rewrite</code>, and another which uses a CGI
    script.</p>
d135 1
a135 3
    <p>First: if all you need to do is migrate a server from one
    name to another, simply use the <code>Redirect</code>
    directive, as supplied by <code>mod_alias</code>:</p>
d137 27
a163 5
    <blockquote>
<pre>
  Redirect / http://www.apache.org/
</pre>
    </blockquote>
d165 4
a168 40
    <p>Since <code>Redirect</code> will forward along the complete
    path, however, it may not be appropriate - for example, when
    the directory structure has changed after the move, and you
    simply want to direct people to the home page.</p>

    <p>The best option is to use the standard Apache module
    <code>mod_rewrite</code>. If that module is compiled in, the
    following lines</p>

    <blockquote>
<pre>
RewriteEngine On
RewriteRule /.* http://www.apache.org/ [R]
</pre>
    </blockquote>
    will send an HTTP 302 Redirect back to the client, and no
    matter what they gave in the original URL, they'll be sent to
    "http://www.apache.org/". 

    <p>The second option is to set up a <code>ScriptAlias</code>
    pointing to a <strong>CGI script</strong> which outputs a 301
    or 302 status and the location of the other server.</p>

    <p>By using a <strong>CGI script</strong> you can intercept
    various requests and treat them specially, <em>e.g.</em>, you
    might want to intercept <strong>POST</strong> requests, so that
    the client isn't redirected to a script on the other server
    which expects POST information (a redirect will lose the POST
    information.) You might also want to use a CGI script if you
    don't want to compile mod_rewrite into your server.</p>

    <p>Here's how to redirect all requests to a script... In the
    server configuration file,</p>

    <blockquote>
<pre>
ScriptAlias / /usr/local/httpd/cgi-bin/redirect_script/
</pre>
    </blockquote>
    and here's a simple perl script to redirect requests: 
d170 1
a170 3
    <blockquote>
<pre>
#!/usr/local/bin/perl
d172 3
a174 132
print "Status: 302 Moved Temporarily\r\n" .
      "Location: http://www.some.where.else.com/\r\n" .
      "\r\n";

</pre>
    </blockquote>
    <hr />

    <h2><a id="logreset" name="logreset">How to reset your log
    files</a></h2>

    <p>Sooner or later, you'll want to reset your log files
    (access_log and error_log) because they are too big, or full of
    old information you don't need.</p>

    <p><code>access.log</code> typically grows by 1Mb for each
    10,000 requests.</p>

    <p>Most people's first attempt at replacing the logfile is to
    just move the logfile or remove the logfile. This doesn't
    work.</p>

    <p>Apache will continue writing to the logfile at the same
    offset as before the logfile moved. This results in a new
    logfile being created which is just as big as the old one, but
    it now contains thousands (or millions) of null characters.</p>

    <p>The correct procedure is to move the logfile, then signal
    Apache to tell it to reopen the logfiles.</p>

    <p>Apache is signaled using the <strong>SIGHUP</strong> (-1)
    signal. <em>e.g.</em></p>

    <blockquote>
      <code>mv access_log access_log.old<br />
       kill -1 `cat httpd.pid`</code>
    </blockquote>

    <p>Note: <code>httpd.pid</code> is a file containing the
    <strong>p</strong>rocess <strong>id</strong> of the Apache
    httpd daemon, Apache saves this in the same directory as the
    log files.</p>

    <p>Many people use this method to replace (and backup) their
    logfiles on a nightly or weekly basis.</p>
    <hr />

    <h2><a id="stoprob" name="stoprob">How to stop or restrict
    robots</a></h2>

    <p>Ever wondered why so many clients are interested in a file
    called <code>robots.txt</code> which you don't have, and never
    did have?</p>

    <p>These clients are called <strong>robots</strong> (also known
    as crawlers, spiders and other cute names) - special automated
    clients which wander around the web looking for interesting
    resources.</p>

    <p>Most robots are used to generate some kind of <em>web
    index</em> which is then used by a <em>search engine</em> to
    help locate information.</p>

    <p><code>robots.txt</code> provides a means to request that
    robots limit their activities at the site, or more often than
    not, to leave the site alone.</p>

    <p>When the first robots were developed, they had a bad
    reputation for sending hundreds/thousands of requests to each
    site, often resulting in the site being overloaded. Things have
    improved dramatically since then, thanks to <a
    href="http://info.webcrawler.com/mak/projects/robots/guidelines.html">
    Guidelines for Robot Writers</a>, but even so, some robots may
    exhibit unfriendly behavior which the webmaster isn't willing
    to tolerate, and will want to stop.</p>

    <p>Another reason some webmasters want to block access to
    robots, is to stop them indexing dynamic information. Many
    search engines will use the data collected from your pages for
    months to come - not much use if you're serving stock quotes,
    news, weather reports or anything else that will be stale by
    the time people find it in a search engine.</p>

    <p>If you decide to exclude robots completely, or just limit
    the areas in which they can roam, create a
    <code>robots.txt</code> file; refer to the <a
    href="http://info.webcrawler.com/mak/projects/robots/robots.html">
    robot information pages</a> provided by Martijn Koster for the
    syntax.</p>
    <hr />

    <h2><a id="proxyssl" name="proxyssl">How to proxy SSL requests
    <em>through</em> your non-SSL Apache server</a><br />
     <small>(<em>submitted by David Sedlock</em>)</small></h2>

    <p>SSL uses port 443 for requests for secure pages. If your
    browser just sits there for a long time when you attempt to
    access a secure page over your Apache proxy, then the proxy may
    not be configured to handle SSL. You need to instruct Apache to
    listen on port 443 in addition to any of the ports on which it
    is already listening:</p>
<pre>
    Listen 80
    Listen 443
</pre>

    <p>Then set the security proxy in your browser to 443. That
    might be it!</p>

    <p>If your proxy is sending requests to another proxy, then you
    may have to set the directive ProxyRemote differently. Here are
    my settings:</p>
<pre>
    ProxyRemote http://nicklas:80/ http://proxy.mayn.franken.de:8080
    ProxyRemote http://nicklas:443/ http://proxy.mayn.franken.de:443
</pre>

    <p>Requests on port 80 of my proxy <samp>nicklas</samp> are
    forwarded to <samp>proxy.mayn.franken.de:8080</samp>, while
    requests on port 443 are forwarded to
    <samp>proxy.mayn.franken.de:443</samp>. If the remote proxy is
    not set up to handle port 443, then the last directive can be
    left out. SSL requests will only go over the first proxy.</p>

    <p>Note that your Apache does NOT have to be set up to serve
    secure pages with SSL. Proxying SSL is a different thing from
    using it.</p>
        <hr />

    <h3 align="CENTER">Apache HTTP Server Version 1.3</h3>
    <a href="./"><img src="../images/index.gif" alt="Index" /></a>
    <a href="../"><img src="../images/home.gif" alt="Home" /></a>
d176 2
a177 2
  </body>
</html>
d179 2
@


1.1.1.4
log
@import apache 1.3.28 and mod_ssl 2.8.15
@
text
@d175 1
a175 1
    href="http://www.robotstxt.org/wc/guidelines.html">
d190 1
a190 1
    href="http://www.robotstxt.org/wc/robots.html">
@


