head	1.40;
access;
symbols
	OPENBSD_6_1:1.40.0.4
	OPENBSD_6_1_BASE:1.40
	OPENBSD_6_0:1.16.0.2
	OPENBSD_6_0_BASE:1.16
	OPENBSD_5_9:1.9.0.2
	OPENBSD_5_9_BASE:1.9;
locks; strict;
comment	@ * @;


1.40
date	2017.03.26.22.19.47;	author mlarkin;	state Exp;
branches;
next	1.39;
commitid	5S0m9OgzDRebEsHD;

1.39
date	2017.03.25.22.36.53;	author mlarkin;	state Exp;
branches;
next	1.38;
commitid	8Fl7pGk8QzRFB3im;

1.38
date	2017.03.25.16.34.26;	author mlarkin;	state Exp;
branches;
next	1.37;
commitid	jqcKKaQH5lrgFnyW;

1.37
date	2017.03.25.16.24.44;	author mlarkin;	state Exp;
branches;
next	1.36;
commitid	wE2jc2VBbZZljWJU;

1.36
date	2017.03.25.16.05.33;	author mlarkin;	state Exp;
branches;
next	1.35;
commitid	cZysoCXytViDlRmt;

1.35
date	2017.03.25.15.47.37;	author mlarkin;	state Exp;
branches;
next	1.34;
commitid	EGo7WkcHTulgDXQx;

1.34
date	2017.03.15.18.06.18;	author reyk;	state Exp;
branches;
next	1.33;
commitid	olrxKy4ehROyBXsm;

1.33
date	2017.03.02.07.33.37;	author reyk;	state Exp;
branches;
next	1.32;
commitid	0uvzhmQVR7Ehsjus;

1.32
date	2017.02.13.18.49.01;	author reyk;	state Exp;
branches;
next	1.31;
commitid	0ilLPBIaOzQenWTd;

1.31
date	2017.01.21.11.36.54;	author reyk;	state Exp;
branches;
next	1.30;
commitid	XCHrKhcoG24sPzRN;

1.30
date	2017.01.19.10.16.22;	author reyk;	state Exp;
branches;
next	1.29;
commitid	ET5UHzBPhPGp3gGS;

1.29
date	2017.01.17.21.51.01;	author krw;	state Exp;
branches;
next	1.28;
commitid	7aASaBYKS1WgBK9H;

1.28
date	2017.01.13.15.12.13;	author reyk;	state Exp;
branches;
next	1.27;
commitid	RsPIppcr8Fri2h1M;

1.27
date	2017.01.13.14.50.56;	author reyk;	state Exp;
branches;
next	1.26;
commitid	O6Az38HSBXh6vWTC;

1.26
date	2016.12.05.09.28.11;	author reyk;	state Exp;
branches;
next	1.25;
commitid	paJa4IvXGZQfF6WI;

1.25
date	2016.11.26.16.05.11;	author sf;	state Exp;
branches;
next	1.24;
commitid	r9Qg2KxvS622A6iF;

1.24
date	2016.10.18.05.33.57;	author reyk;	state Exp;
branches;
next	1.23;
commitid	CxFBgKY6y5DF2OVP;

1.23
date	2016.10.16.19.07.05;	author guenther;	state Exp;
branches;
next	1.22;
commitid	RqYlGPFyThB7YLxw;

1.22
date	2016.10.12.06.56.54;	author mlarkin;	state Exp;
branches;
next	1.21;
commitid	d9eAYZoqV2dTESpL;

1.21
date	2016.10.05.17.30.13;	author reyk;	state Exp;
branches;
next	1.20;
commitid	c3mC9fNmRbfUPluH;

1.20
date	2016.10.03.05.59.24;	author mlarkin;	state Exp;
branches;
next	1.19;
commitid	lyp0KpPLIT7T9n70;

1.19
date	2016.09.03.11.35.24;	author nayden;	state Exp;
branches;
next	1.18;
commitid	C8lSdv0gvvVFIDi8;

1.18
date	2016.09.02.17.08.28;	author stefan;	state Exp;
branches;
next	1.17;
commitid	rVs1JMfl9No82fHm;

1.17
date	2016.08.17.05.07.13;	author deraadt;	state Exp;
branches;
next	1.16;
commitid	qqWoBqCGCbsKOAqY;

1.16
date	2016.07.19.09.52.34;	author natano;	state Exp;
branches;
next	1.15;
commitid	n1xINDCiMqjfky3a;

1.15
date	2016.07.09.09.06.22;	author stefan;	state Exp;
branches;
next	1.14;
commitid	yIWxq4qxCQhfbPDu;

1.14
date	2016.07.07.00.58.31;	author mlarkin;	state Exp;
branches;
next	1.13;
commitid	2sGq29g3ZwzSaLsn;

1.13
date	2016.07.04.23.03.52;	author mlarkin;	state Exp;
branches;
next	1.12;
commitid	ZMIOpVfPBNv4eDK8;

1.12
date	2016.06.30.02.29.22;	author mlarkin;	state Exp;
branches;
next	1.11;
commitid	ImQDOYsYLKUHbnjJ;

1.11
date	2016.04.04.17.13.54;	author stefan;	state Exp;
branches;
next	1.10;
commitid	Y1YC68R7VkcaKZ1f;

1.10
date	2016.03.13.13.11.47;	author stefan;	state Exp;
branches;
next	1.9;
commitid	tNFHzbetplEWT4Tg;

1.9
date	2016.02.07.10.17.19;	author jsg;	state Exp;
branches;
next	1.8;
commitid	VG4cPioKXUFd8U8E;

1.8
date	2016.01.16.08.55.40;	author stefan;	state Exp;
branches;
next	1.7;
commitid	AJ7cyewTvXIt6nft;

1.7
date	2016.01.14.02.46.40;	author mlarkin;	state Exp;
branches;
next	1.6;
commitid	e9sYZpKi0kYXbM8G;

1.6
date	2016.01.04.02.07.28;	author mlarkin;	state Exp;
branches;
next	1.5;
commitid	jWhixnwknFs5C2Jx;

1.5
date	2016.01.03.22.36.09;	author mlarkin;	state Exp;
branches;
next	1.4;
commitid	6ILTrAEzbXGcpCSC;

1.4
date	2015.12.03.08.42.11;	author reyk;	state Exp;
branches;
next	1.3;
commitid	deOxsg54am7A6Za6;

1.3
date	2015.11.23.13.04.49;	author reyk;	state Exp;
branches;
next	1.2;
commitid	8LSTqui1EesFdS0T;

1.2
date	2015.11.22.21.51.32;	author reyk;	state Exp;
branches;
next	1.1;
commitid	nvO8KnrtEKRaTvXw;

1.1
date	2015.11.22.20.20.32;	author mlarkin;	state Exp;
branches;
next	;
commitid	O5Ay1ZamHQn9lamK;


desc
@@


1.40
log
@Implement a missing command in vioblk and allow > MAXPHYS transfers.

This diff (with the others previously committed) allows ubuntu 14.04
amd64 guests to work.
@
text
@/*	$OpenBSD: virtio.c,v 1.39 2017/03/25 22:36:53 mlarkin Exp $	*/

/*
 * Copyright (c) 2015 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>	/* PAGE_SIZE */
#include <sys/socket.h>

#include <machine/vmmvar.h>
#include <dev/pci/pcireg.h>
#include <dev/pci/pcidevs.h>
#include <dev/pv/virtioreg.h>
#include <dev/pv/vioblkreg.h>

#include <net/if.h>
#include <netinet/in.h>
#include <netinet/if_ether.h>

#include <errno.h>
#include <event.h>
#include <poll.h>
#include <stddef.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

#include "pci.h"
#include "vmd.h"
#include "vmm.h"
#include "virtio.h"
#include "loadfile.h"

extern char *__progname;

struct viornd_dev viornd;
struct vioblk_dev *vioblk;
struct vionet_dev *vionet;
struct vmmci_dev vmmci;

int nr_vionet;

#define MAXPHYS	(64 * 1024)	/* max raw I/O transfer size */

#define VIRTIO_NET_F_MAC	(1<<5)

#define VMMCI_F_TIMESYNC	(1<<0)
#define VMMCI_F_ACK		(1<<1)

const char *
vioblk_cmd_name(uint32_t type)
{
	switch (type) {
	case VIRTIO_BLK_T_IN: return "read";
	case VIRTIO_BLK_T_OUT: return "write";
	case VIRTIO_BLK_T_SCSI_CMD: return "scsi read";
	case VIRTIO_BLK_T_SCSI_CMD_OUT: return "scsi write";
	case VIRTIO_BLK_T_FLUSH: return "flush";
	case VIRTIO_BLK_T_FLUSH_OUT: return "flush out";
	case VIRTIO_BLK_T_GET_ID: return "get id";
	default: return "unknown";
	}
}

static void
dump_descriptor_chain(struct vring_desc *desc, int16_t dxx)
{
	log_debug("descriptor chain @@ %d", dxx);
	do {
		log_debug("desc @@%d addr/len/flags/next = 0x%llx / 0x%x "
		    "/ 0x%x / 0x%x",
		    dxx,
		    desc[dxx].addr,
		    desc[dxx].len,
		    desc[dxx].flags,
		    desc[dxx].next);
		dxx = desc[dxx].next;
	} while (desc[dxx].flags & VRING_DESC_F_NEXT);

	log_debug("desc @@%d addr/len/flags/next = 0x%llx / 0x%x / 0x%x "
	    "/ 0x%x",
	    dxx,
	    desc[dxx].addr,
	    desc[dxx].len,
	    desc[dxx].flags,
	    desc[dxx].next);
}

static const char *
virtio_reg_name(uint8_t reg)
{
	switch (reg) {
	case VIRTIO_CONFIG_DEVICE_FEATURES: return "device feature";
	case VIRTIO_CONFIG_GUEST_FEATURES: return "guest feature";
	case VIRTIO_CONFIG_QUEUE_ADDRESS: return "queue address";
	case VIRTIO_CONFIG_QUEUE_SIZE: return "queue size";
	case VIRTIO_CONFIG_QUEUE_SELECT: return "queue select";
	case VIRTIO_CONFIG_QUEUE_NOTIFY: return "queue notify";
	case VIRTIO_CONFIG_DEVICE_STATUS: return "device status";
	case VIRTIO_CONFIG_ISR_STATUS: return "isr status";
	case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI: return "device config 0";
	case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 4: return "device config 1";
	case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 8: return "device config 2";
	case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 12: return "device config 3";
	case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 16: return "device config 4";
	default: return "unknown";
	}
}

uint32_t
vring_size(uint32_t vq_size)
{
	uint32_t allocsize1, allocsize2;

	/* allocsize1: descriptor table + avail ring + pad */
	allocsize1 = VIRTQUEUE_ALIGN(sizeof(struct vring_desc) * vq_size
	    + sizeof(uint16_t) * (2 + vq_size));
	/* allocsize2: used ring + pad */
	allocsize2 = VIRTQUEUE_ALIGN(sizeof(uint16_t) * 2
	    + sizeof(struct vring_used_elem) * vq_size);

	return allocsize1 + allocsize2;
}

/* Update queue select */
void
viornd_update_qs(void)
{
	/* Invalid queue? */
	if (viornd.cfg.queue_select > 0)
		return;

	/* Update queue address/size based on queue select */
	viornd.cfg.queue_address = viornd.vq[viornd.cfg.queue_select].qa;
	viornd.cfg.queue_size = viornd.vq[viornd.cfg.queue_select].qs;
}

/* Update queue address */
void
viornd_update_qa(void)
{
	/* Invalid queue? */
	if (viornd.cfg.queue_select > 0)
		return;

	viornd.vq[viornd.cfg.queue_select].qa = viornd.cfg.queue_address;
}

int
viornd_notifyq(void)
{
	uint64_t q_gpa;
	uint32_t vr_sz;
	size_t sz;
	int ret;
	char *buf, *rnd_data;
	struct vring_desc *desc;
	struct vring_avail *avail;
	struct vring_used *used;

	ret = 0;

	/* Invalid queue? */
	if (viornd.cfg.queue_notify > 0)
		return (0);

	vr_sz = vring_size(VIORND_QUEUE_SIZE);
	q_gpa = viornd.vq[viornd.cfg.queue_notify].qa;
	q_gpa = q_gpa * VIRTIO_PAGE_SIZE;

	buf = calloc(1, vr_sz);
	if (buf == NULL) {
		log_warn("calloc error getting viornd ring");
		return (0);
	}

	if (read_mem(q_gpa, buf, vr_sz)) {
		free(buf);
		return (0);
	}

	desc = (struct vring_desc *)(buf);
	avail = (struct vring_avail *)(buf +
	    viornd.vq[viornd.cfg.queue_notify].vq_availoffset);
	used = (struct vring_used *)(buf +
	    viornd.vq[viornd.cfg.queue_notify].vq_usedoffset);

	sz = desc[avail->ring[avail->idx]].len;
	if (sz > MAXPHYS)
		fatal("viornd descriptor size too large (%zu)", sz);

	rnd_data = malloc(sz);

	if (rnd_data != NULL) {
		arc4random_buf(rnd_data, desc[avail->ring[avail->idx]].len);
		if (write_mem(desc[avail->ring[avail->idx]].addr,
		    rnd_data, desc[avail->ring[avail->idx]].len)) {
			log_warnx("viornd: can't write random data @@ "
			    "0x%llx",
			    desc[avail->ring[avail->idx]].addr);
		} else {
			/* ret == 1 -> interrupt needed */
			/* XXX check VIRTIO_F_NO_INTR */
			ret = 1;
			viornd.cfg.isr_status = 1;
			used->ring[used->idx].id = avail->ring[avail->idx];
			used->ring[used->idx].len =
			    desc[avail->ring[avail->idx]].len;
			used->idx++;

			if (write_mem(q_gpa, buf, vr_sz)) {
				log_warnx("viornd: error writing vio ring");
			}
		}
		free(rnd_data);
	} else
		fatal("memory allocation error for viornd data");

	free(buf);

	return (ret);
}

int
virtio_rnd_io(int dir, uint16_t reg, uint32_t *data, uint8_t *intr,
    void *unused, uint8_t sz)
{
	*intr = 0xFF;

	if (dir == 0) {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_FEATURES:
		case VIRTIO_CONFIG_QUEUE_SIZE:
		case VIRTIO_CONFIG_ISR_STATUS:
			log_warnx("%s: illegal write %x to %s",
			    __progname, *data, virtio_reg_name(reg));
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			viornd.cfg.guest_feature = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			viornd.cfg.queue_address = *data;
			viornd_update_qa();
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			viornd.cfg.queue_select = *data;
			viornd_update_qs();
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			viornd.cfg.queue_notify = *data;
			if (viornd_notifyq())
				*intr = 1;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			viornd.cfg.device_status = *data;
			break;
		}
	} else {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_FEATURES:
			*data = viornd.cfg.device_feature;
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			*data = viornd.cfg.guest_feature;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			*data = viornd.cfg.queue_address;
			break;
		case VIRTIO_CONFIG_QUEUE_SIZE:
			*data = viornd.cfg.queue_size;
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			*data = viornd.cfg.queue_select;
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			*data = viornd.cfg.queue_notify;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			*data = viornd.cfg.device_status;
			break;
		case VIRTIO_CONFIG_ISR_STATUS:
			*data = viornd.cfg.isr_status;
			viornd.cfg.isr_status = 0;
			break;
		}
	}
	return (0);
}

void
vioblk_update_qa(struct vioblk_dev *dev)
{
	/* Invalid queue? */
	if (dev->cfg.queue_select > 0)
		return;

	dev->vq[dev->cfg.queue_select].qa = dev->cfg.queue_address;
}

void
vioblk_update_qs(struct vioblk_dev *dev)
{
	/* Invalid queue? */
	if (dev->cfg.queue_select > 0)
		return;

	/* Update queue address/size based on queue select */
	dev->cfg.queue_address = dev->vq[dev->cfg.queue_select].qa;
	dev->cfg.queue_size = dev->vq[dev->cfg.queue_select].qs;
}

static char *
vioblk_do_read(struct vioblk_dev *dev, off_t sector, ssize_t sz)
{
	char *buf;

	buf = malloc(sz);
	if (buf == NULL) {
		log_warn("malloc errror vioblk read");
		return (NULL);
	}

	if (lseek(dev->fd, sector * VIRTIO_BLK_SECTOR_SIZE,
	    SEEK_SET) == -1) {
		log_warn("seek error in vioblk read");
		free(buf);
		return (NULL);
	}

	if (read(dev->fd, buf, sz) != sz) {
		log_warn("vioblk read error");
		free(buf);
		return (NULL);
	}

	return buf;
}

static int
vioblk_do_write(struct vioblk_dev *dev, off_t sector, char *buf, ssize_t sz)
{
	if (lseek(dev->fd, sector * VIRTIO_BLK_SECTOR_SIZE,
	    SEEK_SET) == -1) {
		log_warn("seek error in vioblk write");
		return (1);
	}

	if (write(dev->fd, buf, sz) != sz) {
		log_warn("vioblk write error");
		return (1);
	}

	return (0);
}

/*
 * XXX in various cases, ds should be set to VIRTIO_BLK_S_IOERR, if we can
 * XXX cant trust ring data from VM, be extra cautious.
 */
int
vioblk_notifyq(struct vioblk_dev *dev)
{
	uint64_t q_gpa;
	uint32_t vr_sz;
	uint16_t idx, cmd_desc_idx, secdata_desc_idx, ds_desc_idx;
	uint8_t ds;
	int ret;
	off_t secbias;
	char *vr, *secdata;
	struct vring_desc *desc, *cmd_desc, *secdata_desc, *ds_desc;
	struct vring_avail *avail;
	struct vring_used *used;
	struct virtio_blk_req_hdr cmd;

	ret = 0;

	/* Invalid queue? */
	if (dev->cfg.queue_notify > 0)
		return (0);

	vr_sz = vring_size(VIOBLK_QUEUE_SIZE);
	q_gpa = dev->vq[dev->cfg.queue_notify].qa;
	q_gpa = q_gpa * VIRTIO_PAGE_SIZE;

	vr = calloc(1, vr_sz);
	if (vr == NULL) {
		log_warn("calloc error getting vioblk ring");
		return (0);
	}

	if (read_mem(q_gpa, vr, vr_sz)) {
		log_warnx("error reading gpa 0x%llx", q_gpa);
		goto out;
	}

	/* Compute offsets in ring of descriptors, avail ring, and used ring */
	desc = (struct vring_desc *)(vr);
	avail = (struct vring_avail *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_availoffset);
	used = (struct vring_used *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_usedoffset);

	idx = dev->vq[dev->cfg.queue_notify].last_avail & VIOBLK_QUEUE_MASK;

	if ((avail->idx & VIOBLK_QUEUE_MASK) == idx) {
		log_warnx("vioblk queue notify - nothing to do?");
		goto out;
	}

	while (idx != (avail->idx & VIOBLK_QUEUE_MASK)) {

		cmd_desc_idx = avail->ring[idx] & VIOBLK_QUEUE_MASK;
		cmd_desc = &desc[cmd_desc_idx];

		if ((cmd_desc->flags & VRING_DESC_F_NEXT) == 0) {
			log_warnx("unchained vioblk cmd descriptor received "
			    "(idx %d)", cmd_desc_idx);
			goto out;
		}

		/* Read command from descriptor ring */
		if (read_mem(cmd_desc->addr, &cmd, cmd_desc->len)) {
			log_warnx("vioblk: command read_mem error @@ 0x%llx",
			    cmd_desc->addr);
			goto out;
		}

		switch (cmd.type) {
		case VIRTIO_BLK_T_IN:
			/* first descriptor */
			secdata_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
			secdata_desc = &desc[secdata_desc_idx];

			if ((secdata_desc->flags & VRING_DESC_F_NEXT) == 0) {
				log_warnx("unchained vioblk data descriptor "
				    "received (idx %d)", cmd_desc_idx);
				goto out;
			}

			secbias = 0;
			do {
				/* read the data (use current data descriptor) */
				/*
				 * XXX waste to malloc secdata in vioblk_do_read
				 * and free it here over and over
				 */
				secdata = vioblk_do_read(dev, cmd.sector + secbias,
				    (ssize_t)secdata_desc->len);
				if (secdata == NULL) {
					log_warnx("vioblk: block read error, "
					    "sector %lld", cmd.sector);
					goto out;
				}

				if (write_mem(secdata_desc->addr, secdata,
				    secdata_desc->len)) {
					log_warnx("can't write sector "
					    "data to gpa @@ 0x%llx",
					    secdata_desc->addr);
					dump_descriptor_chain(desc, cmd_desc_idx);
					free(secdata);
					goto out;
				}

				free(secdata);

				secbias += (secdata_desc->len / VIRTIO_BLK_SECTOR_SIZE);
				secdata_desc_idx = secdata_desc->next &
				    VIOBLK_QUEUE_MASK;
				secdata_desc = &desc[secdata_desc_idx];
			} while (secdata_desc->flags & VRING_DESC_F_NEXT);

			ds_desc_idx = secdata_desc_idx;
			ds_desc = secdata_desc;

			ds = VIRTIO_BLK_S_OK;
			if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
				log_warnx("can't write device status data @@ "
				    "0x%llx", ds_desc->addr);
				dump_descriptor_chain(desc, cmd_desc_idx);
				goto out;
			}


			ret = 1;
			dev->cfg.isr_status = 1;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
			used->idx++;

			dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
			    VIOBLK_QUEUE_MASK;

			if (write_mem(q_gpa, vr, vr_sz)) {
				log_warnx("vioblk: error writing vio ring");
			}
			break;
		case VIRTIO_BLK_T_OUT:
			secdata_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
			secdata_desc = &desc[secdata_desc_idx];

			if ((secdata_desc->flags & VRING_DESC_F_NEXT) == 0) {
				log_warnx("wr vioblk: unchained vioblk data "
				    "descriptor received (idx %d)",
				    cmd_desc_idx);
				goto out;
			}

			if (secdata_desc->len > dev->max_xfer) {
				log_warnx("%s: invalid read size %d requested",
				    __func__, secdata_desc->len);
				goto out;
			}

			secdata = NULL;
			secbias = 0;
			do {
				secdata = realloc(secdata, secdata_desc->len);
				if (secdata == NULL) {
					log_warn("wr vioblk: malloc error, "
					    "len %d", secdata_desc->len);
					goto out;
				}

				if (read_mem(secdata_desc->addr, secdata,
				    secdata_desc->len)) {
					log_warnx("wr vioblk: can't read "
					    "sector data @@ 0x%llx",
					    secdata_desc->addr);
					dump_descriptor_chain(desc,
					    cmd_desc_idx);
					free(secdata);
					goto out;
				}

				if (vioblk_do_write(dev, cmd.sector + secbias,
				    secdata, (ssize_t)secdata_desc->len)) {
					log_warnx("wr vioblk: disk write "
					    "error");
					free(secdata);
					goto out;
				}

				secbias += secdata_desc->len /
				    VIRTIO_BLK_SECTOR_SIZE;

				secdata_desc_idx = secdata_desc->next &
				    VIOBLK_QUEUE_MASK;
				secdata_desc = &desc[secdata_desc_idx];
			} while (secdata_desc->flags & VRING_DESC_F_NEXT);

			free(secdata);

			ds_desc_idx = secdata_desc_idx;
			ds_desc = secdata_desc;

			ds = VIRTIO_BLK_S_OK;
			if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
				log_warnx("wr vioblk: can't write device "
				    "status data @@ 0x%llx", ds_desc->addr);
				dump_descriptor_chain(desc, cmd_desc_idx);
				goto out;
			}

			ret = 1;
			dev->cfg.isr_status = 1;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id =
			    cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len =
			    cmd_desc->len;
			used->idx++;

			dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
			    VIOBLK_QUEUE_MASK;
			if (write_mem(q_gpa, vr, vr_sz))
				log_warnx("wr vioblk: error writing vio ring");
			break;
		case VIRTIO_BLK_T_FLUSH:
		case VIRTIO_BLK_T_FLUSH_OUT:
			ds_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
			ds_desc = &desc[ds_desc_idx];

			ds = VIRTIO_BLK_S_OK;
			if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
				log_warnx("fl vioblk: can't write device status "
				    "data @@ 0x%llx", ds_desc->addr);
				dump_descriptor_chain(desc, cmd_desc_idx);
				goto out;
			}

			ret = 1;
			dev->cfg.isr_status = 1;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id =
			    cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len =
			    cmd_desc->len;
			used->idx++;

			dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
			    VIOBLK_QUEUE_MASK;
			if (write_mem(q_gpa, vr, vr_sz)) {
				log_warnx("fl vioblk: error writing vio ring");
			}
			break;
		case VIRTIO_BLK_T_GET_ID:
			ds_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
			ds_desc = &desc[ds_desc_idx];

			ds = VIRTIO_BLK_S_UNSUPP;
			if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
				log_warnx("%s: get id : can't write device "
				    "status data @@ 0x%llx", __func__,
				    ds_desc->addr);
				dump_descriptor_chain(desc, cmd_desc_idx);
				goto out;
			}

			ret = 1;
			dev->cfg.isr_status = 1;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id =
			    cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len =
			    cmd_desc->len;
			used->idx++;

			dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
			    VIOBLK_QUEUE_MASK;
			if (write_mem(q_gpa, vr, vr_sz)) {
				log_warnx("%s: get id : error writing vio ring",
				    __func__);
			}
			break;
		default:
			log_warnx("%s: unknown command 0x%x", __func__,
			    cmd.type);
			break;
		}

		idx = (idx + 1) & VIOBLK_QUEUE_MASK;
	}
out:
	free(vr);
	return (ret);
}

int
virtio_blk_io(int dir, uint16_t reg, uint32_t *data, uint8_t *intr,
    void *cookie, uint8_t sz)
{
	struct vioblk_dev *dev = (struct vioblk_dev *)cookie;

	*intr = 0xFF;


	if (dir == 0) {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_FEATURES:
		case VIRTIO_CONFIG_QUEUE_SIZE:
		case VIRTIO_CONFIG_ISR_STATUS:
			log_warnx("%s: illegal write %x to %s",
			    __progname, *data, virtio_reg_name(reg));
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			dev->cfg.guest_feature = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			dev->cfg.queue_address = *data;
			vioblk_update_qa(dev);
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			dev->cfg.queue_select = *data;
			vioblk_update_qs(dev);
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			dev->cfg.queue_notify = *data;
			if (vioblk_notifyq(dev))
				*intr = 1;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			dev->cfg.device_status = *data;
			if (dev->cfg.device_status == 0) {
				log_debug("%s: device reset", __func__);
				dev->cfg.guest_feature = 0;
				dev->cfg.queue_address = 0;
				vioblk_update_qa(dev);
				dev->cfg.queue_size = 0;
				vioblk_update_qs(dev);
				dev->cfg.queue_select = 0;
				dev->cfg.queue_notify = 0;
				dev->cfg.isr_status = 0;
				dev->vq[0].last_avail = 0;
			}
			break;
		default:
			break;
		}
	} else {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI:
			switch (sz) {
			case 4:
				*data = (uint32_t)(dev->sz);
				break;
			case 2:
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->sz) & 0xFFFF;
				break;
			case 1:
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz) & 0xFF;
				break;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 1:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 8) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 2:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 16) & 0xFF;
			} else if (sz == 2) {
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->sz >> 16) & 0xFFFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 3:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 24) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 4:
			switch (sz) {
			case 4:
				*data = (uint32_t)(dev->sz >> 32);
				break;
			case 2:
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->sz >> 32) & 0xFFFF;
				break;
			case 1:
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 32) & 0xFF;
				break;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 5:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 40) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 6:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 48) & 0xFF;
			} else if (sz == 2) {
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->sz >> 48) & 0xFFFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 7:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->sz >> 56) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 8:
			switch (sz) {
			case 4:
				*data = (uint32_t)(dev->max_xfer);
				break;
			case 2:
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->max_xfer) & 0xFFFF;
				break;
			case 1:
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->max_xfer) & 0xFF;
				break;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 9:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->max_xfer >> 8) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 10:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->max_xfer >> 16) & 0xFF;
			} else if (sz == 2) {
				*data &= 0xFFFF0000;
				*data |= (uint32_t)(dev->max_xfer >> 16) & 0xFFFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 11:
			if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint32_t)(dev->max_xfer >> 24) & 0xFF;
			}
			/* XXX handle invalid sz */
			break;
		case VIRTIO_CONFIG_DEVICE_FEATURES:
			*data = dev->cfg.device_feature;
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			*data = dev->cfg.guest_feature;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			*data = dev->cfg.queue_address;
			break;
		case VIRTIO_CONFIG_QUEUE_SIZE:
			if (sz == 4)
				*data = dev->cfg.queue_size;
			else if (sz == 2) {
				*data &= 0xFFFF0000;
				*data |= (uint16_t)dev->cfg.queue_size;
			} else if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint8_t)dev->cfg.queue_size;
			}
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			*data = dev->cfg.queue_select;
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			*data = dev->cfg.queue_notify;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			if (sz == 4)
				*data = dev->cfg.device_status;
			else if (sz == 2) {
				*data &= 0xFFFF0000;
				*data |= (uint16_t)dev->cfg.device_status;
			} else if (sz == 1) {
				*data &= 0xFFFFFF00;
				*data |= (uint8_t)dev->cfg.device_status;
			}
			break;
		case VIRTIO_CONFIG_ISR_STATUS:
			*data = dev->cfg.isr_status;
			dev->cfg.isr_status = 0;
			break;
		}
	}
	return (0);
}

int
virtio_net_io(int dir, uint16_t reg, uint32_t *data, uint8_t *intr,
    void *cookie, uint8_t sz)
{
	struct vionet_dev *dev = (struct vionet_dev *)cookie;

	*intr = 0xFF;
	mutex_lock(&dev->mutex);

	if (dir == 0) {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_FEATURES:
		case VIRTIO_CONFIG_QUEUE_SIZE:
		case VIRTIO_CONFIG_ISR_STATUS:
			log_warnx("%s: illegal write %x to %s",
			    __progname, *data, virtio_reg_name(reg));
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			dev->cfg.guest_feature = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			dev->cfg.queue_address = *data;
			vionet_update_qa(dev);
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			dev->cfg.queue_select = *data;
			vionet_update_qs(dev);
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			dev->cfg.queue_notify = *data;
			if (vionet_notifyq(dev))
				*intr = 1;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			dev->cfg.device_status = *data;
			if (dev->cfg.device_status == 0) {
				log_debug("%s: device reset", __func__);
				dev->cfg.guest_feature = 0;
				dev->cfg.queue_address = 0;
				vionet_update_qa(dev);
				dev->cfg.queue_size = 0;
				vionet_update_qs(dev);
				dev->cfg.queue_select = 0;
				dev->cfg.queue_notify = 0;
				dev->cfg.isr_status = 0;
				dev->vq[0].last_avail = 0;
				dev->vq[0].notified_avail = 0;
				dev->vq[1].last_avail = 0;
				dev->vq[1].notified_avail = 0;
			}
			break;
		default:
			break;
		}
	} else {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI:
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 1:
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 2:
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 3:
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 4:
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 5:
			*data = dev->mac[reg -
			    VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI];
			break;
		case VIRTIO_CONFIG_DEVICE_FEATURES:
			*data = dev->cfg.device_feature;
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			*data = dev->cfg.guest_feature;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			*data = dev->cfg.queue_address;
			break;
		case VIRTIO_CONFIG_QUEUE_SIZE:
			*data = dev->cfg.queue_size;
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			*data = dev->cfg.queue_select;
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			*data = dev->cfg.queue_notify;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			*data = dev->cfg.device_status;
			break;
		case VIRTIO_CONFIG_ISR_STATUS:
			*data = dev->cfg.isr_status;
			dev->cfg.isr_status = 0;
			break;
		}
	}

	mutex_unlock(&dev->mutex);
	return (0);
}

/*
 * Must be called with dev->mutex acquired.
 */
void
vionet_update_qa(struct vionet_dev *dev)
{
	/* Invalid queue? */
	if (dev->cfg.queue_select > 1)
		return;

	dev->vq[dev->cfg.queue_select].qa = dev->cfg.queue_address;
}

/*
 * Must be called with dev->mutex acquired.
 */
void
vionet_update_qs(struct vionet_dev *dev)
{
	/* Invalid queue? */
	if (dev->cfg.queue_select > 1)
		return;

	/* Update queue address/size based on queue select */
	dev->cfg.queue_address = dev->vq[dev->cfg.queue_select].qa;
	dev->cfg.queue_size = dev->vq[dev->cfg.queue_select].qs;
}

/*
 * Must be called with dev->mutex acquired.
 */
int
vionet_enq_rx(struct vionet_dev *dev, char *pkt, ssize_t sz, int *spc)
{
	uint64_t q_gpa;
	uint32_t vr_sz;
	uint16_t idx, pkt_desc_idx, hdr_desc_idx;
	ptrdiff_t off;
	int ret;
	char *vr;
	struct vring_desc *desc, *pkt_desc, *hdr_desc;
	struct vring_avail *avail;
	struct vring_used *used;
	struct vring_used_elem *ue;

	ret = 0;

	if (!(dev->cfg.device_status & VIRTIO_CONFIG_DEVICE_STATUS_DRIVER_OK))
		return ret;

	vr_sz = vring_size(VIONET_QUEUE_SIZE);
	q_gpa = dev->vq[0].qa;
	q_gpa = q_gpa * VIRTIO_PAGE_SIZE;

	vr = calloc(1, vr_sz);
	if (vr == NULL) {
		log_warn("rx enq: calloc error getting vionet ring");
		return (0);
	}

	if (read_mem(q_gpa, vr, vr_sz)) {
		log_warnx("rx enq: error reading gpa 0x%llx", q_gpa);
		goto out;
	}

	/* Compute offsets in ring of descriptors, avail ring, and used ring */
	desc = (struct vring_desc *)(vr);
	avail = (struct vring_avail *)(vr + dev->vq[0].vq_availoffset);
	used = (struct vring_used *)(vr + dev->vq[0].vq_usedoffset);

	idx = dev->vq[0].last_avail & VIONET_QUEUE_MASK;

	if ((dev->vq[0].notified_avail & VIONET_QUEUE_MASK) == idx) {
		log_warnx("vionet queue notify - no space, dropping packet");
		goto out;
	}

	hdr_desc_idx = avail->ring[idx] & VIONET_QUEUE_MASK;
	hdr_desc = &desc[hdr_desc_idx];

	pkt_desc_idx = hdr_desc->next & VIONET_QUEUE_MASK;
	pkt_desc = &desc[pkt_desc_idx];

	/* must be not readable */
	if ((pkt_desc->flags & VRING_DESC_F_WRITE) == 0) {
		log_warnx("unexpected readable rx descriptor %d",
		    pkt_desc_idx);
		goto out;
	}

	/* Write packet to descriptor ring */
	if (write_mem(pkt_desc->addr, pkt, sz)) {
		log_warnx("vionet: rx enq packet write_mem error @@ "
		    "0x%llx", pkt_desc->addr);
		goto out;
	}

	ret = 1;
	dev->cfg.isr_status = 1;
	ue = &used->ring[used->idx & VIONET_QUEUE_MASK];
	ue->id = hdr_desc_idx;
	ue->len = hdr_desc->len + sz;
	used->idx++;
	dev->vq[0].last_avail = (dev->vq[0].last_avail + 1);
	*spc = dev->vq[0].notified_avail - dev->vq[0].last_avail;

	off = (char *)ue - vr;
	if (write_mem(q_gpa + off, ue, sizeof *ue))
		log_warnx("vionet: error writing vio ring");
	else {
		off = (char *)&used->idx - vr;
		if (write_mem(q_gpa + off, &used->idx, sizeof used->idx))
			log_warnx("vionet: error writing vio ring");
	}
out:
	free(vr);
	return (ret);
}

/*
 * vionet_rx
 *
 * Enqueue data that was received on a tap file descriptor
 * to the vionet device queue.
 *
 * Must be called with dev->mutex acquired.
 */
static int
vionet_rx(struct vionet_dev *dev)
{
	char buf[PAGE_SIZE];
	int hasdata, num_enq = 0, spc = 0;
	struct ether_header *eh;
	ssize_t sz;

	do {
		sz = read(dev->fd, buf, sizeof buf);
		if (sz == -1) {
			/*
			 * If we get EAGAIN, No data is currently available.
			 * Do not treat this as an error.
			 */
			if (errno != EAGAIN)
				log_warn("unexpected read error on vionet "
				    "device");
		} else if (sz != 0) {
			eh = (struct ether_header *)buf;
			if (!dev->lockedmac || sz < ETHER_HDR_LEN ||
			    ETHER_IS_MULTICAST(eh->ether_dhost) ||
			    memcmp(eh->ether_dhost, dev->mac,
			    sizeof(eh->ether_dhost)) == 0)
				num_enq += vionet_enq_rx(dev, buf, sz, &spc);
		} else if (sz == 0) {
			log_debug("process_rx: no data");
			hasdata = 0;
			break;
		}

		hasdata = fd_hasdata(dev->fd);
	} while (spc && hasdata);

	dev->rx_pending = hasdata;
	return (num_enq);
}

/*
 * vionet_rx_event
 *
 * Called from the event handling thread when new data can be
 * received on the tap fd of a vionet device.
 */
static void
vionet_rx_event(int fd, short kind, void *arg)
{
	struct vionet_dev *dev = arg;

	mutex_lock(&dev->mutex);

	/*
	 * We already have other data pending to be received. The data that
	 * has become available now will be enqueued to the vionet_dev
	 * later.
	 */
	if (dev->rx_pending) {
		mutex_unlock(&dev->mutex);
		return;
	}

	if (vionet_rx(dev) > 0) {
		/* XXX: vcpu_id */
		vcpu_assert_pic_irq(dev->vm_id, 0, dev->irq);
	}

	mutex_unlock(&dev->mutex);
}

/*
 * vionet_process_rx
 *
 * Processes any remaining pending receivable data for a vionet device.
 * Called on VCPU exit. Although we poll on the tap file descriptor of
 * a vionet_dev in a separate thread, this function still needs to be
 * called on VCPU exit: it can happen that not all data fits into the
 * receive queue of the vionet_dev immediately. So any outstanding data
 * is handled here.
 *
 * Parameters:
 *  vm_id: VM ID of the VM for which to process vionet events
 */
void
vionet_process_rx(uint32_t vm_id)
{
	int i;

	for (i = 0 ; i < nr_vionet; i++) {
		mutex_lock(&vionet[i].mutex);
		if (!vionet[i].rx_added) {
			mutex_unlock(&vionet[i].mutex);
			continue;
		}

		if (vionet[i].rx_pending) {
			if (vionet_rx(&vionet[i])) {
				vcpu_assert_pic_irq(vm_id, 0, vionet[i].irq);
			}
		}
		mutex_unlock(&vionet[i].mutex);
	}
}

/*
 * Must be called with dev->mutex acquired.
 */
void
vionet_notify_rx(struct vionet_dev *dev)
{
	uint64_t q_gpa;
	uint32_t vr_sz;
	char *vr;
	struct vring_avail *avail;

	vr_sz = vring_size(VIONET_QUEUE_SIZE);
	q_gpa = dev->vq[dev->cfg.queue_notify].qa;
	q_gpa = q_gpa * VIRTIO_PAGE_SIZE;

	vr = malloc(vr_sz);
	if (vr == NULL) {
		log_warn("malloc error getting vionet ring");
		return;
	}

	if (read_mem(q_gpa, vr, vr_sz)) {
		log_warnx("error reading gpa 0x%llx", q_gpa);
		free(vr);
		return;
	}

	/* Compute offset into avail ring */
	avail = (struct vring_avail *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_availoffset);

	dev->rx_added = 1;
	dev->vq[0].notified_avail = avail->idx;

	free(vr);
}

/*
 * Must be called with dev->mutex acquired.
 *
 * XXX cant trust ring data from VM, be extra cautious.
 * XXX advertise link status to guest
 */
int
vionet_notifyq(struct vionet_dev *dev)
{
	uint64_t q_gpa;
	uint32_t vr_sz;
	uint16_t idx, pkt_desc_idx, hdr_desc_idx, dxx;
	size_t pktsz;
	int ret, num_enq, ofs;
	char *vr, *pkt;
	struct vring_desc *desc, *pkt_desc, *hdr_desc;
	struct vring_avail *avail;
	struct vring_used *used;
	struct ether_header *eh;

	vr = pkt = NULL;
	ret = 0;

	/* Invalid queue? */
	if (dev->cfg.queue_notify != 1) {
		vionet_notify_rx(dev);
		goto out;
	}

	vr_sz = vring_size(VIONET_QUEUE_SIZE);
	q_gpa = dev->vq[dev->cfg.queue_notify].qa;
	q_gpa = q_gpa * VIRTIO_PAGE_SIZE;

	vr = calloc(1, vr_sz);
	if (vr == NULL) {
		log_warn("calloc error getting vionet ring");
		goto out;
	}

	if (read_mem(q_gpa, vr, vr_sz)) {
		log_warnx("error reading gpa 0x%llx", q_gpa);
		goto out;
	}

	/* Compute offsets in ring of descriptors, avail ring, and used ring */
	desc = (struct vring_desc *)(vr);
	avail = (struct vring_avail *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_availoffset);
	used = (struct vring_used *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_usedoffset);

	num_enq = 0;

	idx = dev->vq[dev->cfg.queue_notify].last_avail & VIONET_QUEUE_MASK;

	if ((avail->idx & VIONET_QUEUE_MASK) == idx) {
		log_warnx("vionet tx queue notify - nothing to do?");
		goto out;
	}

	while ((avail->idx & VIONET_QUEUE_MASK) != idx) {
		hdr_desc_idx = avail->ring[idx] & VIONET_QUEUE_MASK;
		hdr_desc = &desc[hdr_desc_idx];
		pktsz = 0;

		dxx = hdr_desc_idx;
		do {
			pktsz += desc[dxx].len;
			dxx = desc[dxx].next;
		} while (desc[dxx].flags & VRING_DESC_F_NEXT);

		pktsz += desc[dxx].len;

		/* Remove virtio header descriptor len */
		pktsz -= hdr_desc->len;

		/*
		 * XXX check sanity pktsz
		 * XXX too long and  > PAGE_SIZE checks
		 *     (PAGE_SIZE can be relaxed to 16384 later)
		 */
		pkt = malloc(pktsz);
		if (pkt == NULL) {
			log_warn("malloc error alloc packet buf");
			goto out;
		}

		ofs = 0;
		pkt_desc_idx = hdr_desc->next & VIONET_QUEUE_MASK;
		pkt_desc = &desc[pkt_desc_idx];

		while (pkt_desc->flags & VRING_DESC_F_NEXT) {
			/* must be not writable */
			if (pkt_desc->flags & VRING_DESC_F_WRITE) {
				log_warnx("unexpected writable tx desc "
				    "%d", pkt_desc_idx);
				goto out;
			}

			/* Read packet from descriptor ring */
			if (read_mem(pkt_desc->addr, pkt + ofs,
			    pkt_desc->len)) {
				log_warnx("vionet: packet read_mem error "
				    "@@ 0x%llx", pkt_desc->addr);
				goto out;
			}

			ofs += pkt_desc->len;
			pkt_desc_idx = pkt_desc->next & VIONET_QUEUE_MASK;
			pkt_desc = &desc[pkt_desc_idx];
		}

		/* Now handle tail descriptor - must be not writable */
		if (pkt_desc->flags & VRING_DESC_F_WRITE) {
			log_warnx("unexpected writable tx descriptor %d",
			    pkt_desc_idx);
			goto out;
		}

		/* Read packet from descriptor ring */
		if (read_mem(pkt_desc->addr, pkt + ofs,
		    pkt_desc->len)) {
			log_warnx("vionet: packet read_mem error @@ "
			    "0x%llx", pkt_desc->addr);
			goto out;
		}

		/* reject other source addresses */
		if (dev->lockedmac && pktsz >= ETHER_HDR_LEN &&
		    (eh = (struct ether_header *)pkt) &&
		    memcmp(eh->ether_shost, dev->mac,
		    sizeof(eh->ether_shost)) != 0)
			log_debug("vionet: wrong source address %s for vm %d",
			    ether_ntoa((struct ether_addr *)
			    eh->ether_shost), dev->vm_id);
		/* XXX signed vs unsigned here, funky cast */
		else if (write(dev->fd, pkt, pktsz) != (int)pktsz) {
			log_warnx("vionet: tx failed writing to tap: "
			    "%d", errno);
			goto out;
		}

		ret = 1;
		dev->cfg.isr_status = 1;
		used->ring[used->idx & VIONET_QUEUE_MASK].id = hdr_desc_idx;
		used->ring[used->idx & VIONET_QUEUE_MASK].len = hdr_desc->len;
		used->idx++;

		dev->vq[dev->cfg.queue_notify].last_avail =
		    (dev->vq[dev->cfg.queue_notify].last_avail + 1);
		num_enq++;

		idx = dev->vq[dev->cfg.queue_notify].last_avail &
		    VIONET_QUEUE_MASK;
	}

	if (write_mem(q_gpa, vr, vr_sz)) {
		log_warnx("vionet: tx error writing vio ring");
	}

out:
	free(vr);
	free(pkt);

	return (ret);
}

int
vmmci_ctl(unsigned int cmd)
{
	struct timeval tv = { 0, 0 };

	if ((vmmci.cfg.device_status &
	    VIRTIO_CONFIG_DEVICE_STATUS_DRIVER_OK) == 0)
		return (-1);

	if (cmd == vmmci.cmd)
		return (0);

	switch (cmd) {
	case VMMCI_NONE:
		break;
	case VMMCI_SHUTDOWN:
	case VMMCI_REBOOT:
		/* Update command */
		vmmci.cmd = cmd;

		/*
		 * vmm VMs do not support powerdown, send a reboot request
		 * instead and turn it off after the triple fault.
		 */
		if (cmd == VMMCI_SHUTDOWN)
			cmd = VMMCI_REBOOT;

		/* Trigger interrupt */
		vmmci.cfg.isr_status = VIRTIO_CONFIG_ISR_CONFIG_CHANGE;
		vcpu_assert_pic_irq(vmmci.vm_id, 0, vmmci.irq);

		/* Add ACK timeout */
		tv.tv_sec = VMMCI_TIMEOUT;
		evtimer_add(&vmmci.timeout, &tv);
		break;
	default:
		fatalx("invalid vmmci command: %d", cmd);
	}

	return (0);
}

void
vmmci_ack(unsigned int cmd)
{
	struct timeval	 tv = { 0, 0 };

	switch (cmd) {
	case VMMCI_NONE:
		break;
	case VMMCI_SHUTDOWN:
		/*
		 * The shutdown was requested by the VM if we don't have
		 * a pending shutdown request.  In this case add a short
		 * timeout to give the VM a chance to reboot before the
		 * timer is expired.
		 */
		if (vmmci.cmd == 0) {
			log_debug("%s: vm %u requested shutdown", __func__,
			    vmmci.vm_id);
			tv.tv_sec = VMMCI_TIMEOUT;
			evtimer_add(&vmmci.timeout, &tv);
			return;
		}
		/* FALLTHROUGH */
	case VMMCI_REBOOT:
		/*
		 * If the VM acknowleged our shutdown request, give it
		 * enough time to shutdown or reboot gracefully.  This
		 * might take a considerable amount of time (running
		 * rc.shutdown on the VM), so increase the timeout before
		 * killing it forcefully.
		 */
		if (cmd == vmmci.cmd &&
		    evtimer_pending(&vmmci.timeout, NULL)) {
			log_debug("%s: vm %u acknowledged shutdown request",
			    __func__, vmmci.vm_id);
			tv.tv_sec = VMMCI_SHUTDOWN_TIMEOUT;
			evtimer_add(&vmmci.timeout, &tv);
		}
		break;
	default:
		log_warnx("%s: illegal request %u", __func__, cmd);
		break;
	}
}

void
vmmci_timeout(int fd, short type, void *arg)
{
	log_debug("%s: vm %u shutdown", __progname, vmmci.vm_id);
	vm_shutdown(vmmci.cmd == VMMCI_REBOOT ? VMMCI_REBOOT : VMMCI_SHUTDOWN);
}

int
vmmci_io(int dir, uint16_t reg, uint32_t *data, uint8_t *intr,
    void *unused, uint8_t sz)
{
	*intr = 0xFF;

	if (dir == 0) {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_FEATURES:
		case VIRTIO_CONFIG_QUEUE_SIZE:
		case VIRTIO_CONFIG_ISR_STATUS:
			log_warnx("%s: illegal write %x to %s",
			    __progname, *data, virtio_reg_name(reg));
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			vmmci.cfg.guest_feature = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			vmmci.cfg.queue_address = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			vmmci.cfg.queue_select = *data;
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			vmmci.cfg.queue_notify = *data;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			vmmci.cfg.device_status = *data;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI:
			vmmci_ack(*data);
			break;
		}
	} else {
		switch (reg) {
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI:
			*data = vmmci.cmd;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 4:
			/* Update time once when reading the first register */
			gettimeofday(&vmmci.time, NULL);
			*data = (uint64_t)vmmci.time.tv_sec;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 8:
			*data = (uint64_t)vmmci.time.tv_sec << 32;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 12:
			*data = (uint64_t)vmmci.time.tv_usec;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 16:
			*data = (uint64_t)vmmci.time.tv_usec << 32;
			break;
		case VIRTIO_CONFIG_DEVICE_FEATURES:
			*data = vmmci.cfg.device_feature;
			break;
		case VIRTIO_CONFIG_GUEST_FEATURES:
			*data = vmmci.cfg.guest_feature;
			break;
		case VIRTIO_CONFIG_QUEUE_ADDRESS:
			*data = vmmci.cfg.queue_address;
			break;
		case VIRTIO_CONFIG_QUEUE_SIZE:
			*data = vmmci.cfg.queue_size;
			break;
		case VIRTIO_CONFIG_QUEUE_SELECT:
			*data = vmmci.cfg.queue_select;
			break;
		case VIRTIO_CONFIG_QUEUE_NOTIFY:
			*data = vmmci.cfg.queue_notify;
			break;
		case VIRTIO_CONFIG_DEVICE_STATUS:
			*data = vmmci.cfg.device_status;
			break;
		case VIRTIO_CONFIG_ISR_STATUS:
			*data = vmmci.cfg.isr_status;
			vmmci.cfg.isr_status = 0;
			break;
		}
	}
	return (0);
}

void
virtio_init(struct vmop_create_params *vmc, int *child_disks, int *child_taps)
{
	struct vm_create_params *vcp = &vmc->vmc_params;
	static const uint8_t zero_mac[6];
	uint8_t id;
	uint8_t i;
	int ret, rng;
	off_t sz;

	/* Virtio entropy device */
	if (pci_add_device(&id, PCI_VENDOR_QUMRANET,
	    PCI_PRODUCT_QUMRANET_VIO_RNG, PCI_CLASS_SYSTEM,
	    PCI_SUBCLASS_SYSTEM_MISC,
	    PCI_VENDOR_OPENBSD,
	    PCI_PRODUCT_VIRTIO_ENTROPY, 1, NULL)) {
		log_warnx("%s: can't add PCI virtio rng device",
		    __progname);
		return;
	}

	if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, virtio_rnd_io, NULL)) {
		log_warnx("%s: can't add bar for virtio rng device",
		    __progname);
		return;
	}

	memset(&viornd, 0, sizeof(viornd));
	viornd.vq[0].qs = VIORND_QUEUE_SIZE;
	viornd.vq[0].vq_availoffset = sizeof(struct vring_desc) *
	    VIORND_QUEUE_SIZE;
	viornd.vq[0].vq_usedoffset = VIRTQUEUE_ALIGN(
	    sizeof(struct vring_desc) * VIORND_QUEUE_SIZE
	    + sizeof(uint16_t) * (2 + VIORND_QUEUE_SIZE));

	if (vcp->vcp_ndisks > 0) {
		vioblk = calloc(vcp->vcp_ndisks, sizeof(struct vioblk_dev));
		if (vioblk == NULL) {
			log_warn("%s: calloc failure allocating vioblks",
			    __progname);
			return;
		}

		/* One virtio block device for each disk defined in vcp */
		for (i = 0; i < vcp->vcp_ndisks; i++) {
			if ((sz = lseek(child_disks[i], 0, SEEK_END)) == -1)
				continue;

			if (pci_add_device(&id, PCI_VENDOR_QUMRANET,
			    PCI_PRODUCT_QUMRANET_VIO_BLOCK,
			    PCI_CLASS_MASS_STORAGE,
			    PCI_SUBCLASS_MASS_STORAGE_SCSI,
			    PCI_VENDOR_OPENBSD,
			    PCI_PRODUCT_VIRTIO_BLOCK, 1, NULL)) {
				log_warnx("%s: can't add PCI virtio block "
				    "device", __progname);
				return;
			}
			if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, virtio_blk_io,
			    &vioblk[i])) {
				log_warnx("%s: can't add bar for virtio block "
				    "device", __progname);
				return;
			}
			vioblk[i].vq[0].qs = VIOBLK_QUEUE_SIZE;
			vioblk[i].vq[0].vq_availoffset =
			    sizeof(struct vring_desc) * VIORND_QUEUE_SIZE;
			vioblk[i].vq[0].vq_usedoffset = VIRTQUEUE_ALIGN(
			    sizeof(struct vring_desc) * VIOBLK_QUEUE_SIZE
			    + sizeof(uint16_t) * (2 + VIOBLK_QUEUE_SIZE));
			vioblk[i].vq[0].last_avail = 0;
			vioblk[i].fd = child_disks[i];
			vioblk[i].sz = sz / 512;
			vioblk[i].cfg.device_feature = VIRTIO_BLK_F_SIZE_MAX;
			vioblk[i].max_xfer = 1048576;
		}
	}

	if (vcp->vcp_nnics > 0) {
		vionet = calloc(vcp->vcp_nnics, sizeof(struct vionet_dev));
		if (vionet == NULL) {
			log_warn("%s: calloc failure allocating vionets",
			    __progname);
			return;
		}

		nr_vionet = vcp->vcp_nnics;
		/* Virtio network */
		for (i = 0; i < vcp->vcp_nnics; i++) {
			if (pci_add_device(&id, PCI_VENDOR_QUMRANET,
			    PCI_PRODUCT_QUMRANET_VIO_NET, PCI_CLASS_SYSTEM,
			    PCI_SUBCLASS_SYSTEM_MISC,
			    PCI_VENDOR_OPENBSD,
			    PCI_PRODUCT_VIRTIO_NETWORK, 1, NULL)) {
				log_warnx("%s: can't add PCI virtio net device",
				    __progname);
				return;
			}

			if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, virtio_net_io,
			    &vionet[i])) {
				log_warnx("%s: can't add bar for virtio net "
				    "device", __progname);
				return;
			}

			ret = pthread_mutex_init(&vionet[i].mutex, NULL);
			if (ret) {
				errno = ret;
				log_warn("%s: could not initialize mutex "
				    "for vionet device", __progname);
				return;
			}

			vionet[i].vq[0].qs = VIONET_QUEUE_SIZE;
			vionet[i].vq[0].vq_availoffset =
			    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE;
			vionet[i].vq[0].vq_usedoffset = VIRTQUEUE_ALIGN(
			    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE
			    + sizeof(uint16_t) * (2 + VIONET_QUEUE_SIZE));
			vionet[i].vq[0].last_avail = 0;
			vionet[i].vq[1].qs = VIONET_QUEUE_SIZE;
			vionet[i].vq[1].vq_availoffset =
			    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE;
			vionet[i].vq[1].vq_usedoffset = VIRTQUEUE_ALIGN(
			    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE
			    + sizeof(uint16_t) * (2 + VIONET_QUEUE_SIZE));
			vionet[i].vq[1].last_avail = 0;
			vionet[i].vq[1].notified_avail = 0;
			vionet[i].fd = child_taps[i];
			vionet[i].rx_pending = 0;
			vionet[i].vm_id = vcp->vcp_id;
			vionet[i].irq = pci_get_dev_irq(id);

			event_set(&vionet[i].event, vionet[i].fd,
			    EV_READ | EV_PERSIST, vionet_rx_event, &vionet[i]);
			if (event_add(&vionet[i].event, NULL)) {
				log_warn("could not initialize vionet event "
				    "handler");
				return;
			}

			vionet[i].cfg.device_feature = VIRTIO_NET_F_MAC;

			if (memcmp(zero_mac, &vcp->vcp_macs[i], 6) != 0) {
				/* User-defined address */
				memcpy(&vionet[i].mac, &vcp->vcp_macs[i], 6);
			} else {
				/*
				 * If the address is zero, always randomize
				 * it in vmd(8) because we cannot rely on
				 * the guest OS to do the right thing like
				 * OpenBSD does.  Based on ether_fakeaddr()
				 * from the kernel, incremented by one to
				 * differentiate the source.
				 */
				rng = arc4random();
				vionet[i].mac[0] = 0xfe;
				vionet[i].mac[1] = 0xe1;
				vionet[i].mac[2] = 0xba + 1;
				vionet[i].mac[3] = 0xd0 | ((i + 1) & 0xf);
				vionet[i].mac[4] = rng;
				vionet[i].mac[5] = rng >> 8;
			}
			vionet[i].lockedmac =
			    vmc->vmc_ifflags[i] & VMIFF_LOCKED ? 1 : 0;

			log_debug("%s: vm \"%s\" vio%u lladdr %s%s",
			    __func__, vcp->vcp_name, i,
			    ether_ntoa((void *)vionet[i].mac),
			    vionet[i].lockedmac ? " (locked)" : "");
		}
	}

	/* virtio control device */
	if (pci_add_device(&id, PCI_VENDOR_OPENBSD,
	    PCI_PRODUCT_OPENBSD_CONTROL,
	    PCI_CLASS_COMMUNICATIONS,
	    PCI_SUBCLASS_COMMUNICATIONS_MISC,
	    PCI_VENDOR_OPENBSD,
	    PCI_PRODUCT_VIRTIO_VMMCI, 1, NULL)) {
		log_warnx("%s: can't add PCI vmm control device",
		    __progname);
		return;
	}

	if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, vmmci_io, NULL)) {
		log_warnx("%s: can't add bar for vmm control device",
		    __progname);
		return;
	}

	memset(&vmmci, 0, sizeof(vmmci));
	vmmci.cfg.device_feature = VMMCI_F_TIMESYNC|VMMCI_F_ACK;
	vmmci.vm_id = vcp->vcp_id;
	vmmci.irq = pci_get_dev_irq(id);

	evtimer_set(&vmmci.timeout, vmmci_timeout, NULL);
}
@


1.39
log
@Last bits needed to get seabios + alpine linux working. This is enough
to get started and let more people help finding and fixing bugs.

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.38 2017/03/25 16:34:26 mlarkin Exp $	*/
d516 2
a517 1
				    "descriptor received (idx %d)", cmd_desc_idx);
d521 3
a523 4
			secdata = malloc(MAXPHYS);
			if (secdata == NULL) {
				log_warn("wr vioblk: malloc error, len %d",
				    secdata_desc->len);
d527 1
d530 7
d542 2
a543 1
					dump_descriptor_chain(desc, cmd_desc_idx);
d550 2
a551 1
					log_warnx("wr vioblk: disk write error");
d556 2
a557 1
				secbias += secdata_desc->len / VIRTIO_BLK_SECTOR_SIZE;
d571 2
a572 2
				log_warnx("wr vioblk: can't write device status "
				    "data @@ 0x%llx", ds_desc->addr);
d579 4
a582 2
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
d605 4
a608 2
			used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
			used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
d617 32
d666 1
d791 40
d1657 2
@


1.38
log
@Process more than one avail slot in vioblk ring, if requested by the
guest VM. We probably need to do this same fix for vionet later.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.37 2017/03/25 16:24:44 mlarkin Exp $	*/
d753 9
a761 1
			*data = dev->cfg.queue_size;
d770 9
a778 1
			*data = dev->cfg.device_status;
@


1.37
log
@The virtio spec says isr_status should be cleared to 0 on read, which
we were not doing. That confused the virtio subsystem in linux.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.36 2017/03/25 16:05:33 mlarkin Exp $	*/
a414 1

d422 1
a422 2
	cmd_desc_idx = avail->ring[idx] & VIOBLK_QUEUE_MASK;
	cmd_desc = &desc[cmd_desc_idx];
d424 2
a425 5
	if ((cmd_desc->flags & VRING_DESC_F_NEXT) == 0) {
		log_warnx("unchained vioblk cmd descriptor received "
		    "(idx %d)", cmd_desc_idx);
		goto out;
	}
d427 5
a431 6
	/* Read command from descriptor ring */
	if (read_mem(cmd_desc->addr, &cmd, cmd_desc->len)) {
		log_warnx("vioblk: command read_mem error @@ 0x%llx",
		    cmd_desc->addr);
		goto out;
	}
d433 4
a436 9
	switch (cmd.type) {
	case VIRTIO_BLK_T_IN:
		/* first descriptor */
		secdata_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
		secdata_desc = &desc[secdata_desc_idx];

		if ((secdata_desc->flags & VRING_DESC_F_NEXT) == 0) {
			log_warnx("unchained vioblk data descriptor "
			    "received (idx %d)", cmd_desc_idx);
d440 9
a448 12
		secbias = 0;
		do {
			/* read the data (use current data descriptor) */
			/*
			 * XXX waste to malloc secdata in vioblk_do_read
			 * and free it here over and over
			 */
			secdata = vioblk_do_read(dev, cmd.sector + secbias,
			    (ssize_t)secdata_desc->len);
			if (secdata == NULL) {
				log_warnx("vioblk: block read error, "
				    "sector %lld", cmd.sector);
d452 40
a491 5
			if (write_mem(secdata_desc->addr, secdata,
			    secdata_desc->len)) {
				log_warnx("can't write sector "
				    "data to gpa @@ 0x%llx",
				    secdata_desc->addr);
a492 1
				free(secdata);
a495 1
			free(secdata);
d497 7
a503 2
			secbias += (secdata_desc->len / VIRTIO_BLK_SECTOR_SIZE);
			secdata_desc_idx = secdata_desc->next &
d505 7
a512 1
		} while (secdata_desc->flags & VRING_DESC_F_NEXT);
d514 5
a518 2
		ds_desc_idx = secdata_desc_idx;
		ds_desc = secdata_desc;
d520 6
a525 7
		ds = VIRTIO_BLK_S_OK;
		if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
			log_warnx("can't write device status data @@ "
			    "0x%llx", ds_desc->addr);
			dump_descriptor_chain(desc, cmd_desc_idx);
			goto out;
		}
d527 25
d553 1
a553 5
		ret = 1;
		dev->cfg.isr_status = 1;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
		used->idx++;
d555 2
a556 2
		dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
		    VIOBLK_QUEUE_MASK;
d558 4
a561 28
		if (write_mem(q_gpa, vr, vr_sz)) {
			log_warnx("vioblk: error writing vio ring");
		}
		break;
	case VIRTIO_BLK_T_OUT:
		secdata_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
		secdata_desc = &desc[secdata_desc_idx];

		if ((secdata_desc->flags & VRING_DESC_F_NEXT) == 0) {
			log_warnx("wr vioblk: unchained vioblk data "
			    "descriptor received (idx %d)", cmd_desc_idx);
			goto out;
		}

		secdata = malloc(MAXPHYS);
		if (secdata == NULL) {
			log_warn("wr vioblk: malloc error, len %d",
			    secdata_desc->len);
			goto out;
		}

		secbias = 0;
		do {
			if (read_mem(secdata_desc->addr, secdata,
			    secdata_desc->len)) {
				log_warnx("wr vioblk: can't read "
				    "sector data @@ 0x%llx",
				    secdata_desc->addr);
a562 1
				free(secdata);
d566 21
a586 4
			if (vioblk_do_write(dev, cmd.sector + secbias,
			    secdata, (ssize_t)secdata_desc->len)) {
				log_warnx("wr vioblk: disk write error");
				free(secdata);
d590 5
a594 1
			secbias += secdata_desc->len / VIRTIO_BLK_SECTOR_SIZE;
d596 1
a596 1
			secdata_desc_idx = secdata_desc->next &
d598 4
a601 14
			secdata_desc = &desc[secdata_desc_idx];
		} while (secdata_desc->flags & VRING_DESC_F_NEXT);

		free(secdata);

		ds_desc_idx = secdata_desc_idx;
		ds_desc = secdata_desc;

		ds = VIRTIO_BLK_S_OK;
		if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
			log_warnx("wr vioblk: can't write device status "
			    "data @@ 0x%llx", ds_desc->addr);
			dump_descriptor_chain(desc, cmd_desc_idx);
			goto out;
d604 1
a604 36
		ret = 1;
		dev->cfg.isr_status = 1;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
		used->idx++;

		dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
		    VIOBLK_QUEUE_MASK;
		if (write_mem(q_gpa, vr, vr_sz))
			log_warnx("wr vioblk: error writing vio ring");
		break;
	case VIRTIO_BLK_T_FLUSH:
	case VIRTIO_BLK_T_FLUSH_OUT:
		ds_desc_idx = cmd_desc->next & VIOBLK_QUEUE_MASK;
		ds_desc = &desc[ds_desc_idx];

		ds = VIRTIO_BLK_S_OK;
		if (write_mem(ds_desc->addr, &ds, ds_desc->len)) {
			log_warnx("fl vioblk: can't write device status "
			    "data @@ 0x%llx", ds_desc->addr);
			dump_descriptor_chain(desc, cmd_desc_idx);
			goto out;
		}

		ret = 1;
		dev->cfg.isr_status = 1;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].id = cmd_desc_idx;
		used->ring[used->idx & VIOBLK_QUEUE_MASK].len = cmd_desc->len;
		used->idx++;

		dev->vq[dev->cfg.queue_notify].last_avail = avail->idx &
		    VIOBLK_QUEUE_MASK;
		if (write_mem(q_gpa, vr, vr_sz)) {
			log_warnx("fl vioblk: error writing vio ring");
		}
		break;
@


1.36
log
@implement missing vioblk reset function and improve the partially
implemented vionet one
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.35 2017/03/25 15:47:37 mlarkin Exp $	*/
d295 1
d762 1
d857 1
d1469 1
@


1.35
log
@Implement some missing functionality and clean up some code in vmd
pci emulation.

ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.34 2017/03/15 18:06:18 reyk Exp $	*/
d640 12
d802 10
a811 1
			if (*data == 0) {
a815 1
				/* XXX do proper reset */
@


1.34
log
@Improve vmmci(4) shutdown and reboot.

This change handles various cases to power off the VM, even if it is
unresponsive, stuck in ddb, or when the shutdown was initiated from
the VM guest side.  Usage of timeout and VM ACKs make sure that the VM
is really turned off at some point.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.33 2017/03/02 07:33:37 reyk Exp $	*/
d238 1
a238 1
    void *unused)
d608 1
a608 1
    void *cookie)
d646 40
d687 31
a717 1
			*data = (uint32_t)(dev->sz >> 32);
d719 6
a724 2
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI:
			*data = (uint32_t)(dev->sz);
d757 1
a757 1
    void *cookie)
d1373 1
a1373 1
    void *unused)
@


1.33
log
@Add "locked lladdr" option to prevent VMs from spoofing MAC addresses.

This is especially useful when multiple VMs share a switch, the
implementation is independent from the underlying switch or bridge.

no objections mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.32 2017/02/13 18:49:01 reyk Exp $	*/
d60 1
d1206 2
d1217 1
d1233 4
d1245 52
d1326 3
d1568 2
a1569 1
	vmmci.cfg.device_feature = VMMCI_F_TIMESYNC;
d1572 2
@


1.32
log
@Fix powerdown with vmmci(4) VMs using a shutdown and no reset.

vmm VMs don't support powerdown - no ACPI or power management - so we
use a trick to issue a reboot and just don't reset after the triple
fault.  This worked before but was broken with the previous fix to
pvbus_shutdown() - move the trick to vmd instead.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.31 2017/01/21 11:36:54 reyk Exp $	*/
d900 1
d913 8
a920 3
		} else if (sz != 0)
			num_enq += vionet_enq_rx(dev, buf, sz, &spc);
		else if (sz == 0) {
d1053 1
d1162 8
d1171 1
a1171 1
		if (write(dev->fd, pkt, pktsz) != (int)pktsz) {
d1316 1
a1316 1
virtio_init(struct vm_create_params *vcp, int *child_disks, int *child_taps)
d1318 1
d1477 2
d1480 1
a1480 1
			log_debug("%s: vm \"%s\" vio%u lladdr %s",
d1482 2
a1483 1
			    ether_ntoa((void *)vionet[i].mac));
@


1.31
log
@Switch include of virtio header from dev/pci/ to dev/pv/
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.30 2017/01/19 10:16:22 reyk Exp $	*/
d1203 7
@


1.30
log
@Export the host time to the guest, add it as a timedelta sensor in vmmci(4)

OK kettenis@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.29 2017/01/17 21:51:01 krw Exp $	*/
d25 2
a26 2
#include <dev/pci/virtioreg.h>
#include <dev/pci/vioblkreg.h>
@


1.29
log
@Nuke some whitespace that keeps poking me in the eye as I try to
steal code.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.28 2017/01/13 15:12:13 reyk Exp $	*/
d59 2
d115 2
d1250 14
d1479 1
a1479 1
	vmmci.cfg.device_feature = 0;
@


1.28
log
@I accidentally committed three lines from the future.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.27 2017/01/13 14:50:56 reyk Exp $	*/
d1422 1
a1422 1
				 * it in vmd(8) because we cannot rely on 
d1424 1
a1424 1
			 	 * OpenBSD does.  Based on ether_fakeaddr()
@


1.27
log
@Add host side of vmmci(4) to vmd(8).

It currently uses the device to request graceful shutdown of a VM on
"vmctl stop myvm" but will be extended for reboot and a other edge cases.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.26 2016/12/05 09:28:11 reyk Exp $	*/
a1243 3
			*data = vmmci.cmd;
			break;
		case VIRTIO_CONFIG_DEVICE_CONFIG_NOMSI + 1:
@


1.26
log
@Add debug message to print VM's mac addresses (lladdr).
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.25 2016/11/26 16:05:11 sf Exp $	*/
d51 1
a58 1

d1183 95
d1445 22
@


1.25
log
@virtio-net: Partially implement device reset

Implement some bits of the device reset and don't do DMA if the DRIVER_OK bit
is not set. This is enough to avoid vio(4) panicking on if down/up.

Tested by rzalamena@@ and mikeb@@
OK reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.24 2016/10/18 05:33:57 reyk Exp $	*/
d20 1
d28 4
d1344 4
@


1.24
log
@When the guest OS gets a zero MAC address on a network interface, it should
create a randomized locally administrated address.  OpenBSD as a guest OS
does this correctly but we cannot rely on it for other guests, so randomize
the MAC address in vmd(8) on the host side if it is not specified by the
user.  I incremented the prefix by one to differentiate from the ones
that are generated by OpenBSD in the kernel; fe:e1:bb:xx:xx:xx.

OK deraadt@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.23 2016/10/16 19:07:05 guenther Exp $	*/
d706 7
d805 3
@


1.23
log
@Prefer memcpy/memmove over bcopy

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.22 2016/10/12 06:56:54 mlarkin Exp $	*/
d1174 1
a1174 1
	int ret;
d1307 2
a1308 1
			/* User defined MAC */
d1310 1
a1310 2
				vionet[i].cfg.device_feature =
				    VIRTIO_NET_F_MAC;
d1312 16
@


1.22
log
@
Allow 4 vio(4) interfaces in each VM. Also fix a bad interrupt assignment that
caused IRQ9 to be shared between the second disk device and the vio(4)s,
which caused poor network performance.

ok reyk, stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.21 2016/10/05 17:30:13 reyk Exp $	*/
d1311 1
a1311 1
				bcopy(&vcp->vcp_macs[i], &vionet[i].mac, 6);
@


1.21
log
@Add support for enhanced networking configuration and virtual switches.
See vm.conf(5) for more details.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.20 2016/10/03 05:59:24 mlarkin Exp $	*/
d948 3
d952 2
a953 2
int
vionet_process_rx(void)
d955 1
a955 1
	int i, num_enq;
a956 1
	num_enq = 0;
d964 5
a968 2
		if (vionet[i].rx_pending)
			num_enq += vionet_rx(&vionet[i]);
a970 6

	/*
	 * XXX returns the number of packets enqueued across all vionet, which
	 * may not be right for VMs with more than one vionet.
	 */
	return (num_enq);
d1297 1
a1297 1
			vionet[i].irq = 9; /* XXX */
@


1.20
log
@
style cleanup
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.19 2016/09/03 11:35:24 nayden Exp $	*/
d1172 1
a1307 1
#if 0
d1309 5
a1313 3
			vionet[i].cfg.device_feature = VIRTIO_NET_F_MAC;
			bcopy(&vcp->vcp_macs[i], &vionet[i].mac, 6);
#endif
@


1.19
log
@Adding a clean up block to vioblk_notifyq() and vionet_enq_rx()
OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.18 2016/09/02 17:08:28 stefan Exp $	*/
d816 2
a817 4
	avail = (struct vring_avail *)(vr +
	    dev->vq[0].vq_availoffset);
	used = (struct vring_used *)(vr +
	    dev->vq[0].vq_usedoffset);
@


1.18
log
@Process incoming host->guest packets asynchronously to running VCPU

This registers a handler with libevent that is called on incoming packets
for the guest. If they cannot be handled immediately (because the virtq is
full), make sure they are handled on VCPU exits.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.17 2016/08/17 05:07:13 deraadt Exp $	*/
d358 1
a358 2
 * XXX this function needs a cleanup block, lots of free(blah); return (0)
 *     in various cases, ds should be set to VIRTIO_BLK_S_IOERR, if we can
d394 1
a394 2
		free(vr);
		return (0);
d409 1
a409 2
		free(vr);
		return (0);
d418 1
a418 2
		free(vr);
		return (0);
d425 1
a425 2
		free(vr);
		return (0);
d437 1
a437 2
			free(vr);
			return (0);
d452 1
a452 2
				free(vr);
				return (0);
a460 1
				free(vr);
d462 1
a462 1
				return (0);
d481 1
a481 2
			free(vr);
			return (0);
d505 1
a505 2
			free(vr);
			return (0);
d512 1
a512 2
			free(vr);
			return (0);
a522 1
				free(vr);
d524 1
a524 1
				return (0);
a529 1
				free(vr);
d531 1
a531 1
				return (0);
d551 1
a551 2
			free(vr);
			return (0);
d575 1
a575 2
			free(vr);
			return (0);
d591 1
a591 1

a592 1

d811 1
a811 2
		free(vr);
		return (0);
d825 1
a825 2
		free(vr);
		return (0);
d838 1
a838 2
		free(vr);
		return (0);
d845 1
a845 2
		free(vr);
		return (0);
d865 1
a865 1

a866 1

@


1.17
log
@small bits of header cleanup; ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.16 2016/07/19 09:52:34 natano Exp $	*/
d28 1
d37 1
d694 1
d763 2
d768 3
d781 3
d796 3
d896 2
d931 41
d979 3
a981 1
		if (!vionet[i].rx_added)
d983 1
d985 1
a985 1
		if (vionet[i].rx_pending || fd_hasdata(vionet[i].fd))
d987 1
d997 3
a1033 1

d1035 2
d1197 1
d1294 8
d1316 1
d1318 11
@


1.16
log
@Replace malloc() + memset() with calloc().
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.15 2016/07/09 09:06:22 stefan Exp $	*/
d19 8
a31 1
#include <sys/types.h>
d33 1
a33 6
#include <dev/pci/pcireg.h>
#include <dev/pci/pcidevs.h>
#include <dev/pci/virtioreg.h>
#include <dev/pci/vioblkreg.h>
#include <machine/vmmvar.h>
#include <machine/param.h>
@


1.15
log
@Prepare vionet to be handled asynchronously to the VCPU thread

This splits the handling of received data into a separate function
that can later be called in parallel to the VCPU thread instead of
handling received packets on VCPU exits only.

It also makes virtq accesses in the rx path safe to run in parallel
to the VCPU thread: the last index into the 'avail' ring the driver
has notified to the host is kept track of. It also makes sure that
the host only writes back to the 'avail' ring instead of modifying
the whole receive virtq.

While there, describe what virtio_vq_info and virtio_io_cfg are used
for, as suggested by mlarkin@@

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.14 2016/07/07 00:58:31 mlarkin Exp $	*/
d169 1
a169 1
	buf = malloc(vr_sz);
d171 1
a171 1
		log_warn("malloc error getting viornd ring");
a174 2
	memset(buf, 0, vr_sz);

d383 1
a383 1
	vr = malloc(vr_sz);
d385 1
a385 1
		log_warn("malloc error getting vioblk ring");
a388 2
	memset(vr, 0, vr_sz);

d803 1
a803 1
	vr = malloc(vr_sz);
d805 1
a805 1
		log_warn("rx enq: malloc error getting vionet ring");
a808 2
	memset(vr, 0, vr_sz);

d999 1
a999 1
	vr = malloc(vr_sz);
d1001 1
a1001 1
		log_warn("malloc error getting vionet ring");
a1004 2
	memset(vr, 0, vr_sz);

d1158 1
a1158 1
		vioblk = malloc(sizeof(struct vioblk_dev) * vcp->vcp_ndisks);
d1160 1
a1160 1
			log_warn("%s: malloc failure allocating vioblks",
a1164 2
		memset(vioblk, 0, sizeof(struct vioblk_dev) * vcp->vcp_ndisks);

d1199 1
a1199 1
		vionet = malloc(sizeof(struct vionet_dev) * vcp->vcp_nnics);
d1201 1
a1201 1
			log_warn("%s: malloc failure allocating vionets",
a1204 2

		memset(vionet, 0, sizeof(struct vionet_dev) * vcp->vcp_nnics);
@


1.14
log
@
sanity check vm create and run args earlier
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.13 2016/07/04 23:03:52 mlarkin Exp $	*/
d21 1
d793 1
d799 1
d830 1
a830 1
	if ((avail->idx & VIONET_QUEUE_MASK) == idx) {
d860 3
a862 2
	used->ring[used->idx & VIONET_QUEUE_MASK].id = hdr_desc_idx;
	used->ring[used->idx & VIONET_QUEUE_MASK].len = hdr_desc->len + sz;
d865 4
a868 2
	*spc = avail->idx - dev->vq[0].last_avail;
	if (write_mem(q_gpa, vr, vr_sz)) {
d870 4
d881 38
d922 1
a922 4
	int i, num_enq, spc, hasdata;
	ssize_t sz;
	char *buf;
	struct pollfd pfd;
a924 1
	buf = malloc(PAGE_SIZE);
d929 2
a930 19
		spc = 1;
		hasdata = 1;
		memset(buf, 0, PAGE_SIZE);
		memset(&pfd, 0, sizeof(struct pollfd));
		pfd.fd = vionet[i].fd;
		pfd.events = POLLIN;
		while (spc && hasdata) {
			hasdata = poll(&pfd, 1, 0);
			if (hasdata == 1) {
				sz = read(vionet[i].fd, buf, PAGE_SIZE);
				if (sz != 0) {
					num_enq += vionet_enq_rx(&vionet[i],
					    buf, sz, &spc);
				} else if (sz == 0) {
					log_debug("process_rx: no data");
					hasdata = 0;
				}
			}
		}
a932 2
	free(buf);

a944 1
	uint16_t idx, pkt_desc_idx;
a945 1
	struct vring_desc *desc, *pkt_desc;
a946 1
	struct vring_used *used;
a957 2
	memset(vr, 0, vr_sz);

d964 1
a964 2
	/* Compute offsets in ring of descriptors, avail ring, and used ring */
	desc = (struct vring_desc *)(vr);
a966 6
	used = (struct vring_used *)(vr +
	    dev->vq[dev->cfg.queue_notify].vq_usedoffset);

	idx = dev->vq[dev->cfg.queue_notify].last_avail & VIONET_QUEUE_MASK;
	pkt_desc_idx = avail->ring[idx] & VIONET_QUEUE_MASK;
	pkt_desc = &desc[pkt_desc_idx];
d969 1
@


1.13
log
@
limit each viornd request to 64KB.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.12 2016/06/30 02:29:22 mlarkin Exp $	*/
d1151 7
d1159 1
a1159 6
	vioblk = malloc(sizeof(struct vioblk_dev) * vcp->vcp_ndisks);
	if (vioblk == NULL) {
		log_warn("%s: malloc failure allocating vioblks",
		    __progname);
		return;
	}
d1161 30
a1190 21
	memset(vioblk, 0, sizeof(struct vioblk_dev) * vcp->vcp_ndisks);

	/* One virtio block device for each disk defined in vcp */
	for (i = 0; i < vcp->vcp_ndisks; i++) {
		if ((sz = lseek(child_disks[i], 0, SEEK_END)) == -1)
			continue;

		if (pci_add_device(&id, PCI_VENDOR_QUMRANET,
		    PCI_PRODUCT_QUMRANET_VIO_BLOCK, PCI_CLASS_MASS_STORAGE,
		    PCI_SUBCLASS_MASS_STORAGE_SCSI,
		    PCI_VENDOR_OPENBSD,
		    PCI_PRODUCT_VIRTIO_BLOCK, 1, NULL)) {
			log_warnx("%s: can't add PCI virtio block "
			    "device", __progname);
			return;
		}
		if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, virtio_blk_io,
		    &vioblk[i])) {
			log_warnx("%s: can't add bar for virtio block "
			    "device", __progname);
			return;
a1191 9
		vioblk[i].vq[0].qs = VIOBLK_QUEUE_SIZE;
		vioblk[i].vq[0].vq_availoffset = sizeof(struct vring_desc) *
		    VIORND_QUEUE_SIZE;
		vioblk[i].vq[0].vq_usedoffset = VIRTQUEUE_ALIGN(
		    sizeof(struct vring_desc) * VIOBLK_QUEUE_SIZE
		    + sizeof(uint16_t) * (2 + VIOBLK_QUEUE_SIZE));
		vioblk[i].vq[0].last_avail = 0;
		vioblk[i].fd = child_disks[i];
		vioblk[i].sz = sz / 512;
d1194 4
a1197 18
	vionet = malloc(sizeof(struct vionet_dev) * vcp->vcp_nnics);
	if (vionet == NULL) {
		log_warn("%s: malloc failure allocating vionets",
		    __progname);
		return;
	}

	memset(vionet, 0, sizeof(struct vionet_dev) * vcp->vcp_nnics);

	nr_vionet = vcp->vcp_nnics;
	/* Virtio network */
	for (i = 0; i < vcp->vcp_nnics; i++) {
		if (pci_add_device(&id, PCI_VENDOR_QUMRANET,
		    PCI_PRODUCT_QUMRANET_VIO_NET, PCI_CLASS_SYSTEM,
		    PCI_SUBCLASS_SYSTEM_MISC,
		    PCI_VENDOR_OPENBSD,
		    PCI_PRODUCT_VIRTIO_NETWORK, 1, NULL)) {
			log_warnx("%s: can't add PCI virtio net device",
d1202 21
a1222 6
		if (pci_add_bar(id, PCI_MAPREG_TYPE_IO, virtio_net_io,
		    &vionet[i])) {
			log_warnx("%s: can't add bar for virtio net "
			    "device", __progname);
			return;
		}
d1224 15
a1238 15
		vionet[i].vq[0].qs = VIONET_QUEUE_SIZE;
		vionet[i].vq[0].vq_availoffset = sizeof(struct vring_desc) *
		    VIONET_QUEUE_SIZE;
		vionet[i].vq[0].vq_usedoffset = VIRTQUEUE_ALIGN(
		    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE
		    + sizeof(uint16_t) * (2 + VIONET_QUEUE_SIZE));
		vionet[i].vq[0].last_avail = 0;
		vionet[i].vq[1].qs = VIONET_QUEUE_SIZE;
		vionet[i].vq[1].vq_availoffset = sizeof(struct vring_desc) *
		    VIONET_QUEUE_SIZE;
		vionet[i].vq[1].vq_usedoffset = VIRTQUEUE_ALIGN(
		    sizeof(struct vring_desc) * VIONET_QUEUE_SIZE
		    + sizeof(uint16_t) * (2 + VIONET_QUEUE_SIZE));
		vionet[i].vq[1].last_avail = 0;
		vionet[i].fd = child_taps[i];
d1241 3
a1243 3
		/* User defined MAC */
		vionet[i].cfg.device_feature = VIRTIO_NET_F_MAC;
		bcopy(&vcp->vcp_macs[i], &vionet[i].mac, 6);
d1245 1
@


1.12
log
@
remove some unused variables (that were commented out anyway)
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.11 2016/04/04 17:13:54 stefan Exp $	*/
d151 1
d187 5
a191 2
	/* XXX sanity check len here */
	rnd_data = malloc(desc[avail->ring[avail->idx]].len);
d215 2
a216 1
	}
@


1.11
log
@Directly use physical addresses from ELF header for kernel loading.

This allows us to remove the 'do_mask' parameters in read_mem and
write_mem as well as the address mask operaton itself.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.10 2016/03/13 13:11:47 stefan Exp $	*/
d358 2
a359 2
	uint32_t vr_sz; //, osz;
	uint16_t idx, cmd_desc_idx, secdata_desc_idx, ds_desc_idx; //, dxx;
@


1.10
log
@Introduce memory ranges to support VMs with >= 4G RAM

Kernel bits:
- When creating a VM, a list of memory ranges has to be specified,
  similar to the BIOS memory map. This is necessary for VMs with
  RAM sizes approaching 4G because we'll need PCI MMIO space in
  the higher parts of the 32 bit address space.

vmctl and vmd bits:
- Construct appropriate memory ranges to create a VM with a given
  RAM size
- Construct a corresponding BIOS memory map from the memory ranges
  and update the boot params page accordingly.
- Make sure that all variables that represent guest physical addresses
  match the address width of the target CPU instead of using uint32_t.
- Fix some integer promotion glitches that actually restricted VM
  RAM size to 2G.

This changes the VM create ioctl interface, so update your kernel,
vmd, and vmctl.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.9 2016/02/07 10:17:19 jsg Exp $	*/
d175 1
a175 1
	if (read_mem(q_gpa, buf, vr_sz, 0)) {
d192 1
a192 1
		    rnd_data, desc[avail->ring[avail->idx]].len, 0)) {
d206 1
a206 1
			if (write_mem(q_gpa, buf, vr_sz, 0)) {
d387 1
a387 1
	if (read_mem(q_gpa, vr, vr_sz, 0)) {
d420 1
a420 1
	if (read_mem(cmd_desc->addr, &cmd, cmd_desc->len, 0)) {
d457 1
a457 1
			    secdata_desc->len, 0)) {
d479 1
a479 1
		if (write_mem(ds_desc->addr, &ds, ds_desc->len, 0)) {
d497 1
a497 1
		if (write_mem(q_gpa, vr, vr_sz, 0)) {
d523 1
a523 1
			    secdata_desc->len, 0)) {
d554 1
a554 1
		if (write_mem(ds_desc->addr, &ds, ds_desc->len, 0)) {
d570 1
a570 1
		if (write_mem(q_gpa, vr, vr_sz, 0))
d579 1
a579 1
		if (write_mem(ds_desc->addr, &ds, ds_desc->len, 0)) {
d595 1
a595 1
		if (write_mem(q_gpa, vr, vr_sz, 0)) {
d807 1
a807 1
	if (read_mem(q_gpa, vr, vr_sz, 0)) {
d843 1
a843 1
	if (write_mem(pkt_desc->addr, pkt, sz, 0)) {
d857 1
a857 1
	if (write_mem(q_gpa, vr, vr_sz, 0)) {
d933 1
a933 1
	if (read_mem(q_gpa, vr, vr_sz, 0)) {
d994 1
a994 1
	if (read_mem(q_gpa, vr, vr_sz, 0)) {
d1056 1
a1056 1
			    pkt_desc->len, 0)) {
d1076 1
a1076 1
		    pkt_desc->len, 0)) {
d1103 1
a1103 1
	if (write_mem(q_gpa, vr, vr_sz, 0)) {
@


1.9
log
@avoid a double free in an error path
ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.8 2016/01/16 08:55:40 stefan Exp $	*/
d175 1
a175 1
	if (read_mem((uint32_t)q_gpa, buf, vr_sz, 0)) {
d191 1
a191 1
		if (write_mem((uint32_t)(desc[avail->ring[avail->idx]].addr),
d387 2
a388 2
	if (read_mem((uint32_t)q_gpa, vr, vr_sz, 0)) {
		log_warnx("error reading gpa 0x%x", (uint32_t)q_gpa);
d420 1
a420 1
	if (read_mem((uint32_t)cmd_desc->addr, &cmd, cmd_desc->len, 0)) {
d456 1
a456 1
			if (write_mem((uint32_t)secdata_desc->addr, secdata,
d479 1
a479 2
		if (write_mem((uint32_t)ds_desc->addr,
		    &ds, ds_desc->len, 0)) {
d522 1
a522 1
			if (read_mem((uint32_t)secdata_desc->addr, secdata,
d554 1
a554 2
		if (write_mem((uint32_t)ds_desc->addr,
		    &ds, ds_desc->len, 0)) {
d579 1
a579 2
		if (write_mem((uint32_t)ds_desc->addr,
		    &ds, ds_desc->len, 0)) {
d807 2
a808 2
	if (read_mem((uint32_t)q_gpa, vr, vr_sz, 0)) {
		log_warnx("rx enq: error reading gpa 0x%x", (uint32_t)q_gpa);
d843 1
a843 1
	if (write_mem((uint32_t)pkt_desc->addr, pkt, sz, 0)) {
d933 2
a934 2
	if (read_mem((uint32_t)q_gpa, vr, vr_sz, 0)) {
		log_warnx("error reading gpa 0x%x", (uint32_t)q_gpa);
d994 2
a995 2
	if (read_mem((uint32_t)q_gpa, vr, vr_sz, 0)) {
		log_warnx("error reading gpa 0x%x", (uint32_t)q_gpa);
d1055 1
a1055 1
			if (read_mem((uint32_t)pkt_desc->addr, pkt + ofs,
d1075 1
a1075 1
		if (read_mem((uint32_t)pkt_desc->addr, pkt + ofs,
@


1.8
log
@vmd(8) sometimes attempts page-crossing data copies between the host
and guest. The readpage/writepage ioctls of vmm(4) do not support this
and they return EINVAL on such attempts since recently.

Avoid page-crossing guest memory accesses by changing read_page() and
write_page() into read_mem() and write_mem() that can copy arbitrary
lengths of data between host<->guest without page-crossing accesses.

This also allows us to remove page-wise copy-loops in a few places.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.7 2016/01/14 02:46:40 mlarkin Exp $	*/
a342 1
		free(buf);
@


1.7
log
@
stdio.h is not needed here anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.6 2016/01/04 02:07:28 mlarkin Exp $	*/
a145 14
static int
push_virtio_ring(char *buf, uint32_t gpa, uint32_t vr_sz)
{
	uint32_t i;

	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (write_page((uint32_t)gpa + i, buf + i, PAGE_SIZE, 0)) {
			return (1);
		}
	}

	return (0);
}

d150 1
a150 1
	uint32_t vr_sz, i;
d175 3
a177 5
	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (read_page((uint32_t)q_gpa + i, buf + i, PAGE_SIZE, 0)) {
			free(buf);
			return (0);
		}
d191 1
a191 1
		if (write_page((uint32_t)(desc[avail->ring[avail->idx]].addr),
d206 1
a206 1
			if (push_virtio_ring(buf, q_gpa, vr_sz)) {
d359 1
a359 1
	uint32_t vr_sz, i, j, len; //, osz;
d388 4
a391 7
	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (read_page((uint32_t)q_gpa + i, vr + i, PAGE_SIZE, 0)) {
			log_warnx("error reading gpa 0x%x",
			    (uint32_t)q_gpa + i);
			free(vr);
			return (0);
		}
d421 2
a422 2
	if (read_page((uint32_t)cmd_desc->addr, &cmd, cmd_desc->len, 0)) {
		log_warnx("vioblk: command read_page error @@ 0x%llx",
d457 9
a465 20
			j = 0;
			while (j < secdata_desc->len) {
				if (secdata_desc->len - j >= PAGE_SIZE)
					len = PAGE_SIZE;
				else
					len = secdata_desc->len - j;

				if (write_page((uint32_t)secdata_desc->addr + j,
				    secdata + j, len, 0)) {
					log_warnx("can't write sector "
					    "data to gpa @@ 0x%llx",
					    secdata_desc->addr);
					dump_descriptor_chain(desc,
					    cmd_desc_idx);
					free(vr);
					free(secdata);
					return (0);
				}

				j += PAGE_SIZE;
d480 1
a480 1
		if (write_page((uint32_t)ds_desc->addr,
d499 1
a499 1
		if (push_virtio_ring(vr, q_gpa, vr_sz)) {
d524 9
a532 19
			j = 0;
			while (j < secdata_desc->len) {
				if (secdata_desc->len - j >= PAGE_SIZE)
					len = PAGE_SIZE;
				else
					len = secdata_desc->len - j;
				if (read_page((uint32_t)secdata_desc->addr + j,
				    secdata + j, len, 0)) {
					log_warnx("wr vioblk: can't read "
					    "sector data @@ 0x%llx",
					    secdata_desc->addr);
					dump_descriptor_chain(desc,
					     cmd_desc_idx);
					free(vr);
					free(secdata);
					return (0);
				}

				j += PAGE_SIZE;
d556 1
a556 1
		if (write_page((uint32_t)ds_desc->addr,
d573 1
a573 1
		if (push_virtio_ring(vr, q_gpa, vr_sz))
d582 1
a582 1
		if (write_page((uint32_t)ds_desc->addr,
d599 1
a599 1
		if (push_virtio_ring(vr, q_gpa, vr_sz)) {
d789 1
a789 1
	uint32_t vr_sz, i;
d811 4
a814 7
	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (read_page((uint32_t)q_gpa + i, vr + i, PAGE_SIZE, 0)) {
			log_warnx("rx enq: error reading gpa 0x%x",
			    (uint32_t)q_gpa + i);
			free(vr);
			return (0);
		}
d847 2
a848 2
	if (write_page((uint32_t)pkt_desc->addr, pkt, sz, 0)) {
		log_warnx("vionet: rx enq packet write_page error @@ "
d861 1
a861 1
	if (push_virtio_ring(vr, q_gpa, vr_sz)) {
d918 1
a918 1
	uint32_t vr_sz, i;
d937 4
a940 7
	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (read_page((uint32_t)q_gpa + i, vr + i, PAGE_SIZE, 0)) {
			log_warnx("error reading gpa 0x%x",
			    (uint32_t)q_gpa + i);
			free(vr);
			return;
		}
d968 1
a968 1
	uint32_t vr_sz, i;
d998 3
a1000 6
	for (i = 0; i < vr_sz; i += VIRTIO_PAGE_SIZE) {
		if (read_page((uint32_t)q_gpa + i, vr + i, PAGE_SIZE, 0)) {
			log_warnx("error reading gpa 0x%x",
			    (uint32_t)q_gpa + i);
			goto out;
		}
d1059 1
a1059 1
			if (read_page((uint32_t)pkt_desc->addr, pkt + ofs,
d1061 1
a1061 1
				log_warnx("vionet: packet read_page error "
d1079 1
a1079 1
		if (read_page((uint32_t)pkt_desc->addr, pkt + ofs,
d1081 1
a1081 1
			log_warnx("vionet: packet read_page error @@ "
d1107 1
a1107 1
	if (push_virtio_ring(vr, q_gpa, vr_sz)) {
@


1.6
log
@
bzero -> memset for consistency
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.5 2016/01/03 22:36:09 mlarkin Exp $	*/
a20 1
#include <stdio.h>
@


1.5
log
@
Add a cleanup block, fix a couple of memory leaks, fix a typo and a bit
of KNF.

Submitted by Michal Mazurek, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.4 2015/12/03 08:42:11 reyk Exp $	*/
d188 1
a188 1
	bzero(buf, vr_sz);
d403 1
a403 1
	bzero(vr, vr_sz);
d850 1
a850 1
	bzero(vr, vr_sz);
d930 2
a931 2
		bzero(buf, PAGE_SIZE);
		bzero(&pfd, sizeof(struct pollfd));
d979 1
a979 1
	bzero(vr, vr_sz);
d1043 1
a1043 1
	bzero(vr, vr_sz);
d1192 1
a1192 1
	bzero(&viornd, sizeof(viornd));
d1208 1
a1208 1
	bzero(vioblk, sizeof(struct vioblk_dev) * vcp->vcp_ndisks);
d1248 1
a1248 1
	bzero(vionet, sizeof(struct vionet_dev) * vcp->vcp_nnics);
@


1.4
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.3 2015/11/23 13:04:49 reyk Exp $	*/
a1007 1
 * XXX this function needs a cleanup block, lots of free(blah); return (0)
d1024 1
d1030 1
a1030 1
		return (0);
d1040 1
a1040 1
		return (0);
d1049 1
a1049 2
			free(vr);
			return (0);
d1066 1
a1066 2
		free(vr);
		return (0);
a1069 1

d1093 1
a1093 2
			free(vr);
			return (0);
d1103 1
a1103 1
				log_warnx("unexpceted writable tx desc "
d1105 1
a1105 2
				free(vr);
				return (0);
d1113 1
a1113 3
				free(pkt);
				free(vr);
				return (0);
d1123 1
a1123 1
			log_warnx("unexpceted writable tx descriptor %d",
d1125 1
a1125 2
			free(vr);
			return (0);
d1133 1
a1133 3
			free(pkt);
			free(vr);
			return (0);
d1140 1
a1140 3
			free(pkt);
			free(vr);
			return (0);
d1161 1
@


1.3
log
@Add support for logging to stderr or syslog, and to run vmd in
foreground with -d.

OK mlarkin@@ jung@@
@
text
@d1 1
a1 1
/*	$OpenBSD: virtio.c,v 1.2 2015/11/22 21:51:32 reyk Exp $	*/
d53 1
a53 1
	switch(type) {
d92 1
a92 1
	switch(reg) {
d128 1
a128 1
	if (viornd.cfg.queue_select > 0) 
a225 1
			
d242 1
a242 1
		switch(reg) {
d270 1
a270 1
		switch(reg) {
d339 1
a339 1
 
d476 1
a476 1
			
d489 2
a490 1
					dump_descriptor_chain(desc, cmd_desc_idx);
d497 1
a497 1
			}	
d540 1
a540 1
			   "descriptor received (idx %d)", cmd_desc_idx);
d574 1
a574 1
			}	
d660 1
a660 1
		switch(reg) {
d690 1
a690 1
		switch(reg) {
d735 1
a735 1
		switch(reg) {
d765 1
a765 1
		switch(reg) {
d818 1
a818 1
	if (dev->cfg.queue_select > 1) 
d1103 1
a1103 1
		
@


1.2
log
@Add $ Ids
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d68 1
a68 1
	fprintf(stderr, "descriptor chain @@ %d\n\r", dxx);
d70 2
a71 2
		fprintf(stderr, "desc @@%d addr/len/flags/next = 0x%llx / 0x%x "
		    "/ 0x%x / 0x%x\n\r",
d80 2
a81 2
	fprintf(stderr, "desc @@%d addr/len/flags/next = 0x%llx / 0x%x / 0x%x "
	    "/ 0x%x\n\r",
d184 1
a184 1
		fprintf(stderr, "malloc error getting viornd ring\n\r");
d210 2
a211 2
			fprintf(stderr, "viornd: can't write random data @@ "
			    "0x%llx\n\r",
d224 1
a224 2
				fprintf(stderr, "viornd: error writing vio "
				    "ring\n\r");
d247 1
a247 1
			fprintf(stderr, "%s: illegal write %x to %s\n\r",
a328 1

d330 1
a330 1
		fprintf(stderr, "malloc errror vioblk read\n\r");
d336 1
a336 2
		fprintf(stderr, "seek error in vioblk read: %d\n\r",
		    errno);
d342 1
a342 2
		fprintf(stderr, "vioblk read error: %d\n\r",
		    errno);
d355 1
a355 2
		fprintf(stderr, "seek error in vioblk write: %d\n\r",
		    errno);
d360 1
a360 2
		fprintf(stderr, "vioblk write error: %d\n\r",
		    errno);
d400 1
a400 1
		fprintf(stderr, "malloc error getting vioblk ring\n\r");
d408 1
a408 1
			fprintf(stderr, "error reading gpa 0x%x\n\r",
d426 1
a426 1
		fprintf(stderr, "vioblk queue notify - nothing to do?\n\r");
d435 2
a436 2
		fprintf(stderr, "unchained vioblk cmd descriptor received "
		    "(idx %d)\n\r", cmd_desc_idx);
d443 1
a443 1
		fprintf(stderr, "vioblk: command read_page error @@ 0x%llx\n\r",
d456 2
a457 2
			fprintf(stderr, "unchained vioblk data descriptor "
			    "received (idx %d)\n\r", cmd_desc_idx);
d472 2
a473 2
				fprintf(stderr, "vioblk: block read error, "
				    "sector %lld\n\r", cmd.sector);
d487 2
a488 2
					fprintf(stderr, "can't write sector "
					    "data to gpa @@ 0x%llx\n\r",
d513 2
a514 2
			fprintf(stderr, "can't write device status data @@ "
			    "0x%llx\n\r", ds_desc->addr);
d531 1
a531 1
			fprintf(stderr, "vioblk: error writing vio ring\n\r");
d539 2
a540 2
			fprintf(stderr, "wr vioblk: unchained vioblk data "
			   "descriptor received (idx %d)\n\r", cmd_desc_idx);
d547 1
a547 1
			fprintf(stderr, "wr vioblk: malloc error, len %d\n\r",
d563 2
a564 2
					fprintf(stderr, "wr vioblk: can't read "
					    "sector data @@ 0x%llx\n\r",
d578 1
a578 2
				fprintf(stderr, "wr vioblk: disk write "
				    "error\n\r");
d599 2
a600 2
			fprintf(stderr, "wr vioblk: can't write device status "
			    "data @@ 0x%llx\n\r", ds_desc->addr);
d615 1
a615 1
			fprintf(stderr, "wr vioblk: error writing vio ring\n\r");
d625 2
a626 2
			fprintf(stderr, "fl vioblk: can't write device status "
			    "data @@ 0x%llx\n\r", ds_desc->addr);
d641 1
a641 1
			fprintf(stderr, "fl vioblk: error writing vio ring\n\r");
d664 1
a664 1
			fprintf(stderr, "%s: illegal write %x to %s\n\r",
d739 1
a739 1
			fprintf(stderr, "%s: illegal write %x to %s\n\r",
d846 1
a846 1
		fprintf(stderr, "rx enq :malloc error getting vionet ring\n\r");
d854 1
a854 1
			fprintf(stderr, "rx enq: error reading gpa 0x%x\n\r",
d871 1
a871 2
		fprintf(stderr, "vionet queue notify - no space, dropping "
		    "packet\n\r");
d884 1
a884 1
		fprintf(stderr, "unexpected readable rx descriptor %d\n\r",
d892 2
a893 2
		fprintf(stderr, "vionet: rx enq packet write_page error @@ "
		    "0x%llx\n\r", pkt_desc->addr);
d906 1
a906 1
		fprintf(stderr, "vionet: error writing vio ring\n\r");
d942 1
a942 2
					fprintf(stderr, "process_rx: no data"
					    "\n\r");
d975 1
a975 1
		fprintf(stderr, "malloc error getting vionet ring\n\r");
d983 1
a983 1
			fprintf(stderr, "error reading gpa 0x%x\n\r",
d1039 1
a1039 1
		fprintf(stderr, "malloc error getting vionet ring\n\r");
d1047 1
a1047 1
			fprintf(stderr, "error reading gpa 0x%x\n\r",
d1066 1
a1066 1
		fprintf(stderr, "vionet tx queue notify - nothing to do?\n\r");
d1095 1
a1095 1
			fprintf(stderr, "malloc error alloc packet buf\n\r");
d1107 2
a1108 2
				fprintf(stderr, "unexpceted writable tx desc "
				    "%d\n\r", pkt_desc_idx);
d1116 2
a1117 2
				fprintf(stderr, "vionet: packet read_page error "
				    "@@ 0x%llx\n\r", pkt_desc->addr);
d1130 2
a1131 2
			fprintf(stderr, "unexpceted writable tx descriptor %d"
			    "\n\r", pkt_desc_idx);
d1139 2
a1140 2
			fprintf(stderr, "vionet: packet read_page error @@ "
			    "0x%llx\n\r", pkt_desc->addr);
d1148 2
a1149 2
			fprintf(stderr, "vionet: tx failed writing to tap: "
			    "%d\n\r", errno);
d1170 1
a1170 1
		fprintf(stderr, "vionet: tx error writing vio ring\n\r");
d1192 1
a1192 1
		fprintf(stderr, "%s: can't add PCI virtio rng device\n",
d1198 1
a1198 1
		fprintf(stderr, "%s: can't add bar for virtio rng device\n",
d1214 1
a1214 1
		fprintf(stderr, "%s: malloc failure allocating vioblks\n",
d1231 2
a1232 2
			fprintf(stderr, "%s: can't add PCI virtio block "
			    "device\n", __progname);
d1237 2
a1238 2
			fprintf(stderr, "%s: can't add bar for virtio block "
			    "device\n", __progname);
d1254 1
a1254 1
		fprintf(stderr, "%s: malloc failure allocating vionets\n",
d1269 1
a1269 1
			fprintf(stderr, "%s: can't add PCI virtio net device\n",
d1276 2
a1277 2
			fprintf(stderr, "%s: can't add bar for virtio net "
			    "device\n", __progname);
@


1.1
log
@
vmd(8) - virtual machine daemon.

There is still a lot to be done, and fixed, in these userland components
but I have received enough "it works, commit it" emails that it's time
to finish those things in tree.

discussed with many, tested by many.
@
text
@d1 2
@

