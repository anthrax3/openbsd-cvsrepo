head	1.33;
access;
symbols
	OPENBSD_5_1_BASE:1.32
	OPENBSD_5_1:1.32.0.2
	OPENBSD_5_0:1.31.0.6
	OPENBSD_5_0_BASE:1.31
	OPENBSD_4_9:1.31.0.4
	OPENBSD_4_9_BASE:1.31
	OPENBSD_4_8:1.31.0.2
	OPENBSD_4_8_BASE:1.31
	OPENBSD_4_7:1.30.0.6
	OPENBSD_4_7_BASE:1.30
	OPENBSD_4_6:1.30.0.8
	OPENBSD_4_6_BASE:1.30
	OPENBSD_4_5:1.30.0.4
	OPENBSD_4_5_BASE:1.30
	OPENBSD_4_4:1.30.0.2
	OPENBSD_4_4_BASE:1.30
	OPENBSD_4_3:1.29.0.4
	OPENBSD_4_3_BASE:1.29
	OPENBSD_4_2:1.29.0.2
	OPENBSD_4_2_BASE:1.29
	OPENBSD_4_1:1.28.0.8
	OPENBSD_4_1_BASE:1.28
	OPENBSD_4_0:1.28.0.6
	OPENBSD_4_0_BASE:1.28
	OPENBSD_3_9:1.28.0.4
	OPENBSD_3_9_BASE:1.28
	OPENBSD_3_8:1.28.0.2
	OPENBSD_3_8_BASE:1.28
	OPENBSD_3_7:1.27.0.2
	OPENBSD_3_7_BASE:1.27
	OPENBSD_3_6:1.26.0.6
	OPENBSD_3_6_BASE:1.26
	OPENBSD_3_5:1.26.0.4
	OPENBSD_3_5_BASE:1.26
	OPENBSD_3_4:1.26.0.2
	OPENBSD_3_4_BASE:1.26
	OPENBSD_3_3:1.24.0.2
	OPENBSD_3_3_BASE:1.24
	OPENBSD_3_2:1.19.0.2
	OPENBSD_3_2_BASE:1.19
	OPENBSD_3_1:1.18.0.2
	OPENBSD_3_1_BASE:1.18
	OPENBSD_3_0:1.16.0.2
	OPENBSD_3_0_BASE:1.16
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_8:1.11.0.4
	OPENBSD_2_8_BASE:1.11
	OPENBSD_2_7:1.11.0.2
	OPENBSD_2_7_BASE:1.11
	OPENBSD_2_6:1.8.0.2
	OPENBSD_2_6_BASE:1.8
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@.\" @;


1.33
date	2012.04.06.15.42.56;	author jsing;	state dead;
branches;
next	1.32;

1.32
date	2011.10.06.22.22.11;	author jmc;	state Exp;
branches;
next	1.31;

1.31
date	2010.04.01.17.06.55;	author jmc;	state Exp;
branches;
next	1.30;

1.30
date	2008.06.26.05.42.07;	author ray;	state Exp;
branches;
next	1.29;

1.29
date	2007.05.31.19.19.51;	author jmc;	state Exp;
branches;
next	1.28;

1.28
date	2005.08.19.16.21.30;	author jsg;	state Exp;
branches;
next	1.27;

1.27
date	2004.09.30.19.59.25;	author mickey;	state Exp;
branches;
next	1.26;

1.26
date	2003.04.01.11.15.12;	author jmc;	state Exp;
branches;
next	1.25;

1.25
date	2003.03.30.12.14.31;	author jmc;	state Exp;
branches;
next	1.24;

1.24
date	2003.01.13.19.06.42;	author kjell;	state Exp;
branches;
next	1.23;

1.23
date	2002.12.06.19.04.24;	author avsm;	state Exp;
branches;
next	1.22;

1.22
date	2002.11.08.08.08.47;	author mpech;	state Exp;
branches;
next	1.21;

1.21
date	2002.10.16.18.41.17;	author deraadt;	state Exp;
branches;
next	1.20;

1.20
date	2002.10.15.11.59.02;	author deraadt;	state Exp;
branches;
next	1.19;

1.19
date	2002.09.26.07.55.40;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2002.04.07.00.03.29;	author tdeval;	state Exp;
branches;
next	1.17;

1.17
date	2002.01.05.13.47.28;	author tdeval;	state Exp;
branches;
next	1.16;

1.16
date	2001.10.05.14.45.53;	author mpech;	state Exp;
branches;
next	1.15;

1.15
date	2001.08.03.15.21.16;	author mpech;	state Exp;
branches;
next	1.14;

1.14
date	2001.06.23.07.03.58;	author pjanzen;	state Exp;
branches;
next	1.13;

1.13
date	2001.06.22.12.15.46;	author mpech;	state Exp;
branches;
next	1.12;

1.12
date	2000.12.21.18.15.13;	author avsm;	state Exp;
branches;
next	1.11;

1.11
date	2000.04.03.21.19.37;	author aaron;	state Exp;
branches;
next	1.10;

1.10
date	2000.03.06.21.46.57;	author aaron;	state Exp;
branches;
next	1.9;

1.9
date	2000.01.07.14.52.28;	author peter;	state Exp;
branches;
next	1.8;

1.8
date	99.09.23.04.12.04;	author alex;	state Exp;
branches;
next	1.7;

1.7
date	99.07.30.14.45.31;	author peter;	state Exp;
branches;
next	1.6;

1.6
date	99.07.09.13.35.48;	author aaron;	state Exp;
branches;
next	1.5;

1.5
date	99.07.08.09.41.43;	author hugh;	state Exp;
branches;
next	1.4;

1.4
date	99.06.05.04.16.07;	author aaron;	state Exp;
branches;
next	1.3;

1.3
date	99.05.16.19.56.35;	author alex;	state Exp;
branches;
next	1.2;

1.2
date	99.02.19.00.19.52;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	99.01.11.15.02.04;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.33
log
@Remove raidframe related references.
@
text
@.\"	$OpenBSD: raid.4,v 1.32 2011/10/06 22:22:11 jmc Exp $
.\"     $NetBSD: raid.4,v 1.20 2001/09/22 16:03:58 wiz Exp $
.\"
.\" Copyright (c) 1998 The NetBSD Foundation, Inc.
.\" All rights reserved.
.\"
.\" This code is derived from software contributed to The NetBSD Foundation
.\" by Greg Oster
.\"
.\" Redistribution and use in source and binary forms, with or without
.\" modification, are permitted provided that the following conditions
.\" are met:
.\" 1. Redistributions of source code must retain the above copyright
.\"    notice, this list of conditions and the following disclaimer.
.\" 2. Redistributions in binary form must reproduce the above copyright
.\"    notice, this list of conditions and the following disclaimer in the
.\"    documentation and/or other materials provided with the distribution.
.\"
.\" THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
.\" ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
.\" TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
.\" PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
.\" BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
.\" CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
.\" SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
.\" INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
.\" CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
.\" ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
.\" POSSIBILITY OF SUCH DAMAGE.
.\"
.\"
.\" Copyright (c) 1995 Carnegie-Mellon University.
.\" All rights reserved.
.\"
.\" Author: Mark Holland
.\"
.\" Permission to use, copy, modify and distribute this software and
.\" its documentation is hereby granted, provided that both the copyright
.\" notice and this permission notice appear in all copies of the
.\" software, derivative works or modified versions, and any portions
.\" thereof, and that both notices appear in supporting documentation.
.\"
.\" CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
.\" CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
.\" FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
.\"
.\" Carnegie Mellon requests users of this software to return to
.\"
.\"  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
.\"  School of Computer Science
.\"  Carnegie Mellon University
.\"  Pittsburgh PA 15213-3890
.\"
.\" any improvements or extensions that they make and grant Carnegie the
.\" rights to redistribute these changes.
.\"
.Dd $Mdocdate: October 6 2011 $
.Dt RAID 4
.Os
.Sh NAME
.Nm raid
.Nd RAIDframe disk driver
.Sh SYNOPSIS
.Cd "pseudo-device raid" Op Ar count
.Sh DESCRIPTION
The
.Nm
driver provides RAID 0, 1, 4, and 5 (and more!) capabilities to
.Ox .
This
document assumes that the reader has at least some familiarity with RAID
and RAID concepts.
The reader is also assumed to know how to configure
disks and pseudo-devices into kernels, how to generate kernels, and how
to partition disks.
.Pp
RAIDframe provides a number of different RAID levels including:
.Bl -tag -width indent
.It RAID 0
provides simple data striping across the components.
.It RAID 1
provides mirroring.
.It RAID 4
provides data striping across the components, with parity
stored on a dedicated drive (in this case, the last component).
.It RAID 5
provides data striping across the components, with parity
distributed across all the components.
.El
.Pp
There are a wide variety of other RAID levels supported by RAIDframe,
including Even-Odd parity, RAID level 5 with rotated sparing, Chained
declustering, and Interleaved declustering.
The reader is referred to the RAIDframe documentation mentioned in the
.Sx HISTORY
section for more detail on these various RAID configurations.
.Pp
Depending on the parity level configured, the device driver can
support the failure of component drives.
The number of failures allowed depends on the parity level selected.
If the driver is able to handle drive failures, and a drive does fail,
then the system is operating in "degraded mode".
In this mode, all missing data must be reconstructed from the data and
parity present on the other components.
This results in much slower data accesses, but does mean that a failure
need not bring the system to a complete halt.
.Pp
The RAID driver supports and enforces the use of
.Sq component labels .
A
.Sq component label
contains important information about the component, including a
user-specified serial number, the row and column of that component in
the RAID set, and whether the data (and parity) on the component is
.Sq clean .
If the driver determines that the labels are very inconsistent with
respect to each other (e.g. two or more serial numbers do not match)
or that the component label is not consistent with its assigned place
in the set (e.g., the component label claims the component should be
the 3rd one of a 6-disk set, but the RAID set has it as the 3rd component
in a 5-disk set) then the device will fail to configure.
If the driver determines that exactly one component label seems to be
incorrect, and the RAID set is being configured as a set that supports
a single failure, then the RAID set will be allowed to configure, but
the incorrectly labeled component will be marked as
.Sq failed ,
and the RAID set will begin operation in degraded mode.
If all of the components are consistent among themselves, the RAID set
will configure normally.
.Pp
Component labels are also used to support the auto-detection and
auto-configuration of RAID sets.
A RAID set can be flagged as auto-configurable, in which case it will be
configured automatically during the kernel boot process.
RAID filesystems which are
automatically configured are also eligible to be the root filesystem.
There is currently no support for booting a kernel directly from a RAID
set.
To use a RAID set as the root filesystem, a kernel is usually
obtained from a small non-RAID partition, after which any
auto-configuring RAID set can be used for the root filesystem.
See
.Xr raidctl 8
for more information on auto-configuration of RAID sets.
.Pp
The driver supports
.Sq hot spares ,
disks which are on-line, but are not actively used in an existing
filesystem.
Should a disk fail, the driver is capable of reconstructing
the failed disk onto a hot spare or back onto a replacement drive.
If the components are hot swapable, the failed disk can then be
removed, a new disk put in its place, and a copyback operation
performed.
The copyback operation, as its name indicates, will copy
the reconstructed data from the hot spare to the previously failed
(and now replaced) disk.
Hot spares can also be hot-added using
.Xr raidctl 8 .
.Pp
If a component cannot be detected when the RAID device is configured,
that component will be simply marked as 'failed'.
.Pp
The user-land utility for doing all
.Nm
configuration and other operations
is
.Xr raidctl 8 .
Most importantly,
.Xr raidctl 8
must be used with the
.Fl i
option to initialize all RAID sets.
In particular, this initialization includes re-building the parity data.
This rebuilding of parity data is also required when either a) a new RAID
device is brought up for the first time or b) after an un-clean shutdown of a
RAID device.
By using the
.Fl P
option to
.Xr raidctl 8 ,
and performing this on-demand recomputation of all parity
before doing a
.Xr fsck 8
or a
.Xr newfs 8 ,
filesystem integrity and parity integrity can be ensured.
It bears repeating again that parity recomputation is
.Ar required
before any filesystems are created or used on the RAID device.
If the parity is not correct, then missing data cannot be correctly recovered.
.Pp
RAID levels may be combined in a hierarchical fashion.
For example, a RAID 0 device can be constructed out of a number of RAID 5
devices (which, in turn, may be constructed out of the physical disks,
or of other RAID devices).
.Pp
It is important that drives be hard-coded at their respective
addresses (i.e., not left free-floating, where a drive with SCSI ID of
4 can end up as
.Pa /dev/sd0c )
for well-behaved functioning of the RAID device.
This is true for all types of drives, including IDE, HP-IB, etc.
For normal SCSI drives, for example, the following can be used
to fix the device addresses:
.Bd -unfilled -offset indent
sd0     at scsibus0 target 0       # SCSI disk drives
sd1     at scsibus0 target 1       # SCSI disk drives
sd2     at scsibus0 target 2       # SCSI disk drives
sd3     at scsibus0 target 3       # SCSI disk drives
sd4     at scsibus0 target 4       # SCSI disk drives
sd5     at scsibus0 target 5       # SCSI disk drives
sd6     at scsibus0 target 6       # SCSI disk drives
.Ed
.Pp
See
.Xr sd 4
for more information.
The rationale for fixing the device addresses is as follows:
Consider a system with three SCSI drives at SCSI ID's 4, 5, and 6,
and which map to components
.Pa /dev/sd0e , /dev/sd1e ,
and
.Pa /dev/sd2e
of a RAID 5 set.
If the drive with SCSI ID 5 fails, and the system reboots, the old
.Pa /dev/sd2e
will show up as
.Pa /dev/sd1e .
The RAID driver is able to detect that component positions have changed, and
will not allow normal configuration.
If the device addresses are hard
coded, however, the RAID driver would detect that the middle component
is unavailable, and bring the RAID 5 set up in degraded mode.
Note that the auto-detection and auto-configuration code does not care
about where the components live.
The auto-configuration code will
correctly configure a device even after any number of the components
have been re-arranged.
.Pp
The first step to using the
.Nm
driver is to ensure that it is suitably configured in the kernel.
This is done by adding a line similar to:
.Bd -unfilled -offset indent
pseudo-device   raid   4       # RAIDframe disk device
.Ed
.Pp
to the kernel configuration file.
The
.Sq count
argument (
.Sq 4 ,
in this case), specifies the number of RAIDframe drivers to configure.
To turn on component auto-detection and auto-configuration of RAID
sets, simply add:
.Bd -unfilled -offset indent
option	RAID_AUTOCONFIG
.Ed
.Pp
to the kernel configuration file.
.Pp
All component partitions must be of the type
.Dv FS_BSDFFS
(e.g., 4.2BSD) or
.Dv FS_RAID
(e.g., RAID).
The use of the latter is strongly encouraged, and is
required if auto-configuration of the RAID set is desired.
Since RAIDframe leaves room for disklabels, RAID components can be simply
raw disks, or partitions which use an entire disk.
Note that some platforms (such as SUN) do not allow using the FS_RAID
partition type.
On these platforms, the
.Nm
driver can still auto-configure from FS_BSDFFS partitions.
.Pp
A more detailed treatment of actually using a
.Nm
device is found in
.Xr raidctl 8 .
It is highly recommended that the steps to reconstruct, copyback, and
re-compute parity are well understood by the system administrator(s)
.Ar before
a component failure.
Doing the wrong thing when a component fails may result in data loss.
.Pp
Additional debug information can be sent to the console by specifying:
.Bd -unfilled -offset indent
option	RAIDDEBUG
.Ed
.Sh FILES
.Bl -tag -width /dev/XXrXraidX -compact
.It Pa /dev/{,r}raid*
.Nm
device special files.
.El
.Sh SEE ALSO
.Xr sd 4 ,
.Xr wd 4 ,
.Xr config 8 ,
.Xr fsck 8 ,
.Xr MAKEDEV 8 ,
.Xr mount 8 ,
.Xr newfs 8 ,
.Xr raidctl 8
.Sh HISTORY
The
.Nm
driver in
.Ox
is a port of RAIDframe, a framework for rapid prototyping of RAID
structures developed by the folks at the Parallel Data Laboratory at
Carnegie Mellon University (CMU).
RAIDframe, as originally distributed
by CMU, provides a RAID simulator for a number of different
architectures, and a user-level device driver and a kernel device
driver for Digital UNIX.
The
.Nm
driver is a kernelized version of RAIDframe v1.1.
.Pp
A more complete description of the internals and functionality of
RAIDframe is found in the paper "RAIDframe: A Rapid Prototyping Tool
for RAID Systems", by William V. Courtright II, Garth Gibson, Mark
Holland, LeAnn Neal Reilly, and Jim Zelenka, and published by the
Parallel Data Laboratory of Carnegie Mellon University.
The
.Nm
driver first appeared in
.Nx 1.4
from where it was ported to
.Ox 2.5 .
.Sh CAVEATS
Certain RAID levels (1, 4, 5, 6, and others) can protect against some
data loss due to component failure.
However the loss of two components of a RAID 4 or 5 system, or the loss
of a single component of a RAID 0 system, will result in the entire
filesystems on that RAID device being lost.
RAID is
.Ar NOT
a substitute for good backup practices.
.Pp
Recomputation of parity
.Ar MUST
be performed whenever there is a chance that it may have been
compromised.
This includes after system crashes, or before a RAID
device has been used for the first time.
Failure to keep parity correct will be catastrophic should a component
ever fail -- it is better to use RAID 0 and get the additional space and
speed, than it is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
@


1.32
log
@no more ccd(4);
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.31 2010/04/01 17:06:55 jmc Exp $
d57 1
a57 1
.Dd $Mdocdate: April 1 2010 $
@


1.31
log
@WARNINGS -> CAVEATS, and a little neccessary cleanup;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.30 2008/06/26 05:42:07 ray Exp $
d57 1
a57 1
.Dd $Mdocdate: June 26 2008 $
a298 1
.Xr ccd 4 ,
@


1.30
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.29 2007/05/31 19:19:51 jmc Exp $
d57 1
a57 1
.Dd $Mdocdate: May 31 2007 $
a291 20
.Sh WARNINGS
Certain RAID levels (1, 4, 5, 6, and others) can protect against some
data loss due to component failure.
However the loss of two components of a RAID 4 or 5 system, or the loss
of a single component of a RAID 0 system, will result in the entire
filesystems on that RAID device being lost.
RAID is
.Ar NOT
a substitute for good backup practices.
.Pp
Recomputation of parity
.Ar MUST
be performed whenever there is a chance that it may have been
compromised.
This includes after system crashes, or before a RAID
device has been used for the first time.
Failure to keep parity correct will be catastrophic should a component
ever fail -- it is better to use RAID 0 and get the additional space and
speed, than it is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
a301 1
.Xr MAKEDEV 8 ,
d304 1
d335 20
@


1.29
log
@convert to new .Dd format;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.28 2005/08/19 16:21:30 jsg Exp $
a17 7
.\" 3. All advertising materials mentioning features or use of this software
.\"    must display the following acknowledgement:
.\"        This product includes software developed by the NetBSD
.\"        Foundation, Inc. and its contributors.
.\" 4. Neither the name of The NetBSD Foundation nor the names of its
.\"    contributors may be used to endorse or promote products derived
.\"    from this software without specific prior written permission.
d57 1
a57 1
.Dd $Mdocdate$
@


1.28
log
@Copyright does not need its own .Sh, it is already commented.
ok jmc@@ deraadt@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.27 2004/09/30 19:59:25 mickey Exp $
d64 1
a64 1
.Dd November 9, 1998
@


1.27
log
@kill default locators
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.26 2003/04/01 11:15:12 jmc Exp $
a361 28
.Sh COPYRIGHT
.Bd -unfilled
The RAIDframe Copyright is as follows:
.Pp
Copyright (c) 1994-1996 Carnegie-Mellon University.
All rights reserved.
.Pp
Permission to use, copy, modify and distribute this software and
its documentation is hereby granted, provided that both the copyright
notice and this permission notice appear in all copies of the
software, derivative works or modified versions, and any portions
thereof, and that both notices appear in supporting documentation.
.Pp
CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
CONDITION.
CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR ANY DAMAGES
WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
.Pp
Carnegie Mellon requests users of this software to return to
.Pp
 Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 School of Computer Science
 Carnegie Mellon University
 Pittsburgh PA 15213-3890
.Pp
any improvements or extensions that they make and grant Carnegie the
rights to redistribute these changes.
.Ed
@


1.26
log
@Digital Unix -> Digital UNIX
Tru64 Unix -> Tru64 UNIX

checked against HP web site.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.25 2003/03/30 12:14:31 jmc Exp $
d214 7
a220 7
sd0     at scsibus0 target 0 lun ?      # SCSI disk drives
sd1     at scsibus0 target 1 lun ?      # SCSI disk drives
sd2     at scsibus0 target 2 lun ?      # SCSI disk drives
sd3     at scsibus0 target 3 lun ?      # SCSI disk drives
sd4     at scsibus0 target 4 lun ?      # SCSI disk drives
sd5     at scsibus0 target 5 lun ?      # SCSI disk drives
sd6     at scsibus0 target 6 lun ?      # SCSI disk drives
@


1.25
log
@typos;
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.24 2003/01/13 19:06:42 kjell Exp $
d346 1
a346 1
driver for Digital Unix.
@


1.24
log
@Start sentence on new line. ok mpech@@. "whatever" theo@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.23 2002/12/06 19:04:24 avsm Exp $
d127 1
a127 1
the 3rd one a 6-disk set, but the RAID set has it as the 3rd component
d207 3
a209 2
4 can end up as /dev/sd0c) for well-behaved functioning of the RAID
device.
d228 9
a236 4
and which map to components /dev/sd0e, /dev/sd1e, and
/dev/sd2e of a RAID 5 set.
If the drive with SCSI ID 5 fails, and the system reboots,
the old /dev/sd2e will show up as /dev/sd1e.
@


1.23
log
@it's -> its where appropriate
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.22 2002/11/08 08:08:47 mpech Exp $
d100 2
a101 2
declustering,  and Interleaved declustering.  The reader is referred
to the RAIDframe documentation mentioned in the
d370 3
a372 2
CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
@


1.22
log
@Time to cleanup:
o) start new sentence on a new line;
o) wrap long lines;
o) don't use .Pp before/after .Sh, .Ss;
o) OpenBSD -> .Ox;
o) typos;
o) close .Rs;
o) use space between arguments in tag, for example:
   .Xr blabla ) .

miod@@ ok
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.21 2002/10/16 18:41:17 deraadt Exp $
d125 1
a125 1
or that the component label is not consistent with it's assigned place
@


1.21
log
@Xr ccd for kicks
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.20 2002/10/15 11:59:02 deraadt Exp $
d79 2
a80 1
and RAID concepts.  The reader is also assumed to know how to configure
d106 8
a113 7
support the failure of component drives.  The number of failures
allowed depends on the parity level selected.  If the driver is able
to handle drive failures, and a drive does fail, then the system is
operating in "degraded mode".  In this mode, all missing data must be
reconstructed from the data and parity present on the other
components.  This results in much slower data accesses, but
does mean that a failure need not bring the system to a complete halt.
d126 1
a126 1
in the set (e.g. the component label claims the component should be
d128 2
a129 2
in a 5-disk set) then the device will fail to configure.  If the
driver determines that exactly one component label seems to be
d139 4
a142 3
auto-configuration of RAID sets.  A RAID set can be flagged as
auto-configurable, in which case it will be configured automatically
during the kernel boot process.  RAID filesystems which are
d145 2
a146 1
set.  To use a RAID set as the root filesystem, a kernel is usually
d148 2
a149 1
auto-configuring RAID set can be used for the root filesystem.  See
d156 2
a157 1
filesystem.  Should a disk fail, the driver is capable of reconstructing
d161 2
a162 1
performed.  The copyback operation, as its name indicates, will copy
d164 2
a165 1
(and now replaced) disk.  Hot spares can also be hot-added using
d180 6
a185 5
option to initialize all RAID sets.  In particular, this
initialization includes re-building the parity data.  This rebuilding
of parity data is also required when either a) a new RAID device is
brought up for the first time or b) after an un-clean shutdown of a
RAID device.  By using the
d194 2
a195 2
filesystem integrity and parity integrity can be ensured.  It bears
repeating again that parity recomputation is
d197 2
a198 2
before any filesystems are created or used on the RAID device.  If the
parity is not correct, then missing data cannot be correctly recovered.
d200 4
a203 3
RAID levels may be combined in a hierarchical fashion.  For example, a RAID 0
device can be constructed out of a number of RAID 5 devices (which, in turn,
may be constructed out of the physical disks, or of other RAID devices).
d206 1
a206 1
addresses (i.e. not left free-floating, where a drive with SCSI ID of
d208 3
a210 2
device.  This is true for all types of drives, including IDE, HP-IB,
etc.  For normal SCSI drives, for example, the following can be used
d224 10
a233 7
for more information.  The rationale for fixing the device addresses
is as follows: Consider a system with three SCSI drives at SCSI ID's
4, 5, and 6, and which map to components /dev/sd0e, /dev/sd1e, and
/dev/sd2e of a RAID 5 set.  If the drive with SCSI ID 5 fails, and the
system reboots, the old /dev/sd2e will show up as /dev/sd1e.  The RAID
driver is able to detect that component positions have changed, and
will not allow normal configuration.  If the device addresses are hard
d235 4
a238 3
is unavailable, and bring the RAID 5 set up in degraded mode.  Note
that the auto-detection and auto-configuration code does not care
about where the components live.  The auto-configuration code will
d244 2
a245 2
driver is to ensure that it is suitably configured in the kernel.  This is
done by adding a line similar to:
d250 2
a251 1
to the kernel configuration file.  The
d266 1
a266 1
(e.g. 4.2BSD) or
d268 7
a274 5
(e.g. RAID).  The use of the latter is strongly encouraged, and is
required if auto-configuration of the RAID set is desired.  Since
RAIDframe leaves room for disklabels, RAID components can be simply
raw disks, or partitions which use an entire disk.  Note that some
platforms (such as SUN) do not allow using the FS_RAID partition type. 
d286 2
a287 2
a component failure.  Doing the wrong thing when a component fails may
result in data loss.
a292 1
.Pp
d295 4
a298 4
data loss due to component failure.  However the loss of two
components of a RAID 4 or 5 system, or the loss of a single component
of a RAID 0 system, will result in the entire filesystems on that RAID
device being lost.
d306 7
a312 6
compromised.  This includes after system crashes, or before a RAID
device has been used for the first time.  Failure to keep parity
correct will be catastrophic should a component ever fail -- it is
better to use RAID 0 and get the additional space and speed, than it
is to use parity, but not keep the parity correct.  At least with RAID
0 there is no perception of increased data security.
d336 2
a337 1
Carnegie Mellon University (CMU).  RAIDframe, as originally distributed
d340 2
a341 1
driver for Digital Unix.  The
@


1.20
log
@option, not options; hamajima@@nagoya.ydc.co.jp
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.19 2002/09/26 07:55:40 miod Exp $
d302 1
@


1.19
log
@Incomplete section 4 mi manual page cleanup.
- make configuration lines match GENERIC files
- more Xr (hopefully usefule Xr)
- never mention intro(4) on non-physical devices (i.e. protocols), always
mention it for physical devices
- random text style and quoting fixes
- drivers ported from FreeBSD did mention rc.conf instead of hostname.if as
the place to put mediaopt
- some .Tn UNIX -> .Ox replacements
- etc, etc...

reviewd in part by deraadt@@ (until he got bored of this)
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.18 2002/04/07 00:03:29 tdeval Exp $
d243 1
a243 1
options    RAID_AUTOCONFIG
d273 1
a273 1
options    RAIDDEBUG
@


1.18
log
@Clarify RAIDframe kernel options(4)
Closes PR#2523 (thanks Dries Schellekens)
  ok miod@@
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.17 2002/01/05 13:47:28 tdeval Exp $
d303 1
@


1.17
log
@Documentation update.  Adapted from NetBSD.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.16 2001/10/05 14:45:53 mpech Exp $
d271 1
a271 1
Additional internal consistency checking can be enabled by specifying:
d273 1
a273 1
options    RAID_DIAGNOSTIC
a275 2
These assertions are disabled by default in order to improve
performance.
@


1.16
log
@Powered by @@mantoya:
o) start new sentence on a new line;
o) minor mdoc fixes;
millert@@ ok

Tip of the day:   www.mpechismazohist.com
@
text
@d1 2
a2 3
.\"	$OpenBSD: raid.4,v 1.15 2001/08/03 15:21:16 mpech Exp $
.\"     $NetBSD: raid.4,v 1.8 1999/12/15 22:07:33 abs Exp $
.\"
d69 1
a69 1
.Nd RAIDframe Disk Driver
d75 7
a81 5
driver provides RAID 0, 1, 4, and 5 (and more!) capabilities.
This document assumes that the reader has at least some familiarity with RAID
and RAID concepts.
The reader is also assumed to know how to configure disks and pseudo-devices
into kernels, how to generate kernels, and how to partition disks.
d99 2
a100 2
declustering, and Interleaved declustering.
The reader is referred to the RAIDframe documentation mentioned in the
d105 7
a111 8
support the failure of component drives.
The number of failures allowed depends on the parity level selected.
If the driver is able to handle drive failures, and a drive does fail,
then the system is operating in "degraded mode".
In this mode, all missing data must be reconstructed from the data and
parity present on the other components.
This results in much slower data accesses, but does mean that a failure
need not bring the system to a complete halt.
d118 2
a119 2
user-specified serial number, the row and column of that component in the RAID
set, and whether the data (and parity) on the component is
d123 1
a123 1
or that the component label is not consistent with its assigned place
d126 2
a127 2
in a 5-disk set) then the device will fail to configure.
If the driver determines that exactly one component label seems to be
d136 12
d150 3
a152 4
disks which are on-line, but are not
actively used in an existing filesystem.
Should a disk fail, the driver is capable of reconstructing the failed disk
onto a hot spare or back onto a replacement drive.
d155 3
a157 4
performed.
The copyback operation, as its name indicates, will copy the reconstructed
data from the hot spare to the previously failed (and now replaced) disk.
Hot spares can also be hot-added using
d168 1
a168 1
For any of the RAID flavours which have parity data,
d172 3
a174 1
option to re-write the data when either a) a new RAID device is
d176 6
a181 2
RAID device.
By performing this on-demand recomputation of all parity before doing a
d184 10
a193 11
.Xr newfs 8
filesystem integrity and parity integrity can be ensured.
It bears repeating again that parity recomputation is
.Em required
before any filesystems are created or used on the RAID device.
If the parity is not correct, then missing data cannot be correctly recovered.
.Pp
RAID levels may be combined in a hierarchical fashion.
For example, a RAID 0 device can be constructed out of a number of RAID 5
devices (which, in turn, may be constructed out of the physical disks, or
of other RAID devices).
d198 3
a200 3
device.
For normal SCSI drives, for example, the following can be
used to fix the device addresses:
d213 13
a225 11
for more information.
The rationale for fixing the device addresses is as follows: Consider a
system with three SCSI drives at SCSI IDs 4, 5, and 6, and which map to
components /dev/sd0e, /dev/sd1e, and /dev/sd2e of a RAID 5 set.
If the drive with SCSI ID 5 fails, and the system reboots, the old
/dev/sd2e will show up as /dev/sd1e.
The RAID driver is able to detect that component positions have changed,
and will not allow normal configuration.
If the device addresses are hard coded, however, the RAID driver would
detect that the middle component is unavailable, and bring the RAID 5
set up in degraded mode.
d229 2
a230 2
driver is to ensure that it is suitably configured in the kernel.
This is done by adding a line similar to:
d235 1
a235 2
to the kernel configuration file.
The
d237 2
a238 2
argument
.Pf ( Sq 4 ,
d240 7
a246 4
At the time of this writing, 4 is the MAXIMUM of
.Nm
devices which are supported.
This will change as soon as kernel threads are available.
d248 10
a257 9
In all cases the
.Sq raw
partitions of the disks
.Pa must not
be combined.
Rather, each component partition should be offset by at least one cylinder
from the beginning of that component disk.
This ensures that the disklabels for the component disks do not conflict
with the disklabel for the
d259 1
a259 3
device.
As well, all component partitions must be of the type
.Dv FS_BSDFFS .
d267 11
a277 3
.Em before
a component failure.
Doing the wrong thing when a component fails may result in data loss.
d280 4
a283 4
data loss due to component failure.
However the loss of two components of a RAID 4 or 5 system, or the loss of
a single component of a RAID 0 system, will result in the entire
filesystems on that RAID device being lost.
d285 1
a285 1
.Em not
d289 1
a289 1
.Em must
d291 6
a296 7
compromised.
This includes after system crashes, or before a RAID device has been used for
the first time.
Failure to keep parity correct will be catastrophic should a component ever
fail -- it is better to use RAID 0 and get the additional space and speed,
than it is to use parity, but not keep the parity correct.
At least with RAID 0 there is no perception of increased data security.
d318 4
a321 5
Carnegie Mellon University (CMU).
RAIDframe, as originally distributed by CMU, provides a RAID simulator
for a number of different architectures, and a user-level device driver
and a kernel device driver for Digital Unix.
The
d327 1
a327 2
for RAID Systems", by William V.
Courtright II, Garth Gibson, Mark
d339 1
a339 1

d342 1
a342 1

d348 1
a348 1

d352 1
a352 1

d354 1
a354 1

d359 1
a359 1

@


1.15
log
@o) We don't like .Pp before/after .Sh;
o) .Nm always has argument in .Sh SYNOPSIS;
o) We always closes .Bl and .Bd tags;

millert@@ ok
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.14 2001/06/23 07:03:58 pjanzen Exp $
d76 5
a80 5
driver provides RAID 0, 1, 4, and 5 (and more!) capabilities.  This
document assumes that the reader has at least some familiarity with RAID
and RAID concepts.  The reader is also assumed to know how to configure
disks and pseudo-devices into kernels, how to generate kernels, and how
to partition disks.
d98 2
a99 2
declustering,  and Interleaved declustering.  The reader is referred
to the RAIDframe documentation mentioned in the
d104 8
a111 7
support the failure of component drives.  The number of failures
allowed depends on the parity level selected.  If the driver is able
to handle drive failures, and a drive does fail, then the system is
operating in "degraded mode".  In this mode, all missing data must be
reconstructed from the data and parity present on the other
components.  This results in much slower data accesses, but
does mean that a failure need not bring the system to a complete halt.
d126 2
a127 2
in a 5-disk set) then the device will fail to configure.  If the
driver determines that exactly one component label seems to be
d139 3
a141 3
actively used in an existing filesystem.  Should a disk fail, the
driver is capable of reconstructing the failed disk onto a hot spare
or back onto a replacement drive.
d144 4
a147 3
performed.  The copyback operation, as its name indicates, will copy
the reconstructed data from the hot spare to the previously failed
(and now replaced) disk.  Hot spares can also be hot-added using
d164 2
a165 2
RAID device.  By performing this on-demand recomputation of all parity
before doing a
d169 2
a170 2
filesystem integrity and parity integrity can be ensured.  It bears
repeating again that parity recomputation is
d172 2
a173 2
before any filesystems are created or used on the RAID device.  If the
parity is not correct, then missing data cannot be correctly recovered.
d175 4
a178 3
RAID levels may be combined in a hierarchical fashion.  For example, a RAID 0
device can be constructed out of a number of RAID 5 devices (which, in turn,
may be constructed out of the physical disks, or of other RAID devices).
d183 2
a184 1
device.  For normal SCSI drives, for example, the following can be
d198 11
a208 9
for more information.  The rationale for fixing the device addresses
is as follows: Consider a system with three SCSI drives at SCSI IDs
4, 5, and 6, and which map to components /dev/sd0e, /dev/sd1e, and
/dev/sd2e of a RAID 5 set.  If the drive with SCSI ID 5 fails, and the
system reboots, the old /dev/sd2e will show up as /dev/sd1e.  The RAID
driver is able to detect that component positions have changed, and
will not allow normal configuration.  If the device addresses are hard
coded, however, the RAID driver would detect that the middle component
is unavailable, and bring the RAID 5 set up in degraded mode.
d212 2
a213 2
driver is to ensure that it is suitably configured in the kernel.  This is
done by adding a line similar to:
d218 2
a219 1
to the kernel configuration file.  The
d226 2
a227 2
devices which are supported.  This will change as soon as kernel threads
are available.
d233 5
a237 4
be combined.  Rather, each component partition should be offset by at least one
cylinder from the beginning of that component disk.  This ensures that
the disklabels for the component disks do not conflict with the
disklabel for the
d250 2
a251 2
a component failure.  Doing the wrong thing when a component fails may
result in data loss.
d254 4
a257 4
data loss due to component failure.  However the loss of two
components of a RAID 4 or 5 system, or the loss of a single component
of a RAID 0 system, will result in the entire filesystems on that RAID
device being lost.
d265 7
a271 6
compromised.  This includes after system crashes, or before a RAID
device has been used for the first time.  Failure to keep parity
correct will be catastrophic should a component ever fail -- it is
better to use RAID 0 and get the additional space and speed, than it
is to use parity, but not keep the parity correct.  At least with RAID
0 there is no perception of increased data security.
d293 5
a297 4
Carnegie Mellon University (CMU).  RAIDframe, as originally distributed
by CMU, provides a RAID simulator for a number of different
architectures, and a user-level device driver and a kernel device
driver for Digital Unix.  The
d303 2
a304 1
for RAID Systems", by William V. Courtright II, Garth Gibson, Mark
a314 1

a338 1

@


1.14
log
@spelling fixes and minor repairs almost entirely from jsyn@@nthought.com
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.13 2001/06/22 12:15:46 mpech Exp $
a243 1
.Pp
@


1.13
log
@o) remove unnecessary .Pp;
o) remove unnecessary .Nm args;
o) closes .Bl;
o) .Sh AUTHOR -> .Sh AUTHORS;
o) sort SEE ALSO;
o) better -mdoc style;
o) typos;

aaron@@ ok.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.12 2000/12/21 18:15:13 avsm Exp $
d195 1
a195 1
is as follows: Consider a system with three SCSI drives at SCSI ID's
@


1.12
log
@fix typos, aaron@@ ok
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.11 2000/04/03 21:19:37 aaron Exp $
a263 1
.Pp
a269 1
.Pp
d271 1
a276 1
.Xr sd 4 ,
@


1.11
log
@Remove trailing whitespace.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.10 2000/03/06 21:46:57 aaron Exp $
d122 1
a122 1
or that the component label is not consistent with it's assigned place
d142 1
a142 1
removed, a new disk put in it's place, and a copyback operation
@


1.10
log
@Do not leave `(' characters dangling at the end of lines; instead, use the
appropriate mdoc macros to handle these cases.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.9 2000/01/07 14:52:28 peter Exp $
d112 1
a112 1
The RAID driver supports and enforces the use of 
d118 1
a118 1
set, and whether the data (and parity) on the component is 
d192 1
a192 1
See 
@


1.9
log
@sync with Greg Oster's NetBSD changes to Jan 4th 2000
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.8 1999/09/23 04:12:04 alex Exp $
d214 2
a215 2
argument (
.Sq 4 ,
@


1.8
log
@Typo fixes.
@
text
@d1 2
a2 2
.\"	$OpenBSD: raid.4,v 1.7 1999/07/30 14:45:31 peter Exp $
.\"     $NetBSD: raid.4,v 1.5 1999/03/16 01:19:17 garbled Exp $
d177 26
a202 15
At the time of this writing, it is
.Em imperative
that drives be
.Sq nailed down
at their respective addresses (i.e. not left free-floating, where a
drive with SCSI ID of 4 can end up as /dev/sd0c).  Consider a system
with three SCSI drives at SCSI IDs 4, 5, and 6, and which map to
components /dev/sd0e, /dev/sd1e, and /dev/sd2e.  If the drive with
SCSI ID 5 fails,
and the system reboots, the old /dev/sd2e will show up as /dev/sd1e.
If the RAID driver is automatically configured, it will only detect
that /dev/sd2e has failed, and will not notice that /dev/sd2e has
actually become /dev/sd1e.  Hopefully this will change within a few
days of this writing with the addition of MD5 checksums to each of the
components.
d278 1
@


1.7
log
@Update RAIDframe from NetBSD-current as of 1999/07/26.

Please note that you *must* follow the upgrade instructions at

	http://www.cs.usask.ca/staff/oster/clabel_upgrade.html

before installing the new raidctl and new kernel using this code.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.6 1999/07/09 13:35:48 aaron Exp $
d140 1
a140 1
or back onto a replacment drive.
d215 1
a215 1
be combined.  Rather, each component parition should be offset by at least one
d221 1
a221 1
As well, all component paritions must be of the type
@


1.6
log
@- remove all trailing whitespace
     * except when it is escaped with a `\' at the end of the line
- fix remaining .Nm usage as well
- this is from a patch I received from kwesterback@@home.com, who has been
  working on some scripts for fixing formatting errors in mdoc'd man pages

Ok, so there could be a cost/benefit debate with this commit, but since I have
the patch we might as well commit it...
@
text
@d1 2
a2 1
.\"	$OpenBSD: raid.4,v 1.5 1999/07/08 09:41:43 hugh Exp $
d112 23
d139 2
a140 1
driver is capable of reconstructing the failed disk onto a hot spare.
d145 2
a146 1
(and now replaced) disk.
d159 1
a159 1
.Fl r
@


1.5
log
@spurious words and misc fixes
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.4 1999/06/05 04:16:07 aaron Exp $
d41 1
a41 1
.\" 
d43 1
a43 1
.\" 
d49 1
a49 1
.\" 
d53 1
a53 1
.\" 
d55 1
a55 1
.\" 
d60 1
a60 1
.\" 
d63 1
a63 1
.\" 
d78 1
a78 1
disks and pseudo-devices into kernels, how to generate kernels, and how 
d83 1
a83 1
.It RAID 0 
d85 1
a85 1
.It RAID 1 
d87 1
a87 1
.It RAID 4 
d90 2
a91 2
.It RAID 5 
provides data striping across the components, with parity 
d111 2
a112 2
The driver supports 
.Sq hot spares , 
d125 1
a125 1
The user-land utility for doing all 
d128 1
a128 1
is 
d130 1
a130 1
For any of the RAID flavours which have parity data, 
d132 1
a132 1
must be used with the 
d136 2
a137 2
RAID device.  By performing this on-demand recomputation of all parity 
before doing a 
d142 1
a142 1
repeating again that parity recomputation is 
d147 2
a148 2
RAID levels may be combined in a hierarchical fashion.  For example, a RAID 0 
device can be constructed out of a number of RAID 5 devices (which, in turn, 
d151 1
a151 1
At the time of this writing, it is 
d153 1
a153 1
that drives be 
d156 2
a157 2
drive with SCSI ID of 4 can end up as /dev/sd0c).  Consider a system 
with three SCSI drives at SCSI IDs 4, 5, and 6, and which map to 
d167 1
a167 1
The first step to using the 
d175 1
a175 1
to the kernel configuration file.  The 
d180 1
a180 1
At the time of this writing, 4 is the MAXIMUM of 
d190 1
a190 1
cylinder from the beginning of that component disk.  This ensures that 
d192 1
a192 1
disklabel for the 
d198 1
a198 1
A more detailed treatment of actually using a 
d200 1
a200 1
device is found in 
d214 1
a214 1
RAID is 
d218 1
a218 1
Recomputation of parity 
d226 1
a226 1
0 there is no perception of increased data security. 
d243 1
a243 1
The 
d245 1
a245 1
driver in 
d252 1
a252 1
driver for Digital Unix.  The 
d261 1
a261 1
The 
@


1.4
log
@capitalize the acronym ID
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.3 1999/05/16 19:56:35 alex Exp $
d75 1
a75 1
driver provides RAID 0, 1, 4, and 5 (and more!) capabilities to NetBSD.  This
d118 1
a118 1
performed.  The copyback operation, as it's name indicates, will copy
d163 1
a163 1
actually become /dev/sd1e.  Hopefully this will change withing a few
d252 1
a252 1
driver for for Digital Unix.  The 
@


1.3
log
@Cleanup xrefs under SEE ALSO.  Specifically:

  - Sort xrefs by section, and then alphabetically.
  - Add missing commas between xref items.
  - Remove commas from the last xref entry.
  - Remove duplicate entries.
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.2 1999/02/19 00:19:52 niklas Exp $
d143 1
a143 1
.Ar required
d152 1
a152 1
.Ar imperative
d157 1
a157 1
with three SCSI drives at SCSI ID's 4, 5, and 6, and which map to 
d204 1
a204 1
.Ar before
d215 1
a215 1
.Ar NOT
d219 1
a219 1
.Ar MUST
@


1.2
log
@We are OpenBSD
@
text
@d1 1
a1 1
.\"	$OpenBSD: raid.4,v 1.1 1999/01/11 15:02:04 niklas Exp $
a236 1
.Xr raidctl 8 ,
d240 2
a241 1
.Xr newfs 8
@


1.1
log
@raid(4)
@
text
@d1 1
a1 1
.\"	$OpenBSD$
d246 1
a246 1
.Nx
d264 3
a266 1
.Nx 1.4 .
@

