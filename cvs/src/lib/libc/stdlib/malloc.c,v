head	1.224;
access;
symbols
	OPENBSD_6_1:1.218.0.4
	OPENBSD_6_1_BASE:1.218
	OPENBSD_6_0:1.192.0.2
	OPENBSD_6_0_BASE:1.192
	OPENBSD_5_9:1.181.0.2
	OPENBSD_5_9_BASE:1.181
	OPENBSD_5_8:1.174.0.4
	OPENBSD_5_8_BASE:1.174
	OPENBSD_5_7:1.173.0.2
	OPENBSD_5_7_BASE:1.173
	OPENBSD_5_6:1.170.0.4
	OPENBSD_5_6_BASE:1.170
	OPENBSD_5_5:1.150.0.4
	OPENBSD_5_5_BASE:1.150
	OPENBSD_5_4:1.149.0.4
	OPENBSD_5_4_BASE:1.149
	OPENBSD_5_3:1.149.0.2
	OPENBSD_5_3_BASE:1.149
	OPENBSD_5_2:1.146.0.2
	OPENBSD_5_2_BASE:1.146
	OPENBSD_5_1_BASE:1.140
	OPENBSD_5_1:1.140.0.2
	OPENBSD_5_0:1.139.0.2
	OPENBSD_5_0_BASE:1.139
	OPENBSD_4_9:1.127.0.2
	OPENBSD_4_9_BASE:1.127
	OPENBSD_4_8:1.125.0.2
	OPENBSD_4_8_BASE:1.125
	OPENBSD_4_7:1.124.0.2
	OPENBSD_4_7_BASE:1.124
	OPENBSD_4_6:1.116.0.4
	OPENBSD_4_6_BASE:1.116
	OPENBSD_4_5:1.115.0.2
	OPENBSD_4_5_BASE:1.115
	OPENBSD_4_4:1.92.0.2
	OPENBSD_4_4_BASE:1.92
	OPENBSD_4_3:1.88.0.2
	OPENBSD_4_3_BASE:1.88
	OPENBSD_4_2:1.86.0.4
	OPENBSD_4_2_BASE:1.86
	OPENBSD_4_1:1.86.0.2
	OPENBSD_4_1_BASE:1.86
	OPENBSD_4_0:1.83.0.2
	OPENBSD_4_0_BASE:1.83
	OPENBSD_3_9:1.80.0.2
	OPENBSD_3_9_BASE:1.80
	OPENBSD_3_8:1.76.0.2
	OPENBSD_3_8_BASE:1.76
	OPENBSD_3_7:1.71.0.4
	OPENBSD_3_7_BASE:1.71
	OPENBSD_3_6:1.71.0.2
	OPENBSD_3_6_BASE:1.71
	OPENBSD_3_5:1.66.0.2
	OPENBSD_3_5_BASE:1.66
	OPENBSD_3_4:1.59.0.2
	OPENBSD_3_4_BASE:1.59
	OPENBSD_3_3:1.54.0.2
	OPENBSD_3_3_BASE:1.54
	OPENBSD_3_2:1.48.0.2
	OPENBSD_3_2_BASE:1.48
	OPENBSD_3_1:1.47.0.2
	OPENBSD_3_1_BASE:1.47
	OPENBSD_3_0:1.42.0.2
	OPENBSD_3_0_BASE:1.42
	OPENBSD_2_9:1.40.0.6
	OPENBSD_2_9_BASE:1.40
	OPENBSD_2_8:1.40.0.4
	OPENBSD_2_8_BASE:1.40
	OPENBSD_2_7:1.40.0.2
	OPENBSD_2_7_BASE:1.40
	OPENBSD_2_6:1.36.0.2
	OPENBSD_2_6_BASE:1.36
	OPENBSD_2_5:1.35.0.2
	OPENBSD_2_5_BASE:1.35
	OPENBSD_2_4:1.32.0.2
	OPENBSD_2_4_BASE:1.32
	OPENBSD_2_3:1.30.0.2
	OPENBSD_2_3_BASE:1.30
	OPENBSD_2_2:1.29.0.2
	OPENBSD_2_2_BASE:1.29
	OPENBSD_2_1:1.24.0.2
	OPENBSD_2_1_BASE:1.24
	OPENBSD_2_0:1.16.0.2
	OPENBSD_2_0_BASE:1.16
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.224
date	2017.04.22.09.12.49;	author otto;	state Exp;
branches;
next	1.223;
commitid	8N1FAKeSWW2dfQfP;

1.223
date	2017.04.18.15.46.44;	author otto;	state Exp;
branches;
next	1.222;
commitid	WFyIUC7QJMAMxbEO;

1.222
date	2017.04.17.16.36.35;	author otto;	state Exp;
branches;
next	1.221;
commitid	Ss4DubDmemtiwyyq;

1.221
date	2017.04.13.18.32.55;	author otto;	state Exp;
branches;
next	1.220;
commitid	QwLNer6ydKWzc9I5;

1.220
date	2017.04.10.05.45.02;	author otto;	state Exp;
branches;
next	1.219;
commitid	eHZQ1wx4WtC7i8dc;

1.219
date	2017.04.06.08.39.47;	author otto;	state Exp;
branches;
next	1.218;
commitid	PBAixAw7dcUUFvW2;

1.218
date	2017.03.28.16.56.38;	author otto;	state Exp;
branches;
next	1.217;
commitid	SECqkPki1FkOTfjM;

1.217
date	2017.03.24.16.23.05;	author otto;	state Exp;
branches;
next	1.216;
commitid	c6a697hYFDCK3NTl;

1.216
date	2017.03.24.16.15.31;	author otto;	state Exp;
branches;
next	1.215;
commitid	E7atUClJQW0H55na;

1.215
date	2017.02.15.12.31.57;	author jsg;	state Exp;
branches;
next	1.214;
commitid	DTFXRJDghqiAddur;

1.214
date	2017.02.02.10.35.34;	author otto;	state Exp;
branches;
next	1.213;
commitid	VIZ730XNnQMD4NgT;

1.213
date	2017.02.01.06.17.42;	author otto;	state Exp;
branches;
next	1.212;
commitid	7LdLu1UCbVRogZf2;

1.212
date	2017.01.21.07.47.42;	author otto;	state Exp;
branches;
next	1.211;
commitid	8izqOrEBZ5f6q8lC;

1.211
date	2016.11.04.09.11.20;	author otto;	state Exp;
branches;
next	1.210;
commitid	KXoqVCCY3FBEmOGg;

1.210
date	2016.11.03.18.51.49;	author otto;	state Exp;
branches;
next	1.209;
commitid	zFn5hKSenUHvoh8S;

1.209
date	2016.10.31.10.06.56;	author otto;	state Exp;
branches;
next	1.208;
commitid	Lq5CiypVVi4WxupC;

1.208
date	2016.10.28.17.03.22;	author otto;	state Exp;
branches;
next	1.207;
commitid	zdZKJVhyptCKImqu;

1.207
date	2016.10.22.14.27.19;	author otto;	state Exp;
branches;
next	1.206;
commitid	GBWsERW5Jq4IJaxg;

1.206
date	2016.10.21.15.39.31;	author otto;	state Exp;
branches;
next	1.205;
commitid	EKu3mOMLPd1drrwx;

1.205
date	2016.10.21.06.55.09;	author otto;	state Exp;
branches;
next	1.204;
commitid	hSBBzrdAvcubPirg;

1.204
date	2016.10.20.11.29.34;	author otto;	state Exp;
branches;
next	1.203;
commitid	MFi5hc5A0x0TSHu5;

1.203
date	2016.10.20.05.38.41;	author otto;	state Exp;
branches;
next	1.202;
commitid	SBAFUIC4Oh3NLBpP;

1.202
date	2016.10.15.18.24.40;	author guenther;	state Exp;
branches;
next	1.201;
commitid	XGWAFLwctKs8oeFL;

1.201
date	2016.10.14.17.33.36;	author otto;	state Exp;
branches;
next	1.200;
commitid	AnZeYNv8Tswzby4L;

1.200
date	2016.10.12.07.36.38;	author otto;	state Exp;
branches;
next	1.199;
commitid	rhEhoklnj1a0Ww2h;

1.199
date	2016.10.07.05.55.37;	author otto;	state Exp;
branches;
next	1.198;
commitid	Ja3xNMt9bmEOZkK5;

1.198
date	2016.10.07.05.54.35;	author otto;	state Exp;
branches;
next	1.197;
commitid	g0jmqp0UJlglJ2G4;

1.197
date	2016.09.21.04.38.56;	author guenther;	state Exp;
branches;
next	1.196;
commitid	pqjnYVtACzQ9ctai;

1.196
date	2016.09.18.13.46.28;	author otto;	state Exp;
branches;
next	1.195;
commitid	ohE0js2B0E7UdGyj;

1.195
date	2016.09.01.10.41.02;	author otto;	state Exp;
branches;
next	1.194;
commitid	HatwrthzJhfQpDr7;

1.194
date	2016.09.01.10.20.22;	author tedu;	state Exp;
branches;
next	1.193;
commitid	RIopII8D6BuixGUT;

1.193
date	2016.08.17.05.33.54;	author otto;	state Exp;
branches;
next	1.192;
commitid	TBF5tGTSzEBsXzP2;

1.192
date	2016.07.06.20.32.02;	author otto;	state Exp;
branches;
next	1.191;
commitid	AYsEYDeA9HAJozst;

1.191
date	2016.06.30.09.00.48;	author otto;	state Exp;
branches;
next	1.190;
commitid	hfuhnBXVjdnV2QSQ;

1.190
date	2016.06.28.06.40.11;	author tb;	state Exp;
branches;
next	1.189;
commitid	7i3QyrXJwXxqL8s0;

1.189
date	2016.06.27.15.33.40;	author tedu;	state Exp;
branches;
next	1.188;
commitid	dlFTbtdULZGMX3F6;

1.188
date	2016.04.12.18.14.02;	author otto;	state Exp;
branches;
next	1.187;
commitid	YvMEXPWSSDQHKLMm;

1.187
date	2016.04.09.14.08.40;	author otto;	state Exp;
branches;
next	1.186;
commitid	uQfRuxLmeJ2nCybM;

1.186
date	2016.04.09.12.23.59;	author otto;	state Exp;
branches;
next	1.185;
commitid	myISmmSuk2PdL3WC;

1.185
date	2016.03.17.17.55.33;	author mmcc;	state Exp;
branches;
next	1.184;
commitid	xx7mESXrxO5zuLo4;

1.184
date	2016.03.14.17.20.26;	author otto;	state Exp;
branches;
next	1.183;
commitid	WEkHMNAK2H0YcBXg;

1.183
date	2016.03.13.18.34.21;	author guenther;	state Exp;
branches;
next	1.182;
commitid	AStIlKdF20sYXV6x;

1.182
date	2016.02.25.00.38.51;	author deraadt;	state Exp;
branches;
next	1.181;
commitid	2KWMyRJsfj0yz6Bh;

1.181
date	2016.01.26.15.44.28;	author otto;	state Exp;
branches;
next	1.180;
commitid	jPoKGuCRVL0kpM88;

1.180
date	2016.01.06.17.57.22;	author tedu;	state Exp;
branches;
next	1.179;
commitid	6hdC6citgQ7pUGrN;

1.179
date	2015.12.30.06.04.39;	author tedu;	state Exp;
branches;
next	1.178;
commitid	YWHPklL2qFNX2DFC;

1.178
date	2015.12.30.06.01.18;	author tedu;	state Exp;
branches;
next	1.177;
commitid	Hnp5szDXCJfBr3eP;

1.177
date	2015.12.09.02.45.23;	author tedu;	state Exp;
branches;
next	1.176;
commitid	4oXBICKpx91PF8wY;

1.176
date	2015.09.13.20.29.23;	author guenther;	state Exp;
branches;
next	1.175;
commitid	u2fz8qkdx56qeEmF;

1.175
date	2015.09.13.08.31.47;	author guenther;	state Exp;
branches;
next	1.174;
commitid	QZ177IcjQzeRxHAC;

1.174
date	2015.04.06.09.18.51;	author tedu;	state Exp;
branches;
next	1.173;
commitid	qRZh26lkDvHqRRxK;

1.173
date	2015.01.16.16.48.51;	author deraadt;	state Exp;
branches;
next	1.172;
commitid	0DYulI8hhujBHMcR;

1.172
date	2015.01.05.21.04.04;	author tedu;	state Exp;
branches;
next	1.171;
commitid	CLo13nSUPn69I32N;

1.171
date	2014.08.18.14.34.58;	author tedu;	state Exp;
branches;
next	1.170;
commitid	t4dV7Cz176j66adn;

1.170
date	2014.07.09.19.11.00;	author tedu;	state Exp;
branches;
next	1.169;
commitid	fV3u9GI3TCoja7KE;

1.169
date	2014.06.27.18.17.03;	author deraadt;	state Exp;
branches;
next	1.168;
commitid	hRAgSW0YuCwQfGg3;

1.168
date	2014.06.27.17.37.42;	author otto;	state Exp;
branches;
next	1.167;
commitid	l1N12JwLDuiVRAQ1;

1.167
date	2014.06.02.08.49.38;	author otto;	state Exp;
branches;
next	1.166;
commitid	MjZnapnUYXaoFATu;

1.166
date	2014.05.26.06.19.07;	author otto;	state Exp;
branches;
next	1.165;

1.165
date	2014.05.21.15.47.51;	author otto;	state Exp;
branches;
next	1.164;

1.164
date	2014.05.18.17.49.47;	author tedu;	state Exp;
branches;
next	1.163;

1.163
date	2014.05.12.19.02.20;	author tedu;	state Exp;
branches;
next	1.162;

1.162
date	2014.05.10.18.14.55;	author otto;	state Exp;
branches;
next	1.161;

1.161
date	2014.05.08.21.43.49;	author deraadt;	state Exp;
branches;
next	1.160;

1.160
date	2014.05.07.20.07.59;	author halex;	state Exp;
branches;
next	1.159;

1.159
date	2014.05.01.04.08.13;	author tedu;	state Exp;
branches;
next	1.158;

1.158
date	2014.04.23.15.07.27;	author tedu;	state Exp;
branches;
next	1.157;

1.157
date	2014.04.23.10.47.15;	author espie;	state Exp;
branches;
next	1.156;

1.156
date	2014.04.23.05.43.25;	author otto;	state Exp;
branches;
next	1.155;

1.155
date	2014.04.22.14.26.26;	author tedu;	state Exp;
branches;
next	1.154;

1.154
date	2014.04.21.13.17.32;	author deraadt;	state Exp;
branches;
next	1.153;

1.153
date	2014.04.14.10.29.41;	author otto;	state Exp;
branches;
next	1.152;

1.152
date	2014.04.03.16.18.11;	author schwarze;	state Exp;
branches;
next	1.151;

1.151
date	2014.03.25.17.00.02;	author beck;	state Exp;
branches;
next	1.150;

1.150
date	2013.11.12.06.57.54;	author deraadt;	state Exp;
branches;
next	1.149;

1.149
date	2012.12.22.07.32.17;	author otto;	state Exp;
branches;
next	1.148;

1.148
date	2012.11.02.18.18.15;	author djm;	state Exp;
branches;
next	1.147;

1.147
date	2012.09.13.10.45.41;	author pirofti;	state Exp;
branches;
next	1.146;

1.146
date	2012.07.09.08.39.24;	author deraadt;	state Exp;
branches;
next	1.145;

1.145
date	2012.06.26.21.36.25;	author tedu;	state Exp;
branches;
next	1.144;

1.144
date	2012.06.22.01.30.17;	author tedu;	state Exp;
branches;
next	1.143;

1.143
date	2012.06.20.13.13.15;	author tedu;	state Exp;
branches;
next	1.142;

1.142
date	2012.06.18.17.03.51;	author matthew;	state Exp;
branches;
next	1.141;

1.141
date	2012.02.29.08.44.14;	author otto;	state Exp;
branches;
next	1.140;

1.140
date	2011.10.06.14.37.04;	author otto;	state Exp;
branches;
next	1.139;

1.139
date	2011.07.12.14.43.42;	author otto;	state Exp;
branches;
next	1.138;

1.138
date	2011.06.20.18.04.06;	author tedu;	state Exp;
branches;
next	1.137;

1.137
date	2011.05.20.20.02.08;	author otto;	state Exp;
branches;
next	1.136;

1.136
date	2011.05.18.18.09.37;	author otto;	state Exp;
branches;
next	1.135;

1.135
date	2011.05.18.18.07.20;	author otto;	state Exp;
branches;
next	1.134;

1.134
date	2011.05.12.12.03.40;	author otto;	state Exp;
branches;
next	1.133;

1.133
date	2011.05.12.09.35.37;	author otto;	state Exp;
branches;
next	1.132;

1.132
date	2011.05.12.09.29.30;	author otto;	state Exp;
branches;
next	1.131;

1.131
date	2011.05.08.07.08.13;	author otto;	state Exp;
branches;
next	1.130;

1.130
date	2011.05.05.12.11.20;	author otto;	state Exp;
branches;
next	1.129;

1.129
date	2011.04.30.15.46.46;	author otto;	state Exp;
branches;
next	1.128;

1.128
date	2011.04.30.14.56.20;	author otto;	state Exp;
branches;
next	1.127;

1.127
date	2010.12.16.18.47.01;	author dhill;	state Exp;
branches;
next	1.126;

1.126
date	2010.10.21.08.09.35;	author otto;	state Exp;
branches;
next	1.125;

1.125
date	2010.05.18.22.24.55;	author tedu;	state Exp;
branches;
next	1.124;

1.124
date	2010.01.13.12.40.11;	author otto;	state Exp;
branches;
next	1.123;

1.123
date	2009.12.16.08.23.53;	author otto;	state Exp;
branches;
next	1.122;

1.122
date	2009.12.07.18.47.38;	author miod;	state Exp;
branches;
next	1.121;

1.121
date	2009.11.27.20.11.01;	author otto;	state Exp;
branches;
next	1.120;

1.120
date	2009.11.27.20.05.03;	author otto;	state Exp;
branches;
next	1.119;

1.119
date	2009.11.27.19.59.49;	author otto;	state Exp;
branches;
next	1.118;

1.118
date	2009.11.02.19.26.17;	author todd;	state Exp;
branches;
next	1.117;

1.117
date	2009.10.20.21.19.37;	author pirofti;	state Exp;
branches;
next	1.116;

1.116
date	2009.06.08.19.21.08;	author deraadt;	state Exp;
branches;
next	1.115;

1.115
date	2009.01.03.12.58.28;	author djm;	state Exp;
branches;
next	1.114;

1.114
date	2008.12.31.05.21.46;	author deraadt;	state Exp;
branches;
next	1.113;

1.113
date	2008.12.30.07.44.51;	author djm;	state Exp;
branches;
next	1.112;

1.112
date	2008.12.29.22.25.50;	author djm;	state Exp;
branches;
next	1.111;

1.111
date	2008.12.15.19.47.49;	author otto;	state Exp;
branches;
next	1.110;

1.110
date	2008.11.20.09.05.15;	author otto;	state Exp;
branches;
next	1.109;

1.109
date	2008.11.20.09.01.24;	author otto;	state Exp;
branches;
next	1.108;

1.108
date	2008.11.13.07.38.45;	author otto;	state Exp;
branches;
next	1.107;

1.107
date	2008.11.12.09.41.49;	author otto;	state Exp;
branches;
next	1.106;

1.106
date	2008.11.06.12.32.45;	author otto;	state Exp;
branches;
next	1.105;

1.105
date	2008.11.02.08.50.41;	author otto;	state Exp;
branches;
next	1.104;

1.104
date	2008.10.29.14.05.15;	author otto;	state Exp;
branches;
next	1.103;

1.103
date	2008.10.20.06.19.02;	author otto;	state Exp;
branches;
next	1.102;

1.102
date	2008.10.03.19.31.49;	author otto;	state Exp;
branches;
next	1.101;

1.101
date	2008.10.03.19.01.12;	author otto;	state Exp;
branches;
next	1.100;

1.100
date	2008.10.03.18.44.29;	author otto;	state Exp;
branches;
next	1.99;

1.99
date	2008.10.03.18.42.45;	author otto;	state Exp;
branches;
next	1.98;

1.98
date	2008.08.25.17.56.17;	author otto;	state Exp;
branches;
next	1.97;

1.97
date	2008.08.23.07.49.38;	author djm;	state Exp;
branches;
next	1.96;

1.96
date	2008.08.23.06.15.16;	author djm;	state Exp;
branches;
next	1.95;

1.95
date	2008.08.22.21.25.10;	author otto;	state Exp;
branches;
next	1.94;

1.94
date	2008.08.22.17.14.57;	author otto;	state Exp;
branches;
next	1.93;

1.93
date	2008.08.07.18.41.47;	author otto;	state Exp;
branches;
next	1.92;

1.92
date	2008.07.28.04.56.38;	author otto;	state Exp;
branches;
next	1.91;

1.91
date	2008.06.13.21.18.42;	author otto;	state Exp;
branches;
next	1.90;

1.90
date	2008.05.19.19.36.15;	author otto;	state Exp;
branches;
next	1.89;

1.89
date	2008.04.13.00.22.16;	author djm;	state Exp;
branches;
next	1.88;

1.88
date	2008.02.20.18.31.34;	author otto;	state Exp;
branches;
next	1.87;

1.87
date	2007.09.03.14.37.02;	author millert;	state Exp;
branches;
next	1.86;

1.86
date	2007.02.12.20.00.14;	author otto;	state Exp;
branches;
next	1.85;

1.85
date	2006.12.19.13.00.50;	author otto;	state Exp;
branches;
next	1.84;

1.84
date	2006.10.24.04.35.30;	author tedu;	state Exp;
branches;
next	1.83;

1.83
date	2006.05.14.19.53.40;	author otto;	state Exp;
branches;
next	1.82;

1.82
date	2006.04.24.19.23.42;	author otto;	state Exp;
branches;
next	1.81;

1.81
date	2006.04.18.18.26.13;	author otto;	state Exp;
branches;
next	1.80;

1.80
date	2006.02.14.11.14.11;	author espie;	state Exp;
branches;
next	1.79;

1.79
date	2005.10.10.12.00.52;	author espie;	state Exp;
branches;
next	1.78;

1.78
date	2005.10.05.18.38.10;	author deraadt;	state Exp;
branches;
next	1.77;

1.77
date	2005.10.05.02.18.29;	author deraadt;	state Exp;
branches;
next	1.76;

1.76
date	2005.08.08.08.05.36;	author espie;	state Exp;
branches;
next	1.75;

1.75
date	2005.07.07.05.28.53;	author tdeval;	state Exp;
branches;
next	1.74;

1.74
date	2005.06.07.04.42.42;	author tedu;	state Exp;
branches;
next	1.73;

1.73
date	2005.05.24.16.39.05;	author tedu;	state Exp;
branches;
next	1.72;

1.72
date	2005.03.31.21.24.46;	author tdeval;	state Exp;
branches;
next	1.71;

1.71
date	2004.08.11.06.22.45;	author tdeval;	state Exp;
branches;
next	1.70;

1.70
date	2004.08.05.21.55.21;	author tdeval;	state Exp;
branches;
next	1.69;

1.69
date	2004.08.04.19.12.53;	author tdeval;	state Exp;
branches;
next	1.68;

1.68
date	2004.08.01.08.45.39;	author tdeval;	state Exp;
branches;
next	1.67;

1.67
date	2004.04.12.09.25.11;	author tdeval;	state Exp;
branches;
next	1.66;

1.66
date	2004.02.19.23.20.53;	author tdeval;	state Exp;
branches;
next	1.65;

1.65
date	2003.11.19.02.27.18;	author tedu;	state Exp;
branches;
next	1.64;

1.64
date	2003.10.16.17.05.05;	author tedu;	state Exp;
branches;
next	1.63;

1.63
date	2003.10.15.21.37.01;	author tedu;	state Exp;
branches;
next	1.62;

1.62
date	2003.10.02.00.02.10;	author tedu;	state Exp;
branches;
next	1.61;

1.61
date	2003.09.30.00.22.03;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2003.09.27.21.09.15;	author tedu;	state Exp;
branches;
next	1.59;

1.59
date	2003.08.04.16.51.49;	author jfb;	state Exp;
branches;
next	1.58;

1.58
date	2003.07.19.23.52.27;	author tdeval;	state Exp;
branches;
next	1.57;

1.57
date	2003.07.13.08.35.44;	author otto;	state Exp;
branches;
next	1.56;

1.56
date	2003.05.14.17.46.39;	author tdeval;	state Exp;
branches;
next	1.55;

1.55
date	2003.05.14.15.41.51;	author tdeval;	state Exp;
branches;
next	1.54;

1.54
date	2003.01.14.02.27.16;	author millert;	state Exp;
branches;
next	1.53;

1.53
date	2002.11.27.21.40.32;	author tdeval;	state Exp;
branches;
next	1.52;

1.52
date	2002.11.25.00.06.51;	author cloder;	state Exp;
branches;
next	1.51;

1.51
date	2002.11.05.22.19.55;	author marc;	state Exp;
branches;
next	1.50;

1.50
date	2002.11.03.23.58.39;	author marc;	state Exp;
branches;
next	1.49;

1.49
date	2002.11.03.20.36.43;	author marc;	state Exp;
branches;
next	1.48;

1.48
date	2002.05.27.03.13.23;	author deraadt;	state Exp;
branches;
next	1.47;

1.47
date	2002.02.16.21.27.24;	author millert;	state Exp;
branches;
next	1.46;

1.46
date	2002.01.23.20.42.24;	author fgsch;	state Exp;
branches;
next	1.45;

1.45
date	2001.12.05.22.54.01;	author tdeval;	state Exp;
branches;
next	1.44;

1.44
date	2001.11.01.07.00.51;	author mickey;	state Exp;
branches;
next	1.43;

1.43
date	2001.10.30.17.01.07;	author tdeval;	state Exp;
branches;
next	1.42;

1.42
date	2001.05.11.15.30.14;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2001.05.10.16.14.19;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2000.04.10.19.36.29;	author deraadt;	state Exp;
branches;
next	1.39;

1.39
date	2000.03.01.03.09.08;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	99.11.10.20.12.31;	author millert;	state Exp;
branches;
next	1.37;

1.37
date	99.11.09.19.25.33;	author millert;	state Exp;
branches;
next	1.36;

1.36
date	99.09.16.19.06.06;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	99.02.03.03.58.05;	author d;	state Exp;
branches;
next	1.34;

1.34
date	99.02.01.07.58.30;	author d;	state Exp;
branches;
next	1.33;

1.33
date	98.11.20.11.18.50;	author d;	state Exp;
branches;
next	1.32;

1.32
date	98.08.06.16.26.32;	author millert;	state Exp;
branches;
next	1.31;

1.31
date	98.06.28.06.30.34;	author rahnds;	state Exp;
branches;
next	1.30;

1.30
date	98.01.02.05.32.49;	author deraadt;	state Exp;
branches;
next	1.29;

1.29
date	97.08.23.10.43.25;	author pefo;	state Exp;
branches;
next	1.28;

1.28
date	97.08.22.17.06.59;	author deraadt;	state Exp;
branches;
next	1.27;

1.27
date	97.07.02.16.26.27;	author millert;	state Exp;
branches;
next	1.26;

1.26
date	97.05.31.08.55.06;	author tholo;	state Exp;
branches;
next	1.25;

1.25
date	97.05.31.08.47.56;	author tholo;	state Exp;
branches;
next	1.24;

1.24
date	97.04.30.05.52.50;	author tholo;	state Exp;
branches;
next	1.23;

1.23
date	97.04.05.05.05.44;	author tholo;	state Exp;
branches;
next	1.22;

1.22
date	97.02.11.17.46.36;	author niklas;	state Exp;
branches;
next	1.21;

1.21
date	97.02.09.22.55.38;	author tholo;	state Exp;
branches;
next	1.20;

1.20
date	97.01.05.22.12.48;	author tholo;	state Exp;
branches;
next	1.19;

1.19
date	96.11.24.00.41.30;	author niklas;	state Exp;
branches;
next	1.18;

1.18
date	96.11.23.19.10.26;	author niklas;	state Exp;
branches;
next	1.17;

1.17
date	96.11.22.16.15.23;	author kstailey;	state Exp;
branches;
next	1.16;

1.16
date	96.09.26.15.22.19;	author tholo;	state Exp;
branches;
next	1.15;

1.15
date	96.09.26.04.49.56;	author tholo;	state Exp;
branches;
next	1.14;

1.14
date	96.09.26.04.19.42;	author tholo;	state Exp;
branches;
next	1.13;

1.13
date	96.09.19.20.38.48;	author tholo;	state Exp;
branches;
next	1.12;

1.12
date	96.09.16.05.43.40;	author tholo;	state Exp;
branches;
next	1.11;

1.11
date	96.09.15.09.31.49;	author tholo;	state Exp;
branches;
next	1.10;

1.10
date	96.09.11.03.04.43;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	96.09.06.16.14.36;	author tholo;	state Exp;
branches;
next	1.8;

1.8
date	96.08.21.03.47.22;	author tholo;	state Exp;
branches;
next	1.7;

1.7
date	96.08.20.17.56.52;	author tholo;	state Exp;
branches;
next	1.6;

1.6
date	96.08.20.17.30.49;	author downsj;	state Exp;
branches;
next	1.5;

1.5
date	96.08.19.08.33.37;	author tholo;	state Exp;
branches;
next	1.4;

1.4
date	96.08.02.18.08.09;	author tholo;	state Exp;
branches;
next	1.3;

1.3
date	96.03.25.22.16.37;	author tholo;	state Exp;
branches;
next	1.2;

1.2
date	96.01.29.02.00.21;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.42.18;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.42.18;	author deraadt;	state Exp;
branches;
next	;


desc
@@


1.224
log
@For small allocations (chunk) freezero only validates the given
size if canaries are enabled. In that case we have the exact requested
size of the allocation.  But we can at least check the given size
against the chunk size if C is not enabled. Plus add some braces
so my brain doesn't have to scan for dangling else problems when I
see this code.
@
text
@/*	$OpenBSD: malloc.c,v 1.223 2017/04/18 15:46:44 otto Exp $	*/
/*
 * Copyright (c) 2008, 2010, 2011, 2016 Otto Moerbeek <otto@@drijf.net>
 * Copyright (c) 2012 Matthew Dempsky <matthew@@openbsd.org>
 * Copyright (c) 2008 Damien Miller <djm@@openbsd.org>
 * Copyright (c) 2000 Poul-Henning Kamp <phk@@FreeBSD.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * If we meet some day, and you think this stuff is worth it, you
 * can buy me a beer in return. Poul-Henning Kamp
 */

/* #define MALLOC_STATS */

#include <sys/types.h>
#include <sys/param.h>	/* PAGE_SHIFT ALIGN */
#include <sys/queue.h>
#include <sys/mman.h>
#include <sys/uio.h>
#include <errno.h>
#include <stdarg.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <unistd.h>

#ifdef MALLOC_STATS
#include <sys/tree.h>
#include <fcntl.h>
#endif

#include "thread_private.h"
#include <tib.h>

#if defined(__mips64__)
#define MALLOC_PAGESHIFT	(14U)
#else
#define MALLOC_PAGESHIFT	(PAGE_SHIFT)
#endif

#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		(MALLOC_PAGESHIFT - 1)
#define MALLOC_PAGESIZE		(1UL << MALLOC_PAGESHIFT)
#define MALLOC_MINSIZE		(1UL << MALLOC_MINSHIFT)
#define MALLOC_PAGEMASK		(MALLOC_PAGESIZE - 1)
#define MASK_POINTER(p)		((void *)(((uintptr_t)(p)) & ~MALLOC_PAGEMASK))

#define MALLOC_MAXCHUNK		(1 << MALLOC_MAXSHIFT)
#define MALLOC_MAXCACHE		256
#define MALLOC_DELAYED_CHUNK_MASK	15
#define MALLOC_INITIAL_REGIONS	512
#define MALLOC_DEFAULT_CACHE	64
#define MALLOC_CHUNK_LISTS	4
#define CHUNK_CHECK_LENGTH	32

/*
 * We move allocations between half a page and a whole page towards the end,
 * subject to alignment constraints. This is the extra headroom we allow.
 * Set to zero to be the most strict.
 */
#define MALLOC_LEEWAY		0
#define MALLOC_MOVE_COND(sz)	((sz) - mopts.malloc_guard < 		\
				    MALLOC_PAGESIZE - MALLOC_LEEWAY)
#define MALLOC_MOVE(p, sz)  	(((char *)(p)) +			\
				    ((MALLOC_PAGESIZE - MALLOC_LEEWAY -	\
			    	    ((sz) - mopts.malloc_guard)) & 	\
				    ~(MALLOC_MINSIZE - 1)))

#define PAGEROUND(x)  (((x) + (MALLOC_PAGEMASK)) & ~MALLOC_PAGEMASK)

/*
 * What to use for Junk.  This is the byte value we use to fill with
 * when the 'J' option is enabled. Use SOME_JUNK right after alloc,
 * and SOME_FREEJUNK right before free.
 */
#define SOME_JUNK		0xdb	/* deadbeef */
#define SOME_FREEJUNK		0xdf	/* dead, free */

#define MMAP(sz)	mmap(NULL, (sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, 0)

#define MMAPA(a,sz)	mmap((a), (sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, 0)

#define MQUERY(a, sz)	mquery((a), (sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE | MAP_FIXED, -1, 0)

struct region_info {
	void *p;		/* page; low bits used to mark chunks */
	uintptr_t size;		/* size for pages, or chunk_info pointer */
#ifdef MALLOC_STATS
	void *f;		/* where allocated from */
#endif
};

LIST_HEAD(chunk_head, chunk_info);

struct dir_info {
	u_int32_t canary1;
	int active;			/* status of malloc */
	struct region_info *r;		/* region slots */
	size_t regions_total;		/* number of region slots */
	size_t regions_free;		/* number of free slots */
					/* lists of free chunk info structs */
	struct chunk_head chunk_info_list[MALLOC_MAXSHIFT + 1];
					/* lists of chunks with free slots */
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1][MALLOC_CHUNK_LISTS];
	size_t free_regions_size;	/* free pages cached */
					/* free pages cache */
	struct region_info free_regions[MALLOC_MAXCACHE];
					/* delayed free chunk slots */
	void *delayed_chunks[MALLOC_DELAYED_CHUNK_MASK + 1];
	size_t rbytesused;		/* random bytes used */
	char *func;			/* current function */
	int mutex;
	u_char rbytes[32];		/* random bytes */
	u_short chunk_start;
#ifdef MALLOC_STATS
	size_t inserts;
	size_t insert_collisions;
	size_t finds;
	size_t find_collisions;
	size_t deletes;
	size_t delete_moves;
	size_t cheap_realloc_tries;
	size_t cheap_reallocs;
	size_t malloc_used;		/* bytes allocated */
	size_t malloc_guarded;		/* bytes used for guards */
#define STATS_ADD(x,y)	((x) += (y))
#define STATS_SUB(x,y)	((x) -= (y))
#define STATS_INC(x)	((x)++)
#define STATS_ZERO(x)	((x) = 0)
#define STATS_SETF(x,y)	((x)->f = (y))
#else
#define STATS_ADD(x,y)	/* nothing */
#define STATS_SUB(x,y)	/* nothing */
#define STATS_INC(x)	/* nothing */
#define STATS_ZERO(x)	/* nothing */
#define STATS_SETF(x,y)	/* nothing */
#endif /* MALLOC_STATS */
	u_int32_t canary2;
};
#define DIR_INFO_RSZ	((sizeof(struct dir_info) + MALLOC_PAGEMASK) & \
			~MALLOC_PAGEMASK)

/*
 * This structure describes a page worth of chunks.
 *
 * How many bits per u_short in the bitmap
 */
#define MALLOC_BITS		(NBBY * sizeof(u_short))
struct chunk_info {
	LIST_ENTRY(chunk_info) entries;
	void *page;			/* pointer to the page */
	u_int32_t canary;
	u_short size;			/* size of this page's chunks */
	u_short shift;			/* how far to shift for this size */
	u_short free;			/* how many free chunks */
	u_short total;			/* how many chunks */
	u_short offset;			/* requested size table offset */
					/* which chunks are free */
	u_short bits[1];
};

struct malloc_readonly {
	struct dir_info *malloc_pool[_MALLOC_MUTEXES];	/* Main bookkeeping information */
	int	malloc_mt;		/* multi-threaded mode? */
	int	malloc_freenow;		/* Free quickly - disable chunk rnd */
	int	malloc_freeunmap;	/* mprotect free pages PROT_NONE? */
	int	malloc_junk;		/* junk fill? */
	int	malloc_realloc;		/* always realloc? */
	int	malloc_xmalloc;		/* xmalloc behaviour? */
	int	chunk_canaries;		/* use canaries after chunks? */
	int	internal_funcs;		/* use better recallocarray/freezero? */
	u_int	malloc_cache;		/* free pages we cache */
	size_t	malloc_guard;		/* use guard pages after allocations? */
#ifdef MALLOC_STATS
	int	malloc_stats;		/* dump statistics at end */
#endif
	u_int32_t malloc_canary;	/* Matched against ones in malloc_pool */
};

/* This object is mapped PROT_READ after initialisation to prevent tampering */
static union {
	struct malloc_readonly mopts;
	u_char _pad[MALLOC_PAGESIZE];
} malloc_readonly __attribute__((aligned(MALLOC_PAGESIZE)));
#define mopts	malloc_readonly.mopts

char		*malloc_options;	/* compile-time options */

static u_char getrbyte(struct dir_info *d);
static __dead void wrterror(struct dir_info *d, char *msg, ...)
    __attribute__((__format__ (printf, 2, 3)));
static void fill_canary(char *ptr, size_t sz, size_t allocated);

#ifdef MALLOC_STATS
void malloc_dump(int, int, struct dir_info *);
PROTO_NORMAL(malloc_dump);
void malloc_gdump(int);
PROTO_NORMAL(malloc_gdump);
static void malloc_exit(void);
#define CALLER	__builtin_return_address(0)
#else
#define CALLER	NULL
#endif

/* low bits of r->p determine size: 0 means >= page size and r->size holding
 * real size, otherwise low bits are a shift count, or 1 for malloc(0)
 */
#define REALSIZE(sz, r)						\
	(sz) = (uintptr_t)(r)->p & MALLOC_PAGEMASK,		\
	(sz) = ((sz) == 0 ? (r)->size : ((sz) == 1 ? 0 : (1 << ((sz)-1))))

static inline void
_MALLOC_LEAVE(struct dir_info *d)
{
	if (mopts.malloc_mt) {
		d->active--;
		_MALLOC_UNLOCK(d->mutex);
	}
}

static inline void
_MALLOC_ENTER(struct dir_info *d)
{
	if (mopts.malloc_mt) {
		_MALLOC_LOCK(d->mutex);
		d->active++;
	}
}

static inline size_t
hash(void *p)
{
	size_t sum;
	uintptr_t u;

	u = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u;
	sum = (sum << 7) - sum + (u >> 16);
#ifdef __LP64__
	sum = (sum << 7) - sum + (u >> 32);
	sum = (sum << 7) - sum + (u >> 48);
#endif
	return sum;
}

static inline
struct dir_info *getpool(void)
{
	if (!mopts.malloc_mt)
		return mopts.malloc_pool[0];
	else
		return mopts.malloc_pool[TIB_GET()->tib_tid &
		    (_MALLOC_MUTEXES - 1)];
}

static __dead void
wrterror(struct dir_info *d, char *msg, ...)
{
	struct iovec	iov[3];
	char		pidbuf[80];
	char		buf[80];
	int		saved_errno = errno;
	va_list		ap;

	iov[0].iov_base = pidbuf;
	snprintf(pidbuf, sizeof(pidbuf), "%s(%d) in %s(): ", __progname,
	    getpid(), (d != NULL && d->func) ? d->func : "unknown");
	iov[0].iov_len = strlen(pidbuf);
	iov[1].iov_base = buf;
	va_start(ap, msg);
	vsnprintf(buf, sizeof(buf), msg, ap);
	va_end(ap);
	iov[1].iov_len = strlen(buf);
	iov[2].iov_base = "\n";
	iov[2].iov_len = 1;
	writev(STDERR_FILENO, iov, 3);

#ifdef MALLOC_STATS
	if (mopts.malloc_stats) {
		int i;

		for (i = 0; i < _MALLOC_MUTEXES; i++)
			malloc_dump(STDERR_FILENO, i, mopts.malloc_pool[i]);
	}
#endif /* MALLOC_STATS */

	errno = saved_errno;

	abort();
}

static void
rbytes_init(struct dir_info *d)
{
	arc4random_buf(d->rbytes, sizeof(d->rbytes));
	/* add 1 to account for using d->rbytes[0] */
	d->rbytesused = 1 + d->rbytes[0] % (sizeof(d->rbytes) / 2);
}

static inline u_char
getrbyte(struct dir_info *d)
{
	u_char x;

	if (d->rbytesused >= sizeof(d->rbytes))
		rbytes_init(d);
	x = d->rbytes[d->rbytesused++];
	return x;
}

/*
 * Cache maintenance. We keep at most malloc_cache pages cached.
 * If the cache is becoming full, unmap pages in the cache for real,
 * and then add the region to the cache
 * Opposed to the regular region data structure, the sizes in the
 * cache are in MALLOC_PAGESIZE units.
 */
static void
unmap(struct dir_info *d, void *p, size_t sz, int clear)
{
	size_t psz = sz >> MALLOC_PAGESHIFT;
	size_t rsz, tounmap;
	struct region_info *r;
	u_int i, offset;

	if (sz != PAGEROUND(sz))
		wrterror(d, "munmap round");

	rsz = mopts.malloc_cache - d->free_regions_size;

	/*
	 * normally the cache holds recently freed regions, but if the region
	 * to unmap is larger than the cache size or we're clearing and the
	 * cache is full, just munmap
	 */
	if (psz > mopts.malloc_cache || (clear && rsz == 0)) {
		i = munmap(p, sz);
		if (i)
			wrterror(d, "munmap %p", p);
		STATS_SUB(d->malloc_used, sz);
		return;
	}
	tounmap = 0;
	if (psz > rsz)
		tounmap = psz - rsz;
	offset = getrbyte(d);
	for (i = 0; tounmap > 0 && i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror(d, "munmap %p", r->p);
			r->p = NULL;
			if (tounmap > r->size)
				tounmap -= r->size;
			else
				tounmap = 0;
			d->free_regions_size -= r->size;
			r->size = 0;
			STATS_SUB(d->malloc_used, rsz);
		}
	}
	if (tounmap > 0)
		wrterror(d, "malloc cache underflow");
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p == NULL) {
			if (clear)
				memset(p, 0, sz - mopts.malloc_guard);
			if (mopts.malloc_junk && !mopts.malloc_freeunmap) {
				size_t amt = mopts.malloc_junk == 1 ?
				    MALLOC_MAXCHUNK : sz;
				memset(p, SOME_FREEJUNK, amt);
			}
			if (mopts.malloc_freeunmap)
				mprotect(p, sz, PROT_NONE);
			r->p = p;
			r->size = psz;
			d->free_regions_size += psz;
			break;
		}
	}
	if (i == mopts.malloc_cache)
		wrterror(d, "malloc free slot lost");
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror(d, "malloc cache overflow");
}

static void
zapcacheregion(struct dir_info *d, void *p, size_t len)
{
	u_int i;
	struct region_info *r;
	size_t rsz;

	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[i];
		if (r->p >= p && r->p <= (void *)((char *)p + len)) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror(d, "munmap %p", r->p);
			r->p = NULL;
			d->free_regions_size -= r->size;
			r->size = 0;
			STATS_SUB(d->malloc_used, rsz);
		}
	}
}

static void *
map(struct dir_info *d, void *hint, size_t sz, int zero_fill)
{
	size_t psz = sz >> MALLOC_PAGESHIFT;
	struct region_info *r, *big = NULL;
	u_int i, offset;
	void *p;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror(d, "internal struct corrupt");
	if (sz != PAGEROUND(sz))
		wrterror(d, "map round");

	if (!hint && psz > d->free_regions_size) {
		_MALLOC_LEAVE(d);
		p = MMAP(sz);
		_MALLOC_ENTER(d);
		if (p != MAP_FAILED)
			STATS_ADD(d->malloc_used, sz);
		/* zero fill not needed */
		return p;
	}
	offset = getrbyte(d);
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			if (hint && r->p != hint)
				continue;
			if (r->size == psz) {
				p = r->p;
				r->p = NULL;
				r->size = 0;
				d->free_regions_size -= psz;
				if (mopts.malloc_freeunmap)
					mprotect(p, sz, PROT_READ | PROT_WRITE);
				if (zero_fill)
					memset(p, 0, sz);
				else if (mopts.malloc_junk == 2 &&
				    mopts.malloc_freeunmap)
					memset(p, SOME_FREEJUNK, sz);
				return p;
			} else if (r->size > psz)
				big = r;
		}
	}
	if (big != NULL) {
		r = big;
		p = r->p;
		r->p = (char *)r->p + (psz << MALLOC_PAGESHIFT);
		if (mopts.malloc_freeunmap)
			mprotect(p, sz, PROT_READ | PROT_WRITE);
		r->size -= psz;
		d->free_regions_size -= psz;
		if (zero_fill)
			memset(p, 0, sz);
		else if (mopts.malloc_junk == 2 && mopts.malloc_freeunmap)
			memset(p, SOME_FREEJUNK, sz);
		return p;
	}
	if (hint)
		return MAP_FAILED;
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror(d, "malloc cache");
	_MALLOC_LEAVE(d);
	p = MMAP(sz);
	_MALLOC_ENTER(d);
	if (p != MAP_FAILED)
		STATS_ADD(d->malloc_used, sz);
	/* zero fill not needed */
	return p;
}

static void
omalloc_parseopt(char opt)
{
	switch (opt) {
	case '>':
		mopts.malloc_cache <<= 1;
		if (mopts.malloc_cache > MALLOC_MAXCACHE)
			mopts.malloc_cache = MALLOC_MAXCACHE;
		break;
	case '<':
		mopts.malloc_cache >>= 1;
		break;
	case 'c':
		mopts.chunk_canaries = 0;
		break;
	case 'C':
		mopts.chunk_canaries = 1;
		break;
#ifdef MALLOC_STATS
	case 'd':
		mopts.malloc_stats = 0;
		break;
	case 'D':
		mopts.malloc_stats = 1;
		break;
#endif /* MALLOC_STATS */
	case 'f':
		mopts.malloc_freenow = 0;
		mopts.malloc_freeunmap = 0;
		break;
	case 'F':
		mopts.malloc_freenow = 1;
		mopts.malloc_freeunmap = 1;
		break;
	case 'g':
		mopts.malloc_guard = 0;
		break;
	case 'G':
		mopts.malloc_guard = MALLOC_PAGESIZE;
		break;
	case 'j':
		if (mopts.malloc_junk > 0)
			mopts.malloc_junk--;
		break;
	case 'J':
		if (mopts.malloc_junk < 2)
			mopts.malloc_junk++;
		break;
	case 'r':
		mopts.malloc_realloc = 0;
		break;
	case 'R':
		mopts.malloc_realloc = 1;
		break;
	case 'u':
		mopts.malloc_freeunmap = 0;
		break;
	case 'U':
		mopts.malloc_freeunmap = 1;
		break;
	case 'x':
		mopts.malloc_xmalloc = 0;
		break;
	case 'X':
		mopts.malloc_xmalloc = 1;
		break;
	default: {
		static const char q[] = "malloc() warning: "
		    "unknown char in MALLOC_OPTIONS\n";
		write(STDERR_FILENO, q, sizeof(q) - 1);
		break;
	}
	}
}

static void
omalloc_init(void)
{
	char *p, *q, b[64];
	int i, j;

	/*
	 * Default options
	 */
	mopts.malloc_junk = 1;
	mopts.malloc_cache = MALLOC_DEFAULT_CACHE;

	for (i = 0; i < 3; i++) {
		switch (i) {
		case 0:
			j = readlink("/etc/malloc.conf", b, sizeof b - 1);
			if (j <= 0)
				continue;
			b[j] = '\0';
			p = b;
			break;
		case 1:
			if (issetugid() == 0)
				p = getenv("MALLOC_OPTIONS");
			else
				continue;
			break;
		case 2:
			p = malloc_options;
			break;
		default:
			p = NULL;
		}

		for (; p != NULL && *p != '\0'; p++) {
			switch (*p) {
			case 'S':
				for (q = "CGJ"; *q != '\0'; q++)
					omalloc_parseopt(*q);
				mopts.malloc_cache = 0;
				break;
			case 's':
				for (q = "cgj"; *q != '\0'; q++)
					omalloc_parseopt(*q);
				mopts.malloc_cache = MALLOC_DEFAULT_CACHE;
				break;
			default:
				omalloc_parseopt(*p);
				break;
			}
		}
	}

#ifdef MALLOC_STATS
	if (mopts.malloc_stats && (atexit(malloc_exit) == -1)) {
		static const char q[] = "malloc() warning: atexit(2) failed."
		    " Will not be able to dump stats on exit\n";
		write(STDERR_FILENO, q, sizeof(q) - 1);
	}
#endif /* MALLOC_STATS */

	while ((mopts.malloc_canary = arc4random()) == 0)
		;
}

/*
 * Initialize a dir_info, which should have been cleared by caller
 */
static void
omalloc_poolinit(struct dir_info **dp)
{
	void *p;
	size_t d_avail, regioninfo_size;
	struct dir_info *d;
	int i, j;

	/*
	 * Allocate dir_info with a guard page on either side. Also
	 * randomise offset inside the page at which the dir_info
	 * lies (subject to alignment by 1 << MALLOC_MINSHIFT)
	 */
	if ((p = MMAP(DIR_INFO_RSZ + (MALLOC_PAGESIZE * 2))) == MAP_FAILED)
		wrterror(NULL, "malloc init mmap failed");
	mprotect(p, MALLOC_PAGESIZE, PROT_NONE);
	mprotect((char *)p + MALLOC_PAGESIZE + DIR_INFO_RSZ,
	    MALLOC_PAGESIZE, PROT_NONE);
	d_avail = (DIR_INFO_RSZ - sizeof(*d)) >> MALLOC_MINSHIFT;
	d = (struct dir_info *)((char *)p + MALLOC_PAGESIZE +
	    (arc4random_uniform(d_avail) << MALLOC_MINSHIFT));

	rbytes_init(d);
	d->regions_free = d->regions_total = MALLOC_INITIAL_REGIONS;
	regioninfo_size = d->regions_total * sizeof(struct region_info);
	d->r = MMAP(regioninfo_size);
	if (d->r == MAP_FAILED) {
		d->regions_total = 0;
		wrterror(NULL, "malloc init mmap failed");
	}
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		LIST_INIT(&d->chunk_info_list[i]);
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++)
			LIST_INIT(&d->chunk_dir[i][j]);
	}
	STATS_ADD(d->malloc_used, regioninfo_size);
	d->canary1 = mopts.malloc_canary ^ (u_int32_t)(uintptr_t)d;
	d->canary2 = ~d->canary1;

	*dp = d;
}

static int
omalloc_grow(struct dir_info *d)
{
	size_t newtotal;
	size_t newsize;
	size_t mask;
	size_t i;
	struct region_info *p;

	if (d->regions_total > SIZE_MAX / sizeof(struct region_info) / 2 )
		return 1;

	newtotal = d->regions_total * 2;
	newsize = newtotal * sizeof(struct region_info);
	mask = newtotal - 1;

	p = MMAP(newsize);
	if (p == MAP_FAILED)
		return 1;

	STATS_ADD(d->malloc_used, newsize);
	STATS_ZERO(d->inserts);
	STATS_ZERO(d->insert_collisions);
	for (i = 0; i < d->regions_total; i++) {
		void *q = d->r[i].p;
		if (q != NULL) {
			size_t index = hash(q) & mask;
			STATS_INC(d->inserts);
			while (p[index].p != NULL) {
				index = (index - 1) & mask;
				STATS_INC(d->insert_collisions);
			}
			p[index] = d->r[i];
		}
	}
	/* avoid pages containing meta info to end up in cache */
	if (munmap(d->r, d->regions_total * sizeof(struct region_info)))
		wrterror(d, "munmap %p", (void *)d->r);
	else
		STATS_SUB(d->malloc_used,
		    d->regions_total * sizeof(struct region_info));
	d->regions_free = d->regions_free + d->regions_total;
	d->regions_total = newtotal;
	d->r = p;
	return 0;
}

static struct chunk_info *
alloc_chunk_info(struct dir_info *d, int bits)
{
	struct chunk_info *p;
	size_t size, count;

	if (bits == 0)
		count = MALLOC_PAGESIZE / MALLOC_MINSIZE;
	else
		count = MALLOC_PAGESIZE >> bits;

	size = howmany(count, MALLOC_BITS);
	size = sizeof(struct chunk_info) + (size - 1) * sizeof(u_short);
	if (mopts.chunk_canaries)
		size += count * sizeof(u_short);
	size = ALIGN(size);

	if (LIST_EMPTY(&d->chunk_info_list[bits])) {
		char *q;
		int i;

		q = MMAP(MALLOC_PAGESIZE);
		if (q == MAP_FAILED)
			return NULL;
		STATS_ADD(d->malloc_used, MALLOC_PAGESIZE);
		count = MALLOC_PAGESIZE / size;
		for (i = 0; i < count; i++, q += size)
			LIST_INSERT_HEAD(&d->chunk_info_list[bits],
			    (struct chunk_info *)q, entries);
	}
	p = LIST_FIRST(&d->chunk_info_list[bits]);
	LIST_REMOVE(p, entries);
	memset(p, 0, size);
	p->canary = d->canary1;
	return p;
}


/*
 * The hashtable uses the assumption that p is never NULL. This holds since
 * non-MAP_FIXED mappings with hint 0 start at BRKSIZ.
 */
static int
insert(struct dir_info *d, void *p, size_t sz, void *f)
{
	size_t index;
	size_t mask;
	void *q;

	if (d->regions_free * 4 < d->regions_total) {
		if (omalloc_grow(d))
			return 1;
	}
	mask = d->regions_total - 1;
	index = hash(p) & mask;
	q = d->r[index].p;
	STATS_INC(d->inserts);
	while (q != NULL) {
		index = (index - 1) & mask;
		q = d->r[index].p;
		STATS_INC(d->insert_collisions);
	}
	d->r[index].p = p;
	d->r[index].size = sz;
#ifdef MALLOC_STATS
	d->r[index].f = f;
#endif
	d->regions_free--;
	return 0;
}

static struct region_info *
find(struct dir_info *d, void *p)
{
	size_t index;
	size_t mask = d->regions_total - 1;
	void *q, *r;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror(d, "internal struct corrupt");
	p = MASK_POINTER(p);
	index = hash(p) & mask;
	r = d->r[index].p;
	q = MASK_POINTER(r);
	STATS_INC(d->finds);
	while (q != p && r != NULL) {
		index = (index - 1) & mask;
		r = d->r[index].p;
		q = MASK_POINTER(r);
		STATS_INC(d->find_collisions);
	}
	return (q == p && r != NULL) ? &d->r[index] : NULL;
}

static void
delete(struct dir_info *d, struct region_info *ri)
{
	/* algorithm R, Knuth Vol III section 6.4 */
	size_t mask = d->regions_total - 1;
	size_t i, j, r;

	if (d->regions_total & (d->regions_total - 1))
		wrterror(d, "regions_total not 2^x");
	d->regions_free++;
	STATS_INC(d->deletes);

	i = ri - d->r;
	for (;;) {
		d->r[i].p = NULL;
		d->r[i].size = 0;
		j = i;
		for (;;) {
			i = (i - 1) & mask;
			if (d->r[i].p == NULL)
				return;
			r = hash(d->r[i].p) & mask;
			if ((i <= r && r < j) || (r < j && j < i) ||
			    (j < i && i <= r))
				continue;
			d->r[j] = d->r[i];
			STATS_INC(d->delete_moves);
			break;
		}

	}
}

/*
 * Allocate a page of chunks
 */
static struct chunk_info *
omalloc_make_chunks(struct dir_info *d, int bits, int listnum)
{
	struct chunk_info *bp;
	void		*pp;
	int		i, k;

	/* Allocate a new bucket */
	pp = map(d, NULL, MALLOC_PAGESIZE, 0);
	if (pp == MAP_FAILED)
		return NULL;

	bp = alloc_chunk_info(d, bits);
	if (bp == NULL) {
		unmap(d, pp, MALLOC_PAGESIZE, 0);
		return NULL;
	}

	/* memory protect the page allocated in the malloc(0) case */
	if (bits == 0) {
		bp->size = 0;
		bp->shift = 1;
		i = MALLOC_MINSIZE - 1;
		while (i >>= 1)
			bp->shift++;
		bp->total = bp->free = MALLOC_PAGESIZE >> bp->shift;
		bp->page = pp;

		k = mprotect(pp, MALLOC_PAGESIZE, PROT_NONE);
		if (k < 0) {
			unmap(d, pp, MALLOC_PAGESIZE, 0);
			LIST_INSERT_HEAD(&d->chunk_info_list[0], bp, entries);
			return NULL;
		}
	} else {
		bp->size = 1U << bits;
		bp->shift = bits;
		bp->total = bp->free = MALLOC_PAGESIZE >> bits;
		bp->offset = howmany(bp->total, MALLOC_BITS);
		bp->page = pp;
	}

	/* set all valid bits in the bitmap */
	k = bp->total;
	i = 0;

	/* Do a bunch at a time */
	for (; (k - i) >= MALLOC_BITS; i += MALLOC_BITS)
		bp->bits[i / MALLOC_BITS] = (u_short)~0U;

	for (; i < k; i++)
		bp->bits[i / MALLOC_BITS] |= (u_short)1U << (i % MALLOC_BITS);

	LIST_INSERT_HEAD(&d->chunk_dir[bits][listnum], bp, entries);

	bits++;
	if ((uintptr_t)pp & bits)
		wrterror(d, "pp & bits %p", pp);

	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp, NULL);
	return bp;
}

static int
find_chunksize(size_t size)
{
	int		i, j;

	/* Don't bother with anything less than this */
	/* unless we have a malloc(0) requests */
	if (size != 0 && size < MALLOC_MINSIZE)
		size = MALLOC_MINSIZE;

	/* Find the right bucket */
	if (size == 0)
		j = 0;
	else {
		j = MALLOC_MINSHIFT;
		i = (size - 1) >> (MALLOC_MINSHIFT - 1);
		while (i >>= 1)
			j++;
	}
	return j;
}

/*
 * Allocate a chunk
 */
static void *
malloc_bytes(struct dir_info *d, size_t size, void *f)
{
	int		i, j, listnum;
	size_t		k;
	u_short		u, *lp;
	struct chunk_info *bp;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror(d, "internal struct corrupt");

	j = find_chunksize(size);

	listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
	/* If it's empty, make a page more of that size chunks */
	if ((bp = LIST_FIRST(&d->chunk_dir[j][listnum])) == NULL) {
		bp = omalloc_make_chunks(d, j, listnum);
		if (bp == NULL)
			return NULL;
	}

	if (bp->canary != d->canary1)
		wrterror(d, "chunk info corrupted");

	i = d->chunk_start;
	if (bp->free > 1)
		i += getrbyte(d);
	if (i >= bp->total)
		i &= bp->total - 1;
	for (;;) {
		for (;;) {
			lp = &bp->bits[i / MALLOC_BITS];
			if (!*lp) {
				i += MALLOC_BITS;
				i &= ~(MALLOC_BITS - 1);
				if (i >= bp->total)
					i = 0;
			} else
				break;
		}
		k = i % MALLOC_BITS;
		u = 1 << k;
		if (*lp & u)
			break;
		if (++i >= bp->total)
			i = 0;
	}
	d->chunk_start += i + 1;
#ifdef MALLOC_STATS
	if (i == 0) {
		struct region_info *r = find(d, bp->page);
		r->f = f;
	}
#endif

	*lp ^= u;

	/* If there are no more free, remove from free-list */
	if (!--bp->free)
		LIST_REMOVE(bp, entries);

	/* Adjust to the real offset of that chunk */
	k += (lp - bp->bits) * MALLOC_BITS;

	if (mopts.chunk_canaries)
		bp->bits[bp->offset + k] = size;

	k <<= bp->shift;

	if (bp->size > 0) {
		if (mopts.malloc_junk == 2)
			memset((char *)bp->page + k, SOME_JUNK, bp->size);
		else if (mopts.chunk_canaries)
			fill_canary((char *)bp->page + k, size, bp->size);
	}
	return ((char *)bp->page + k);
}

static void
fill_canary(char *ptr, size_t sz, size_t allocated)
{
	size_t check_sz = allocated - sz;

	if (check_sz > CHUNK_CHECK_LENGTH)
		check_sz = CHUNK_CHECK_LENGTH;
	memset(ptr + sz, SOME_JUNK, check_sz);
}

static void
validate_canary(struct dir_info *d, u_char *ptr, size_t sz, size_t allocated)
{
	size_t check_sz = allocated - sz;
	u_char *p, *q;

	if (check_sz > CHUNK_CHECK_LENGTH)
		check_sz = CHUNK_CHECK_LENGTH;
	p = ptr + sz;
	q = p + check_sz;

	while (p < q) {
		if (*p++ != SOME_JUNK) {
			wrterror(d, "chunk canary corrupted %p %#tx@@%#zx",
			    ptr, p - ptr - 1, sz);
		}
	}
}

static uint32_t
find_chunknum(struct dir_info *d, struct region_info *r, void *ptr, int check)
{
	struct chunk_info *info;
	uint32_t chunknum;

	info = (struct chunk_info *)r->size;
	if (info->canary != d->canary1)
		wrterror(d, "chunk info corrupted");

	/* Find the chunk number on the page */
	chunknum = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;
	if (check && info->size > 0) {
		validate_canary(d, ptr, info->bits[info->offset + chunknum],
		    info->size);
	}

	if ((uintptr_t)ptr & ((1U << (info->shift)) - 1))
		wrterror(d, "modified chunk-pointer %p", ptr);
	if (info->bits[chunknum / MALLOC_BITS] &
	    (1U << (chunknum % MALLOC_BITS)))
		wrterror(d, "chunk is already free %p", ptr);
	return chunknum;
}

/*
 * Free a chunk, and possibly the page it's on, if the page becomes empty.
 */
static void
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
{
	struct chunk_head *mp;
	struct chunk_info *info;
	uint32_t chunknum;
	int listnum;

	info = (struct chunk_info *)r->size;
	chunknum = find_chunknum(d, r, ptr, 0);

	info->bits[chunknum / MALLOC_BITS] |= 1U << (chunknum % MALLOC_BITS);
	info->free++;

	if (info->free == 1) {
		/* Page became non-full */
		listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
		if (info->size != 0)
			mp = &d->chunk_dir[info->shift][listnum];
		else
			mp = &d->chunk_dir[0][listnum];

		LIST_INSERT_HEAD(mp, info, entries);
		return;
	}

	if (info->free != info->total)
		return;

	LIST_REMOVE(info, entries);

	if (info->size == 0 && !mopts.malloc_freeunmap)
		mprotect(info->page, MALLOC_PAGESIZE, PROT_READ | PROT_WRITE);
	unmap(d, info->page, MALLOC_PAGESIZE, 0);

	delete(d, r);
	if (info->size != 0)
		mp = &d->chunk_info_list[info->shift];
	else
		mp = &d->chunk_info_list[0];
	LIST_INSERT_HEAD(mp, info, entries);
}



static void *
omalloc(struct dir_info *pool, size_t sz, int zero_fill, void *f)
{
	void *p;
	size_t psz;

	if (sz > MALLOC_MAXCHUNK) {
		if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
			errno = ENOMEM;
			return NULL;
		}
		sz += mopts.malloc_guard;
		psz = PAGEROUND(sz);
		p = map(pool, NULL, psz, zero_fill);
		if (p == MAP_FAILED) {
			errno = ENOMEM;
			return NULL;
		}
		if (insert(pool, p, sz, f)) {
			unmap(pool, p, psz, 0);
			errno = ENOMEM;
			return NULL;
		}
		if (mopts.malloc_guard) {
			if (mprotect((char *)p + psz - mopts.malloc_guard,
			    mopts.malloc_guard, PROT_NONE))
				wrterror(pool, "mprotect");
			STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
		}

		if (MALLOC_MOVE_COND(sz)) {
			/* fill whole allocation */
			if (mopts.malloc_junk == 2)
				memset(p, SOME_JUNK, psz - mopts.malloc_guard);
			/* shift towards the end */
			p = MALLOC_MOVE(p, sz);
			/* fill zeros if needed and overwritten above */
			if (zero_fill && mopts.malloc_junk == 2)
				memset(p, 0, sz - mopts.malloc_guard);
		} else {
			if (mopts.malloc_junk == 2) {
				if (zero_fill)
					memset((char *)p + sz - mopts.malloc_guard,
					    SOME_JUNK, psz - sz);
				else
					memset(p, SOME_JUNK,
					    psz - mopts.malloc_guard);
			}
			else if (mopts.chunk_canaries)
				fill_canary(p, sz - mopts.malloc_guard,
				    psz - mopts.malloc_guard);
		}

	} else {
		/* takes care of SOME_JUNK */
		p = malloc_bytes(pool, sz, f);
		if (zero_fill && p != NULL && sz > 0)
			memset(p, 0, sz);
	}

	return p;
}

/*
 * Common function for handling recursion.  Only
 * print the error message once, to avoid making the problem
 * potentially worse.
 */
static void
malloc_recurse(struct dir_info *d)
{
	static int noprint;

	if (noprint == 0) {
		noprint = 1;
		wrterror(d, "recursive call");
	}
	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	errno = EDEADLK;
}

void
_malloc_init(int from_rthreads)
{
	int i, max;
	struct dir_info *d;

	_MALLOC_LOCK(0);
	if (!from_rthreads && mopts.malloc_pool[0]) {
		_MALLOC_UNLOCK(0);
		return;
	}
	if (!mopts.malloc_canary)
		omalloc_init();

	max = from_rthreads ? _MALLOC_MUTEXES : 1;
	if (((uintptr_t)&malloc_readonly & MALLOC_PAGEMASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly),
		     PROT_READ | PROT_WRITE);
	for (i = 0; i < max; i++) {
		if (mopts.malloc_pool[i])
			continue;
		omalloc_poolinit(&d);
		d->mutex = i;
		mopts.malloc_pool[i] = d;
	}

	if (from_rthreads)
		mopts.malloc_mt = 1;
	else
		mopts.internal_funcs = 1;

	/*
	 * Options have been set and will never be reset.
	 * Prevent further tampering with them.
	 */
	if (((uintptr_t)&malloc_readonly & MALLOC_PAGEMASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly), PROT_READ);
	_MALLOC_UNLOCK(0);
}
DEF_STRONG(_malloc_init);

void *
malloc(size_t size)
{
	void *r;
	struct dir_info *d;
	int saved_errno = errno;

	d = getpool();
	if (d == NULL) {
		_malloc_init(0);
		d = getpool();
	}
	_MALLOC_LOCK(d->mutex);
	d->func = "malloc";

	if (d->active++) {
		malloc_recurse(d);
		return NULL;
	}
	r = omalloc(d, size, 0, CALLER);
	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	if (r == NULL && mopts.malloc_xmalloc)
		wrterror(d, "out of memory");
	if (r != NULL)
		errno = saved_errno;
	return r;
}
/*DEF_STRONG(malloc);*/

static void
validate_junk(struct dir_info *pool, void *p)
{
	struct region_info *r;
	size_t byte, sz;

	if (p == NULL)
		return;
	r = find(pool, p);
	if (r == NULL)
		wrterror(pool, "bogus pointer in validate_junk %p", p);
	REALSIZE(sz, r);
	if (sz > CHUNK_CHECK_LENGTH)
		sz = CHUNK_CHECK_LENGTH;
	for (byte = 0; byte < sz; byte++) {
		if (((unsigned char *)p)[byte] != SOME_FREEJUNK)
			wrterror(pool, "use after free %p", p);
	}
}

static void
ofree(struct dir_info *argpool, void *p, int clear, int check, size_t argsz)
{
	struct dir_info *pool;
	struct region_info *r;
	size_t sz;
	int i;

	pool = argpool;
	r = find(pool, p);
	if (r == NULL) {
		if (mopts.malloc_mt)  {
			for (i = 0; i < _MALLOC_MUTEXES; i++) {
				if (i == argpool->mutex)
					continue;
				pool->active--;
				_MALLOC_UNLOCK(pool->mutex);
				pool = mopts.malloc_pool[i];
				_MALLOC_LOCK(pool->mutex);
				pool->active++;
				r = find(pool, p);
				if (r != NULL)
					break;
			}
		}
		if (r == NULL)
			wrterror(pool, "bogus pointer (double free?) %p", p);
	}

	REALSIZE(sz, r);
	if (check) {
		if (sz <= MALLOC_MAXCHUNK) {
			if (mopts.chunk_canaries && sz > 0) {
				struct chunk_info *info =
				    (struct chunk_info *)r->size;
				uint32_t chunknum =
				    find_chunknum(pool, r, p, 0);

				if (info->bits[info->offset + chunknum] <
				    argsz)
					wrterror(pool, "recorded size %hu"
					    " < %zu",
					    info->bits[info->offset + chunknum],
					    argsz);
			} else {
				if (sz < argsz)
					wrterror(pool, "chunk size %zu < %zu",
					    sz, argsz);
			}
		} else if (sz - mopts.malloc_guard < argsz) {
			wrterror(pool, "recorded size %zu < %zu",
			    sz - mopts.malloc_guard, argsz);
		}
	}
	if (sz > MALLOC_MAXCHUNK) {
		if (!MALLOC_MOVE_COND(sz)) {
			if (r->p != p)
				wrterror(pool, "bogus pointer %p", p);
			if (mopts.chunk_canaries)
				validate_canary(pool, p,
				    sz - mopts.malloc_guard,
				    PAGEROUND(sz - mopts.malloc_guard));
		} else {
			/* shifted towards the end */
			if (p != MALLOC_MOVE(r->p, sz))
				wrterror(pool, "bogus moved pointer %p", p);
			p = r->p;
		}
		if (mopts.malloc_guard) {
			if (sz < mopts.malloc_guard)
				wrterror(pool, "guard size");
			if (!mopts.malloc_freeunmap) {
				if (mprotect((char *)p + PAGEROUND(sz) -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror(pool, "mprotect");
			}
			STATS_SUB(pool->malloc_guarded, mopts.malloc_guard);
		}
		unmap(pool, p, PAGEROUND(sz), clear);
		delete(pool, r);
	} else {
		void *tmp;
		int i;

		/* Delayed free or canaries? Extra check */
		if (!mopts.malloc_freenow || mopts.chunk_canaries)
			find_chunknum(pool, r, p, mopts.chunk_canaries);
		if (!clear && !mopts.malloc_freenow) {
			if (mopts.malloc_junk && sz > 0)
				memset(p, SOME_FREEJUNK, sz);
			i = getrbyte(pool) & MALLOC_DELAYED_CHUNK_MASK;
			tmp = p;
			p = pool->delayed_chunks[i];
			if (tmp == p)
				wrterror(pool, "double free %p", tmp);
			if (mopts.malloc_junk)
				validate_junk(pool, p);
			pool->delayed_chunks[i] = tmp;
		} else {
			if ((clear || mopts.malloc_junk) && sz > 0)
				memset(p, clear ? 0 : SOME_FREEJUNK, sz);
		}
		if (p != NULL) {
			r = find(pool, p);
			if (r == NULL)
				wrterror(pool,
				    "bogus pointer (double free?) %p", p);
			free_bytes(pool, r, p);
		}
	}

	if (argpool != pool) {
		pool->active--;
		_MALLOC_UNLOCK(pool->mutex);
		_MALLOC_LOCK(argpool->mutex);
		argpool->active++;
	}
}

void
free(void *ptr)
{
	struct dir_info *d;
	int saved_errno = errno;

	/* This is legal. */
	if (ptr == NULL)
		return;

	d = getpool();
	if (d == NULL)
		wrterror(d, "free() called before allocation");
	_MALLOC_LOCK(d->mutex);
	d->func = "free";
	if (d->active++) {
		malloc_recurse(d);
		return;
	}
	ofree(d, ptr, 0, 0, 0);
	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	errno = saved_errno;
}
/*DEF_STRONG(free);*/

static void
freezero_p(void *ptr, size_t sz)
{
	explicit_bzero(ptr, sz);
	free(ptr);
}

void
freezero(void *ptr, size_t sz)
{
	struct dir_info *d;
	int saved_errno = errno;

	/* This is legal. */
	if (ptr == NULL)
		return;

	if (!mopts.internal_funcs)
		return freezero_p(ptr, sz);

	d = getpool();
	if (d == NULL)
		wrterror(d, "freezero() called before allocation");
	_MALLOC_LOCK(d->mutex);
	d->func = "freezero";
	if (d->active++) {
		malloc_recurse(d);
		return;
	}
	ofree(d, ptr, 1, 1, sz);
	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	errno = saved_errno;
}
DEF_WEAK(freezero);

static void *
orealloc(struct dir_info *argpool, void *p, size_t newsz, void *f)
{
	struct dir_info *pool;
	struct region_info *r;
	struct chunk_info *info;
	size_t oldsz, goldsz, gnewsz;
	void *q, *ret;
	int i;
	uint32_t chunknum;

	pool = argpool;

	if (p == NULL)
		return omalloc(pool, newsz, 0, f);

	r = find(pool, p);
	if (r == NULL) {
		if (mopts.malloc_mt) {
			for (i = 0; i < _MALLOC_MUTEXES; i++) {
				if (i == argpool->mutex)
					continue;
				pool->active--;
				_MALLOC_UNLOCK(pool->mutex);
				pool = mopts.malloc_pool[i];
				_MALLOC_LOCK(pool->mutex);
				pool->active++;
				r = find(pool, p);
				if (r != NULL)
					break;
			}
		}
		if (r == NULL)
			wrterror(pool, "bogus pointer (double free?) %p", p);
	}
	if (newsz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		ret = NULL;
		goto done;
	}

	REALSIZE(oldsz, r);
	if (mopts.chunk_canaries && oldsz <= MALLOC_MAXCHUNK) {
		chunknum = find_chunknum(pool, r, p, 0);
		info = (struct chunk_info *)r->size;
	}

	goldsz = oldsz;
	if (oldsz > MALLOC_MAXCHUNK) {
		if (oldsz < mopts.malloc_guard)
			wrterror(pool, "guard size");
		oldsz -= mopts.malloc_guard;
	}

	gnewsz = newsz;
	if (gnewsz > MALLOC_MAXCHUNK)
		gnewsz += mopts.malloc_guard;

	if (newsz > MALLOC_MAXCHUNK && oldsz > MALLOC_MAXCHUNK &&
	    !mopts.malloc_realloc) {
		/* First case: from n pages sized allocation to m pages sized
		   allocation, m > n */
		size_t roldsz = PAGEROUND(goldsz);
		size_t rnewsz = PAGEROUND(gnewsz);

		if (rnewsz > roldsz) {
			/* try to extend existing region */
			if (!mopts.malloc_guard) {
				void *hint = (char *)r->p + roldsz;
				size_t needed = rnewsz - roldsz;

				STATS_INC(pool->cheap_realloc_tries);
				q = map(pool, hint, needed, 0);
				if (q == hint)
					goto gotit;
				zapcacheregion(pool, hint, needed);
				q = MQUERY(hint, needed);
				if (q == hint)
					q = MMAPA(hint, needed);
				else
					q = MAP_FAILED;
				if (q == hint) {
gotit:
					STATS_ADD(pool->malloc_used, needed);
					if (mopts.malloc_junk == 2)
						memset(q, SOME_JUNK, needed);
					r->size = gnewsz;
					if (r->p != p) {
						/* old pointer is moved */
						memmove(r->p, p, oldsz);
						p = r->p;
					}
					if (mopts.chunk_canaries)
						fill_canary(p, newsz,
						    PAGEROUND(newsz));
					STATS_SETF(r, f);
					STATS_INC(pool->cheap_reallocs);
					ret = p;
					goto done;
				} else if (q != MAP_FAILED) {
					if (munmap(q, needed))
						wrterror(pool, "munmap %p", q);
				}
			}
		} else if (rnewsz < roldsz) {
			/* shrink number of pages */
			if (mopts.malloc_guard) {
				if (mprotect((char *)r->p + roldsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror(pool, "mprotect");
				if (mprotect((char *)r->p + rnewsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_NONE))
					wrterror(pool, "mprotect");
			}
			unmap(pool, (char *)r->p + rnewsz, roldsz - rnewsz, 0);
			r->size = gnewsz;
			if (MALLOC_MOVE_COND(gnewsz)) {
				void *pp = MALLOC_MOVE(r->p, gnewsz);
				memmove(pp, p, newsz);
				p = pp;
			} else if (mopts.chunk_canaries)
				fill_canary(p, newsz, PAGEROUND(newsz));
			STATS_SETF(r, f);
			ret = p;
			goto done;
		} else {
			/* number of pages remains the same */
			void *pp = r->p;

			r->size = gnewsz;
			if (MALLOC_MOVE_COND(gnewsz))
				pp = MALLOC_MOVE(r->p, gnewsz);
			if (p != pp) {
				memmove(pp, p, oldsz < newsz ? oldsz : newsz);
				p = pp;
			}
			if (p == r->p) {
				if (newsz > oldsz && mopts.malloc_junk == 2)
					memset((char *)p + newsz, SOME_JUNK,
					    rnewsz - mopts.malloc_guard -
					    newsz);
				if (mopts.chunk_canaries)
					fill_canary(p, newsz, PAGEROUND(newsz));
			}
			STATS_SETF(r, f);
			ret = p;
			goto done;
		}
	}
	if (oldsz <= MALLOC_MAXCHUNK && oldsz > 0 &&
	    newsz <= MALLOC_MAXCHUNK && newsz > 0 &&
	    1 << find_chunksize(newsz) == oldsz && !mopts.malloc_realloc) {
		/* do not reallocate if new size fits good in existing chunk */
		if (mopts.malloc_junk == 2)
			memset((char *)p + newsz, SOME_JUNK, oldsz - newsz);
		if (mopts.chunk_canaries) {
			info->bits[info->offset + chunknum] = newsz;
			fill_canary(p, newsz, info->size);
		}
		STATS_SETF(r, f);
		ret = p;
	} else if (newsz != oldsz || mopts.malloc_realloc) {
		/* create new allocation */
		q = omalloc(pool, newsz, 0, f);
		if (q == NULL) {
			ret = NULL;
			goto done;
		}
		if (newsz != 0 && oldsz != 0)
			memcpy(q, p, oldsz < newsz ? oldsz : newsz);
		ofree(pool, p, 0, 0, 0);
		ret = q;
	} else {
		/* oldsz == newsz */
		if (newsz != 0)
			wrterror(pool, "realloc internal inconsistency");
		STATS_SETF(r, f);
		ret = p;
	}
done:
	if (argpool != pool) {
		pool->active--;
		_MALLOC_UNLOCK(pool->mutex);
		_MALLOC_LOCK(argpool->mutex);
		argpool->active++;
	}
	return ret;
}

void *
realloc(void *ptr, size_t size)
{
	struct dir_info *d;
	void *r;
	int saved_errno = errno;

	d = getpool();
	if (d == NULL) {
		_malloc_init(0);
		d = getpool();
	}
	_MALLOC_LOCK(d->mutex);
	d->func = "realloc";
	if (d->active++) {
		malloc_recurse(d);
		return NULL;
	}
	r = orealloc(d, ptr, size, CALLER);

	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	if (r == NULL && mopts.malloc_xmalloc)
		wrterror(d, "out of memory");
	if (r != NULL)
		errno = saved_errno;
	return r;
}
/*DEF_STRONG(realloc);*/


/*
 * This is sqrt(SIZE_MAX+1), as s1*s2 <= SIZE_MAX
 * if both s1 < MUL_NO_OVERFLOW and s2 < MUL_NO_OVERFLOW
 */
#define MUL_NO_OVERFLOW	(1UL << (sizeof(size_t) * 4))

void *
calloc(size_t nmemb, size_t size)
{
	struct dir_info *d;
	void *r;
	int saved_errno = errno;

	d = getpool();
	if (d == NULL) {
		_malloc_init(0);
		d = getpool();
	}
	_MALLOC_LOCK(d->mutex);
	d->func = "calloc";
	if ((nmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    nmemb > 0 && SIZE_MAX / nmemb < size) {
		_MALLOC_UNLOCK(d->mutex);
		if (mopts.malloc_xmalloc)
			wrterror(d, "out of memory");
		errno = ENOMEM;
		return NULL;
	}

	if (d->active++) {
		malloc_recurse(d);
		return NULL;
	}

	size *= nmemb;
	r = omalloc(d, size, 1, CALLER);

	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	if (r == NULL && mopts.malloc_xmalloc)
		wrterror(d, "out of memory");
	if (r != NULL)
		errno = saved_errno;
	return r;
}
/*DEF_STRONG(calloc);*/

static void *
orecallocarray(struct dir_info *argpool, void *p, size_t oldsize,
    size_t newsize, void *f)
{
	struct dir_info *pool;
	struct region_info *r;
	void *newptr;
	size_t sz;
	int i;

	pool = argpool;

	if (p == NULL)
		return omalloc(pool, newsize, 1, f);

	if (oldsize == newsize)
		return p;

	r = find(pool, p);
	if (r == NULL) {
		if (mopts.malloc_mt) {
			for (i = 0; i < _MALLOC_MUTEXES; i++) {
				if (i == argpool->mutex)
					continue;
				pool->active--;
				_MALLOC_UNLOCK(pool->mutex);
				pool = mopts.malloc_pool[i];
				_MALLOC_LOCK(pool->mutex);
				pool->active++;
				r = find(pool, p);
				if (r != NULL)
					break;
			}
		}
		if (r == NULL)
			wrterror(pool, "bogus pointer (double free?) %p", p);
	}

	REALSIZE(sz, r);
	if (sz <= MALLOC_MAXCHUNK) {
		if (mopts.chunk_canaries) {
			struct chunk_info *info = (struct chunk_info *)r->size;
			uint32_t chunknum = find_chunknum(pool, r, p, 0);

			if (info->bits[info->offset + chunknum] != oldsize)
				wrterror(pool, "recorded old size %hu != %zu",
				    info->bits[info->offset + chunknum],
				    oldsize);
		}
	} else if (oldsize != sz - mopts.malloc_guard)
		wrterror(pool, "recorded old size %zu != %zu",
		    sz - mopts.malloc_guard, oldsize);

	newptr = omalloc(pool, newsize, 0, f);
	if (newptr == NULL)
		goto done;

	if (newsize > oldsize) {
		memcpy(newptr, p, oldsize);
		memset((char *)newptr + oldsize, 0, newsize - oldsize);
	} else
		memcpy(newptr, p, newsize);

	ofree(pool, p, 1, 0, 0);

done:
	if (argpool != pool) {
		pool->active--;
		_MALLOC_UNLOCK(pool->mutex);
		_MALLOC_LOCK(argpool->mutex);
		argpool->active++;
	}

	return newptr;
}

static void *
recallocarray_p(void *ptr, size_t oldnmemb, size_t newnmemb, size_t size)
{
	size_t oldsize, newsize;
	void *newptr;

	if (ptr == NULL)
		return calloc(newnmemb, size);

	if ((newnmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    newnmemb > 0 && SIZE_MAX / newnmemb < size) {
		errno = ENOMEM;
		return NULL;
	}
	newsize = newnmemb * size;

	if ((oldnmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    oldnmemb > 0 && SIZE_MAX / oldnmemb < size) {
		errno = EINVAL;
		return NULL;
	}
	oldsize = oldnmemb * size;

	/*
	 * Don't bother too much if we're shrinking just a bit,
	 * we do not shrink for series of small steps, oh well.
	 */
	if (newsize <= oldsize) {
		size_t d = oldsize - newsize;

		if (d < oldsize / 2 && d < MALLOC_PAGESIZE) {
			memset((char *)ptr + newsize, 0, d);
			return ptr;
		}
	}

	newptr = malloc(newsize);
	if (newptr == NULL)
		return NULL;

	if (newsize > oldsize) {
		memcpy(newptr, ptr, oldsize);
		memset((char *)newptr + oldsize, 0, newsize - oldsize);
	} else
		memcpy(newptr, ptr, newsize);

	explicit_bzero(ptr, oldsize);
	free(ptr);

	return newptr;
}

void *
recallocarray(void *ptr, size_t oldnmemb, size_t newnmemb, size_t size)
{
	struct dir_info *d;
	size_t oldsize = 0, newsize;
	void *r;
	int saved_errno = errno;

	if (!mopts.internal_funcs)
		return recallocarray_p(ptr, oldnmemb, newnmemb, size);

	d = getpool();
	if (d == NULL) {
		_malloc_init(0);
		d = getpool();
	}

	_MALLOC_LOCK(d->mutex);
	d->func = "recallocarray";

	if ((newnmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    newnmemb > 0 && SIZE_MAX / newnmemb < size) {
		_MALLOC_UNLOCK(d->mutex);
		if (mopts.malloc_xmalloc)
			wrterror(d, "out of memory");
		errno = ENOMEM;
		return NULL;
	}
	newsize = newnmemb * size;

	if (ptr != NULL) {
		if ((oldnmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
		    oldnmemb > 0 && SIZE_MAX / oldnmemb < size) {
			_MALLOC_UNLOCK(d->mutex);
			errno = EINVAL;
			return NULL;
		}
		oldsize = oldnmemb * size;
	}

	if (d->active++) {
		malloc_recurse(d);
		return NULL;
	}

	r = orecallocarray(d, ptr, oldsize, newsize, CALLER);

	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	if (r == NULL && mopts.malloc_xmalloc)
		wrterror(d, "out of memory");
	if (r != NULL)
		errno = saved_errno;
	return r;
}
DEF_WEAK(recallocarray);


static void *
mapalign(struct dir_info *d, size_t alignment, size_t sz, int zero_fill)
{
	char *p, *q;

	if (alignment < MALLOC_PAGESIZE || ((alignment - 1) & alignment) != 0)
		wrterror(d, "mapalign bad alignment");
	if (sz != PAGEROUND(sz))
		wrterror(d, "mapalign round");

	/* Allocate sz + alignment bytes of memory, which must include a
	 * subrange of size bytes that is properly aligned.  Unmap the
	 * other bytes, and then return that subrange.
	 */

	/* We need sz + alignment to fit into a size_t. */
	if (alignment > SIZE_MAX - sz)
		return MAP_FAILED;

	p = map(d, NULL, sz + alignment, zero_fill);
	if (p == MAP_FAILED)
		return MAP_FAILED;
	q = (char *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
	if (q != p) {
		if (munmap(p, q - p))
			wrterror(d, "munmap %p", p);
	}
	if (munmap(q + sz, alignment - (q - p)))
		wrterror(d, "munmap %p", q + sz);
	STATS_SUB(d->malloc_used, alignment);

	return q;
}

static void *
omemalign(struct dir_info *pool, size_t alignment, size_t sz, int zero_fill, void *f)
{
	size_t psz;
	void *p;

	if (alignment <= MALLOC_PAGESIZE) {
		/*
		 * max(size, alignment) is enough to assure the requested alignment,
		 * since the allocator always allocates power-of-two blocks.
		 */
		if (sz < alignment)
			sz = alignment;
		return omalloc(pool, sz, zero_fill, f);
	}

	if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		return NULL;
	}

	sz += mopts.malloc_guard;
	psz = PAGEROUND(sz);

	p = mapalign(pool, alignment, psz, zero_fill);
	if (p == NULL) {
		errno = ENOMEM;
		return NULL;
	}

	if (insert(pool, p, sz, f)) {
		unmap(pool, p, psz, 0);
		errno = ENOMEM;
		return NULL;
	}

	if (mopts.malloc_guard) {
		if (mprotect((char *)p + psz - mopts.malloc_guard,
		    mopts.malloc_guard, PROT_NONE))
			wrterror(pool, "mprotect");
		STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
	}

	if (mopts.malloc_junk == 2) {
		if (zero_fill)
			memset((char *)p + sz - mopts.malloc_guard,
			    SOME_JUNK, psz - sz);
		else
			memset(p, SOME_JUNK, psz - mopts.malloc_guard);
	}
	else if (mopts.chunk_canaries)
		fill_canary(p, sz - mopts.malloc_guard,
		    psz - mopts.malloc_guard);

	return p;
}

int
posix_memalign(void **memptr, size_t alignment, size_t size)
{
	struct dir_info *d;
	int res, saved_errno = errno;
	void *r;

	/* Make sure that alignment is a large enough power of 2. */
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *))
		return EINVAL;

	d = getpool();
	if (d == NULL) {
		_malloc_init(0);
		d = getpool();
	}
	_MALLOC_LOCK(d->mutex);
	d->func = "posix_memalign";
	if (d->active++) {
		malloc_recurse(d);
		goto err;
	}
	r = omemalign(d, alignment, size, 0, CALLER);
	d->active--;
	_MALLOC_UNLOCK(d->mutex);
	if (r == NULL) {
		if (mopts.malloc_xmalloc)
			wrterror(d, "out of memory");
		goto err;
	}
	errno = saved_errno;
	*memptr = r;
	return 0;

err:
	res = errno;
	errno = saved_errno;
	return res;
}
/*DEF_STRONG(posix_memalign);*/

#ifdef MALLOC_STATS

struct malloc_leak {
	void (*f)();
	size_t total_size;
	int count;
};

struct leaknode {
	RB_ENTRY(leaknode) entry;
	struct malloc_leak d;
};

static int
leakcmp(struct leaknode *e1, struct leaknode *e2)
{
	return e1->d.f < e2->d.f ? -1 : e1->d.f > e2->d.f;
}

static RB_HEAD(leaktree, leaknode) leakhead;
RB_GENERATE_STATIC(leaktree, leaknode, entry, leakcmp)

static void
putleakinfo(void *f, size_t sz, int cnt)
{
	struct leaknode key, *p;
	static struct leaknode *page;
	static int used;

	if (cnt == 0 || page == MAP_FAILED)
		return;

	key.d.f = f;
	p = RB_FIND(leaktree, &leakhead, &key);
	if (p == NULL) {
		if (page == NULL ||
		    used >= MALLOC_PAGESIZE / sizeof(struct leaknode)) {
			page = MMAP(MALLOC_PAGESIZE);
			if (page == MAP_FAILED)
				return;
			used = 0;
		}
		p = &page[used++];
		p->d.f = f;
		p->d.total_size = sz * cnt;
		p->d.count = cnt;
		RB_INSERT(leaktree, &leakhead, p);
	} else {
		p->d.total_size += sz * cnt;
		p->d.count += cnt;
	}
}

static struct malloc_leak *malloc_leaks;

static void
writestr(int fd, const char *p)
{
	write(fd, p, strlen(p));
}

static void
dump_leaks(int fd)
{
	struct leaknode *p;
	char buf[64];
	int i = 0;

	writestr(fd, "Leak report\n");
	writestr(fd, "                 f     sum      #    avg\n");
	/* XXX only one page of summary */
	if (malloc_leaks == NULL)
		malloc_leaks = MMAP(MALLOC_PAGESIZE);
	if (malloc_leaks != MAP_FAILED)
		memset(malloc_leaks, 0, MALLOC_PAGESIZE);
	RB_FOREACH(p, leaktree, &leakhead) {
		snprintf(buf, sizeof(buf), "%18p %7zu %6u %6zu\n", p->d.f,
		    p->d.total_size, p->d.count, p->d.total_size / p->d.count);
		write(fd, buf, strlen(buf));
		if (malloc_leaks == MAP_FAILED ||
		    i >= MALLOC_PAGESIZE / sizeof(struct malloc_leak))
			continue;
		malloc_leaks[i].f = p->d.f;
		malloc_leaks[i].total_size = p->d.total_size;
		malloc_leaks[i].count = p->d.count;
		i++;
	}
}

static void
dump_chunk(int fd, struct chunk_info *p, void *f, int fromfreelist)
{
	char buf[64];

	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %18p %18p %4d %d/%d\n",
		    p->page, ((p->bits[0] & 1) ? NULL : f),
		    p->size, p->free, p->total);
		write(fd, buf, strlen(buf));
		if (!fromfreelist) {
			if (p->bits[0] & 1)
				putleakinfo(NULL, p->size, p->total - p->free);
			else {
				putleakinfo(f, p->size, 1);
				putleakinfo(NULL, p->size,
				    p->total - p->free - 1);
			}
			break;
		}
		p = LIST_NEXT(p, entries);
		if (p != NULL)
			writestr(fd, "        ");
	}
}

static void
dump_free_chunk_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i, j, count;
	struct chunk_info *p;

	writestr(fd, "Free chunk structs:\n");
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		count = 0;
		LIST_FOREACH(p, &d->chunk_info_list[i], entries)
			count++;
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++) {
			p = LIST_FIRST(&d->chunk_dir[i][j]);
			if (p == NULL && count == 0)
				continue;
			snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
			write(fd, buf, strlen(buf));
			if (p != NULL)
				dump_chunk(fd, p, NULL, 1);
			else
				write(fd, "\n", 1);
		}
	}

}

static void
dump_free_page_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
			write(fd, buf, strlen(buf));
		}
	}
}

static void
malloc_dump1(int fd, int poolno, struct dir_info *d)
{
	char buf[100];
	size_t i, realsize;

	snprintf(buf, sizeof(buf), "Malloc dir of %s pool %d at %p\n", __progname, poolno, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Region slots free %zu/%zu\n",
		d->regions_free, d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu\n", d->finds,
	    d->find_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu\n", d->inserts,
	    d->insert_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	    d->delete_moves);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "In use %zu\n", d->malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", d->malloc_guarded);
	write(fd, buf, strlen(buf));
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	writestr(fd,
	    "slot)  hash d  type               page                  f size [free/n]\n");
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%4zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				putleakinfo(d->r[i].f, realsize, 1);
				snprintf(buf, sizeof(buf),
				    "pages %18p %18p %zu\n", d->r[i].p,
				    d->r[i].f, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size,
				    d->r[i].f, 0);
		}
	}
	dump_leaks(fd);
	write(fd, "\n", 1);
}

void
malloc_dump(int fd, int poolno, struct dir_info *pool)
{
	int i;
	void *p;
	struct region_info *r;
	int saved_errno = errno;

	if (pool == NULL)
		return;
	for (i = 0; i < MALLOC_DELAYED_CHUNK_MASK + 1; i++) {
		p = pool->delayed_chunks[i];
		if (p == NULL)
			continue;
		r = find(pool, p);
		if (r == NULL)
			wrterror(pool, "bogus pointer in malloc_dump %p", p);
		free_bytes(pool, r, p);
		pool->delayed_chunks[i] = NULL;
	}
	/* XXX leak when run multiple times */
	RB_INIT(&leakhead);
	malloc_dump1(fd, poolno, pool);
	errno = saved_errno;
}
DEF_WEAK(malloc_dump);

void
malloc_gdump(int fd)
{
	int i;
	int saved_errno = errno;

	for (i = 0; i < _MALLOC_MUTEXES; i++)
		malloc_dump(fd, i, mopts.malloc_pool[i]);

	errno = saved_errno;
}
DEF_WEAK(malloc_gdump);

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd, i;
	char buf[100];

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		snprintf(buf, sizeof(buf), "******** Start dump %s *******\n",
		     __progname);
		write(fd, buf, strlen(buf));
		snprintf(buf, sizeof(buf),
		    "MT=%d I=%d F=%d U=%d J=%d R=%d X=%d C=%d cache=%u G=%zu\n",
		    mopts.malloc_mt, mopts.internal_funcs,
		    mopts.malloc_freenow,
		    mopts.malloc_freeunmap, mopts.malloc_junk,
		    mopts.malloc_realloc, mopts.malloc_xmalloc,
		    mopts.chunk_canaries, mopts.malloc_cache,
		    mopts.malloc_guard);
		write(fd, buf, strlen(buf));

		for (i = 0; i < _MALLOC_MUTEXES; i++)
			malloc_dump(fd, i, mopts.malloc_pool[i]);
		snprintf(buf, sizeof(buf), "******** End dump %s *******\n",
		    __progname);
		write(fd, buf, strlen(buf));
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}

#endif /* MALLOC_STATS */
@


1.223
log
@don't forget to fill in canary bytes for posix_memalign(3); reported by
and ok jeremy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.222 2017/04/17 16:36:35 otto Exp $	*/
d1337 1
a1337 1
			if (mopts.chunk_canaries) {
d1345 1
a1345 1
					wrterror(pool, "recorded old size %hu"
d1349 4
d1354 2
a1355 2
		} else if (sz - mopts.malloc_guard < argsz)
			wrterror(pool, "recorded old size %zu < %zu",
d1357 1
@


1.222
log
@whitespace fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.221 2017/04/13 18:32:55 otto Exp $	*/
d2021 3
@


1.221
log
@allow clearing less than allocated and document freezero(3) better
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.220 2017/04/10 05:45:02 otto Exp $	*/
d264 1
a264 1
struct dir_info *getpool(void) 
d1015 1
a1015 1
	
d1269 1
a1269 1
	
d1329 1
a1329 1
		}	
d1339 1
a1339 1
				     (struct chunk_info *)r->size;
d1341 1
a1341 1
				     find_chunknum(pool, r, p, 0);
d1349 1
a1349 1
			}	
d1492 1
a1492 1
	
d1513 1
a1513 1
		}	
d1762 1
a1762 1
	
d1788 1
a1788 1
	
d1849 1
a1849 1
	
d1898 1
a1898 1
	
@


1.220
log
@Introducing freezero(3) a version of free that guarantees the process
no longer has access to the content of a memmory object. It does
this by either clearing (if the object memory remains cached) or
by calling munmap(2). ok millert@@, deraadt@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.219 2017/04/06 08:39:47 otto Exp $	*/
d1343 1
a1343 1
				if (info->bits[info->offset + chunknum] !=
d1346 1
a1346 1
					    " != %zu",
d1350 2
a1351 2
		} else if (argsz != sz - mopts.malloc_guard)
			wrterror(pool, "recorded old size %zu != %zu",
@


1.219
log
@first print size in meta-data then supplied arg size when an inconsistency is
detected wrt recallocarray()
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.218 2017/03/28 16:56:38 otto Exp $	*/
d188 1
a188 1
	int	internal_recallocarray;	/* use better recallocarray? */
d346 8
a353 1
	if (psz > mopts.malloc_cache) {
a360 1
	rsz = mopts.malloc_cache - d->free_regions_size;
d1243 1
a1243 1
		mopts.internal_recallocarray = 1;
d1306 1
a1306 1
ofree(struct dir_info *argpool, void *p, int clear)
d1335 19
d1439 1
a1439 1
	ofree(d, ptr, 0);
d1446 35
d1654 1
a1654 1
		ofree(pool, p, 0);
d1814 1
a1814 1
	ofree(pool, p, 1);
d1887 1
a1887 1
	if (!mopts.internal_recallocarray)
d2338 2
a2339 2
		    "MT=%d IRC=%d F=%d U=%d J=%d R=%d X=%d C=%d cache=%u G=%zu\n",
		    mopts.malloc_mt, mopts.internal_recallocarray,
@


1.218
log
@small cleanup & optimization; ok deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.217 2017/03/24 16:23:05 otto Exp $	*/
d1741 2
a1742 2
		wrterror(pool, "recorded old size %zu != %zu", oldsize,
		    sz - mopts.malloc_guard);
@


1.217
log
@add a helper function to print all pools #ifdef MALLOC_STATS
from David CARLIER
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.216 2017/03/24 16:15:31 otto Exp $	*/
d1706 3
d1797 1
a1797 1
		if (d < oldsize / 2 && d < getpagesize()) {
@


1.216
log
@move recallocarray to malloc.c and
- use internal meta-data to do more consistency checking (especially with
  option C)
- use cheap free if possible
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.215 2017/02/15 12:31:57 jsg Exp $	*/
d214 2
d2248 13
@


1.215
log
@Add a NULL test to wrterror() to avoid a NULL deref when called from a
free() error path.

ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.214 2017/02/02 10:35:34 otto Exp $	*/
d188 1
d334 1
a334 1
unmap(struct dir_info *d, void *p, size_t sz)
d377 2
d869 1
a869 1
		unmap(d, pp, MALLOC_PAGESIZE);
d885 1
a885 1
			unmap(d, pp, MALLOC_PAGESIZE);
d1112 1
a1112 1
	unmap(d, info->page, MALLOC_PAGESIZE);
d1143 1
a1143 1
			unmap(pool, p, psz);
d1234 2
d1298 1
a1298 1
ofree(struct dir_info *argpool, void *p)
d1352 1
a1352 1
		unmap(pool, p, PAGEROUND(sz));
d1361 1
a1361 1
		if (!mopts.malloc_freenow) {
d1373 2
a1374 2
			if (mopts.malloc_junk && sz > 0)
				memset(p, SOME_FREEJUNK, sz);
d1412 1
a1412 1
	ofree(d, ptr);
d1536 1
a1536 1
			unmap(pool, (char *)r->p + rnewsz, roldsz - rnewsz);
d1592 1
a1592 1
		ofree(pool, p);
d1690 183
d1937 1
a1937 1
		unmap(pool, p, psz);
d2260 3
a2262 2
		    "MT=%d F=%d U=%d J=%d R=%d X=%d C=%d cache=%u G=%zu\n",
		    mopts.malloc_mt, mopts.malloc_freenow,
@


1.214
log
@fix a comment and rm some dead code as a result of the previous diff
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.213 2017/02/01 06:17:42 otto Exp $	*/
d281 1
a281 1
	    getpid(), d->func ? d->func : "unknown");
@


1.213
log
@Let realloc handle and produce moved pointers for allocations between
half a page and a page. ok jmatthew@@ tb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.212 2017/01/21 07:47:42 otto Exp $	*/
d1476 1
a1476 1
		   allocation, no malloc_move in effect */
d1590 3
a1592 6
		/* > page size allocation didnt change */
		if (mopts.chunk_canaries && oldsz <= MALLOC_MAXCHUNK) {
			info->bits[info->offset + chunknum] = newsz;
			if (info->size > 0)
				fill_canary(p, newsz, info->size);
		}	
@


1.212
log
@1. When shrinking a chunk allocation, compare the size of the current
allocation to the size of the new allocation (instead of the requested size).
2. Previously realloc takes the easy way and always reallocates if C is
active. This commit fixes by carefully updating the recorded requested
size in all cases, and writing the canary bytes in the proper location
after reallocating.
3. Introduce defines to test if MALLOC_MOVE should be done and to
compute the new value.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.211 2016/11/04 09:11:20 otto Exp $	*/
a1330 1
#if notyetbecause_of_realloc
d1332 2
a1333 5
			if (p != ((char *)r->p) + ((MALLOC_PAGESIZE -
			    MALLOC_MINSIZE - sz - mopts.malloc_guard) &
			    ~(MALLOC_MINSIZE-1))) {
			}
#endif
d1473 1
a1473 1
	if (newsz > MALLOC_MAXCHUNK && oldsz > MALLOC_MAXCHUNK && p == r->p &&
d1483 1
a1483 1
				void *hint = (char *)p + roldsz;
d1501 6
a1506 1
					r->size = newsz;
d1508 2
a1509 1
						fill_canary(p, newsz, PAGEROUND(newsz));
d1522 1
a1522 1
				if (mprotect((char *)p + roldsz -
d1526 1
a1526 1
				if (mprotect((char *)p + rnewsz -
d1531 1
a1531 1
			unmap(pool, (char *)p + rnewsz, roldsz - rnewsz);
d1533 5
a1537 1
			if (mopts.chunk_canaries)
d1544 2
a1545 3
			if (newsz > oldsz && mopts.malloc_junk == 2)
				memset((char *)p + newsz, SOME_JUNK,
				    rnewsz - mopts.malloc_guard - newsz);
d1547 14
a1560 2
			if (mopts.chunk_canaries)
				fill_canary(p, newsz, PAGEROUND(newsz));
@


1.211
log
@MALLOC_STATS tweaks, by default not compiled in
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.210 2016/11/03 18:51:49 otto Exp $	*/
d76 6
d208 1
d219 2
a220 2
/* low bits of r->p determine size: 0 means >= page size and p->size holding
 *  real size, otherwise r->size is a shift count, or 1 for malloc(0)
d915 2
a916 6

/*
 * Allocate a chunk
 */
static void *
malloc_bytes(struct dir_info *d, size_t argsize, void *f)
d918 1
a918 10
	int		i, j, listnum;
	size_t		k, size;
	u_short		u, *lp;
	struct chunk_info *bp;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror(d, "internal struct corrupt");

	size = argsize;
d934 19
d1006 1
a1006 1
		bp->bits[bp->offset + k] = argsize;
d1013 2
a1014 7
		else if (mopts.chunk_canaries) {
			size_t sz = bp->size - argsize;

			if (sz > CHUNK_CHECK_LENGTH)
				sz = CHUNK_CHECK_LENGTH;
			memset((char *)bp->page + k + argsize, SOME_JUNK, sz);
		}
d1020 10
d1151 1
a1151 1
		if (sz - mopts.malloc_guard < MALLOC_PAGESIZE - MALLOC_LEEWAY) {
d1156 1
a1156 2
			p = ((char *)p) + ((MALLOC_PAGESIZE - MALLOC_LEEWAY -
			    (sz - mopts.malloc_guard)) & ~(MALLOC_MINSIZE-1));
d1169 3
a1171 8
			else if (mopts.chunk_canaries) {
				size_t csz = psz - sz;

				if (csz > CHUNK_CHECK_LENGTH)
					csz = CHUNK_CHECK_LENGTH;
				memset((char *)p + sz - mopts.malloc_guard,
				    SOME_JUNK, csz);
			}
d1323 1
a1323 2
		if (sz - mopts.malloc_guard >= MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
d1424 1
d1428 1
d1461 5
d1478 3
a1480 1
	    !mopts.chunk_canaries && !mopts.malloc_realloc) {
d1485 1
d1506 2
d1518 1
d1531 2
d1537 1
d1542 2
d1549 5
a1553 3
	if (newsz <= oldsz && newsz > oldsz / 2 && !mopts.chunk_canaries &&
	    !mopts.malloc_realloc) {
		if (mopts.malloc_junk == 2 && newsz > 0)
d1555 4
d1561 2
a1562 2
	} else if (newsz != oldsz || mopts.chunk_canaries ||
	    mopts.malloc_realloc) {
d1573 6
@


1.210
log
@small tweak to also check canaries if F is in effect
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.209 2016/10/31 10:06:56 otto Exp $	*/
d204 1
a204 1
void malloc_dump(int, struct dir_info *);
d290 1
a290 1
			malloc_dump(STDERR_FILENO, mopts.malloc_pool[i]);
d1921 1
a1921 1
malloc_dump1(int fd, struct dir_info *d)
d1926 1
a1926 1
	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
d1945 4
d1964 1
a1964 1
				    "pages %12p %12p %zu\n", d->r[i].p,
a1972 4
	snprintf(buf, sizeof(buf), "In use %zu\n", d->malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", d->malloc_guarded);
	write(fd, buf, strlen(buf));
d1978 1
a1978 1
malloc_dump(int fd, struct dir_info *pool)
d1999 1
a1999 1
	malloc_dump1(fd, pool);
d2009 1
d2013 12
d2026 4
a2029 1
			malloc_dump(fd, mopts.malloc_pool[i]);
@


1.209
log
@remove some old option letters and also make P non-settable.  It has
been the default for ages, and I see no valid reason to be able to
disable it. ok natano@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.208 2016/10/28 17:03:22 otto Exp $	*/
d1042 1
a1042 1
	if (check && mopts.chunk_canaries && info->size > 0) {
d1346 3
a1349 1
			find_chunknum(pool, r, p, 1);
@


1.208
log
@Pages in the malloc cache are either reused quickly or unmapped
quickly. In both cases it does not make sense to set hints on them.
So remove that option, which is just a remainder of old times when
malloc used to hold on to pages. ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.207 2016/10/22 14:27:19 otto Exp $	*/
d71 3
a73 4
 * When the P option is active, we move allocations between half a page
 * and a whole page towards the end, subject to alignment constraints.
 * This is the extra headroom we allow. Set to zero to be the most
 * strict.
a178 1
	int	malloc_move;		/* move allocations to end of page? */
d182 1
a183 1
	u_int	malloc_cache;		/* free pages we cache */
a493 4
	case 'a':
	case 'A':
		/* ignored */
		break;
a529 9
	case 'n':
	case 'N':
		break;
	case 'p':
		mopts.malloc_move = 0;
		break;
	case 'P':
		mopts.malloc_move = 1;
		break;
a566 1
	mopts.malloc_move = 1;
d1133 1
a1133 3
		if (mopts.malloc_move &&
		    sz - mopts.malloc_guard < MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
@


1.207
log
@- fix MALLOC_STATS compile
- redundant cast is redundant
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.206 2016/10/21 15:39:31 otto Exp $	*/
a178 1
	int	malloc_hint;		/* call madvice on free pages?  */
a375 2
			if (mopts.malloc_hint)
				madvise(p, sz, MADV_FREE);
a446 2
				if (mopts.malloc_hint)
					madvise(p, sz, MADV_NORMAL);
a462 2
		if (mopts.malloc_hint)
			madvise(p, sz, MADV_NORMAL);
a526 6
		break;
	case 'h':
		mopts.malloc_hint = 0;
		break;
	case 'H':
		mopts.malloc_hint = 1;
@


1.206
log
@fix some void * arithmetic by casting
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.205 2016/10/21 06:55:09 otto Exp $	*/
d289 3
a291 1
	if (mopts.malloc_stats)
d294 1
d1048 1
a1048 1
	p = (u_char *)ptr + sz;
@


1.205
log
@and recommit with fixed GC
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.204 2016/10/20 11:29:34 otto Exp $	*/
d667 1
a667 1
	mprotect(p + MALLOC_PAGESIZE + DIR_INFO_RSZ,
d670 1
a670 1
	d = (struct dir_info *)(p + MALLOC_PAGESIZE +
d730 1
a730 1
		wrterror(d, "munmap %p", d->r);
@


1.204
log
@backout for now; flag combination GC is not ok
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.203 2016/10/20 05:38:41 otto Exp $	*/
d34 1
d203 2
d267 1
a267 1
wrterror(struct dir_info *d, char *msg, void *p)
d269 18
a286 32
	char		*q = " error: ";
	struct iovec	iov[7];
	char		pidbuf[20];
	char		buf[20];
	int		saved_errno = errno, i;

	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = pidbuf;
	snprintf(pidbuf, sizeof(pidbuf), "(%d) in ", getpid());
	iov[1].iov_len = strlen(pidbuf);
	if (d != NULL) {
		iov[2].iov_base = d->func;
		iov[2].iov_len = strlen(d->func);
 	} else {
		iov[2].iov_base = "unknown";
		iov[2].iov_len = 7;
	}
	iov[3].iov_base = q;
	iov[3].iov_len = strlen(q);
	iov[4].iov_base = msg;
	iov[4].iov_len = strlen(msg);
	iov[5].iov_base = buf;
	if (p == NULL)
		iov[5].iov_len = 0;
	else {
		snprintf(buf, sizeof(buf), " %010p", p);
		iov[5].iov_len = strlen(buf);
	}
	iov[6].iov_base = "\n";
	iov[6].iov_len = 1;
	writev(STDERR_FILENO, iov, 7);
d334 1
a334 1
		wrterror(d, "munmap round", NULL);
d339 1
a339 1
			wrterror(d, "munmap", p);
d353 1
a353 1
				wrterror(d, "munmap", r->p);
d365 1
a365 1
		wrterror(d, "malloc cache underflow", NULL);
d385 1
a385 1
		wrterror(d, "malloc free slot lost", NULL);
d387 1
a387 1
		wrterror(d, "malloc cache overflow", NULL);
d402 1
a402 1
				wrterror(d, "munmap", r->p);
d421 1
a421 1
		wrterror(d, "internal struct corrupt", NULL);
d423 1
a423 1
		wrterror(d, "map round", NULL);
d478 1
a478 1
		wrterror(d, "malloc cache", NULL);
d665 1
a665 1
		wrterror(NULL, "malloc init mmap failed", NULL);
d679 1
a679 1
		wrterror(NULL, "malloc init mmap failed", NULL);
d730 1
a730 1
		wrterror(d, "munmap", d->r);
d820 1
a820 1
		wrterror(d, "internal struct corrupt", NULL);
d843 1
a843 1
		wrterror(d, "regions_total not 2^x", NULL);
d928 1
a928 1
		wrterror(d, "pp & bits", pp);
d948 1
a948 1
		wrterror(d, "internal struct corrupt", NULL);
d976 1
a976 1
		wrterror(d, "chunk info corrupted", NULL);
d1037 19
d1064 1
a1064 1
		wrterror(d, "chunk info corrupted", NULL);
d1069 2
a1070 14
		size_t sz = info->bits[info->offset + chunknum];
		size_t check_sz = info->size - sz;
		u_char *p, *q;

		if (check_sz > CHUNK_CHECK_LENGTH)
			check_sz = CHUNK_CHECK_LENGTH;
		p = (u_char *)ptr + sz;
		q = p + check_sz;

		while (p < q)
			if (*p++ != SOME_JUNK) {
				q = (void *)(sz << 16 | p - (u_char *)ptr - 1);
				wrterror(d, "chunk canary corrupted: ", q);
			}
d1074 1
a1074 1
		wrterror(d, "modified chunk-pointer", ptr);
d1077 1
a1077 1
		wrterror(d, "chunk is already free", ptr);
d1155 1
a1155 1
				wrterror(pool, "mprotect", NULL);
d1180 8
d1212 1
a1212 1
		wrterror(d, "recursive call", NULL);
d1271 1
a1271 1
	d->func = "malloc():";
d1281 1
a1281 1
		wrterror(d, "out of memory", NULL);
d1298 1
a1298 1
		wrterror(pool, "bogus pointer in validate_junk", p);
d1304 1
a1304 1
			wrterror(pool, "use after free", p);
d1334 1
a1334 1
			wrterror(pool, "bogus pointer (double free?)", p);
d1342 5
a1346 1
				wrterror(pool, "bogus pointer", p);
d1359 1
a1359 1
				wrterror(pool, "guard size", NULL);
d1364 1
a1364 1
					wrterror(pool, "mprotect", NULL);
d1382 1
a1382 1
				wrterror(pool, "double free", p);
d1393 2
a1394 1
				wrterror(pool, "bogus pointer (double free?)", p);
d1398 1
a1398 1
done:
d1419 1
a1419 1
		wrterror(d, "free() called before allocation", NULL);
d1421 1
a1421 1
	d->func = "free():";
d1465 1
a1465 1
			wrterror(pool, "bogus pointer (double free?)", p);
d1477 1
a1477 1
			wrterror(pool, "guard size", NULL);
d1486 1
a1486 1
	    !mopts.malloc_realloc) {
d1517 1
a1517 1
						wrterror(pool, "munmap", q);
d1525 1
a1525 1
					wrterror(pool, "mprotect", NULL);
d1529 1
a1529 1
					wrterror(pool, "mprotect", NULL);
d1590 1
a1590 1
	d->func = "realloc():";
d1600 1
a1600 1
		wrterror(d, "out of memory", NULL);
d1627 1
a1627 1
	d->func = "calloc():";
d1632 1
a1632 1
			wrterror(d, "out of memory", NULL);
d1648 1
a1648 1
		wrterror(d, "out of memory", NULL);
d1661 1
a1661 1
		wrterror(d, "mapalign bad alignment", NULL);
d1663 1
a1663 1
		wrterror(d, "mapalign round", NULL);
d1680 1
a1680 1
			wrterror(d, "munmap", p);
d1683 1
a1683 1
		wrterror(d, "munmap", q + sz);
d1728 1
a1728 1
			wrterror(pool, "mprotect", NULL);
d1760 1
a1760 1
	d->func = "posix_memalign():";
d1770 1
a1770 1
			wrterror(d, "out of memory", NULL);
d2019 1
a2019 1
			wrterror(pool, "bogus pointer in malloc_dump", p);
@


1.203
log
@Also place canaries in > page sized objects (if C is in effect); ok tb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.202 2016/10/15 18:24:40 guenther Exp $	*/
a33 1
#include <stdarg.h>
a201 2
static __dead void wrterror(struct dir_info *d, char *msg, ...)
    __attribute__((__format__ (printf, 2, 3)));
d264 1
a264 1
wrterror(struct dir_info *d, char *msg, ...)
d266 32
a297 18
	struct iovec	iov[3];
	char		pidbuf[80];
	char		buf[80];
	int		saved_errno = errno, ret;
	va_list		ap;

	iov[0].iov_base = pidbuf;
	ret = snprintf(pidbuf, sizeof(pidbuf), "%.50s(%d) in %s(): ",
	    __progname, getpid(), d->func ? d->func : "unknown");
	iov[0].iov_len = ret > 0 ? strlen(pidbuf) : 0;
	iov[1].iov_base = buf;
	va_start(ap, msg);
	ret = vsnprintf(buf, sizeof(buf), msg, ap);
	va_end(ap);
	iov[1].iov_len = ret > 0 ? strlen(buf) : 0;
	iov[2].iov_base = "\n";
	iov[2].iov_len = 1;
	writev(STDERR_FILENO, iov, 3);
d345 1
a345 1
		wrterror(d, "munmap round");
d350 1
a350 1
			wrterror(d, "munmap %p", p);
d364 1
a364 1
				wrterror(d, "munmap %p", r->p);
d376 1
a376 1
		wrterror(d, "malloc cache underflow");
d396 1
a396 1
		wrterror(d, "malloc free slot lost");
d398 1
a398 1
		wrterror(d, "malloc cache overflow");
d413 1
a413 1
				wrterror(d, "munmap %p", r->p);
d432 1
a432 1
		wrterror(d, "internal struct corrupt");
d434 1
a434 1
		wrterror(d, "map round");
d489 1
a489 1
		wrterror(d, "malloc cache");
d676 1
a676 1
		wrterror(NULL, "malloc init mmap failed");
d690 1
a690 1
		wrterror(NULL, "malloc init mmap failed");
d741 1
a741 1
		wrterror(d, "munmap %p", d->r);
d831 1
a831 1
		wrterror(d, "internal struct corrupt");
d854 1
a854 1
		wrterror(d, "regions_total not 2^x");
d939 1
a939 1
		wrterror(d, "pp & bits %p", pp);
d959 1
a959 1
		wrterror(d, "internal struct corrupt");
d987 1
a987 1
		wrterror(d, "chunk info corrupted");
a1047 19
static void
validate_canary(struct dir_info *d, u_char *ptr, size_t sz, size_t allocated)
{
	size_t check_sz = allocated - sz;
	u_char *p, *q;

	if (check_sz > CHUNK_CHECK_LENGTH)
		check_sz = CHUNK_CHECK_LENGTH;
	p = (u_char *)ptr + sz;
	q = p + check_sz;

	while (p < q) {
		if (*p++ != SOME_JUNK) {
			wrterror(d, "chunk canary corrupted %p %#tx@@%#zx",
			    ptr, p - ptr - 1, sz);
		}
	}
}

d1056 1
a1056 1
		wrterror(d, "chunk info corrupted");
d1061 14
a1074 2
		validate_canary(d, ptr, info->bits[info->offset + chunknum],
		    info->size);
d1078 1
a1078 1
		wrterror(d, "modified chunk-pointer %p", ptr);
d1081 1
a1081 1
		wrterror(d, "chunk is already free %p", ptr);
d1159 1
a1159 1
				wrterror(pool, "mprotect");
a1182 6
			} else if (mopts.chunk_canaries) {
				size_t csz = psz - mopts.malloc_guard - sz;

				if (csz > CHUNK_CHECK_LENGTH)
					csz = CHUNK_CHECK_LENGTH;
				memset(p + sz, SOME_JUNK, csz);
d1208 1
a1208 1
		wrterror(d, "recursive call");
d1267 1
a1267 1
	d->func = "malloc";
d1277 1
a1277 1
		wrterror(d, "out of memory");
d1294 1
a1294 1
		wrterror(pool, "bogus pointer in validate_junk %p", p);
d1300 1
a1300 1
			wrterror(pool, "use after free %p", p);
d1330 1
a1330 1
			wrterror(pool, "bogus pointer (double free?) %p", p);
d1338 1
a1338 5
				wrterror(pool, "bogus pointer %p", p);
			if (mopts.chunk_canaries)
				validate_canary(pool, p,
				    sz - mopts.malloc_guard,
				    PAGEROUND(sz - mopts.malloc_guard));
d1351 1
a1351 1
				wrterror(pool, "guard size");
d1356 1
a1356 1
					wrterror(pool, "mprotect");
d1374 1
a1374 1
				wrterror(pool, "double free %p", tmp);
d1385 1
a1385 2
				wrterror(pool,
				    "bogus pointer (double free?) %p", p);
d1389 1
a1389 1

d1410 1
a1410 1
		wrterror(d, "free() called before allocation");
d1412 1
a1412 1
	d->func = "free";
d1456 1
a1456 1
			wrterror(pool, "bogus pointer (double free?) %p", p);
d1468 1
a1468 1
			wrterror(pool, "guard size");
d1477 1
a1477 1
	    !mopts.chunk_canaries && !mopts.malloc_realloc) {
d1508 1
a1508 1
						wrterror(pool, "munmap %p", q);
d1516 1
a1516 1
					wrterror(pool, "mprotect");
d1520 1
a1520 1
					wrterror(pool, "mprotect");
d1581 1
a1581 1
	d->func = "realloc";
d1591 1
a1591 1
		wrterror(d, "out of memory");
d1618 1
a1618 1
	d->func = "calloc";
d1623 1
a1623 1
			wrterror(d, "out of memory");
d1639 1
a1639 1
		wrterror(d, "out of memory");
d1652 1
a1652 1
		wrterror(d, "mapalign bad alignment");
d1654 1
a1654 1
		wrterror(d, "mapalign round");
d1671 1
a1671 1
			wrterror(d, "munmap %p", p);
d1674 1
a1674 1
		wrterror(d, "munmap %p", q + sz);
d1719 1
a1719 1
			wrterror(pool, "mprotect");
d1751 1
a1751 1
	d->func = "posix_memalign";
d1761 1
a1761 1
			wrterror(d, "out of memory");
d2010 1
a2010 1
			wrterror(pool, "bogus pointer in malloc_dump %p", p);
@


1.202
log
@Wrap _malloc_init() so internal calls go directly

prodded by otto@@
ok kettenis@@ otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.201 2016/10/14 17:33:36 otto Exp $	*/
d34 1
d203 2
d267 1
a267 1
wrterror(struct dir_info *d, char *msg, void *p)
d269 18
a286 32
	char		*q = " error: ";
	struct iovec	iov[7];
	char		pidbuf[20];
	char		buf[20];
	int		saved_errno = errno, i;

	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = pidbuf;
	snprintf(pidbuf, sizeof(pidbuf), "(%d) in ", getpid());
	iov[1].iov_len = strlen(pidbuf);
	if (d != NULL) {
		iov[2].iov_base = d->func;
		iov[2].iov_len = strlen(d->func);
 	} else {
		iov[2].iov_base = "unknown";
		iov[2].iov_len = 7;
	}
	iov[3].iov_base = q;
	iov[3].iov_len = strlen(q);
	iov[4].iov_base = msg;
	iov[4].iov_len = strlen(msg);
	iov[5].iov_base = buf;
	if (p == NULL)
		iov[5].iov_len = 0;
	else {
		snprintf(buf, sizeof(buf), " %010p", p);
		iov[5].iov_len = strlen(buf);
	}
	iov[6].iov_base = "\n";
	iov[6].iov_len = 1;
	writev(STDERR_FILENO, iov, 7);
d334 1
a334 1
		wrterror(d, "munmap round", NULL);
d339 1
a339 1
			wrterror(d, "munmap", p);
d353 1
a353 1
				wrterror(d, "munmap", r->p);
d365 1
a365 1
		wrterror(d, "malloc cache underflow", NULL);
d385 1
a385 1
		wrterror(d, "malloc free slot lost", NULL);
d387 1
a387 1
		wrterror(d, "malloc cache overflow", NULL);
d402 1
a402 1
				wrterror(d, "munmap", r->p);
d421 1
a421 1
		wrterror(d, "internal struct corrupt", NULL);
d423 1
a423 1
		wrterror(d, "map round", NULL);
d478 1
a478 1
		wrterror(d, "malloc cache", NULL);
d665 1
a665 1
		wrterror(NULL, "malloc init mmap failed", NULL);
d679 1
a679 1
		wrterror(NULL, "malloc init mmap failed", NULL);
d730 1
a730 1
		wrterror(d, "munmap", d->r);
d820 1
a820 1
		wrterror(d, "internal struct corrupt", NULL);
d843 1
a843 1
		wrterror(d, "regions_total not 2^x", NULL);
d928 1
a928 1
		wrterror(d, "pp & bits", pp);
d948 1
a948 1
		wrterror(d, "internal struct corrupt", NULL);
d976 1
a976 1
		wrterror(d, "chunk info corrupted", NULL);
d1037 19
d1064 1
a1064 1
		wrterror(d, "chunk info corrupted", NULL);
d1069 2
a1070 14
		size_t sz = info->bits[info->offset + chunknum];
		size_t check_sz = info->size - sz;
		u_char *p, *q;

		if (check_sz > CHUNK_CHECK_LENGTH)
			check_sz = CHUNK_CHECK_LENGTH;
		p = (u_char *)ptr + sz;
		q = p + check_sz;

		while (p < q)
			if (*p++ != SOME_JUNK) {
				q = (void *)(sz << 16 | p - (u_char *)ptr - 1);
				wrterror(d, "chunk canary corrupted: ", q);
			}
d1074 1
a1074 1
		wrterror(d, "modified chunk-pointer", ptr);
d1077 1
a1077 1
		wrterror(d, "chunk is already free", ptr);
d1155 1
a1155 1
				wrterror(pool, "mprotect", NULL);
d1179 6
d1210 1
a1210 1
		wrterror(d, "recursive call", NULL);
d1269 1
a1269 1
	d->func = "malloc():";
d1279 1
a1279 1
		wrterror(d, "out of memory", NULL);
d1296 1
a1296 1
		wrterror(pool, "bogus pointer in validate_junk", p);
d1302 1
a1302 1
			wrterror(pool, "use after free", p);
d1332 1
a1332 1
			wrterror(pool, "bogus pointer (double free?)", p);
d1340 5
a1344 1
				wrterror(pool, "bogus pointer", p);
d1357 1
a1357 1
				wrterror(pool, "guard size", NULL);
d1362 1
a1362 1
					wrterror(pool, "mprotect", NULL);
d1380 1
a1380 1
				wrterror(pool, "double free", p);
d1391 2
a1392 1
				wrterror(pool, "bogus pointer (double free?)", p);
d1396 1
a1396 1
done:
d1417 1
a1417 1
		wrterror(d, "free() called before allocation", NULL);
d1419 1
a1419 1
	d->func = "free():";
d1463 1
a1463 1
			wrterror(pool, "bogus pointer (double free?)", p);
d1475 1
a1475 1
			wrterror(pool, "guard size", NULL);
d1484 1
a1484 1
	    !mopts.malloc_realloc) {
d1515 1
a1515 1
						wrterror(pool, "munmap", q);
d1523 1
a1523 1
					wrterror(pool, "mprotect", NULL);
d1527 1
a1527 1
					wrterror(pool, "mprotect", NULL);
d1588 1
a1588 1
	d->func = "realloc():";
d1598 1
a1598 1
		wrterror(d, "out of memory", NULL);
d1625 1
a1625 1
	d->func = "calloc():";
d1630 1
a1630 1
			wrterror(d, "out of memory", NULL);
d1646 1
a1646 1
		wrterror(d, "out of memory", NULL);
d1659 1
a1659 1
		wrterror(d, "mapalign bad alignment", NULL);
d1661 1
a1661 1
		wrterror(d, "mapalign round", NULL);
d1678 1
a1678 1
			wrterror(d, "munmap", p);
d1681 1
a1681 1
		wrterror(d, "munmap", q + sz);
d1726 1
a1726 1
			wrterror(pool, "mprotect", NULL);
d1758 1
a1758 1
	d->func = "posix_memalign():";
d1768 1
a1768 1
			wrterror(d, "out of memory", NULL);
d2017 1
a2017 1
			wrterror(pool, "bogus pointer in malloc_dump", p);
@


1.201
log
@0xd0 -> 0xdb; ok deraadt@@ millert@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.200 2016/10/12 07:36:38 otto Exp $	*/
d1252 1
@


1.200
log
@optimize canary code a bit by storing offset of sizes table instead of
recomputing it all the time
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.199 2016/10/07 05:55:37 otto Exp $	*/
d84 2
a85 2
#define SOME_JUNK		0xd0	/* as in "Duh" :-) */
#define SOME_FREEJUNK		0xdf
@


1.199
log
@stray tab
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.198 2016/10/07 05:54:35 otto Exp $	*/
d168 1
d920 1
d1030 1
a1030 1
		bp->bits[howmany(bp->total, MALLOC_BITS) + k] = argsize;
d1061 1
a1061 2
		size_t sz = info->bits[howmany(info->total, MALLOC_BITS) +
		    chunknum];
d1284 2
a1285 1
validate_junk(struct dir_info *pool, void *p) {
@


1.198
log
@Beter implementation of chunk canaries: store size in chunk meta data
instead of chunk itself; does not change actual allocated size; ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.197 2016/09/21 04:38:56 guenther Exp $	*/
d66 1
a66 1
#define	MALLOC_CHUNK_LISTS	4
@


1.197
log
@Delete casts to off_t and size_t that are implied by assignments
or prototypes.  Ditto for some of the char* and void* casts too.

verified no change to instructions on ILP32 (i386) and LP64 (amd64)
ok natano@@ abluhm@@ deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.196 2016/09/18 13:46:28 otto Exp $	*/
d67 1
d182 1
a182 1
	size_t	malloc_canaries;	/* use canaries after chunks? */
a188 1
	uintptr_t malloc_chunk_canary;
d291 1
a291 1
		snprintf(buf, sizeof(buf), " %p", p);
d515 1
a515 1
		mopts.malloc_canaries = 0;
d518 1
a518 1
		mopts.malloc_canaries = sizeof(void *);
a655 3

	arc4random_buf(&mopts.malloc_chunk_canary,
	    sizeof(mopts.malloc_chunk_canary));
d763 2
d948 1
a948 1
malloc_bytes(struct dir_info *d, size_t size, void *f)
d951 1
a951 1
	size_t		k;
d958 3
d1026 4
d1032 10
a1041 4
	if (mopts.malloc_canaries && bp->size > 0) {
		char *end = (char *)bp->page + k + bp->size;
		uintptr_t *canary = (uintptr_t *)(end - mopts.malloc_canaries);
		*canary = mopts.malloc_chunk_canary ^ hash(canary);
a1042 4

	if (mopts.malloc_junk == 2 && bp->size > 0)
		memset((char *)bp->page + k, SOME_JUNK,
		    bp->size - mopts.malloc_canaries);
d1047 1
a1047 1
find_chunknum(struct dir_info *d, struct region_info *r, void *ptr)
a1055 7
	if (mopts.malloc_canaries && info->size > 0) {
		char *end = (char *)ptr + info->size;
		uintptr_t *canary = (uintptr_t *)(end - mopts.malloc_canaries);
		if (*canary != (mopts.malloc_chunk_canary ^ hash(canary)))
			wrterror(d, "chunk canary corrupted", ptr);
	}

d1058 17
d1096 1
a1096 2
	if ((chunknum = find_chunknum(d, r, ptr)) == -1)
		return;
d1189 1
a1189 1
			memset(p, 0, sz - mopts.malloc_canaries);
a1270 2
	if (size > 0 && size <= MALLOC_MAXCHUNK)
		size += mopts.malloc_canaries;
d1293 2
a1294 4
	if (sz > 0 && sz <= MALLOC_MAXCHUNK)
		sz -= mopts.malloc_canaries;
	if (sz > 32)
		sz = 32;
a1362 2
		if (mopts.malloc_junk && sz > 0)
			memset(p, SOME_FREEJUNK, sz - mopts.malloc_canaries);
d1364 3
a1366 2
			if (find_chunknum(pool, r, p) == -1)
				goto done;
d1375 3
d1534 4
a1537 8
	if (newsz <= oldsz && newsz > oldsz / 2 && !mopts.malloc_realloc) {
		if (mopts.malloc_junk == 2 && newsz > 0) {
			size_t usable_oldsz = oldsz;
			if (oldsz <= MALLOC_MAXCHUNK)
				usable_oldsz -= mopts.malloc_canaries;
			if (newsz < usable_oldsz)
				memset((char *)p + newsz, SOME_JUNK, usable_oldsz - newsz);
		}
d1540 2
a1541 1
	} else if (newsz != oldsz || mopts.malloc_realloc) {
d1547 2
a1548 6
		if (newsz != 0 && oldsz != 0) {
			size_t copysz = oldsz < newsz ? oldsz : newsz;
			if (copysz <= MALLOC_MAXCHUNK)
				copysz -= mopts.malloc_canaries;
			memcpy(q, p, copysz);
		}
a1582 2
	if (size > 0 && size <= MALLOC_MAXCHUNK)
		size += mopts.malloc_canaries;
a1630 2
	if (size > 0 && size <= MALLOC_MAXCHUNK)
		size += mopts.malloc_canaries;
a1752 2
	if (size > 0 && size <= MALLOC_MAXCHUNK)
		size += mopts.malloc_canaries;
@


1.196
log
@move page junking tp unmap(), right before we stick the region in the cache;
ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.195 2016/09/01 10:41:02 otto Exp $	*/
d86 2
a87 2
#define MMAP(sz)	mmap(NULL, (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)
d89 2
a90 2
#define MMAPA(a,sz)	mmap((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)
d92 2
a93 2
#define MQUERY(a, sz)	mquery((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE | MAP_FIXED, -1, (off_t)0)
@


1.195
log
@Less lock contention by using more pools for mult-threaded programs.
tested by many (thanks!) ok tedu, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.192 2016/07/06 20:32:02 otto Exp $	*/
d379 5
a1342 5
		}
		if (mopts.malloc_junk && !mopts.malloc_freeunmap) {
			size_t amt = mopts.malloc_junk == 1 ? MALLOC_MAXCHUNK :
			    PAGEROUND(sz) - mopts.malloc_guard;
			memset(p, SOME_FREEJUNK, amt);
@


1.194
log
@black magic for sparc page size can go
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.193 2016/08/17 05:33:54 otto Exp $	*/
d3 1
a3 1
 * Copyright (c) 2008, 2010, 2011 Otto Moerbeek <otto@@drijf.net>
d46 1
d122 1
d172 2
a173 1
	struct dir_info *malloc_pool;	/* Main bookkeeping information */
a196 1
#define getpool() mopts.malloc_pool
d203 1
a203 1
void malloc_dump(int);
d221 1
a221 1
	if (__isthreaded) {
d223 1
a223 1
		_MALLOC_UNLOCK();
d230 2
a231 2
	if (__isthreaded) {
		_MALLOC_LOCK();
d252 11
a262 1
__dead static void
d269 1
a269 1
	int		saved_errno = errno;
d300 2
a301 1
		malloc_dump(STDERR_FILENO);
d429 1
d587 2
a588 5
/*
 * Initialize a dir_info, which should have been cleared by caller
 */
static int
omalloc_init(struct dir_info **dp)
a591 2
	size_t d_avail, regioninfo_size;
	struct dir_info *d;
d654 12
d673 1
a673 1
		return -1;
d685 2
a686 1
	if (d->r == MAP_FAILED)
d688 1
a688 1

a698 9

	/*
	 * Options have been set and will never be reset.
	 * Prevent further tampering with them.
	 */
	if (((uintptr_t)&malloc_readonly & MALLOC_PAGEMASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly), PROT_READ);

	return 0;
d1188 1
a1188 1
	_MALLOC_UNLOCK();
d1192 2
a1193 2
static int
malloc_init(void)
d1195 21
a1215 6
	if (omalloc_init(&mopts.malloc_pool)) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror(NULL, "out of memory", NULL);
		errno = ENOMEM;
		return -1;
d1217 11
a1227 1
	return 0;
a1236 1
	_MALLOC_LOCK();
d1239 1
a1239 2
		if (malloc_init() != 0)
			return NULL;
d1242 1
d1253 1
a1253 1
	_MALLOC_UNLOCK();
d1284 1
a1284 1
ofree(struct dir_info *pool, void *p)
d1286 1
d1289 1
d1291 1
d1293 19
a1311 2
	if (r == NULL)
		wrterror(pool, "bogus pointer (double free?)", p);
d1354 1
a1354 1
				return;
d1371 7
a1389 1
	_MALLOC_LOCK();
d1391 1
a1391 2
	if (d == NULL) {
		_MALLOC_UNLOCK();
d1393 1
a1393 1
	}
d1401 1
a1401 1
	_MALLOC_UNLOCK();
d1408 1
a1408 1
orealloc(struct dir_info *pool, void *p, size_t newsz, void *f)
d1410 1
d1413 4
a1416 1
	void *q;
d1422 18
a1439 2
	if (r == NULL)
		wrterror(pool, "bogus pointer (double free?)", p);
d1442 2
a1443 1
		return NULL;
d1486 2
a1487 1
					return p;
d1507 2
a1508 1
			return p;
d1515 2
a1516 1
			return p;
d1528 1
a1528 1
		return p;
d1531 4
a1534 2
		if (q == NULL)
			return NULL;
d1542 1
a1542 1
		return q;
d1545 8
a1552 1
		return p;
d1554 1
a1563 1
	_MALLOC_LOCK();
d1566 1
a1566 2
		if (malloc_init() != 0)
			return NULL;
d1569 1
d1580 1
a1580 1
	_MALLOC_UNLOCK();
a1602 1
	_MALLOC_LOCK();
d1605 1
a1605 2
		if (malloc_init() != 0)
			return NULL;
d1608 1
d1612 1
a1612 1
		_MALLOC_UNLOCK();
d1630 1
a1630 1
	_MALLOC_UNLOCK();
a1737 1
	_MALLOC_LOCK();
d1740 1
a1740 2
		if (malloc_init() != 0)
			goto err;
d1743 1
d1753 1
a1753 1
	_MALLOC_UNLOCK();
d1799 1
a1799 1
	if (cnt == 0)
d1990 1
a1990 1
malloc_dump(int fd)
a1991 1
	struct dir_info *pool = getpool();
d2020 1
a2020 1
	int save_errno = errno, fd;
d2024 2
a2025 1
		malloc_dump(fd);
@


1.193
log
@wrterror() is fatal, delete dead code; ok tom@@ natano@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.192 2016/07/06 20:32:02 otto Exp $	*/
d47 1
a47 3
#if defined(__sparc__) && !defined(__sparcv9__)
#define MALLOC_PAGESHIFT	(13U)
#elif defined(__mips64__)
@


1.192
log
@J/j is a three valued option, document and fix code to actuall support that
with a little help from jmc@@ for the man page bits
ok jca@@  and a reluctant tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.191 2016/06/30 09:00:48 otto Exp $	*/
d252 1
a252 1
static void
d332 1
a332 1
	if (sz != PAGEROUND(sz)) {
a333 2
		return;
	}
d416 1
a416 1
	if (sz != PAGEROUND(sz)) {
a417 2
		return MAP_FAILED;
	}
d666 1
a666 1
	if (d->r == MAP_FAILED) {
d668 1
a668 3
		d->regions_total = 0;
		return 1;
	}
d1042 1
a1042 1
	if ((uintptr_t)ptr & ((1U << (info->shift)) - 1)) {
a1043 2
		return -1;
	}
d1045 1
a1045 1
	    (1U << (chunknum % MALLOC_BITS))) {
a1046 2
		return -1;
	}
d1219 1
a1219 1
	if (r == NULL && mopts.malloc_xmalloc) {
a1220 2
		errno = ENOMEM;
	}
d1235 1
a1235 1
	if (r == NULL) {
a1236 2
		return;
	}
d1243 1
a1243 1
		if (((unsigned char *)p)[byte] != SOME_FREEJUNK) {
a1244 2
			return;
		}
d1255 1
a1255 1
	if (r == NULL) {
a1256 2
		return;
	}
d1261 1
a1261 1
			if (r->p != p) {
a1262 2
				return;
			}
d1303 1
a1303 1
			if (tmp == p) {
a1304 2
				return;
			}
d1311 1
a1311 1
			if (r == NULL) {
a1312 2
				return;
			}
a1332 1
		return;
d1358 1
a1358 1
	if (r == NULL) {
a1359 2
		return NULL;
	}
d1488 1
a1488 1
	if (r == NULL && mopts.malloc_xmalloc) {
a1489 2
		errno = ENOMEM;
	}
d1539 1
a1539 1
	if (r == NULL && mopts.malloc_xmalloc) {
a1540 2
		errno = ENOMEM;
	}
d1552 1
a1552 1
	if (alignment < MALLOC_PAGESIZE || ((alignment - 1) & alignment) != 0) {
d1554 1
a1554 3
		return MAP_FAILED;
	}
	if (sz != PAGEROUND(sz)) {
a1555 2
		return MAP_FAILED;
	}
d1664 1
a1664 1
		if (mopts.malloc_xmalloc) {
a1665 2
			errno = ENOMEM;
		}
d1914 1
a1914 1
		if (r == NULL) {
a1915 2
			continue;
		}
@


1.191
log
@adapt S option: add C, rm F (not relevant with 0 cache and disables
chunk rnd), rm P: is default
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.190 2016/06/28 06:40:11 tb Exp $	*/
d536 2
a537 1
		mopts.malloc_junk = 0;
d540 2
a541 1
		mopts.malloc_junk = 2;
@


1.190
log
@Back out previous; otto saw a potential race that could lead to a
double unmap and I experienced a much more unstable firefox.

discussed with otto on icb
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.189 2016/06/27 15:33:40 tedu Exp $	*/
d620 1
a620 1
				for (q = "FGJP"; *q != '\0'; q++)
d625 1
a625 1
				for (q = "fgj"; *q != '\0'; q++)
@


1.189
log
@defer munmap to after unlocking malloc. this can (unfortunately) be an
expensive syscall, and we don't want to tie up other threads. there's no
need to hold the lock, so defer it to afterwards.
from Michael McConville
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.188 2016/04/12 18:14:02 otto Exp $	*/
a124 2
	void *unmap_me;
	size_t unmap_me_sz;
a220 3
	void *unmap_me = d->unmap_me;
	size_t unmap_me_sz = d->unmap_me_sz;

a224 3

	if (unmap_me != NULL)
		munmap(unmap_me, unmap_me_sz);
a233 2
	d->unmap_me = NULL;
	d->unmap_me_sz = 0;
a297 10
static inline void
_MUNMAP(struct dir_info *d, void *p, size_t sz)
{
	if (d->unmap_me == NULL) {
		d->unmap_me = p;
		d->unmap_me_sz = sz;
	} else if (munmap(p, sz) == -1)
		wrterror(d, "munmap", p);
}

d338 3
a340 1
		_MUNMAP(d, p, sz);
d353 2
a354 1
			_MUNMAP(d, r->p, rsz);
d397 2
a398 1
			_MUNMAP(d, r->p, rsz);
d730 5
a734 3
	_MUNMAP(d, d->r, d->regions_total * sizeof(struct region_info));
	STATS_SUB(d->malloc_used,
	    d->regions_total * sizeof(struct region_info));
d1431 4
a1434 2
				} else if (q != MAP_FAILED)
					_MUNMAP(pool, q, needed);
d1603 6
a1608 3
	if (q != p)
		_MUNMAP(d, p, q - p);
	_MUNMAP(d, q + sz, alignment - (q - p));
@


1.188
log
@two times a define to an inline function, from Michael McConville; ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.187 2016/04/09 14:08:40 otto Exp $	*/
d125 2
d223 3
d230 3
d242 2
d308 10
d358 1
a358 3
		i = munmap(p, sz);
		if (i)
			wrterror(d, "munmap", p);
d371 1
a371 2
			if (munmap(r->p, rsz))
				wrterror(d, "munmap", r->p);
d414 1
a414 2
			if (munmap(r->p, rsz))
				wrterror(d, "munmap", r->p);
d746 3
a748 5
	if (munmap(d->r, d->regions_total * sizeof(struct region_info)))
		wrterror(d, "munmap", d->r);
	else
		STATS_SUB(d->malloc_used,
		    d->regions_total * sizeof(struct region_info));
d1445 2
a1446 4
				} else if (q != MAP_FAILED) {
					if (munmap(q, needed))
						wrterror(pool, "munmap", q);
				}
d1615 3
a1617 6
	if (q != p) {
		if (munmap(p, q - p))
			wrterror(d, "munmap", p);
	}
	if (munmap(q + sz, alignment - (q - p)))
		wrterror(d, "munmap", q + sz);
@


1.187
log
@tweak MALLOC_STATS printing (switched off by default), prodded by
Michael McConville
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.186 2016/04/09 12:23:59 otto Exp $	*/
a95 9
#define _MALLOC_LEAVE(d) do { if (__isthreaded) { \
	(d)->active--; \
	_MALLOC_UNLOCK(); } \
} while (0)
#define _MALLOC_ENTER(d) do { if (__isthreaded) { \
	_MALLOC_LOCK(); \
	(d)->active++; } \
} while (0)

d217 18
@


1.186
log
@redundant memset(3), from Michael McConville, ok armani@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.185 2016/03/17 17:55:33 mmcc Exp $	*/
d175 1
a175 1
	u_short total;			/* how many chunk */
d1761 6
d1773 2
a1774 4
	snprintf(buf, sizeof(buf), "Leak report\n");
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "                 f     sum      #    avg\n");
	write(fd, buf, strlen(buf));
d1815 2
a1816 4
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "        ");
			write(fd, buf, strlen(buf));
		}
d1827 1
a1827 2
	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
d1894 1
a1894 1
	snprintf(buf, sizeof(buf),
a1895 1
	write(fd, buf, strlen(buf));
@


1.185
log
@properly guard to macros

ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.184 2016/03/14 17:20:26 otto Exp $	*/
a705 1
	memset(p, 0, newsize);
@


1.184
log
@small step towards multiple pools: move two globls into the struct dir_info
ok @@stefan armani@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.183 2016/03/13 18:34:21 guenther Exp $	*/
d96 1
a96 1
#define _MALLOC_LEAVE(d) if (__isthreaded) do { \
d98 1
a98 1
	_MALLOC_UNLOCK(); \
d100 1
a100 1
#define _MALLOC_ENTER(d) if (__isthreaded) do { \
d102 1
a102 1
	(d)->active++; \
@


1.183
log
@environ and __progname are not declared in a public header; declare them
in libc's hidden/stdlib.h instead of in each .c file that needs one

ok deraadt@@ gsoares@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.182 2016/02/25 00:38:51 deraadt Exp $	*/
d96 2
a97 2
#define _MALLOC_LEAVE() if (__isthreaded) do { \
	malloc_active--; \
d100 1
a100 1
#define _MALLOC_ENTER() if (__isthreaded) do { \
d102 1
a102 1
	malloc_active++; \
d117 1
d131 1
a207 2
static char	*malloc_func;		/* current function */
static int	malloc_active;		/* status of malloc */
d244 1
a244 1
wrterror(char *msg, void *p)
d257 7
a263 2
	iov[2].iov_base = malloc_func;
	iov[2].iov_len = strlen(malloc_func);
d324 1
a324 1
		wrterror("munmap round", NULL);
d331 1
a331 1
			wrterror("munmap", p);
d345 1
a345 1
				wrterror("munmap", r->p);
d357 1
a357 1
		wrterror("malloc cache underflow", NULL);
d372 1
a372 1
		wrterror("malloc free slot lost", NULL);
d374 1
a374 1
		wrterror("malloc cache overflow", NULL);
d389 1
a389 1
				wrterror("munmap", r->p);
d408 1
a408 1
		wrterror("internal struct corrupt", NULL);
d410 1
a410 1
		wrterror("map round", NULL);
d414 1
a414 1
		_MALLOC_LEAVE();
d416 1
a416 1
		_MALLOC_ENTER();
d466 2
a467 2
		wrterror("malloc cache", NULL);
	_MALLOC_LEAVE();
d469 1
a469 1
	_MALLOC_ENTER();
d660 1
a660 1
		wrterror("malloc init mmap failed", NULL);
d723 1
a723 1
		wrterror("munmap", d->r);
d811 1
a811 1
		wrterror("internal struct corrupt", NULL);
d834 1
a834 1
		wrterror("regions_total not 2^x", NULL);
d836 1
a836 1
	STATS_INC(getpool()->deletes);
d852 1
a852 1
			STATS_INC(getpool()->delete_moves);
d918 1
a918 1
		wrterror("pp & bits", pp);
d938 1
a938 1
		wrterror("internal struct corrupt", NULL);
d963 1
a963 1
		wrterror("chunk info corrupted", NULL);
d1026 1
a1026 1
		wrterror("chunk info corrupted", NULL);
d1032 1
a1032 1
			wrterror("chunk canary corrupted", ptr);
d1039 1
a1039 1
		wrterror("modified chunk-pointer", ptr);
d1044 1
a1044 1
		wrterror("chunk is already free", ptr);
d1100 1
a1100 1
omalloc(size_t sz, int zero_fill, void *f)
a1101 1
	struct dir_info *pool = getpool();
d1125 1
a1125 1
				wrterror("mprotect", NULL);
d1168 1
a1168 1
malloc_recurse(void)
d1174 1
a1174 1
		wrterror("recursive call", NULL);
d1176 1
a1176 1
	malloc_active--;
d1187 1
a1187 1
			wrterror("out of memory", NULL);
d1198 1
d1202 2
a1203 2
	malloc_func = "malloc():";
	if (getpool() == NULL) {
d1206 1
d1208 1
d1210 2
a1211 2
	if (malloc_active++) {
		malloc_recurse();
d1216 2
a1217 2
	r = omalloc(size, 0, CALLER);
	malloc_active--;
d1220 1
a1220 1
		wrterror("out of memory", NULL);
d1230 1
a1230 1
validate_junk(void *p) {
a1231 1
	struct dir_info *pool = getpool();
d1238 1
a1238 1
		wrterror("bogus pointer in validate_junk", p);
d1248 1
a1248 1
			wrterror("use after free", p);
d1255 1
a1255 1
ofree(void *p)
a1256 1
	struct dir_info *pool = getpool();
d1262 1
a1262 1
		wrterror("bogus pointer (double free?)", p);
d1270 1
a1270 1
				wrterror("bogus pointer", p);
d1285 1
a1285 1
				wrterror("guard size", NULL);
d1290 1
a1290 1
					wrterror("mprotect", NULL);
d1314 1
a1314 1
				wrterror("double free", p);
d1318 1
a1318 1
				validate_junk(p);
d1324 1
a1324 1
				wrterror("bogus pointer (double free?)", p);
d1335 1
d1343 2
a1344 2
	malloc_func = "free():";
	if (getpool() == NULL) {
d1346 1
a1346 1
		wrterror("free() called before allocation", NULL);
d1349 3
a1351 2
	if (malloc_active++) {
		malloc_recurse();
d1354 2
a1355 2
	ofree(ptr);
	malloc_active--;
d1363 1
a1363 1
orealloc(void *p, size_t newsz, void *f)
a1364 1
	struct dir_info *pool = getpool();
d1370 1
a1370 1
		return omalloc(newsz, 0, f);
d1374 1
a1374 1
		wrterror("bogus pointer (double free?)", p);
d1386 1
a1386 1
			wrterror("guard size", NULL);
d1425 1
a1425 1
						wrterror("munmap", q);
d1433 1
a1433 1
					wrterror("mprotect", NULL);
d1437 1
a1437 1
					wrterror("mprotect", NULL);
d1463 1
a1463 1
		q = omalloc(newsz, 0, f);
d1472 1
a1472 1
		ofree(p);
d1483 1
d1488 2
a1489 2
	malloc_func = "realloc():";
	if (getpool() == NULL) {
d1492 1
d1494 3
a1496 2
	if (malloc_active++) {
		malloc_recurse();
d1501 1
a1501 1
	r = orealloc(ptr, size, CALLER);
d1503 1
a1503 1
	malloc_active--;
d1506 1
a1506 1
		wrterror("out of memory", NULL);
d1525 1
d1530 2
a1531 2
	malloc_func = "calloc():";
	if (getpool() == NULL) {
d1534 1
d1536 1
d1541 1
a1541 1
			wrterror("out of memory", NULL);
d1546 2
a1547 2
	if (malloc_active++) {
		malloc_recurse();
d1554 1
a1554 1
	r = omalloc(size, 1, CALLER);
d1556 1
a1556 1
	malloc_active--;
d1559 1
a1559 1
		wrterror("out of memory", NULL);
d1574 1
a1574 1
		wrterror("mapalign bad alignment", NULL);
d1578 1
a1578 1
		wrterror("mapalign round", NULL);
d1597 1
a1597 1
			wrterror("munmap", p);
d1600 1
a1600 1
		wrterror("munmap", q + sz);
d1607 1
a1607 1
omemalign(size_t alignment, size_t sz, int zero_fill, void *f)
a1608 1
	struct dir_info *pool = getpool();
d1619 1
a1619 1
		return omalloc(sz, zero_fill, f);
d1645 1
a1645 1
			wrterror("mprotect", NULL);
d1663 1
d1672 2
a1673 2
	malloc_func = "posix_memalign():";
	if (getpool() == NULL) {
d1676 1
d1678 3
a1680 2
	if (malloc_active++) {
		malloc_recurse();
d1685 2
a1686 2
	r = omemalign(alignment, size, 0, CALLER);
	malloc_active--;
d1690 1
a1690 1
			wrterror("out of memory", NULL);
d1942 1
a1942 1
			wrterror("bogus pointer in malloc_dump", p);
@


1.182
log
@refactor option letter parsing into a subfunction, to increase clarity
about which options are turned on/off by 's' and 'S'
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.181 2016/01/26 15:44:28 otto Exp $	*/
a209 2

extern char	*__progname;
@


1.181
log
@Don't crash dumping malloc stats if malloc_init hasn't been called, noted by
David CARLIER
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.180 2016/01/06 17:57:22 tedu Exp $	*/
d473 92
d571 1
a571 1
	char *p, b[64];
d607 4
a610 66
			case '>':
				mopts.malloc_cache <<= 1;
				if (mopts.malloc_cache > MALLOC_MAXCACHE)
					mopts.malloc_cache = MALLOC_MAXCACHE;
				break;
			case '<':
				mopts.malloc_cache >>= 1;
				break;
			case 'a':
			case 'A':
				/* ignored */
				break;
			case 'c':
				mopts.malloc_canaries = 0;
				break;
			case 'C':
				mopts.malloc_canaries = sizeof(void *);
				break;
#ifdef MALLOC_STATS
			case 'd':
				mopts.malloc_stats = 0;
				break;
			case 'D':
				mopts.malloc_stats = 1;
				break;
#endif /* MALLOC_STATS */
			case 'f':
				mopts.malloc_freenow = 0;
				mopts.malloc_freeunmap = 0;
				break;
			case 'F':
				mopts.malloc_freenow = 1;
				mopts.malloc_freeunmap = 1;
				break;
			case 'g':
				mopts.malloc_guard = 0;
				break;
			case 'G':
				mopts.malloc_guard = MALLOC_PAGESIZE;
				break;
			case 'h':
				mopts.malloc_hint = 0;
				break;
			case 'H':
				mopts.malloc_hint = 1;
				break;
			case 'j':
				mopts.malloc_junk = 0;
				break;
			case 'J':
				mopts.malloc_junk = 2;
				break;
			case 'n':
			case 'N':
				break;
			case 'p':
				mopts.malloc_move = 0;
				break;
			case 'P':
				mopts.malloc_move = 1;
				break;
			case 'r':
				mopts.malloc_realloc = 0;
				break;
			case 'R':
				mopts.malloc_realloc = 1;
d613 2
a614 2
				mopts.malloc_freeunmap = mopts.malloc_junk = 0;
				mopts.malloc_guard = 0;
d617 2
a618 22
			case 'S':
				mopts.malloc_freeunmap = 1;
				mopts.malloc_junk = 2;
				mopts.malloc_guard = MALLOC_PAGESIZE;
				mopts.malloc_cache = 0;
				break;
			case 'u':
				mopts.malloc_freeunmap = 0;
				break;
			case 'U':
				mopts.malloc_freeunmap = 1;
				break;
			case 'x':
				mopts.malloc_xmalloc = 0;
				break;
			case 'X':
				mopts.malloc_xmalloc = 1;
				break;
			default: {
				static const char q[] = "malloc() warning: "
				    "unknown char in MALLOC_OPTIONS\n";
				write(STDERR_FILENO, q, sizeof(q) - 1);
a619 1
			}
@


1.180
log
@Long ago, malloc internally had two kinds of failures, warnings and errors.
The 'A' option elevated warnings to errors, and has been the default for some
time. Then warnings were effectively eliminated in favor of everything
being an error, but then the 'a' flag turned real errors into warnings!
Remove the 'a' option entirely. You shouldn't have used it anyway.
ok tb tdeval
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.179 2015/12/30 06:04:39 tedu Exp $	*/
d1913 2
@


1.179
log
@another case where bad things would happen after wrterror
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.178 2015/12/30 06:01:18 tedu Exp $	*/
a179 1
	int	malloc_abort;		/* abort() on error */
d282 2
a283 2
	if (mopts.malloc_abort)
		abort();
a486 1
	mopts.malloc_abort = 1;
a523 2
				mopts.malloc_abort = 0;
				break;
d525 1
a525 1
				mopts.malloc_abort = 1;
@


1.178
log
@if somebody makes the mistake of disabling abort, don't deref null in
validate_junk. from Michal Mazurek
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.177 2015/12/09 02:45:23 tedu Exp $	*/
d1922 1
a1922 1
		if (r == NULL)
d1924 2
@


1.177
log
@Integrate two patches originally from Daniel Micay.
1. Optionally add random "canaries" to the end of an allocation. This
requires increasing the internal size of the allocation slightly, which
probably results in a large effective increase with current power of two
sizing. Therefore, this option is only enabled via 'C'.
2. When writing junk (0xdf) to freed chunks (current default behavior),
check that the junk is still intact when finally freeing the delayed chunk
to catch some potential use after free. This should be pretty cheap so
there's no option to control it separately.
ok deraadt tb
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.176 2015/09/13 20:29:23 guenther Exp $	*/
d1228 1
a1228 1
	if (r == NULL)
d1230 2
@


1.176
log
@For now, permit overriding of the malloc family, to make emacs happy
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.175 2015/09/13 08:31:47 guenther Exp $	*/
d188 1
d195 1
d531 6
d630 3
d998 6
d1005 2
a1006 1
		memset((char *)bp->page + k, SOME_JUNK, bp->size);
d1020 7
d1149 1
a1149 1
			memset(p, 0, sz);
d1204 2
d1220 24
d1296 1
a1296 1
			memset(p, SOME_FREEJUNK, sz);
d1307 2
d1442 7
a1448 2
		if (mopts.malloc_junk == 2 && newsz > 0)
			memset((char *)p + newsz, SOME_JUNK, oldsz - newsz);
d1455 6
a1460 2
		if (newsz != 0 && oldsz != 0)
			memcpy(q, p, oldsz < newsz ? oldsz : newsz);
d1485 2
d1535 2
d1664 2
@


1.175
log
@Wrap <stdlib.h> so that calls go direct and the symbols not in the
C standard are all weak.
Apply __{BEGIN,END}_HIDDEN_DECLS to gdtoa{,imp}.h, hiding the
arch-specific __strtorx, __ULtox_D2A, __strtorQ, __ULtoQ_D2A symbols.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.174 2015/04/06 09:18:51 tedu Exp $	*/
d1190 1
a1190 1
DEF_STRONG(malloc);
d1294 1
a1294 1
DEF_STRONG(free);
d1435 1
a1435 1
DEF_STRONG(realloc);
d1483 1
a1483 1
DEF_STRONG(calloc);
d1617 1
a1617 1
DEF_STRONG(posix_memalign);
@


1.174
log
@improve realloc. when expanding a region, actually use the free page cache
instead of simply zapping it. this can save many syscalls in a program
that repeatedly grows and shrinks a buffer, as observed in the wild.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.173 2015/01/16 16:48:51 deraadt Exp $	*/
d214 1
d1190 1
d1294 1
d1435 1
d1483 1
d1617 1
d1862 1
@


1.173
log
@Move to the <limits.h> universe.
review by millert, binary checking process with doug, concept with guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.172 2015/01/05 21:04:04 tedu Exp $	*/
d394 1
a394 1
map(struct dir_info *d, size_t sz, int zero_fill)
d408 1
a408 1
	if (psz > d->free_regions_size) {
d421 2
d444 2
a445 1
		p = (char *)r->p + ((r->size - psz) << MALLOC_PAGESHIFT);
d458 2
d850 1
a850 1
	pp = map(d, MALLOC_PAGESIZE, 0);
d1079 1
a1079 1
		p = map(pool, psz, zero_fill);
d1338 3
d1348 1
d1503 1
a1503 1
	p = map(d, sz + alignment, zero_fill);
@


1.172
log
@rename kern enter/exit macros to malloc enter/leave to better reflect
what's going on.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.171 2014/08/18 14:34:58 tedu Exp $	*/
d29 1
a29 1
#include <sys/param.h>
@


1.171
log
@a small tweak to improve malloc in multithreaded programs. we don't need
to hold the malloc lock across mmap syscalls in all cases. dropping it
allows another thread to access the existing chunk cache if necessary.
could be improved to be a bit more aggressive, but i've been testing this
simple diff for some time now with good results.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.170 2014/07/09 19:11:00 tedu Exp $	*/
d96 1
a96 1
#define KERNENTER() if (__isthreaded) do { \
d100 1
a100 1
#define KERNEXIT() if (__isthreaded) do { \
d409 1
a409 1
		KERNENTER();
d411 1
a411 1
		KERNEXIT();
d457 1
a457 1
	KERNENTER();
d459 1
a459 1
	KERNEXIT();
@


1.170
log
@reduce obvious dependency on global g_pool by moving to local aliases
ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.169 2014/06/27 18:17:03 deraadt Exp $	*/
d96 9
d324 2
a325 1
		if (munmap(p, sz))
d409 1
d411 1
d423 3
a429 3
				r->p = NULL;
				r->size = 0;
				d->free_regions_size -= psz;
d455 3
d459 1
a461 2
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror("malloc cache", NULL);
@


1.169
log
@extra evil spaces snuck in over the last while
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.168 2014/06/27 17:37:42 otto Exp $	*/
d170 1
a170 1
	struct dir_info *g_pool;	/* Main bookkeeping information */
d184 1
a184 1
	u_int32_t malloc_canary;	/* Matched against ones in g_pool */
d193 1
a193 1
#define g_pool	mopts.g_pool
d797 1
a797 1
	STATS_INC(g_pool->deletes);
d813 1
a813 1
			STATS_INC(g_pool->delete_moves);
d1049 1
d1060 1
a1060 1
		p = map(g_pool, psz, zero_fill);
d1065 2
a1066 2
		if (insert(g_pool, p, sz, f)) {
			unmap(g_pool, p, psz);
d1074 1
a1074 1
			STATS_ADD(g_pool->malloc_guarded, mopts.malloc_guard);
d1102 1
a1102 1
		p = malloc_bytes(g_pool, sz, f);
d1132 1
a1132 1
	if (omalloc_init(&g_pool)) {
d1150 1
a1150 1
	if (g_pool == NULL) {
d1154 1
d1174 1
d1178 1
a1178 1
	r = find(g_pool, p);
d1210 1
a1210 1
			STATS_SUB(g_pool->malloc_guarded, mopts.malloc_guard);
d1217 2
a1218 2
		unmap(g_pool, p, PAGEROUND(sz));
		delete(g_pool, r);
d1226 1
a1226 1
			if (find_chunknum(g_pool, r, p) == -1)
d1228 1
a1228 1
			i = getrbyte(g_pool) & MALLOC_DELAYED_CHUNK_MASK;
d1230 1
a1230 1
			p = g_pool->delayed_chunks[i];
d1235 1
a1235 1
			g_pool->delayed_chunks[i] = tmp;
d1238 1
a1238 1
			r = find(g_pool, p);
d1243 1
a1243 1
			free_bytes(g_pool, r, p);
d1259 1
a1259 1
	if (g_pool == NULL) {
d1278 1
d1286 1
a1286 1
	r = find(g_pool, p);
d1318 2
a1319 2
				STATS_INC(g_pool->cheap_realloc_tries);
				zapcacheregion(g_pool, hint, needed);
d1326 1
a1326 1
					STATS_ADD(g_pool->malloc_used, needed);
d1331 1
a1331 1
					STATS_INC(g_pool->cheap_reallocs);
d1349 1
a1349 1
			unmap(g_pool, (char *)p + rnewsz, roldsz - rnewsz);
d1389 1
a1389 1
	if (g_pool == NULL) {
d1425 1
a1425 1
	if (g_pool == NULL) {
d1498 1
d1520 1
a1520 1
	p = mapalign(g_pool, alignment, psz, zero_fill);
d1526 2
a1527 2
	if (insert(g_pool, p, sz, f)) {
		unmap(g_pool, p, psz);
d1536 1
a1536 1
		STATS_ADD(g_pool->malloc_guarded, mopts.malloc_guard);
d1562 1
a1562 1
	if (g_pool == NULL) {
d1812 1
d1819 1
a1819 1
		p = g_pool->delayed_chunks[i];
d1822 1
a1822 1
		r = find(g_pool, p);
d1825 2
a1826 2
		free_bytes(g_pool, r, p);
		g_pool->delayed_chunks[i] = NULL;
d1830 1
a1830 1
	malloc_dump1(fd, g_pool);
@


1.168
log
@Move to a smaller rbytes buffer and skip a random part. Not to
improve the random stream itself (it doesn't), but to introduce
noise in the arc4random calling pattern. Thanks to matthew@@ who
pointed out bias in a previous diff, ok deraadt@@ matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.167 2014/06/02 08:49:38 otto Exp $	*/
d214 1
a214 1
#define REALSIZE(sz, r) 					\
d665 1
a665 1
	
d670 1
a670 1
	for (i = 0; i < d->regions_total; i++) { 
d683 1
a683 1
	if (munmap(d->r, d->regions_total * sizeof(struct region_info))) 
d730 1
a730 1
/* 
d819 1
a819 1
 
d936 1
a936 1
				i &= ~(MALLOC_BITS - 1); 
d1114 1
a1114 1
static void  
d1255 1
a1255 1
	malloc_func = "free():";  
d1382 1
a1382 1
  
d1384 1
a1384 1
	malloc_func = "realloc():";  
d1394 1
a1394 1
  
d1420 1
a1420 1
	malloc_func = "calloc():";  
d1441 1
a1441 1
  
d1681 1
a1681 1
			if (p->bits[0] & 1) 
d1766 1
a1766 1
	     d->delete_moves);
d1787 1
a1787 1
				    "pages %12p %12p %zu\n", d->r[i].p, 
d1792 1
a1792 1
				    (struct chunk_info *)d->r[i].size, 
d1817 1
a1817 1
		if (r == NULL) 
@


1.167
log
@move random bytes buffer to be part of mmaped pages; ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.166 2014/05/26 06:19:07 otto Exp $	*/
d121 1
a121 1
	u_char rbytes[512];		/* random bytes */
d279 2
a280 1
	d->rbytesused = 0;
@


1.166
log
@move all stats collecting under MALLOC_STATS; ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.165 2014/05/21 15:47:51 otto Exp $	*/
d120 2
a195 1

d199 1
a199 4

static size_t rbytesused;		/* random bytes used */
static u_char rbytes[512];		/* random bytes */
static u_char getrbyte(void);
d276 1
a276 1
rbytes_init(void)
d278 2
a279 2
	arc4random_buf(rbytes, sizeof(rbytes));
	rbytesused = 0;
d283 1
a283 1
getrbyte(void)
d287 3
a289 3
	if (rbytesused >= sizeof(rbytes))
		rbytes_init();
	x = rbytes[rbytesused++];
d323 1
a323 1
	offset = getrbyte();
d404 1
a404 1
	offset = getrbyte();
a461 2
	rbytes_init();

d615 1
d914 1
a914 1
	listnum = getrbyte() % MALLOC_CHUNK_LISTS;
d927 1
a927 1
		i += getrbyte();
d1016 1
a1016 1
		listnum = getrbyte() % MALLOC_CHUNK_LISTS;
d1224 1
a1224 1
			i = getrbyte() & MALLOC_DELAYED_CHUNK_MASK;
@


1.165
log
@fix MALLOC_STATS (not compiled in by default); ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.164 2014/05/18 17:49:47 tedu Exp $	*/
d130 7
a136 3
#define STATS_INC(x) ((x)++)
#define STATS_ZERO(x) ((x) = 0)
#define STATS_SETF(x,y) ((x)->f = (y))
d138 2
a197 2
static size_t	malloc_guarded;		/* bytes used for guards */
static size_t	malloc_used;		/* bytes allocated */
d248 1
a248 1
	snprintf(pidbuf, sizeof(pidbuf), "(%d)", getpid());
d318 1
a318 1
		malloc_used -= sz;
d339 1
a339 1
			malloc_used -= rsz;
d379 1
a379 1
			malloc_used -= rsz;
d402 1
a402 1
			malloc_used += sz;
d446 1
a446 1
		malloc_used += sz;
d632 1
a632 1
	malloc_used += regioninfo_size;
d668 1
a668 1
	malloc_used += newsize;
d688 2
a689 1
		malloc_used -= d->regions_total * sizeof(struct region_info);
d718 1
a718 1
		malloc_used += MALLOC_PAGESIZE;
d1075 1
a1075 1
			malloc_guarded += mopts.malloc_guard;
d1150 1
a1150 1
	malloc_func = " in malloc():";
d1209 1
a1209 1
			malloc_guarded -= mopts.malloc_guard;
d1257 1
a1257 1
	malloc_func = " in free():";  
d1324 1
a1324 1
					malloc_used += needed;
d1386 1
a1386 1
	malloc_func = " in realloc():";  
d1422 1
a1422 1
	malloc_func = " in calloc():";  
d1488 1
a1488 1
	malloc_used -= alignment;
d1533 1
a1533 1
		malloc_guarded += mopts.malloc_guard;
d1558 1
a1558 1
	malloc_func = " in posix_memalign():";
d1798 1
a1798 1
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
d1800 1
a1800 1
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
@


1.164
log
@factor out a bit of the chunk index code and use it to make sure that a
freed chunk is actually freeable immediately. catch more errors.
hints/ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.163 2014/05/12 19:02:20 tedu Exp $	*/
d1646 1
a1646 1
	snprintf(buf, sizeof(buf), "           f     sum      #    avg\n");
d1654 1
a1654 1
		snprintf(buf, sizeof(buf), "%12p %7zu %6u %6zu\n", p->d.f,
d1673 1
a1673 1
		snprintf(buf, sizeof(buf), "chunk %12p %12p %4d %d/%d\n",
d1699 2
a1700 1
	int i, count;
a1704 2
		struct chunk_info *p;

d1708 11
a1718 9
		p = LIST_FIRST(&d->chunk_dir[i]);
		if (p == NULL && count == 0)
			continue;
		snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
		write(fd, buf, strlen(buf));
		if (p != NULL)
			dump_chunk(fd, p, NULL, 1);
		else
			write(fd, "\n", 1);
d1746 1
a1746 1
	char buf[64];
d1771 1
a1771 1
	    "slot)  hash d  type         page            f size [free/n]\n");
@


1.163
log
@change to having four freelists per size, to reduce another source of
deterministic behavior. four selected because it's more than three, less
than five. i.e., no particular reason.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.162 2014/05/10 18:14:55 otto Exp $	*/
d969 2
a970 6

/*
 * Free a chunk, and possibly the page it's on, if the page becomes empty.
 */
static void
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
a971 1
	struct chunk_head *mp;
d973 1
a973 1
	int i, listnum;
d980 1
a980 1
	i = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;
d984 1
a984 1
		return;
d986 2
a987 1
	if (info->bits[i / MALLOC_BITS] & (1U << (i % MALLOC_BITS))) {
d989 18
a1007 1
	}
d1009 1
a1009 1
	info->bits[i / MALLOC_BITS] |= 1U << (i % MALLOC_BITS);
d1220 2
d1225 4
@


1.162
log
@fix MALLOC_STATS code that was broken in rev 1.159, not compiled in by default
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.161 2014/05/08 21:43:49 deraadt Exp $	*/
d67 1
d114 1
a114 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1];
d625 2
a626 1
		LIST_INIT(&d->chunk_dir[i]);
d821 1
a821 1
omalloc_make_chunks(struct dir_info *d, int bits)
d872 1
a872 1
	LIST_INSERT_HEAD(&d->chunk_dir[bits], bp, entries);
d889 1
a889 1
	int		i, j;
d912 1
d914 2
a915 2
	if (LIST_EMPTY(&d->chunk_dir[j])) {
		bp = omalloc_make_chunks(d, j);
d918 1
a918 2
	} else
		bp = LIST_FIRST(&d->chunk_dir[j]);
d978 1
a978 1
	int i;
a998 5
	if (info->size != 0)
		mp = d->chunk_dir + info->shift;
	else
		mp = d->chunk_dir;

d1001 6
d1010 1
@


1.161
log
@move reallocarray() to a seperate file so that -portable applications
can avoid reinventing the wheel
ok guenther schwarze
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.160 2014/05/07 20:07:59 halex Exp $	*/
d1785 1
a1785 1
	for (i = 0; i <= MALLOC_DELAYED_CHUNKS; i++) {
@


1.160
log
@comment style fix

ok crickets@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.159 2014/05/01 04:08:13 tedu Exp $	*/
a1424 11
}

void *
reallocarray(void *optr, size_t nmemb, size_t size)
{
	if ((nmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    nmemb > 0 && SIZE_MAX / nmemb < size) {
		errno = ENOMEM;
		return NULL;
	}
	return realloc(optr, size * nmemb);
@


1.159
log
@nibbles aren't enough random, use bytes. does a better job of picking
a free chunk at random and may allow to increase delayed chunk array.
ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.158 2014/04/23 15:07:27 tedu Exp $	*/
d1381 2
a1382 1
/* this is sqrt(SIZE_MAX+1), as s1*s2 <= SIZE_MAX
@


1.158
log
@remove Z option and default to something halfway to J.
we always junk small chunks now, and the first part of pages,
but only after free. J still does the old thing. j disables everything.
Consider experimental as we evaluate performance in the real world.
ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.157 2014/04/23 10:47:15 espie Exp $	*/
d64 1
a64 1
#define MALLOC_DELAYED_CHUNKS	15	/* max of getrnibble() */
d118 1
a118 1
	void *delayed_chunks[MALLOC_DELAYED_CHUNKS + 1];
d194 1
a194 1
static size_t rnibblesused;		/* random nibbles used */
d196 1
a196 1
static u_char getrnibble(void);
d276 1
a276 1
	rnibblesused = 0;
d280 1
a280 1
getrnibble(void)
d284 1
a284 1
	if (rnibblesused >= 2 * sizeof(rbytes))
d286 2
a287 2
	x = rbytes[rnibblesused++ / 2];
	return (rnibblesused & 1 ? x & 0xf : x >> 4);
d320 1
a320 1
	offset = getrnibble() + (getrnibble() << 4);
d401 1
a401 1
	offset = getrnibble() + (getrnibble() << 4);
d923 1
a923 1
		i += getrnibble();
d1203 1
a1203 1
			i = getrnibble();
@


1.157
log
@explain a bit more what's going on for stupid me.
okay otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.156 2014/04/23 05:43:25 otto Exp $	*/
a169 1
	int	malloc_zero;		/* zero fill? */
d416 1
a416 1
				else if (mopts.malloc_junk &&
d435 1
a435 1
		else if (mopts.malloc_junk && mopts.malloc_freeunmap)
d465 1
d539 1
a539 1
				mopts.malloc_junk = 1;
d562 2
a563 1
				mopts.malloc_freeunmap = mopts.malloc_junk = 1;
a578 6
			case 'z':
				mopts.malloc_zero = 0;
				break;
			case 'Z':
				mopts.malloc_zero = 1;
				break;
a588 7
	/*
	 * We want junk in the entire allocation, and zero only in the part
	 * the user asked for.
	 */
	if (mopts.malloc_zero)
		mopts.malloc_junk = 1;

d962 1
a962 1
	if (mopts.malloc_junk && bp->size > 0)
d1060 1
a1060 1
			if (mopts.malloc_junk)
d1066 1
a1066 1
			if (zero_fill && mopts.malloc_junk)
d1069 1
a1069 1
			if (mopts.malloc_junk) {
d1137 1
a1137 1
	r = omalloc(size, mopts.malloc_zero, CALLER);
d1189 5
a1193 3
		if (mopts.malloc_junk && !mopts.malloc_freeunmap)
			memset(p, SOME_FREEJUNK,
			    PAGEROUND(sz) - mopts.malloc_guard);
d1297 1
a1297 1
					if (mopts.malloc_junk)
d1324 1
a1324 1
			if (newsz > oldsz && mopts.malloc_junk)
d1333 1
a1333 1
		if (mopts.malloc_junk && newsz > 0)
d1518 1
a1518 1
	if (mopts.malloc_junk) {
d1549 1
a1549 1
	r = omemalign(alignment, size, mopts.malloc_zero, CALLER);
@


1.156
log
@Better, cleaner hash function that computes the same on be and le archs.
Should improve sparc64 and other be archs. ok matthew@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.155 2014/04/22 14:26:26 tedu Exp $	*/
d1391 3
@


1.155
log
@change mallocarray to reallocarray. useful in a few more situations.
malloc can, as always, be emulated via realloc(NULL).
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.154 2014/04/21 13:17:32 deraadt Exp $	*/
d220 5
a224 7
	union {
		uintptr_t p;
		unsigned short a[sizeof(void *) / sizeof(short)];
	} u;
	u.p = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u.a[0];
	sum = (sum << 7) - sum + u.a[1];
d226 2
a227 2
	sum = (sum << 7) - sum + u.a[2];
	sum = (sum << 7) - sum + u.a[3];
@


1.154
log
@Introducing:      void *mallocarray(size_t nmemb, size_t size);
Like calloc(), except without the cleared-memory gaurantee
ok beck guenther, discussed for more than a year...
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.153 2014/04/14 10:29:41 otto Exp $	*/
d1436 1
a1436 1
mallocarray(size_t nmemb, size_t size)
d1443 1
a1443 1
	return malloc(size * nmemb);
@


1.153
log
@print pid in error messages; ok reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.152 2014/04/03 16:18:11 schwarze Exp $	*/
d1433 11
@


1.152
log
@Update Copyright notice; ok otto@@ beck@@ deraadt@@.
This is merely a by-product of figuring out the amount of phk@@ code
contained herein; i'm not planning to hack on this file.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.151 2014/03/25 17:00:02 beck Exp $	*/
d238 2
a239 1
	struct iovec	iov[6];
d245 10
a254 7
	iov[1].iov_base = malloc_func;
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = q;
	iov[2].iov_len = strlen(q);
	iov[3].iov_base = msg;
	iov[3].iov_len = strlen(msg);
	iov[4].iov_base = buf;
d256 1
a256 1
		iov[4].iov_len = 0;
d259 1
a259 1
		iov[4].iov_len = strlen(buf);
d261 3
a263 3
	iov[5].iov_base = "\n";
	iov[5].iov_len = 1;
	writev(STDERR_FILENO, iov, 6);
@


1.151
log
@Poul-Henning Kamp informed me he is allright with this licensing change.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.150 2013/11/12 06:57:54 deraadt Exp $	*/
d3 3
a5 1
 * Copyright (c) 2008 Otto Moerbeek <otto@@drijf.net>
@


1.150
log
@avoid arithetic on void *
ok guenther otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.149 2012/12/22 07:32:17 otto Exp $	*/
d4 1
d20 2
a21 10
 * Parts of this code, mainly the sub page sized chunk management code is
 * derived from the malloc implementation with the following license:
 */
/*
 * ----------------------------------------------------------------------------
 * "THE BEER-WARE LICENSE" (Revision 42):
 * <phk@@FreeBSD.ORG> wrote this file.  As long as you retain this notice you
 * can do whatever you want with this stuff. If we meet some day, and you think
 * this stuff is worth it, you can buy me a beer in return.  Poul-Henning Kamp
 * ----------------------------------------------------------------------------
@


1.149
log
@Fix bug in random offset introduced in rev 1.143; random range was
expanded, but not enough due to precedence error. Spotted by Thorsten Glaser.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.148 2012/11/02 18:18:15 djm Exp $	*/
d720 1
a720 1
		void *q;
d1439 1
a1439 1
	void *p, *q;
d1462 1
a1462 1
	q = (void *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
@


1.148
log
@Add a new malloc option 'U' => "Free unmap" that does the guarding/
unmapping of freed allocations without disabling chunk randomisation
like the "Freeguard" ('F') option does. Make security 'S' option
use 'U' and not 'F'.

Rationale: guarding with no chunk randomisation is great for debugging
use-after-free, but chunk randomisation offers better defence against
"heap feng shui" style attacks that depend on carefully constructing a
particular heap layout so we should leave this enabled when requesting
security options.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.147 2012/09/13 10:45:41 pirofti Exp $	*/
d324 1
a324 1
	offset = getrnibble() + getrnibble() << 4;
d405 1
a405 1
	offset = getrnibble() + getrnibble() << 4;
@


1.147
log
@Fix precedence bug (& has lower precedence than !=).

Okay otto@@.

Found by Michal Mazurek <akfaew at jasminek dot net>, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.146 2012/07/09 08:39:24 deraadt Exp $	*/
d168 2
a169 1
	int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
d348 1
a348 1
			if (mopts.malloc_freeprot)
d411 1
a411 1
				if (mopts.malloc_freeprot)
d421 1
a421 1
				    mopts.malloc_freeprot)
d431 1
a431 1
		if (mopts.malloc_freeprot)
d439 1
a439 1
		else if (mopts.malloc_junk && mopts.malloc_freeprot)
d519 2
a520 1
				mopts.malloc_freeprot = 0;
d523 2
a524 1
				mopts.malloc_freeprot = 1;
d560 1
a560 1
				mopts.malloc_freeprot = mopts.malloc_junk = 0;
d565 1
a565 1
				mopts.malloc_freeprot = mopts.malloc_junk = 1;
d569 6
d1027 1
a1027 1
	if (info->size == 0 && !mopts.malloc_freeprot)
d1196 1
a1196 1
			if (!mopts.malloc_freeprot) {
d1204 1
a1204 1
		if (mopts.malloc_junk && !mopts.malloc_freeprot)
d1215 1
a1215 1
		if (!mopts.malloc_freeprot) {
@


1.146
log
@use PAGE_SHIFT instead of PGSHIFT, in preperation for future
param.h symbol reduction.
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.145 2012/06/26 21:36:25 tedu Exp $	*/
d1432 1
a1432 1
	if (alignment < MALLOC_PAGESIZE || alignment & (alignment - 1) != 0) {
@


1.145
log
@after a talk with ariane, use MAP_FIXED for mquery to avoid the cost of
scanning for free space if the hint isn't available.
also, on further inspection, this will prevent pmap_prefer from "improving"
our hint.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.144 2012/06/22 01:30:17 tedu Exp $	*/
d57 1
a57 1
#define MALLOC_PAGESHIFT	(PGSHIFT)
@


1.144
log
@two changes which should improve realloc.  first, fix zapcacheregion to
clear out the entire requested area, not just a perfect fit.  second,
use mquery to check for room to avoid getting an address we don't like
and having to send it back.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.143 2012/06/20 13:13:15 tedu Exp $	*/
d98 1
a98 1
    MAP_ANON | MAP_PRIVATE, -1, (off_t)0)
@


1.143
log
@two small fixes to free page cache.  first, we need two nibbles of random
in order to span the the entire cache.  second, on free use the same offset
to put things in the cache instead of always starting at zero.
ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.142 2012/06/18 17:03:51 matthew Exp $	*/
d97 3
d362 1
a362 1
zapcacheregion(struct dir_info *d, void *p)
d370 1
a370 1
		if (r->p == p) {
d1289 3
d1293 8
a1300 4
				zapcacheregion(g_pool, (char *)p + roldsz);
				q = MMAPA((char *)p + roldsz, rnewsz - roldsz);
				if (q == (char *)p + roldsz) {
					malloc_used += rnewsz - roldsz;
d1302 1
a1302 2
						memset(q, SOME_JUNK,
						    rnewsz - roldsz);
d1308 1
a1308 1
					if (munmap(q, rnewsz - roldsz))
@


1.142
log
@Support larger-than-page-alignment requests in posix_memalign() by
overallocating and then releasing unneeded memory pages.

ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.141 2012/02/29 08:44:14 otto Exp $	*/
d320 1
a320 1
	offset = getrnibble();
d340 1
a340 1
		r = &d->free_regions[i];
d401 1
a401 1
	offset = getrnibble();
@


1.141
log
@- Test for the retrieved page address not being NULL. This turns free((void*)1)
  into an bogus pointer error instead of a segfault.
- Document that we use the assumption that a non-MAP_FIXED mmap() with
  hint 0 never returns NULL.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.140 2011/10/06 14:37:04 otto Exp $	*/
d392 1
a392 1
		return NULL;
d1298 4
a1301 2
				} else if (q != MAP_FAILED)
					munmap(q, rnewsz - roldsz);
d1418 92
d1513 2
a1514 1
	void *result;
d1517 1
a1517 2
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *) ||
	    alignment > MALLOC_PAGESIZE)
d1520 23
a1542 7
	/* 
	 * max(size, alignment) is enough to assure the requested alignment,
	 * since the allocator always allocates power-of-two blocks.
	 */
	if (size < alignment)
		size = alignment;
	result = malloc(size);
d1544 4
a1547 5
	if (result == NULL)
		return ENOMEM;

	*memptr = result;
	return 0;
@


1.140
log
@Make struct chunk_info a variable sized struct, wasting less
space for meta data by only allocating space actually needed for
the bitmap (modulo alignment requirements). ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.139 2011/07/12 14:43:42 otto Exp $	*/
d727 5
d782 1
a782 1
	return q == p ? &d->r[index] : NULL;
@


1.139
log
@on malloc flag S, set cache size to 0; will catch even more
use-after-free bugs; ok krw@@ dlg@@ pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.138 2011/06/20 18:04:06 tedu Exp $	*/
d112 2
a113 2
					/* list of free chunk info structs */
	struct chunk_head chunk_info_list;
d159 1
a159 1
	u_short bits[(MALLOC_PAGESIZE / MALLOC_MINSIZE) / MALLOC_BITS];
d625 2
a626 2
	LIST_INIT(&d->chunk_info_list);
	for (i = 0; i <= MALLOC_MAXSHIFT; i++)
d628 1
d693 1
a693 1
alloc_chunk_info(struct dir_info *d)
d696 17
a712 5
	int i;
	
	if (LIST_EMPTY(&d->chunk_info_list)) {
		p = MMAP(MALLOC_PAGESIZE);
		if (p == MAP_FAILED)
d715 4
a718 2
		for (i = 0; i < MALLOC_PAGESIZE / sizeof(*p); i++)
			LIST_INSERT_HEAD(&d->chunk_info_list, &p[i], entries);
d720 1
a720 1
	p = LIST_FIRST(&d->chunk_info_list);
d722 1
a722 1
	memset(p, 0, sizeof *p);
d821 1
a821 1
	long		i, k;
d828 1
a828 1
	bp = alloc_chunk_info(d);
d847 1
a847 1
			LIST_INSERT_HEAD(&d->chunk_info_list, bp, entries);
d974 1
a974 1
	long i;
d1015 5
a1019 1
	LIST_INSERT_HEAD(&d->chunk_info_list, info, entries);
d1543 1
a1543 1
			snprintf(buf, sizeof(buf), "    ");
d1553 1
a1553 1
	int i;
d1558 11
a1568 4
		struct chunk_info *p = LIST_FIRST(&d->chunk_dir[i]);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
d1570 2
a1571 1
		}
@


1.138
log
@as man page states, lower case undoes upper case.  add support for little s,
no security, for consistency.  use of this option is discouraged. :)
ok deraadt guenther millert
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.137 2011/05/20 20:02:08 otto Exp $	*/
d71 2
d466 1
a466 1
	mopts.malloc_cache = 64;
d556 1
d561 1
@


1.137
log
@save errno dance in wrterror() and malloc_dump(); prompted by and ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.136 2011/05/18 18:09:37 otto Exp $	*/
d550 4
@


1.136
log
@introduce symbolic constant for initial number of regions
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.135 2011/05/18 18:07:20 otto Exp $	*/
d239 1
d264 2
a265 1
	//malloc_active--;
d1625 1
d1640 1
@


1.135
log
@zap regions_bits and rework MALLOC_MAXSHIFT a bit; ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.134 2011/05/12 12:03:40 otto Exp $	*/
d70 1
d607 1
a607 1
	d->regions_free = d->regions_total = 512;
@


1.134
log
@Avoid fp computations for stats, this make calling malloc_dump() safe in more
cases.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.133 2011/05/12 09:35:37 otto Exp $	*/
a51 3
#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		16

d60 2
d67 1
a67 1
#define MALLOC_MAXCHUNK		(1 << (MALLOC_PAGESHIFT-1))
a107 1
	size_t regions_bits;		/* log2 of total */
d112 1
a112 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT];
d606 1
a606 2
	d->regions_bits = 9;
	d->regions_free = d->regions_total = 1 << d->regions_bits;
d615 1
a615 1
	for (i = 0; i < MALLOC_MAXSHIFT; i++)
a635 1
	size_t newbits;
a644 1
	newbits = d->regions_bits + 1;
a675 1
	d->regions_bits = newbits;
d1527 1
a1527 1
	for (i = 0; i < MALLOC_MAXSHIFT; i++) {
@


1.133
log
@fix comment, the bitmap is an array of u_short now
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.132 2011/05/12 09:29:30 otto Exp $	*/
d1574 2
a1575 1
	snprintf(buf, sizeof(buf), "Regions slots %zu\n", d->regions_total);
d1577 2
a1578 3
	snprintf(buf, sizeof(buf), "Finds %zu/%zu %f\n", d->finds,
	    d->find_collisions,
	    1.0 + (double)d->find_collisions / d->finds);
d1580 2
a1581 3
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu %f\n", d->inserts,
	    d->insert_collisions,
	    1.0 + (double)d->insert_collisions / d->inserts);
a1587 2
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Regions slots free %zu\n", d->regions_free);
@


1.132
log
@Introduce leak detection code for MALLOC_STATS
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.131 2011/05/08 07:08:13 otto Exp $	*/
d146 1
a146 1
 * How many bits per u_long in the bitmap
@


1.131
log
@Move MALLOC_STATS code to bottom of file, so the real stuff is more at the top.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.130 2011/05/05 12:11:20 otto Exp $	*/
d46 1
d98 3
d132 1
d136 1
d204 3
d708 1
a708 1
insert(struct dir_info *d, void *p, size_t sz)
d729 3
d854 1
a854 1
	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp);
d863 1
a863 1
malloc_bytes(struct dir_info *d, size_t size)
d923 6
d1001 1
a1001 1
omalloc(size_t sz, int zero_fill)
d1018 1
a1018 1
		if (insert(g_pool, p, sz)) {
d1055 1
a1055 1
		p = malloc_bytes(g_pool, sz);
d1111 1
a1111 1
	r = omalloc(size, mopts.malloc_zero);
d1219 1
a1219 1
orealloc(void *p, size_t newsz)
d1226 1
a1226 1
		return omalloc(newsz, 0);
d1266 1
d1285 1
d1292 1
d1299 1
d1302 1
a1302 1
		q = omalloc(newsz, 0);
d1309 2
a1310 1
	} else
d1312 1
d1331 1
a1331 1
	r = orealloc(ptr, size);
d1374 1
a1374 1
	r = omalloc(size, 1);
d1413 84
d1498 1
a1498 1
dump_chunk(int fd, struct chunk_info *p, int fromfreelist)
d1503 3
a1505 2
		snprintf(buf, sizeof(buf), "chunk %d %d/%d %p\n", p->size,
		    p->free, p->total, p->page);
d1507 8
a1514 1
		if (!fromfreelist)
d1516 1
d1538 1
a1538 1
			dump_chunk(fd, p, 1);
d1592 5
d1601 1
a1601 1
			snprintf(buf, sizeof(buf), "%4zx) #%zx %zd ",
d1606 1
d1608 2
a1609 1
				    "%p: %zu\n", d->r[i].p, realsize);
d1613 2
a1614 1
				    (struct chunk_info *)d->r[i].size, 0);
a1616 2
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
d1621 2
a1624 1

d1628 16
@


1.130
log
@Up until now, malloc scanned the bits of the chunk bitmap from
position zero, skipping a random number of free slots and then
picking the next free one. This slowed things down, especially if
the number of full slots increases.

This changes the scannning to start at a random position in the
bitmap and then taking the first available free slot, wrapping if
the end of the bitmap is reached. Of course we'll still scan more
if the bitmap becomes more full, but the extra iterations skipping
free slots and then some full slots are avoided.

The random number is derived from a global, which is incremented
by a few random bits every time a chunk is needed (with a small optimization
if only one free slot is left).

Thanks to the testers!
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.129 2011/04/30 15:46:46 otto Exp $	*/
d195 5
a224 136
#ifdef MALLOC_STATS
static void
dump_chunk(int fd, struct chunk_info *p, int fromfreelist)
{
	char buf[64];

	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %d %d/%d %p\n", p->size,
		    p->free, p->total, p->page);
		write(fd, buf, strlen(buf));
		if (!fromfreelist)
			break;
		p = LIST_NEXT(p, entries);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "    ");
			write(fd, buf, strlen(buf));
		}
	}
}

static void
dump_free_chunk_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i < MALLOC_MAXSHIFT; i++) {
		struct chunk_info *p = LIST_FIRST(&d->chunk_dir[i]);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			dump_chunk(fd, p, 1);
		}
	}

}

static void
dump_free_page_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
			write(fd, buf, strlen(buf));
		}
	}
}

static void
malloc_dump1(int fd, struct dir_info *d)
{
	char buf[64];
	size_t i, realsize;

	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Regions slots %zu\n", d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu %f\n", d->finds,
	    d->find_collisions,
	    1.0 + (double)d->find_collisions / d->finds);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu %f\n", d->inserts,
	    d->insert_collisions,
	    1.0 + (double)d->insert_collisions / d->inserts);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	     d->delete_moves);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Regions slots free %zu\n", d->regions_free);
	write(fd, buf, strlen(buf));
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				snprintf(buf, sizeof(buf),
				    "%p: %zu\n", d->r[i].p, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size, 0);
		}
	}
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
	write(fd, buf, strlen(buf));
}


void
malloc_dump(int fd)
{
	malloc_dump1(fd, g_pool);
}

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}
#endif /* MALLOC_STATS */


d1388 135
@


1.129
log
@Now that we use an array of u_short for the chunk bitmap change a few
1UL to 1U.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.128 2011/04/30 14:56:20 otto Exp $	*/
d116 1
d1017 16
a1032 32
	/* Find first word of bitmap which isn't empty */
	for (lp = bp->bits; !*lp; lp++)
		/* EMPTY */;

	/* Find that bit, and tweak it */
	u = 1;
	k = 0;
	while (!(*lp & u)) {
		u += u;
		k++;
	}

	/* advance a random # of positions */
	if (bp->free > 1) {
		i = getrnibble() % bp->free;
		while (i > 0) {
			u += u;
			k++;
			if (k >= MALLOC_BITS) {
				while (!*++lp)
					/* EMPTY */;
				u = 1;
				k = 0;
				if (lp - bp->bits > (bp->total - 1) /
				    MALLOC_BITS) {
					wrterror("chunk overflow", NULL);
					errno = EFAULT;
					return (NULL);
				}
			}
			if (*lp & u)
				i--;
d1034 6
d1041 1
@


1.128
log
@More efficient scanning for free chunks while not losing any randomization;
thanks to all testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.127 2010/12/16 18:47:01 dhill Exp $	*/
d949 1
a949 1
		bp->size = (1UL << bits);
d1084 1
a1084 1
	if ((uintptr_t)ptr & ((1UL << (info->shift)) - 1)) {
d1088 1
a1088 1
	if (info->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
d1093 1
a1093 1
	info->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);
@


1.127
log
@avoid pointer arithmetic on void *

tested for a while by me.

ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.126 2010/10/21 08:09:35 otto Exp $	*/
d141 1
a141 1
#define MALLOC_BITS		(NBBY * sizeof(u_long))
d151 1
a151 1
	u_long bits[(MALLOC_PAGESIZE / MALLOC_MINSIZE) / MALLOC_BITS];
d961 1
a961 1
		bp->bits[i / MALLOC_BITS] = ~0UL;
d964 1
a964 1
		bp->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);
d985 1
a985 1
	u_long		u, *lp;
d1029 19
a1047 13
	i = getrnibble() % bp->free;
	while (i > 0) {
		u += u;
		k++;
		if (k >= MALLOC_BITS) {
			lp++;
			u = 1;
			k = 0;
		}
		if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
			wrterror("chunk overflow", NULL);
			errno = EFAULT;
			return (NULL);
a1048 2
		if (*lp & u)
			i--;
@


1.126
log
@print the pointer value that caused the error (if available); ok
deraadt@@ nicm@@ (on an earlier version)
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.125 2010/05/18 22:24:55 tedu Exp $	*/
d1162 1
a1162 1
					memset(p + sz - mopts.malloc_guard,
d1375 3
a1377 3
				zapcacheregion(g_pool, p + roldsz);
				q = MMAPA(p + roldsz, rnewsz - roldsz);
				if (q == p + roldsz) {
@


1.125
log
@add posix_madvise, posix_memalign, strndup, and strnlen.  mostly from
brad and millert, with hints from guenther, jmc, and otto I think.
ok previous.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.124 2010/01/13 12:40:11 otto Exp $	*/
a354 1

d356 1
a356 1
wrterror(char *p)
d359 2
a360 1
	struct iovec	iov[5];
d368 12
a379 5
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;
	writev(STDERR_FILENO, iov, 5);
d424 1
a424 1
		wrterror("munmap round");
d430 1
a430 1
			wrterror("munmap");
d444 1
a444 1
				wrterror("munmap");
d456 1
a456 1
		wrterror("malloc cache underflow");
d471 1
a471 1
		wrterror("malloc free slot lost");
d473 1
a473 1
		wrterror("malloc cache overflow");
d488 1
a488 1
				wrterror("munmap");
d507 1
a507 1
		wrterror("internal struct corrupt");
d509 1
a509 1
		wrterror("map round");
d561 1
a561 1
		wrterror("malloc cache");
d734 1
a734 1
		wrterror("malloc init mmap failed");
d797 1
a797 1
		wrterror("munmap");
d863 1
a863 1
		wrterror("internal struct corrupt");
d886 1
a886 1
		wrterror("regions_total not 2^x");
d970 1
a970 1
		wrterror("pp & bits");
d990 1
a990 1
		wrterror("internal struct corrupt");
d1015 1
a1015 1
		wrterror("chunk info corrupted");
d1039 1
a1039 1
			wrterror("chunk overflow");
d1075 1
a1075 1
		wrterror("chunk info corrupted");
d1081 1
a1081 1
		wrterror("modified chunk-pointer");
d1085 1
a1085 1
		wrterror("chunk is already free");
d1143 1
a1143 1
				wrterror("mprotect");
d1192 1
a1192 1
		wrterror("recursive call");
d1205 1
a1205 1
			wrterror("out of memory");
d1232 1
a1232 1
		wrterror("out of memory");
d1248 1
a1248 1
		wrterror("bogus pointer (double free?)");
d1256 1
a1256 1
				wrterror("bogus pointer");
d1271 1
a1271 1
				wrterror("guard size");
d1276 1
a1276 1
					wrterror("mprotect");
d1300 1
a1300 1
				wrterror("bogus pointer (double free?)");
d1321 1
a1321 1
		wrterror("free() called before allocation");
d1347 1
a1347 1
		wrterror("bogus pointer (double free?)");
d1359 1
a1359 1
			wrterror("guard size");
d1393 1
a1393 1
					wrterror("mprotect");
d1397 1
a1397 1
					wrterror("mprotect");
d1447 1
a1447 1
		wrterror("out of memory");
d1474 1
a1474 1
			wrterror("out of memory");
d1490 1
a1490 1
		wrterror("out of memory");
@


1.124
log
@New options 'S', as a shorthand for the options most suitable as an
extra safeguard (FGJ). Idea from deraadt@@; ok deraadt@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.123 2009/12/16 08:23:53 otto Exp $	*/
d1489 25
@


1.123
log
@save calls to arc4random() by using a nibble at a time; not because
arc4random() is slow, but it induces getpid() calls; also saves a
bit on stirring efforts
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.122 2009/12/07 18:47:38 miod Exp $	*/
d663 4
@


1.122
log
@Make userland malloc use __LDPGSZ granularity on mips, regardless of the
actual kernel page size.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.121 2009/11/27 20:11:01 otto Exp $	*/
d69 1
a69 1
#define MALLOC_DELAYED_CHUNKS	16	/* should be power of 2 */
d115 1
a115 1
	void *delayed_chunks[MALLOC_DELAYED_CHUNKS];
d188 1
a188 1
static size_t rbytesused;		/* random bytes used */
d190 1
a190 1
static u_char getrbyte(void);
d383 18
d431 1
a431 1
	offset = getrbyte();
d512 1
a512 1
	offset = getrbyte();
a558 15
static void
rbytes_init(void)
{
	arc4random_buf(rbytes, sizeof(rbytes));
	rbytesused = 0;
}

static u_char
getrbyte(void)
{
	if (rbytesused >= sizeof(rbytes))
		rbytes_init();
	return rbytes[rbytesused++];
}

d1018 1
a1018 1
	i = (getrbyte() & (MALLOC_DELAYED_CHUNKS - 1)) % bp->free;
d1281 1
a1281 1
			i = getrbyte() & (MALLOC_DELAYED_CHUNKS - 1);
@


1.121
log
@Switch the chunk_info lists to doubly-linked lists and use the queue
macros for them. Avoids walking the lists and greatly enhances speed
of freeing chunks in reverse or random order at the cost of a little
space. Suggested by Fabien Romano and Jonathan Armani; ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.120 2009/11/27 20:05:03 otto Exp $	*/
d56 2
@


1.120
log
@Don't forget to fill region from the cache with junk if needed in one case;
from Fabien Romano and Jonathan Armani
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.119 2009/11/27 19:59:49 otto Exp $	*/
d35 1
d97 2
d106 1
a106 1
	struct chunk_info *chunk_info_list;
d108 1
a108 1
	struct chunk_info *chunk_dir[MALLOC_MAXSHIFT];
d141 1
a141 1
	struct chunk_info *next;	/* next on the free list */
d223 1
a223 1
	while (p) {
d229 1
a229 1
		p = p->next;
d246 1
a246 1
		struct chunk_info *p = d->chunk_dir[i];
d722 3
d797 1
a797 1
	if (d->chunk_info_list == NULL) {
d802 2
a803 4
		for (i = 0; i < MALLOC_PAGESIZE / sizeof(*p); i++) {
			p[i].next = d->chunk_info_list;
			d->chunk_info_list = &p[i];
		}
d805 2
a806 2
	p = d->chunk_info_list;
	d->chunk_info_list = p->next;
a811 8

static void
put_chunk_info(struct dir_info *d, struct chunk_info *p)
{
	p->next = d->chunk_info_list;
	d->chunk_info_list = p;
}  

d929 1
a929 1
			put_chunk_info(d, bp);
d950 1
a950 2
	bp->next = d->chunk_dir[bits];
	d->chunk_dir[bits] = bp;
d991 6
a996 3
	bp = d->chunk_dir[j];
	if (bp == NULL && (bp = omalloc_make_chunks(d, j)) == NULL)
		return NULL;
d1034 3
a1036 4
	if (!--bp->free) {
		d->chunk_dir[j] = bp->next;
		bp->next = NULL;
	}
d1053 2
a1054 1
	struct chunk_info *info, **mp;
d1083 1
a1083 7

		/* Insert in address order */
		while (*mp != NULL && (*mp)->next != NULL &&
		    (*mp)->next->page < info->page)
			mp = &(*mp)->next;
		info->next = *mp;
		*mp = info;
d1089 1
a1089 10
	/* Find & remove this page in the queue */
	while (*mp != info) {
		mp = &((*mp)->next);
		if (!*mp) {
			wrterror("not on queue");
			errno = EFAULT;
			return;
		}
	}
	*mp = info->next;
d1096 1
a1096 1
	put_chunk_info(d, info);
@


1.119
log
@No need to clear a mmapped region; from Fabien Romano and Jonathan
Armani
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.118 2009/11/02 19:26:17 todd Exp $	*/
d523 2
@


1.118
log
@permit -DMALLOC_STATS to compile again
noticed by Jonathan Armani & Fabien Romano
ugh+ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.117 2009/10/20 21:19:37 pirofti Exp $	*/
a717 1
	memset(d->r, 0, regioninfo_size);
@


1.117
log
@Check mmap return value against MAP_FAILED not NULL.

Okay deraadt@@, otto@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.116 2009/06/08 19:21:08 deraadt Exp $	*/
d1374 1
a1374 1
				STATS_INC(g_pool.cheap_realloc_tries);
@


1.116
log
@quieten compiler by converting pointers to uintptr_t before truncating them
to u_int32_t to do integer math with (in a situation where that is legit)
ok otto millert
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.115 2009/01/03 12:58:28 djm Exp $	*/
d699 1
a699 1
	if ((p = MMAP(DIR_INFO_RSZ + (MALLOC_PAGESIZE * 2))) == NULL)
@


1.115
log
@reintroduce extra malloc protections, but avoiding the use of
PAGE_(SIZE|SHIFT|MASK) defines that evaluate to variables on the
sparc architecture;
ok otto@@ tested on my reanimated ss20
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.113 2008/12/30 07:44:51 djm Exp $	*/
d475 1
a475 1
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
d719 1
a719 1
	d->canary1 = mopts.malloc_canary ^ (u_int32_t)d;
d848 1
a848 1
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
d976 1
a976 1
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
@


1.114
log
@PAGE_SIZE is not a valid symbol to use in that way.  In particular,
on sparc, it expands to something that just plain does not work,
because the page size can be variable.  Sorry we didn't spot this
before.  Backing it all out to allow sparc to build; please find a
different way to fix it.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.111 2008/12/15 19:47:49 otto Exp $	*/
d128 2
a129 1

d149 26
a174 2
static struct dir_info	g_pool;
static char	*malloc_func;		/* current function */
d177 1
a177 1
static int	malloc_abort = 1;	/* abort() on error */
a178 8
static int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
static int	malloc_hint;		/* call madvice on free pages?  */
static int	malloc_junk;		/* junk fill? */
static int	malloc_move = 1;	/* move allocations to end of page? */
static int	malloc_realloc;		/* always realloc? */
static int	malloc_xmalloc;		/* xmalloc behaviour? */
static int	malloc_zero;		/* zero fill? */
static size_t	malloc_guard;		/* use guard pages after allocations? */
a179 1
static u_int	malloc_cache = 64;	/* free pages we cache */
a182 4
#ifdef MALLOC_STATS
static int	malloc_stats;		/* dump statistics at end */
#endif

d262 1
a262 1
	for (i = 0; i < malloc_cache; i++) {
d281 2
d330 1
a330 1
	malloc_dump1(fd, &g_pool);
d370 1
a370 1
	if (malloc_stats)
d374 1
a374 1
	if (malloc_abort)
d398 1
a398 1
	if (psz > malloc_cache) {
d405 1
a405 1
	rsz = malloc_cache - d->free_regions_size;
d409 2
a410 2
	for (i = 0; tounmap > 0 && i < malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (malloc_cache - 1)];
d427 1
a427 1
	for (i = 0; i < malloc_cache; i++) {
d430 1
a430 1
			if (malloc_hint)
d432 1
a432 1
			if (malloc_freeprot)
d440 1
a440 1
	if (i == malloc_cache)
d442 1
a442 1
	if (d->free_regions_size > malloc_cache)
d453 1
a453 1
	for (i = 0; i < malloc_cache; i++) {
d475 3
d490 2
a491 2
	for (i = 0; i < malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (malloc_cache - 1)];
d495 1
a495 1
				if (malloc_freeprot)
d497 1
a497 1
				if (malloc_hint)
d504 2
a505 1
				else if (malloc_junk && malloc_freeprot)
d515 1
a515 1
		if (malloc_freeprot)
d517 1
a517 1
		if (malloc_hint)
d528 1
a528 1
	if (d->free_regions_size > malloc_cache)
d553 1
a553 1
omalloc_init(struct dir_info *d)
d557 2
a558 1
	size_t regioninfo_size;
d562 7
d594 3
a596 3
				malloc_cache <<= 1;
				if (malloc_cache > MALLOC_MAXCACHE)
					malloc_cache = MALLOC_MAXCACHE;
d599 1
a599 1
				malloc_cache >>= 1;
d602 1
a602 1
				malloc_abort = 0;
d605 1
a605 1
				malloc_abort = 1;
d609 1
a609 1
				malloc_stats = 0;
d612 1
a612 1
				malloc_stats = 1;
d616 1
a616 1
				malloc_freeprot = 0;
d619 1
a619 1
				malloc_freeprot = 1;
d622 1
a622 1
				malloc_guard = 0;
d625 1
a625 1
				malloc_guard = MALLOC_PAGESIZE;
d628 1
a628 1
				malloc_hint = 0;
d631 1
a631 1
				malloc_hint = 1;
d634 1
a634 1
				malloc_junk = 0;
d637 1
a637 1
				malloc_junk = 1;
d643 1
a643 1
				malloc_move = 0;
d646 1
a646 1
				malloc_move = 1;
d649 1
a649 1
				malloc_realloc = 0;
d652 1
a652 1
				malloc_realloc = 1;
d655 1
a655 1
				malloc_xmalloc = 0;
d658 1
a658 1
				malloc_xmalloc = 1;
d661 1
a661 1
				malloc_zero = 0;
d664 1
a664 1
				malloc_zero = 1;
d680 2
a681 2
	if (malloc_zero)
		malloc_junk = 1;
d684 1
a684 1
	if (malloc_stats && (atexit(malloc_exit) == -1)) {
d691 17
d719 1
a719 1
	d->canary1 = arc4random();
d721 10
d848 2
a849 1
	if (d->canary1 != ~d->canary2)
d875 1
a875 1
	STATS_INC(g_pool.deletes);
d891 1
a891 1
			STATS_INC(g_pool.delete_moves);
d976 3
d1043 1
a1043 1
	if (malloc_junk && bp->size > 0)
d1107 1
a1107 1
	if (info->size == 0 && !malloc_freeprot)
d1124 1
a1124 1
		if (sz >= SIZE_MAX - malloc_guard - MALLOC_PAGESIZE) {
d1128 1
a1128 1
		sz += malloc_guard;
d1130 1
a1130 1
		p = map(&g_pool, psz, zero_fill);
d1135 2
a1136 2
		if (insert(&g_pool, p, sz)) {
			unmap(&g_pool, p, psz);
d1140 3
a1142 3
		if (malloc_guard) {
			if (mprotect((char *)p + psz - malloc_guard,
			    malloc_guard, PROT_NONE))
d1144 1
a1144 1
			malloc_guarded += malloc_guard;
d1147 3
a1149 2
		if (malloc_move &&
		    sz - malloc_guard < MALLOC_PAGESIZE - MALLOC_LEEWAY) {
d1151 2
a1152 2
			if (malloc_junk)
				memset(p, SOME_JUNK, psz - malloc_guard);
d1155 1
a1155 1
			    (sz - malloc_guard)) & ~(MALLOC_MINSIZE-1));
d1157 2
a1158 2
			if (zero_fill && malloc_junk)
				memset(p, 0, sz - malloc_guard);
d1160 1
a1160 1
			if (malloc_junk) {
d1162 1
a1162 1
					memset(p + sz - malloc_guard,
d1165 2
a1166 2
					memset(p,
					    SOME_JUNK, psz - malloc_guard);
d1172 1
a1172 1
		p = malloc_bytes(&g_pool, sz);
d1199 13
d1220 2
a1221 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			_MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1222 1
		}
d1228 1
a1228 1
	r = omalloc(size, malloc_zero);
d1231 1
a1231 1
	if (r == NULL && malloc_xmalloc) {
d1246 1
a1246 1
	r = find(&g_pool, p);
d1253 2
a1254 1
		if (sz - malloc_guard >= MALLOC_PAGESIZE - MALLOC_LEEWAY) {
d1263 1
a1263 1
			    MALLOC_MINSIZE - sz - malloc_guard) &
d1269 2
a1270 2
		if (malloc_guard) {
			if (sz < malloc_guard)
d1272 1
a1272 1
			if (!malloc_freeprot) {
d1274 1
a1274 1
				    malloc_guard, malloc_guard,
d1278 1
a1278 1
			malloc_guarded -= malloc_guard;
d1280 5
a1284 4
		if (malloc_junk && !malloc_freeprot)
			memset(p, SOME_FREEJUNK, PAGEROUND(sz) - malloc_guard);
		unmap(&g_pool, p, PAGEROUND(sz));
		delete(&g_pool, r);
d1289 1
a1289 1
		if (malloc_junk && sz > 0)
d1291 1
a1291 1
		if (!malloc_freeprot) {
d1294 2
a1295 2
			p = g_pool.delayed_chunks[i];
			g_pool.delayed_chunks[i] = tmp;
d1298 1
a1298 1
			r = find(&g_pool, p);
d1303 1
a1303 1
			free_bytes(&g_pool, r, p);
d1319 5
d1345 1
a1345 1
	r = find(&g_pool, p);
d1350 1
a1350 1
	if (newsz >= SIZE_MAX - malloc_guard - MALLOC_PAGESIZE) {
d1358 1
a1358 1
		if (oldsz < malloc_guard)
d1360 1
a1360 1
		oldsz -= malloc_guard;
d1365 1
a1365 1
		gnewsz += malloc_guard;
d1368 1
a1368 1
	    !malloc_realloc) {
d1373 1
a1373 1
			if (!malloc_guard) {
d1375 1
a1375 1
				zapcacheregion(&g_pool, p + roldsz);
d1379 1
a1379 1
					if (malloc_junk)
d1383 1
a1383 1
					STATS_INC(g_pool.cheap_reallocs);
d1389 4
a1392 3
			if (malloc_guard) {
				if (mprotect((char *)p + roldsz - malloc_guard,
				    malloc_guard, PROT_READ | PROT_WRITE))
d1394 3
a1396 2
				if (mprotect((char *)p + rnewsz - malloc_guard,
				    malloc_guard, PROT_NONE))
d1399 1
a1399 1
			unmap(&g_pool, (char *)p + rnewsz, roldsz - rnewsz);
d1403 1
a1403 1
			if (newsz > oldsz && malloc_junk)
d1405 1
a1405 1
				    rnewsz - malloc_guard - newsz);
d1410 2
a1411 2
	if (newsz <= oldsz && newsz > oldsz / 2 && !malloc_realloc) {
		if (malloc_junk && newsz > 0)
d1414 1
a1414 1
	} else if (newsz != oldsz || malloc_realloc) {
d1434 2
a1435 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			 _MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1436 1
		}
a1441 1

d1446 1
a1446 1
	if (r == NULL && malloc_xmalloc) {
d1466 2
a1467 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			 _MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1468 1
		}
d1472 2
a1473 2
		 _MALLOC_UNLOCK();
		if (malloc_xmalloc)
d1489 1
a1489 1
	if (r == NULL && malloc_xmalloc) {
d1497 1
@


1.113
log
@Remove mprotecting of struct dir_info introduced in previous commit
(MALLOC_OPTIONS=L). It was too slow to turn on by default, and we
don't do optional security.

requested by deraadt@@ grumbling ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.112 2008/12/29 22:25:50 djm Exp $	*/
d128 1
a128 1
#define DIR_INFO_RSZ	((sizeof(struct dir_info) + PAGE_MASK) & ~PAGE_MASK)
d148 2
a149 26
struct malloc_readonly {
	struct dir_info *g_pool;	/* Main bookkeeping information */
	int	malloc_abort;		/* abort() on error */
	int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
	int	malloc_hint;		/* call madvice on free pages?  */
	int	malloc_junk;		/* junk fill? */
	int	malloc_move;		/* move allocations to end of page? */
	int	malloc_realloc;		/* always realloc? */
	int	malloc_xmalloc;		/* xmalloc behaviour? */
	int	malloc_zero;		/* zero fill? */
	size_t	malloc_guard;		/* use guard pages after allocations? */
	u_int	malloc_cache;		/* free pages we cache */
#ifdef MALLOC_STATS
	int	malloc_stats;		/* dump statistics at end */
#endif
	u_int32_t malloc_canary;	/* Matched against ones in g_pool */
};

/* This object is mapped PROT_READ after initialisation to prevent tampering */
static union {
	struct malloc_readonly mopts;
	u_char _pad[PAGE_SIZE];
} malloc_readonly __attribute__((aligned(PAGE_SIZE)));
#define mopts	malloc_readonly.mopts
#define g_pool	mopts.g_pool

d152 1
a152 1
static char	*malloc_func;		/* current function */
d154 8
d163 1
d167 4
d250 1
a250 1
	for (i = 0; i < mopts.malloc_cache; i++) {
a268 2
	if (d == NULL)
		return;
d316 1
a316 1
	malloc_dump1(fd, g_pool);
d356 1
a356 1
	if (mopts.malloc_stats)
d360 1
a360 1
	if (mopts.malloc_abort)
d384 1
a384 1
	if (psz > mopts.malloc_cache) {
d391 1
a391 1
	rsz = mopts.malloc_cache - d->free_regions_size;
d395 2
a396 2
	for (i = 0; tounmap > 0 && i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
d413 1
a413 1
	for (i = 0; i < mopts.malloc_cache; i++) {
d416 1
a416 1
			if (mopts.malloc_hint)
d418 1
a418 1
			if (mopts.malloc_freeprot)
d426 1
a426 1
	if (i == mopts.malloc_cache)
d428 1
a428 1
	if (d->free_regions_size > mopts.malloc_cache)
d439 1
a439 1
	for (i = 0; i < mopts.malloc_cache; i++) {
a460 3
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt");
d473 2
a474 2
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
d478 1
a478 1
				if (mopts.malloc_freeprot)
d480 1
a480 1
				if (mopts.malloc_hint)
d487 1
a487 2
				else if (mopts.malloc_junk &&
				    mopts.malloc_freeprot)
d497 1
a497 1
		if (mopts.malloc_freeprot)
d499 1
a499 1
		if (mopts.malloc_hint)
d510 1
a510 1
	if (d->free_regions_size > mopts.malloc_cache)
d535 1
a535 1
omalloc_init(struct dir_info **dp)
d539 1
a539 2
	size_t d_avail, regioninfo_size;
	struct dir_info *d;
a542 7
	/*
	 * Default options
	 */
	mopts.malloc_abort = 1;
	mopts.malloc_move = 1;
	mopts.malloc_cache = 64;

d568 3
a570 3
				mopts.malloc_cache <<= 1;
				if (mopts.malloc_cache > MALLOC_MAXCACHE)
					mopts.malloc_cache = MALLOC_MAXCACHE;
d573 1
a573 1
				mopts.malloc_cache >>= 1;
d576 1
a576 1
				mopts.malloc_abort = 0;
d579 1
a579 1
				mopts.malloc_abort = 1;
d583 1
a583 1
				mopts.malloc_stats = 0;
d586 1
a586 1
				mopts.malloc_stats = 1;
d590 1
a590 1
				mopts.malloc_freeprot = 0;
d593 1
a593 1
				mopts.malloc_freeprot = 1;
d596 1
a596 1
				mopts.malloc_guard = 0;
d599 1
a599 1
				mopts.malloc_guard = MALLOC_PAGESIZE;
d602 1
a602 1
				mopts.malloc_hint = 0;
d605 1
a605 1
				mopts.malloc_hint = 1;
d608 1
a608 1
				mopts.malloc_junk = 0;
d611 1
a611 1
				mopts.malloc_junk = 1;
d617 1
a617 1
				mopts.malloc_move = 0;
d620 1
a620 1
				mopts.malloc_move = 1;
d623 1
a623 1
				mopts.malloc_realloc = 0;
d626 1
a626 1
				mopts.malloc_realloc = 1;
d629 1
a629 1
				mopts.malloc_xmalloc = 0;
d632 1
a632 1
				mopts.malloc_xmalloc = 1;
d635 1
a635 1
				mopts.malloc_zero = 0;
d638 1
a638 1
				mopts.malloc_zero = 1;
d654 2
a655 2
	if (mopts.malloc_zero)
		mopts.malloc_junk = 1;
d658 1
a658 1
	if (mopts.malloc_stats && (atexit(malloc_exit) == -1)) {
a664 16
	while ((mopts.malloc_canary = arc4random()) == 0)
		;

	/*
	 * Allocate dir_info with a guard page on either side. Also
	 * randomise offset inside the page at which the dir_info
	 * lies (subject to alignment by 1 << MALLOC_MINSHIFT)
	 */
	if ((p = MMAP(PAGE_SIZE + DIR_INFO_RSZ + PAGE_SIZE)) == NULL)
		return -1;
	mprotect(p, PAGE_SIZE, PROT_NONE);
	mprotect(p + PAGE_SIZE + DIR_INFO_RSZ, PAGE_SIZE, PROT_NONE);
	d_avail = (DIR_INFO_RSZ - sizeof(*d)) >> MALLOC_MINSHIFT;
	d = (struct dir_info *)(p + PAGE_SIZE +
	    (arc4random_uniform(d_avail) << MALLOC_MINSHIFT));

d676 1
a676 1
	d->canary1 = mopts.malloc_canary ^ (u_int32_t)d;
a677 10

	*dp = d;

	/*
	 * Options have been set and will never be reset.
	 * Prevent further tampering with them.
	 */
	if (((uintptr_t)&malloc_readonly & PAGE_MASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly), PROT_READ);

d795 1
a795 2
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
	    d->canary1 != ~d->canary2)
d821 1
a821 1
	STATS_INC(g_pool->deletes);
d837 1
a837 1
			STATS_INC(g_pool->delete_moves);
a921 3
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt");
d986 1
a986 1
	if (mopts.malloc_junk && bp->size > 0)
d1050 1
a1050 1
	if (info->size == 0 && !mopts.malloc_freeprot)
d1067 1
a1067 1
		if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
d1071 1
a1071 1
		sz += mopts.malloc_guard;
d1073 1
a1073 1
		p = map(g_pool, psz, zero_fill);
d1078 2
a1079 2
		if (insert(g_pool, p, sz)) {
			unmap(g_pool, p, psz);
d1083 3
a1085 3
		if (mopts.malloc_guard) {
			if (mprotect((char *)p + psz - mopts.malloc_guard,
			    mopts.malloc_guard, PROT_NONE))
d1087 1
a1087 1
			malloc_guarded += mopts.malloc_guard;
d1090 2
a1091 3
		if (mopts.malloc_move &&
		    sz - mopts.malloc_guard < MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
d1093 2
a1094 2
			if (mopts.malloc_junk)
				memset(p, SOME_JUNK, psz - mopts.malloc_guard);
d1097 1
a1097 1
			    (sz - mopts.malloc_guard)) & ~(MALLOC_MINSIZE-1));
d1099 2
a1100 2
			if (zero_fill && mopts.malloc_junk)
				memset(p, 0, sz - mopts.malloc_guard);
d1102 1
a1102 1
			if (mopts.malloc_junk) {
d1104 1
a1104 1
					memset(p + sz - mopts.malloc_guard,
d1107 2
a1108 2
					memset(p, SOME_JUNK,
					    psz - mopts.malloc_guard);
d1114 1
a1114 1
		p = malloc_bytes(g_pool, sz);
a1140 21
static void
malloc_global_corrupt(void)
{
	wrterror("global malloc data corrupt");
	_MALLOC_UNLOCK();
	errno = EINVAL;
}

static int
malloc_init(void)
{
	if (omalloc_init(&g_pool)) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror("out of memory");
		errno = ENOMEM;
		return -1;
	}
	return 0;
}

d1149 6
a1154 2
	if (g_pool == NULL) {
		if (malloc_init() != 0)
d1156 1
d1162 1
a1162 1
	r = omalloc(size, mopts.malloc_zero);
d1165 1
a1165 1
	if (r == NULL && mopts.malloc_xmalloc) {
d1180 1
a1180 1
	r = find(g_pool, p);
d1187 1
a1187 2
		if (sz - mopts.malloc_guard >= MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
d1196 1
a1196 1
			    MALLOC_MINSIZE - sz - mopts.malloc_guard) &
d1202 2
a1203 2
		if (mopts.malloc_guard) {
			if (sz < mopts.malloc_guard)
d1205 1
a1205 1
			if (!mopts.malloc_freeprot) {
d1207 1
a1207 1
				    mopts.malloc_guard, mopts.malloc_guard,
d1211 1
a1211 1
			malloc_guarded -= mopts.malloc_guard;
d1213 4
a1216 5
		if (mopts.malloc_junk && !mopts.malloc_freeprot)
			memset(p, SOME_FREEJUNK,
			    PAGEROUND(sz) - mopts.malloc_guard);
		unmap(g_pool, p, PAGEROUND(sz));
		delete(g_pool, r);
d1221 1
a1221 1
		if (mopts.malloc_junk && sz > 0)
d1223 1
a1223 1
		if (!mopts.malloc_freeprot) {
d1226 2
a1227 2
			p = g_pool->delayed_chunks[i];
			g_pool->delayed_chunks[i] = tmp;
d1230 1
a1230 1
			r = find(g_pool, p);
d1235 1
a1235 1
			free_bytes(g_pool, r, p);
a1250 5
	if (g_pool == NULL) {
		_MALLOC_UNLOCK();
		wrterror("free() called before allocation");
		return;
	}
d1272 1
a1272 1
	r = find(g_pool, p);
d1277 1
a1277 1
	if (newsz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
d1285 1
a1285 1
		if (oldsz < mopts.malloc_guard)
d1287 1
a1287 1
		oldsz -= mopts.malloc_guard;
d1292 1
a1292 1
		gnewsz += mopts.malloc_guard;
d1295 1
a1295 1
	    !mopts.malloc_realloc) {
d1300 1
a1300 1
			if (!mopts.malloc_guard) {
d1302 1
a1302 1
				zapcacheregion(g_pool, p + roldsz);
d1306 1
a1306 1
					if (mopts.malloc_junk)
d1310 1
a1310 1
					STATS_INC(g_pool->cheap_reallocs);
d1316 3
a1318 4
			if (mopts.malloc_guard) {
				if (mprotect((char *)p + roldsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
d1320 2
a1321 3
				if (mprotect((char *)p + rnewsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_NONE))
d1324 1
a1324 1
			unmap(g_pool, (char *)p + rnewsz, roldsz - rnewsz);
d1328 1
a1328 1
			if (newsz > oldsz && mopts.malloc_junk)
d1330 1
a1330 1
				    rnewsz - mopts.malloc_guard - newsz);
d1335 2
a1336 2
	if (newsz <= oldsz && newsz > oldsz / 2 && !mopts.malloc_realloc) {
		if (mopts.malloc_junk && newsz > 0)
d1339 1
a1339 1
	} else if (newsz != oldsz || mopts.malloc_realloc) {
d1359 6
a1364 2
	if (g_pool == NULL) {
		if (malloc_init() != 0)
d1366 1
d1372 1
d1377 1
a1377 1
	if (r == NULL && mopts.malloc_xmalloc) {
d1397 6
a1402 2
	if (g_pool == NULL) {
		if (malloc_init() != 0)
d1404 1
d1408 2
a1409 2
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
d1425 1
a1425 1
	if (r == NULL && mopts.malloc_xmalloc) {
a1432 1

@


1.112
log
@extra paranoia for malloc(3):

Move all runtime options into a structure that is made read-only
(via mprotect) after initialisation to protect against attacks that
overwrite options to turn off malloc protections (e.g. use-after-free)

Allocate the main bookkeeping data (struct dir_info) using mmap(),
thereby giving it an unpredictable address. Place a PROT_NONE guard
page on either side to further frustrate attacks on it.

Add a new 'L' option that maps struct dir_info PROT_NONE except when
in the allocator code itself. Makes attacks on it basically impossible.

feedback tedu deraadt otto canacar
ok otto
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.111 2008/12/15 19:47:49 otto Exp $	*/
a90 17
/* Protect and unprotect g_pool structure as we enter/exit the allocator */
#define DIR_INFO_RSZ	((sizeof(struct dir_info) + PAGE_MASK) & ~PAGE_MASK)
#define PROTECT_G_POOL() \
	do { \
		if (g_pool != NULL && mopts.malloc_poolprot) { \
			mprotect((void *)((uintptr_t)g_pool & ~PAGE_MASK), \
			    DIR_INFO_RSZ, PROT_NONE); \
		} \
	} while (0)
#define UNPROTECT_G_POOL() \
	do { \
		if (g_pool != NULL && mopts.malloc_poolprot) { \
			mprotect((void *)((uintptr_t)g_pool & ~PAGE_MASK), \
			    DIR_INFO_RSZ, PROT_READ | PROT_WRITE); \
		} \
	} while (0)

d128 1
a128 1

a150 1
	int	malloc_poolprot;	/* mprotect heap PROT_NONE? */
a637 6
			case 'l':
				mopts.malloc_poolprot = 0;
				break;
			case 'L':
				mopts.malloc_poolprot = 1;
				break;
a1192 1
	PROTECT_G_POOL();
a1200 1
	PROTECT_G_POOL();
a1224 1
	UNPROTECT_G_POOL();
a1235 1
	PROTECT_G_POOL();
a1323 1
	UNPROTECT_G_POOL();
a1335 1
	PROTECT_G_POOL();
a1438 1
	UNPROTECT_G_POOL();
a1450 1
	PROTECT_G_POOL();
a1470 1
	UNPROTECT_G_POOL();
a1477 1
		PROTECT_G_POOL();
a1493 1
	PROTECT_G_POOL();
@


1.111
log
@shave off more bytes than you expect by declaring a few const local arrays
as static const
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.110 2008/11/20 09:05:15 otto Exp $	*/
d91 17
d165 27
a191 2
static struct dir_info	g_pool;
static char	*malloc_func;		/* current function */
d194 1
a194 1
static int	malloc_abort = 1;	/* abort() on error */
a195 8
static int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
static int	malloc_hint;		/* call madvice on free pages?  */
static int	malloc_junk;		/* junk fill? */
static int	malloc_move = 1;	/* move allocations to end of page? */
static int	malloc_realloc;		/* always realloc? */
static int	malloc_xmalloc;		/* xmalloc behaviour? */
static int	malloc_zero;		/* zero fill? */
static size_t	malloc_guard;		/* use guard pages after allocations? */
a196 1
static u_int	malloc_cache = 64;	/* free pages we cache */
a199 4
#ifdef MALLOC_STATS
static int	malloc_stats;		/* dump statistics at end */
#endif

d279 1
a279 1
	for (i = 0; i < malloc_cache; i++) {
d298 2
d347 1
a347 1
	malloc_dump1(fd, &g_pool);
d387 1
a387 1
	if (malloc_stats)
d391 1
a391 1
	if (malloc_abort)
d415 1
a415 1
	if (psz > malloc_cache) {
d422 1
a422 1
	rsz = malloc_cache - d->free_regions_size;
d426 2
a427 2
	for (i = 0; tounmap > 0 && i < malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (malloc_cache - 1)];
d444 1
a444 1
	for (i = 0; i < malloc_cache; i++) {
d447 1
a447 1
			if (malloc_hint)
d449 1
a449 1
			if (malloc_freeprot)
d457 1
a457 1
	if (i == malloc_cache)
d459 1
a459 1
	if (d->free_regions_size > malloc_cache)
d470 1
a470 1
	for (i = 0; i < malloc_cache; i++) {
d492 3
d507 2
a508 2
	for (i = 0; i < malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (malloc_cache - 1)];
d512 1
a512 1
				if (malloc_freeprot)
d514 1
a514 1
				if (malloc_hint)
d521 2
a522 1
				else if (malloc_junk && malloc_freeprot)
d532 1
a532 1
		if (malloc_freeprot)
d534 1
a534 1
		if (malloc_hint)
d545 1
a545 1
	if (d->free_regions_size > malloc_cache)
d570 1
a570 1
omalloc_init(struct dir_info *d)
d574 2
a575 1
	size_t regioninfo_size;
d579 7
d611 3
a613 3
				malloc_cache <<= 1;
				if (malloc_cache > MALLOC_MAXCACHE)
					malloc_cache = MALLOC_MAXCACHE;
d616 1
a616 1
				malloc_cache >>= 1;
d619 1
a619 1
				malloc_abort = 0;
d622 1
a622 1
				malloc_abort = 1;
d626 1
a626 1
				malloc_stats = 0;
d629 1
a629 1
				malloc_stats = 1;
d633 1
a633 1
				malloc_freeprot = 0;
d636 1
a636 1
				malloc_freeprot = 1;
d639 1
a639 1
				malloc_guard = 0;
d642 1
a642 1
				malloc_guard = MALLOC_PAGESIZE;
d645 1
a645 1
				malloc_hint = 0;
d648 1
a648 1
				malloc_hint = 1;
d651 1
a651 1
				malloc_junk = 0;
d654 7
a660 1
				malloc_junk = 1;
d666 1
a666 1
				malloc_move = 0;
d669 1
a669 1
				malloc_move = 1;
d672 1
a672 1
				malloc_realloc = 0;
d675 1
a675 1
				malloc_realloc = 1;
d678 1
a678 1
				malloc_xmalloc = 0;
d681 1
a681 1
				malloc_xmalloc = 1;
d684 1
a684 1
				malloc_zero = 0;
d687 1
a687 1
				malloc_zero = 1;
d703 2
a704 2
	if (malloc_zero)
		malloc_junk = 1;
d707 1
a707 1
	if (malloc_stats && (atexit(malloc_exit) == -1)) {
d714 16
d741 1
a741 1
	d->canary1 = arc4random();
d743 10
d870 2
a871 1
	if (d->canary1 != ~d->canary2)
d897 1
a897 1
	STATS_INC(g_pool.deletes);
d913 1
a913 1
			STATS_INC(g_pool.delete_moves);
d998 3
d1065 1
a1065 1
	if (malloc_junk && bp->size > 0)
d1129 1
a1129 1
	if (info->size == 0 && !malloc_freeprot)
d1146 1
a1146 1
		if (sz >= SIZE_MAX - malloc_guard - MALLOC_PAGESIZE) {
d1150 1
a1150 1
		sz += malloc_guard;
d1152 1
a1152 1
		p = map(&g_pool, psz, zero_fill);
d1157 2
a1158 2
		if (insert(&g_pool, p, sz)) {
			unmap(&g_pool, p, psz);
d1162 3
a1164 3
		if (malloc_guard) {
			if (mprotect((char *)p + psz - malloc_guard,
			    malloc_guard, PROT_NONE))
d1166 1
a1166 1
			malloc_guarded += malloc_guard;
d1169 3
a1171 2
		if (malloc_move &&
		    sz - malloc_guard < MALLOC_PAGESIZE - MALLOC_LEEWAY) {
d1173 2
a1174 2
			if (malloc_junk)
				memset(p, SOME_JUNK, psz - malloc_guard);
d1177 1
a1177 1
			    (sz - malloc_guard)) & ~(MALLOC_MINSIZE-1));
d1179 2
a1180 2
			if (zero_fill && malloc_junk)
				memset(p, 0, sz - malloc_guard);
d1182 1
a1182 1
			if (malloc_junk) {
d1184 1
a1184 1
					memset(p + sz - malloc_guard,
d1187 2
a1188 2
					memset(p,
					    SOME_JUNK, psz - malloc_guard);
d1194 1
a1194 1
		p = malloc_bytes(&g_pool, sz);
d1217 1
d1222 22
d1251 1
d1253 2
a1254 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			_MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1255 1
		}
d1261 1
a1261 1
	r = omalloc(size, malloc_zero);
d1263 1
d1265 1
a1265 1
	if (r == NULL && malloc_xmalloc) {
d1280 1
a1280 1
	r = find(&g_pool, p);
d1287 2
a1288 1
		if (sz - malloc_guard >= MALLOC_PAGESIZE - MALLOC_LEEWAY) {
d1297 1
a1297 1
			    MALLOC_MINSIZE - sz - malloc_guard) &
d1303 2
a1304 2
		if (malloc_guard) {
			if (sz < malloc_guard)
d1306 1
a1306 1
			if (!malloc_freeprot) {
d1308 1
a1308 1
				    malloc_guard, malloc_guard,
d1312 1
a1312 1
			malloc_guarded -= malloc_guard;
d1314 5
a1318 4
		if (malloc_junk && !malloc_freeprot)
			memset(p, SOME_FREEJUNK, PAGEROUND(sz) - malloc_guard);
		unmap(&g_pool, p, PAGEROUND(sz));
		delete(&g_pool, r);
d1323 1
a1323 1
		if (malloc_junk && sz > 0)
d1325 1
a1325 1
		if (!malloc_freeprot) {
d1328 2
a1329 2
			p = g_pool.delayed_chunks[i];
			g_pool.delayed_chunks[i] = tmp;
d1332 1
a1332 1
			r = find(&g_pool, p);
d1337 1
a1337 1
			free_bytes(&g_pool, r, p);
d1352 1
d1354 5
d1365 1
d1381 1
a1381 1
	r = find(&g_pool, p);
d1386 1
a1386 1
	if (newsz >= SIZE_MAX - malloc_guard - MALLOC_PAGESIZE) {
d1394 1
a1394 1
		if (oldsz < malloc_guard)
d1396 1
a1396 1
		oldsz -= malloc_guard;
d1401 1
a1401 1
		gnewsz += malloc_guard;
d1404 1
a1404 1
	    !malloc_realloc) {
d1409 1
a1409 1
			if (!malloc_guard) {
d1411 1
a1411 1
				zapcacheregion(&g_pool, p + roldsz);
d1415 1
a1415 1
					if (malloc_junk)
d1419 1
a1419 1
					STATS_INC(g_pool.cheap_reallocs);
d1425 4
a1428 3
			if (malloc_guard) {
				if (mprotect((char *)p + roldsz - malloc_guard,
				    malloc_guard, PROT_READ | PROT_WRITE))
d1430 3
a1432 2
				if (mprotect((char *)p + rnewsz - malloc_guard,
				    malloc_guard, PROT_NONE))
d1435 1
a1435 1
			unmap(&g_pool, (char *)p + rnewsz, roldsz - rnewsz);
d1439 1
a1439 1
			if (newsz > oldsz && malloc_junk)
d1441 1
a1441 1
				    rnewsz - malloc_guard - newsz);
d1446 2
a1447 2
	if (newsz <= oldsz && newsz > oldsz / 2 && !malloc_realloc) {
		if (malloc_junk && newsz > 0)
d1450 1
a1450 1
	} else if (newsz != oldsz || malloc_realloc) {
d1469 1
d1471 2
a1472 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			 _MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1473 1
		}
a1478 1

d1482 1
d1484 1
a1484 1
	if (r == NULL && malloc_xmalloc) {
d1503 1
d1505 2
a1506 6
	if (!g_pool.regions_total) {
		if (omalloc_init(&g_pool)) {
			 _MALLOC_UNLOCK();
			if (malloc_xmalloc)
				wrterror("out of memory");
			errno = ENOMEM;
a1507 1
		}
d1511 3
a1513 2
		 _MALLOC_UNLOCK();
		if (malloc_xmalloc)
d1528 1
d1530 1
a1530 1
	if (r == NULL && malloc_xmalloc) {
d1538 1
@


1.110
log
@move allocations between half a page and a page as close to the end of
the page as possible (i.e. make malloc option P a default).
ok art@@ millert@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.109 2008/11/20 09:01:24 otto Exp $	*/
d322 1
a322 1
	const char q[] = "malloc() warning: Couldn't dump stats\n";
d641 1
a641 1
				const char q[] = "malloc() warning: "
d659 1
a659 1
		const char q[] = "malloc() warning: atexit(2) failed."
@


1.109
log
@Reduce the leeway malloc allows when moving allocations to the end of
a page to 0. P default will be changed in a separate commit.
ok millert@@ art@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.108 2008/11/13 07:38:45 otto Exp $	*/
d157 1
a157 1
static int	malloc_move;		/* move allocations to end of page? */
@


1.108
log
@To allow for easier playing with more strict settings introduce
a separate symbolic constant for the leeway we allow when moving
allocations towards the end of a page. No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.107 2008/11/12 09:41:49 otto Exp $	*/
d73 1
a73 1
#define MALLOC_LEEWAY		16
@


1.107
log
@avoid a few strlen calls for constant strings; prompted by tg; ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.106 2008/11/06 12:32:45 otto Exp $	*/
d67 7
d1091 1
a1091 1
		    sz - malloc_guard < MALLOC_PAGESIZE - MALLOC_MINSIZE) {
d1096 1
a1096 1
			p = ((char *)p) + ((MALLOC_PAGESIZE - MALLOC_MINSIZE -
d1187 2
a1188 2
		if (sz - malloc_guard >= MALLOC_PAGESIZE - MALLOC_MINSIZE) {
			if (r->p != p)
d1190 2
@


1.106
log
@if the freeprot flag (F) is set, do not do delayed frees for chunks
(might catch errors closer to the trouble spot) and junk fill pages just
before reuse instead of immediate (we can't access the page anyway)
since we set PROT_NONE in the F case. ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.105 2008/11/02 08:50:41 otto Exp $	*/
d315 1
a315 1
	char *q = "malloc() warning: Couldn't dump stats\n";
d323 1
a323 1
		write(STDERR_FILENO, q, strlen(q));
d633 4
a636 5
			default:
				j = malloc_abort;
				malloc_abort = 0;
				wrterror("unknown char in MALLOC_OPTIONS");
				malloc_abort = j;
d639 1
d652 1
a652 1
		char *q = "malloc() warning: atexit(2) failed."
d654 1
a654 1
		write(STDERR_FILENO, q, strlen(q));
@


1.105
log
@remove distinction between warnings and errors, ok deraadt@@ djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.104 2008/10/29 14:05:15 otto Exp $	*/
d480 2
d1204 1
a1204 1
		if (malloc_junk)
d1214 6
a1219 4
		i = getrbyte() & (MALLOC_DELAYED_CHUNKS - 1);
		tmp = p;
		p = g_pool.delayed_chunks[i];
		g_pool.delayed_chunks[i] = tmp;
@


1.104
log
@if MALLOC_STATS is defined, record how many "cheap reallocs" were
tried and how many actually succeeded.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.103 2008/10/20 06:19:02 otto Exp $	*/
a151 1
static int	malloc_silent;		/* avoid outputting warnings? */
a356 25
static void
wrtwarning(char *p)
{
	char		*q = " warning: ";
	struct iovec	iov[5];

	if (malloc_abort)
		wrterror(p);
	else if (malloc_silent)
		return;

	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = malloc_func;
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = q;
	iov[2].iov_len = strlen(q);
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;
	
	writev(STDERR_FILENO, iov, 5);
}

d405 1
a405 1
		wrtwarning("malloc cache underflow");
d420 1
a420 1
		wrtwarning("malloc free slot lost");
d422 1
a422 1
		wrtwarning("malloc cache overflow");
d502 1
a502 1
		wrtwarning("malloc cache");
a604 2
				malloc_silent = 0;
				break;
a605 1
				malloc_silent = 1;
d634 1
a634 1
				wrtwarning("unknown char in MALLOC_OPTIONS");
d649 5
a653 3
	if (malloc_stats && (atexit(malloc_exit) == -1))
		wrtwarning("atexit(2) failed."
		    "  Will not be able to dump malloc stats on exit");
d1000 1
a1000 1
		wrtwarning("modified chunk-pointer");
d1004 1
a1004 1
		wrtwarning("chunk is already free");
d1125 1
a1125 1
		wrtwarning("recursive call");
d1173 1
a1173 1
		wrtwarning("bogus pointer (double free?)");
d1180 1
a1180 1
				wrtwarning("bogus pointer");
d1193 1
a1193 1
				wrtwarning("guard size");
d1219 1
a1219 1
				wrtwarning("bogus pointer (double free?)");
d1261 1
a1261 1
		wrtwarning("bogus pointer (double free?)");
d1273 1
a1273 1
			wrtwarning("guard size");
@


1.103
log
@oops, assign errno the right way. caught by david running regress tests
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.102 2008/10/03 19:31:49 otto Exp $	*/
d111 2
d276 3
d1315 1
d1324 1
@


1.102
log
@reduce rbyte cache to 512 bytes, no measurable slowdown (even in the
threaded case) but much smaller working set; prompted by and ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.101 2008/10/03 19:01:12 otto Exp $	*/
d1183 1
a1183 1
		saved_errno = errno;
@


1.101
log
@save and restore errno on success. while it is not stricly needed for
non-syscalls, there's just too much code not doing the right thing on
error paths; prompted by and ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.100 2008/10/03 18:44:29 otto Exp $	*/
d164 1
a164 1
static u_char rbytes[4096];		/* random bytes */
@


1.100
log
@when increasing the size of a larger than a page allocation try
mapping the region next to the existing one first; there's a pretty
high chance there's a hole there we can use; ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.99 2008/10/03 18:42:45 otto Exp $	*/
d550 1
a550 1
	int i, j, save_errno = errno;
a677 2
	errno = save_errno;

d1158 1
d1182 2
d1252 2
d1267 1
d1362 1
d1388 2
d1400 1
d1436 2
@


1.99
log
@avoid spitting up regions when purging stuff from the cache, it puts
too much pressure on the amaps. ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.98 2008/08/25 17:56:17 otto Exp $	*/
d81 3
d446 21
d1304 15
a1318 1
		if (rnewsz < roldsz) {
d1330 1
a1330 1
		} else if (rnewsz == roldsz) {
@


1.98
log
@Make all combinations of G, P, J and zero-fill work with as little
effort as possible in most cases; ok djm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.97 2008/08/23 07:49:38 djm Exp $	*/
a404 1
	d->free_regions_size -= tounmap;
d409 5
a413 4
			if (r->size <= tounmap) {
				rsz = r->size << MALLOC_PAGESHIFT;
				if (munmap(r->p, rsz))
					wrterror("munmap");
d415 1
a415 9
				r->p = NULL;
				r->size = 0;
				malloc_used -= rsz;
			} else {
				rsz = tounmap << MALLOC_PAGESHIFT;
				if (munmap((char *)r->p + ((r->size - tounmap)
				    << MALLOC_PAGESHIFT), rsz))
					wrterror("munmap");
				r->size -= tounmap ;
d417 3
a419 2
				malloc_used -= rsz;
			}
@


1.97
log
@unbreak MALLOC_OPTIONS=G that I broke in my last commit;
slightly kludgey solution for until otto fixes it properly; ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.96 2008/08/23 06:15:16 djm Exp $	*/
a1080 2
		if (malloc_junk)
			memset(p + sz, SOME_JUNK, psz - sz);
a1087 1
		/* shift towards the end */
d1089 5
a1093 1
		    sz - malloc_guard < MALLOC_PAGESIZE - MALLOC_MINSIZE)
d1096 14
@


1.96
log
@fix calloc() for MALLOC_OPTIONS=J case: SOME_JUNK was being filled into
the freshly mmaped pages disrupting their pure zeroness;
ok otto@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.95 2008/08/22 21:25:10 otto Exp $	*/
d1081 2
a1088 2
		if (malloc_junk)
			memset(p + sz, SOME_JUNK, psz - sz - malloc_guard);
@


1.95
log
@make sure we always map and unmap multiples of MALLOC_PAGESIZE;
case spotted by beck, one by me; ok deraadt@@ beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.94 2008/08/22 17:14:57 otto Exp $	*/
d1088 1
a1088 1
			memset(p, SOME_JUNK, psz - malloc_guard);
@


1.94
log
@Smarter implementation of calloc(3), which uses the fact that mmap(2)
returns zero filled pages; remember to replace this function as well if you
provide your own malloc implementation; ok djm@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.93 2008/08/07 18:41:47 otto Exp $	*/
d385 1
a385 1
	size_t psz = PAGEROUND(sz) >> MALLOC_PAGESHIFT;
d390 5
d453 1
a453 1
	size_t psz = PAGEROUND(sz) >> MALLOC_PAGESHIFT;
d458 4
d1077 1
a1077 1
			unmap(&g_pool, p, sz);
d1193 1
a1193 1
		unmap(&g_pool, p, sz);
@


1.93
log
@small cleanup of error/warning strings
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.92 2008/07/28 04:56:38 otto Exp $	*/
d446 1
a446 1
map(struct dir_info *d, size_t sz)
d457 1
d473 2
d489 2
d498 1
d844 1
a844 1
	pp = map(d, MALLOC_PAGESIZE);
d1062 1
a1062 1
		p = map(&g_pool, psz);
a1085 2
		if (zero_fill)
			memset(p, 0, sz - malloc_guard);
d1321 45
@


1.92
log
@Almost complete rewrite of malloc, to have a more efficient data
structure of tracking pages returned by mmap(). Lots of testing by
lots of people, thanks to you all.
ok djm@@ (for a slighly earlier version) deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d392 1
a392 1
			wrterror("unmap");
d704 1
a704 1
		wrterror("omalloc_grow munmap");
d992 1
a992 1
		wrtwarning("modified (chunk-) pointer");
@


1.91
log
@remove _MALLOC_LOCK_INIT; major bump; ok deraadt@@
@
text
@d1 16
a16 1
/*	$OpenBSD: malloc.c,v 1.90 2008/05/19 19:36:15 otto Exp $	*/
d19 4
d27 1
a27 1
 * this stuff is worth it, you can buy me a beer in return.   Poul-Henning Kamp
d31 1
a31 25
/*
 * Defining MALLOC_EXTRA_SANITY will enable extra checks which are
 * related to internal conditions and consistency in malloc.c. This has
 * a noticeable runtime performance hit, and generally will not do you
 * any good unless you fiddle with the internals of malloc or want
 * to catch random pointer corruption as early as possible.
 */
#ifndef	MALLOC_EXTRA_SANITY
#undef	MALLOC_EXTRA_SANITY
#endif

/*
 * Defining MALLOC_STATS will enable you to call malloc_dump() and set
 * the [dD] options in the MALLOC_OPTIONS environment variable.
 * It has no run-time performance hit, but does pull in stdio...
 */
#ifndef	MALLOC_STATS
#undef	MALLOC_STATS
#endif

/*
 * What to use for Junk.  This is the byte value we use to fill with
 * when the 'J' option is enabled.
 */
#define SOME_JUNK	0xd0	/* as in "Duh" :-) */
a33 2
#include <sys/time.h>
#include <sys/resource.h>
d37 2
a38 1
#include <stdio.h>
d41 1
d43 2
d46 1
a46 3
#include <limits.h>
#include <errno.h>
#include <err.h>
d50 2
a51 18
/*
 * The basic parameters you can tweak.
 *
 * malloc_pageshift	pagesize = 1 << malloc_pageshift
 *			It's probably best if this is the native
 *			page size, but it shouldn't have to be.
 *
 * malloc_minsize	minimum size of an allocation in bytes.
 *			If this is too small it's too much work
 *			to manage them.  This is also the smallest
 *			unit of alignment used for the storage
 *			returned by malloc/realloc.
 *
 */

#if defined(__sparc__)
#define	malloc_pageshift	13U
#endif /* __sparc__ */
d53 4
a56 2
#ifndef malloc_pageshift
#define malloc_pageshift	(PGSHIFT)
d59 8
a66 3
#ifndef malloc_minsize
#define malloc_minsize			16UL
#endif
d68 1
a68 3
#if !defined(malloc_pagesize)
#define malloc_pagesize			(1UL<<malloc_pageshift)
#endif
d70 7
a76 2
/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))
d78 2
d81 3
a83 13
/*
 * No user serviceable parts behind this point.
 *
 * This structure describes a page worth of chunks.
 */
struct pginfo {
	struct pginfo	*next;	/* next on the free list */
	void		*page;	/* Pointer to the page */
	u_short		size;	/* size of this page's chunks */
	u_short		shift;	/* How far to shift for this size chunks */
	u_short		free;	/* How many free chunks */
	u_short		total;	/* How many chunk */
	u_long		bits[(malloc_pagesize / malloc_minsize) / MALLOC_BITS];/* Which chunks are free */
d86 29
a114 9
/*
 * This structure describes a number of free pages.
 */
struct pgfree {
	struct pgfree	*next;	/* next run of free pages */
	struct pgfree	*prev;	/* prev run of free pages */
	void		*page;	/* pointer to free pages */
	void		*pdir;	/* pointer to the base page's dir */
	size_t		size;	/* number of bytes free */
d117 1
d119 3
a121 1
 * Magic values to put in the page_directory
d123 11
a133 33
#define MALLOC_NOT_MINE	((struct pginfo*) 0)
#define MALLOC_FREE	((struct pginfo*) 1)
#define MALLOC_FIRST	((struct pginfo*) 2)
#define MALLOC_FOLLOW	((struct pginfo*) 3)
#define MALLOC_MAGIC	((struct pginfo*) 4)

#if ((1UL<<malloc_pageshift) != malloc_pagesize)
#error	"(1UL<<malloc_pageshift) != malloc_pagesize"
#endif

#ifndef malloc_maxsize
#define malloc_maxsize			((malloc_pagesize)>>1)
#endif

/* A mask for the offset inside a page. */
#define malloc_pagemask	((malloc_pagesize)-1)

#define	pageround(foo)	(((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define	ptr2index(foo)	(((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
#define	index2ptr(idx)	((void*)(((idx)-malloc_pageshift)<<malloc_pageshift))

/* Set when initialization has been done */
static unsigned int	malloc_started;

/* Number of free pages we cache */
static unsigned int	malloc_cache = 16;

/* Structure used for linking discrete directory pages. */
struct pdinfo {
	struct pginfo	**base;
	struct pdinfo	*prev;
	struct pdinfo	*next;
	u_long		dirnum;
a134 18
static struct pdinfo *last_dir;	/* Caches to the last and previous */
static struct pdinfo *prev_dir;	/* referenced directory pages. */

static size_t	pdi_off;
static u_long	pdi_mod;
#define	PD_IDX(num)	((num) / (malloc_pagesize/sizeof(struct pginfo *)))
#define	PD_OFF(num)	((num) & ((malloc_pagesize/sizeof(struct pginfo *))-1))
#define	PI_IDX(index)	((index) / pdi_mod)
#define	PI_OFF(index)	((index) % pdi_mod)

/* The last index in the page directory we care about */
static u_long	last_index;

/* Pointer to page directory. Allocated "as if with" malloc */
static struct pginfo **page_dir;

/* Free pages line up here */
static struct pgfree free_list;
d136 19
a154 5
/* Abort(), user doesn't handle problems. */
static int	malloc_abort = 2;

/* Are we trying to die ? */
static int	suicide;
d157 1
a157 2
/* dump statistics */
static int	malloc_stats;
d160 3
a162 2
/* avoid outputting warnings? */
static int	malloc_silent;
d164 1
a164 2
/* always realloc ? */
static int	malloc_realloc;
d166 21
a186 37
/* mprotect free pages PROT_NONE? */
static int	malloc_freeprot;

/* use guard pages after allocations? */
static size_t	malloc_guard = 0;
static size_t	malloc_guarded;
/* align pointers to end of page? */
static int	malloc_ptrguard;

static int	malloc_hint;

/* xmalloc behaviour ? */
static int	malloc_xmalloc;

/* zero fill ? */
static int	malloc_zero;

/* junk fill ? */
static int	malloc_junk;

#ifdef __FreeBSD__
/* utrace ? */
static int	malloc_utrace;

struct ut {
	void		*p;
	size_t		s;
	void		*r;
};

void		utrace(struct ut *, int);

#define UTRACE(a, b, c) \
	if (malloc_utrace) \
		{struct ut u; u.p=a; u.s = b; u.r=c; utrace(&u, sizeof u);}
#else				/* !__FreeBSD__ */
#define UTRACE(a,b,c)
d188 2
d191 3
a193 36
/* Status of malloc. */
static int	malloc_active;

/* Allocated memory. */
static size_t	malloc_used;

/* My last break. */
static caddr_t	malloc_brk;

/* One location cache for free-list holders. */
static struct pgfree *px;

/* Compile-time options. */
char		*malloc_options;

/* Name of the current public function. */
static char	*malloc_func;

#define MMAP(size) \
	mmap((void *)0, (size), PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, \
	    -1, (off_t)0)

/*
 * Necessary function declarations.
 */
static void	*imalloc(size_t size);
static void	ifree(void *ptr);
static void	*irealloc(void *ptr, size_t size);
static void	*malloc_bytes(size_t size);

static struct pginfo *pginfo_list;

static struct pgfree *pgfree_list;

static struct pgfree *
alloc_pgfree(void)
d195 1
a195 2
	struct pgfree *p;
	int i;
d197 10
a206 7
	if (pgfree_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (p == MAP_FAILED)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pgfree_list;
			pgfree_list = &p[i];
a208 4
	p = pgfree_list;
	pgfree_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
d211 2
a212 2
static struct pginfo *
alloc_pginfo(void)
d214 1
a214 1
	struct pginfo *p;
d217 8
a224 7
	if (pginfo_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (p == MAP_FAILED)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pginfo_list;
			pginfo_list = &p[i];
a226 5
	p = pginfo_list;
	pginfo_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
}
a227 5
static void
put_pgfree(struct pgfree *p)
{
	p->next = pgfree_list;
	pgfree_list = p;
d231 1
a231 1
put_pginfo(struct pginfo *p)
d233 2
a234 3
	p->next = pginfo_list;
	pginfo_list = p;
}
d236 6
a241 77
/*
 * Function for page directory lookup.
 */
static int
pdir_lookup(u_long index, struct pdinfo ** pdi)
{
	struct pdinfo	*spi;
	u_long		pidx = PI_IDX(index);

	if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
		*pdi = last_dir;
	else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
		*pdi = prev_dir;
	else if (last_dir != NULL && prev_dir != NULL) {
		if ((PD_IDX(last_dir->dirnum) > pidx) ?
		    (PD_IDX(last_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(last_dir->dirnum))
		    < (PD_IDX(prev_dir->dirnum) > pidx) ?
		    (PD_IDX(prev_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(prev_dir->dirnum)))
			*pdi = last_dir;
		else
			*pdi = prev_dir;

		if (PD_IDX((*pdi)->dirnum) > pidx) {
			for (spi = (*pdi)->prev;
			    spi != NULL && PD_IDX(spi->dirnum) > pidx;
			    spi = spi->prev)
				*pdi = spi;
			if (spi != NULL)
				*pdi = spi;
		} else
			for (spi = (*pdi)->next;
			    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
			    spi = spi->next)
				*pdi = spi;
	} else {
		*pdi = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
		for (spi = *pdi;
		    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
		    spi = spi->next)
			*pdi = spi;
	}

	return ((PD_IDX((*pdi)->dirnum) == pidx) ? 0 :
	    (PD_IDX((*pdi)->dirnum) > pidx) ? 1 : -1);
}

#ifdef MALLOC_STATS
void
malloc_dump(int fd)
{
	char		buf[1024];
	struct pginfo	**pd;
	struct pgfree	*pf;
	struct pdinfo	*pi;
	u_long		j;

	pd = page_dir;
	pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);

	/* print out all the pages */
	for (j = 0; j <= last_index;) {
		snprintf(buf, sizeof buf, "%08lx %5lu ", j << malloc_pageshift, j);
		write(fd, buf, strlen(buf));
		if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu not mine\n", j);
d243 2
a244 34
		} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu free\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu in use\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
			snprintf(buf, sizeof buf, "(%p)\n", pd[PI_OFF(j)]);
			write(fd, buf, strlen(buf));
		} else {
			snprintf(buf, sizeof buf, "%p %d (of %d) x %d @@ %p --> %p\n",
			    pd[PI_OFF(j)], pd[PI_OFF(j)]->free,
			    pd[PI_OFF(j)]->total, pd[PI_OFF(j)]->size,
			    pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
a246 6
		if (!PI_OFF(++j)) {
			if ((pi = pi->next) == NULL)
				break;
			pd = pi->base;
			j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
		}
d248 1
d250 5
a254 11
	for (pf = free_list.next; pf; pf = pf->next) {
		snprintf(buf, sizeof buf, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		    pf, pf->page, (char *)pf->page + pf->size,
		    pf->size, pf->prev, pf->next);
		write(fd, buf, strlen(buf));
		if (pf == pf->next) {
			snprintf(buf, sizeof buf, "Free_list loops\n");
			write(fd, buf, strlen(buf));
			break;
		}
	}
d256 3
a258 2
	/* print out various info */
	snprintf(buf, sizeof buf, "Minsize\t%lu\n", malloc_minsize);
d260 3
a262 1
	snprintf(buf, sizeof buf, "Maxsize\t%lu\n", malloc_maxsize);
d264 3
a266 1
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", malloc_pagesize);
d268 2
a269 1
	snprintf(buf, sizeof buf, "Pageshift\t%u\n", malloc_pageshift);
d271 1
a271 1
	snprintf(buf, sizeof buf, "In use\t%lu\n", (u_long) malloc_used);
d273 22
a294 1
	snprintf(buf, sizeof buf, "Guarded\t%lu\n", (u_long) malloc_guarded);
d297 22
d321 1
a321 1
extern char	*__progname;
a340 1
	suicide = 1;
d345 1
a345 1
	malloc_active--;
a374 17
#ifdef MALLOC_STATS
static void
malloc_exit(void)
{
	char	*q = "malloc() warning: Couldn't dump stats\n";
	int	save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, strlen(q));
	errno = save_errno;
}
#endif /* MALLOC_STATS */

d376 5
a380 1
 * Allocate a number of pages from the OS
d382 2
a383 2
static void *
map_pages(size_t pages)
d385 10
a394 16
	struct pdinfo	*pi, *spi;
	struct pginfo	**pd;
	u_long		idx, pidx, lidx;
	caddr_t		result, tail;
	u_long		index, lindex;
	void 		*pdregion = NULL;
	size_t		dirs, cnt;

	pages <<= malloc_pageshift;
	result = MMAP(pages + malloc_guard);
	if (result == MAP_FAILED) {
#ifdef MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails");
#endif /* MALLOC_EXTRA_SANITY */
		errno = ENOMEM;
		return (NULL);
d396 17
a412 56
	index = ptr2index(result);
	tail = result + pages + malloc_guard;
	lindex = ptr2index(tail) - 1;
	if (malloc_guard)
		mprotect(result + pages, malloc_guard, PROT_NONE);

	pidx = PI_IDX(index);
	lidx = PI_IDX(lindex);

	if (tail > malloc_brk) {
		malloc_brk = tail;
		last_index = lindex;
	}

	dirs = lidx - pidx;

	/* Insert directory pages, if needed. */
	if (pdir_lookup(index, &pi) != 0)
		dirs++;

	if (dirs > 0) {
		pdregion = MMAP(malloc_pagesize * dirs);
		if (pdregion == MAP_FAILED) {
			munmap(result, tail - result);
#ifdef MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails");
#endif
			errno = ENOMEM;
			return (NULL);
		}
	}

	cnt = 0;
	for (idx = pidx, spi = pi; idx <= lidx; idx++) {
		if (pi == NULL || PD_IDX(pi->dirnum) != idx) {
			pd = (struct pginfo **)((char *)pdregion +
			    cnt * malloc_pagesize);
			cnt++;
			memset(pd, 0, malloc_pagesize);
			pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);
			pi->base = pd;
			pi->prev = spi;
			pi->next = spi->next;
			pi->dirnum = idx * (malloc_pagesize /
			    sizeof(struct pginfo *));

			if (spi->next != NULL)
				spi->next->prev = pi;
			spi->next = pi;
		}
		if (idx > pidx && idx < lidx) {
			pi->dirnum += pdi_mod;
		} else if (idx == pidx) {
			if (pidx == lidx) {
				pi->dirnum += (u_long)(tail - result) >>
				    malloc_pageshift;
d414 7
a420 1
				pi->dirnum += pdi_mod - PI_OFF(index);
a421 2
		} else {
			pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
d423 14
a436 5
#ifdef MALLOC_EXTRA_SANITY
		if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > idx) {
			wrterror("(ES): pages directory overflow");
			errno = EFAULT;
			return (NULL);
d438 64
a501 15
#endif /* MALLOC_EXTRA_SANITY */
		if (idx == pidx && pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		spi = pi;
		pi = spi->next;
	}
#ifdef MALLOC_EXTRA_SANITY
	if (cnt > dirs)
		wrtwarning("(ES): cnt > dirs");
#endif /* MALLOC_EXTRA_SANITY */
	if (cnt < dirs)
		munmap((char *)pdregion + cnt * malloc_pagesize,
		    (dirs - cnt) * malloc_pagesize);
d503 6
a508 1
	return (result);
d512 1
a512 1
 * Initialize the world
d514 2
a515 2
static void
malloc_init(void)
d517 3
a519 2
	char		*p, b[64];
	int		i, j, save_errno = errno;
d521 1
a521 3
#ifdef MALLOC_EXTRA_SANITY
	malloc_junk = 1;
#endif /* MALLOC_EXTRA_SANITY */
d549 2
d579 1
a579 1
				malloc_guard = malloc_pagesize;
d600 1
a600 1
				malloc_ptrguard = 0;
d603 1
a603 1
				malloc_ptrguard = 1;
a610 8
#ifdef __FreeBSD__
			case 'u':
				malloc_utrace = 0;
				break;
			case 'U':
				malloc_utrace = 1;
				break;
#endif /* __FreeBSD__ */
a632 2
	UTRACE(0, 0, 0);

d646 1
a646 10
	/* Allocate one page for the page directory. */
	page_dir = (struct pginfo **)MMAP(malloc_pagesize);

	if (page_dir == MAP_FAILED) {
		wrterror("mmap(2) failed, check limits");
		errno = ENOMEM;
		return;
	}
	pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
	pdi_mod = pdi_off / sizeof(struct pginfo *);
d648 14
a661 13
	last_dir = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
	last_dir->base = page_dir;
	last_dir->prev = last_dir->next = NULL;
	last_dir->dirnum = malloc_pageshift;

	/* Been here, done that. */
	malloc_started++;

	/* Recalculate the cache size in bytes, and make sure it's nonzero. */
	if (!malloc_cache)
		malloc_cache++;
	malloc_cache <<= malloc_pageshift;
	errno = save_errno;
d664 2
a665 5
/*
 * Allocate a number of complete pages
 */
static void *
malloc_pages(size_t size)
d667 33
a699 18
	void		*p, *tp;
	int		i;
	struct pginfo	**pd;
	struct pdinfo	*pi;
	u_long		pidx, index;
	struct pgfree	*pf, *delay_free = NULL;

	size = pageround(size) + malloc_guard;

	p = NULL;
	/* Look for free pages before asking for more */
	for (pf = free_list.next; pf; pf = pf->next) {

#ifdef MALLOC_EXTRA_SANITY
		if (pf->size & malloc_pagemask) {
			wrterror("(ES): junk length entry on free_list");
			errno = EFAULT;
			return (NULL);
d701 12
a712 42
		if (!pf->size) {
			wrterror("(ES): zero length entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if (pf->page > (pf->page + pf->size)) {
			wrterror("(ES): sick entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pi = pf->pdir) == NULL) {
			wrterror("(ES): invalid page directory on free-list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
			wrterror("(ES): directory index mismatch on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
			wrterror("(ES): non-free first page on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pidx = PI_IDX(ptr2index((pf->page) + (pf->size)) - 1);
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): last page not referenced in page directory");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index((pf->page) + (pf->size)) - 1)] != MALLOC_FREE) {
			wrterror("(ES): non-free last page on free-list");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
d714 14
a727 23
		if (pf->size < size)
			continue;

		if (pf->size == size) {
			p = pf->page;
			pi = pf->pdir;
			if (pf->next != NULL)
				pf->next->prev = pf->prev;
			pf->prev->next = pf->next;
			delay_free = pf;
			break;
		}
		p = pf->page;
		pf->page = (char *) pf->page + size;
		pf->size -= size;
		pidx = PI_IDX(ptr2index(pf->page));
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): hole in directories");
			errno = EFAULT;
			return (NULL);
a728 4
		tp = pf->pdir;
		pf->pdir = pi;
		pi = tp;
		break;
d730 7
d738 6
a743 1
	size -= malloc_guard;
d745 45
a789 4
#ifdef MALLOC_EXTRA_SANITY
	if (p != NULL && pi != NULL) {
		pidx = PD_IDX(pi->dirnum);
		pd = pi->base;
d791 2
a792 6
	if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
		wrterror("(ES): allocated non-free page on free-list");
		errno = EFAULT;
		return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */
d794 28
a821 40
	if (p != NULL && (malloc_guard || malloc_freeprot))
		mprotect(p, size, PROT_READ | PROT_WRITE);

	size >>= malloc_pageshift;

	/* Map new pages */
	if (p == NULL)
		p = map_pages(size);

	if (p != NULL) {
		index = ptr2index(p);
		pidx = PI_IDX(index);
		pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): mapped pages not found in directory");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		if (pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		pd = pi->base;
		pd[PI_OFF(index)] = MALLOC_FIRST;
		for (i = 1; i < size; i++) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FOLLOW;
a822 17
		if (malloc_guard) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FIRST;
		}
		malloc_used += size << malloc_pageshift;
		malloc_guarded += malloc_guard;
a823 8
		if (malloc_junk)
			memset(p, SOME_JUNK, size << malloc_pageshift);
	}
	if (delay_free) {
		if (px == NULL)
			px = delay_free;
		else
			put_pgfree(delay_free);
a824 1
	return (p);
d826 1
a826 1

d828 1
a828 1
 * Allocate a page of fragments
d830 2
a831 3

static int
malloc_make_chunks(int bits)
d833 1
a833 5
	struct pginfo	*bp, **pd;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
d838 3
a840 3
	pp = malloc_pages((size_t)malloc_pagesize);
	if (pp == NULL)
		return (0);
d842 1
a842 11
	/* Find length of admin structure */

	/* Don't waste more than two chunks on this */

	/*
	 * If we are to allocate a memory protected page for the malloc(0)
	 * case (when bits=0), it must be from a different page than the
	 * pginfo page.
	 * --> Treat it like the big chunk alloc, get a second data page.
	 */
	bp = alloc_pginfo();
d844 2
a845 2
		ifree(pp);
		return (0);
d852 1
a852 1
		i = malloc_minsize - 1;
d855 1
a855 1
		bp->total = bp->free = malloc_pagesize >> bp->shift;
d858 1
a858 1
		k = mprotect(pp, malloc_pagesize, PROT_NONE);
d860 3
a862 3
			ifree(pp);
			put_pginfo(bp);
			return (0);
d867 1
a867 1
		bp->total = bp->free = malloc_pagesize >> bits;
d882 2
a883 15
	pdir_lookup(ptr2index(pp), &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(ptr2index(pp));
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (0);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(pp))] = bp;
d885 3
a887 2
	bp->next = page_dir[bits];
	page_dir[bits] = bp;
d889 2
a890 2
	/* MALLOC_UNLOCK */
	return (1);
d893 1
d895 1
a895 1
 * Allocate a fragment
d898 1
a898 1
malloc_bytes(size_t size)
d903 1
a903 1
	struct pginfo	*bp;
d907 2
a908 2
	if (size != 0 && size < malloc_minsize)
		size = malloc_minsize;
d914 2
a915 2
		j = 1;
		i = size - 1;
d921 3
a923 4
	if (page_dir[j] == NULL && !malloc_make_chunks(j))
		return (NULL);

	bp = page_dir[j];
d925 2
d928 2
a929 1
	for (lp = bp->bits; !*lp; lp++);
d939 14
a952 20
	if (malloc_guard) {
		/* Walk to a random position. */
		i = arc4random_uniform(bp->free);
		while (i > 0) {
			u += u;
			k++;
			if (k >= MALLOC_BITS) {
				lp++;
				u = 1;
				k = 0;
			}
#ifdef MALLOC_EXTRA_SANITY
			if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
				wrterror("chunk overflow");
				errno = EFAULT;
				return (NULL);
			}
#endif /* MALLOC_EXTRA_SANITY */
			if (*lp & u)
				i--;
d954 2
d957 1
d962 1
a962 1
		page_dir[j] = bp->next;
d969 3
a971 53
	if (malloc_junk && bp->size != 0)
		memset((char *)bp->page + k, SOME_JUNK, (size_t)bp->size);

	return ((u_char *) bp->page + k);
}

/*
 * Magic so that malloc(sizeof(ptr)) is near the end of the page.
 */
#define	PTR_GAP		(malloc_pagesize - sizeof(void *))
#define	PTR_SIZE	(sizeof(void *))
#define	PTR_ALIGNED(p)	(((unsigned long)p & malloc_pagemask) == PTR_GAP)

/*
 * Allocate a piece of memory
 */
static void *
imalloc(size_t size)
{
	void		*result;
	int		ptralloc = 0;

	if (!malloc_started)
		malloc_init();

	if (suicide)
		abort();

	/* does not matter if alloc_pgfree() fails */
	if (px == NULL)
		px = alloc_pgfree();

	if (malloc_ptrguard && size == PTR_SIZE) {
		ptralloc = 1;
		size = malloc_pagesize;
	}
	if ((size + malloc_pagesize) < size) {	/* Check for overflow */
		result = NULL;
		errno = ENOMEM;
	} else if (size <= malloc_maxsize)
		result = malloc_bytes(size);
	else
		result = malloc_pages(size);

	if (malloc_abort == 1 && result == NULL)
		wrterror("allocation failed");

	if (malloc_zero && result != NULL)
		memset(result, 0, size);

	if (result && ptralloc)
		return ((char *) result + PTR_GAP);
	return (result);
a973 437
/*
 * Change the size of an allocation.
 */
static void *
irealloc(void *ptr, size_t size)
{
	void		*p;
	size_t		osize;
	u_long		index, i;
	struct pginfo	**mp;
	struct pginfo	**pd;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */

	if (suicide)
		abort();

	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return (NULL);
	}
	if (malloc_ptrguard && PTR_ALIGNED(ptr)) {
		if (size <= PTR_SIZE)
			return (ptr);

		p = imalloc(size);
		if (p)
			memcpy(p, ptr, PTR_SIZE);
		ifree(ptr);
		return (p);
	}
	index = ptr2index(ptr);

	if (index < malloc_pageshift) {
		wrtwarning("junk pointer, too low to make sense");
		return (NULL);
	}
	if (index > last_index) {
		wrtwarning("junk pointer, too high to make sense");
		return (NULL);
	}
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(index);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	mp = &pd[PI_OFF(index)];

	if (*mp == MALLOC_FIRST) {	/* Page allocation */

		/* Check the pointer */
		if ((u_long) ptr & malloc_pagemask) {
			wrtwarning("modified (page-) pointer");
			return (NULL);
		}
		/* Find the size in bytes */
		i = index;
		if (!PI_OFF(++i)) {
			pi = pi->next;
			if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
				pi = NULL;
			if (pi != NULL)
				pd = pi->base;
		}
		for (osize = malloc_pagesize;
		    pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
			osize += malloc_pagesize;
			if (!PI_OFF(++i)) {
				pi = pi->next;
				if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
					pi = NULL;
				if (pi != NULL)
					pd = pi->base;
			}
		}

		if (!malloc_realloc && size <= osize &&
		    size > osize - malloc_pagesize) {
			if (malloc_junk)
				memset((char *)ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else if (*mp >= MALLOC_MAGIC) {	/* Chunk allocation */

		/* Check the pointer for sane values */
		if ((u_long) ptr & ((1UL << ((*mp)->shift)) - 1)) {
			wrtwarning("modified (chunk-) pointer");
			return (NULL);
		}
		/* Find the chunk index in the page */
		i = ((u_long) ptr & malloc_pagemask) >> (*mp)->shift;

		/* Verify that it isn't a free chunk already */
		if ((*mp)->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
			wrtwarning("chunk is already free");
			return (NULL);
		}
		osize = (*mp)->size;

		if (!malloc_realloc && size <= osize &&
		    (size > osize / 2 || osize == malloc_minsize)) {
			if (malloc_junk)
				memset((char *) ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else {
		wrtwarning("irealloc: pointer to wrong page");
		return (NULL);
	}

	p = imalloc(size);

	if (p != NULL) {
		/* copy the lesser of the two sizes, and free the old one */
		/* Don't move from/to 0 sized region !!! */
		if (osize != 0 && size != 0) {
			if (osize < size)
				memcpy(p, ptr, osize);
			else
				memcpy(p, ptr, size);
		}
		ifree(ptr);
	}
	return (p);
}

/*
 * Free a sequence of pages
 */
static void
free_pages(void *ptr, u_long index, struct pginfo * info)
{
	u_long		i, pidx, lidx;
	size_t		l, cachesize = 0;
	struct pginfo	**pd;
	struct pdinfo	*pi, *spi;
	struct pgfree	*pf, *pt = NULL;
	caddr_t		tail;

	if (info == MALLOC_FREE) {
		wrtwarning("page is already free");
		return;
	}
	if (info != MALLOC_FIRST) {
		wrtwarning("free_pages: pointer to wrong page");
		return;
	}
	if ((u_long) ptr & malloc_pagemask) {
		wrtwarning("modified (page-) pointer");
		return;
	}
	/* Count how many pages and mark them free at the same time */
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */

	spi = pi;		/* Save page index for start of region. */

	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FREE;
	i = 1;
	if (!PI_OFF(index + i)) {
		pi = pi->next;
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i))
			pi = NULL;
		else
			pd = pi->base;
	}
	while (pi != NULL && pd[PI_OFF(index + i)] == MALLOC_FOLLOW) {
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		i++;
		if (!PI_OFF(index + i)) {
			if ((pi = pi->next) == NULL ||
			    PD_IDX(pi->dirnum) != PI_IDX(index + i))
				pi = NULL;
			else
				pd = pi->base;
		}
	}

	l = i << malloc_pageshift;

	if (malloc_junk)
		memset(ptr, SOME_JUNK, l);

	malloc_used -= l;
	malloc_guarded -= malloc_guard;
	if (malloc_guard) {
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i)) {
			wrterror("(ES): hole in mapped pages directory");
			errno = EFAULT;
			return;
		}
#endif /* MALLOC_EXTRA_SANITY */
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		l += malloc_guard;
	}
	tail = (caddr_t)ptr + l;

	if (malloc_hint)
		madvise(ptr, l, MADV_FREE);

	if (malloc_freeprot)
		mprotect(ptr, l, PROT_NONE);

	/* Add to free-list. */
	if (px == NULL && (px = alloc_pgfree()) == NULL)
		goto not_return;
	px->page = ptr;
	px->pdir = spi;
	px->size = l;

	if (free_list.next == NULL) {
		/* Nothing on free list, put this at head. */
		px->next = NULL;
		px->prev = &free_list;
		free_list.next = px;
		pf = px;
		px = NULL;
	} else {
		/*
		 * Find the right spot, leave pf pointing to the modified
		 * entry.
		 */

		/* Race ahead here, while calculating cache size. */
		for (pf = free_list.next;
		    (caddr_t)ptr > ((caddr_t)pf->page + pf->size)
		    && pf->next != NULL;
		    pf = pf->next)
			cachesize += pf->size;

		/* Finish cache size calculation. */
		pt = pf;
		while (pt) {
			cachesize += pt->size;
			pt = pt->next;
		}

		if ((caddr_t)pf->page > tail) {
			/* Insert before entry */
			px->next = pf;
			px->prev = pf->prev;
			pf->prev = px;
			px->prev->next = px;
			pf = px;
			px = NULL;
		} else if (((caddr_t)pf->page + pf->size) == ptr) {
			/* Append to the previous entry. */
			cachesize -= pf->size;
			pf->size += l;
			if (pf->next != NULL &&
			    pf->next->page == ((caddr_t)pf->page + pf->size)) {
				/* And collapse the next too. */
				pt = pf->next;
				pf->size += pt->size;
				pf->next = pt->next;
				if (pf->next != NULL)
					pf->next->prev = pf;
			}
		} else if (pf->page == tail) {
			/* Prepend to entry. */
			cachesize -= pf->size;
			pf->size += l;
			pf->page = ptr;
			pf->pdir = spi;
		} else if (pf->next == NULL) {
			/* Append at tail of chain. */
			px->next = NULL;
			px->prev = pf;
			pf->next = px;
			pf = px;
			px = NULL;
		} else {
			wrterror("freelist is destroyed");
			errno = EFAULT;
			return;
		}
	}

	if (pf->pdir != last_dir) {
		prev_dir = last_dir;
		last_dir = pf->pdir;
	}

	/* Return something to OS ? */
	if (pf->size > (malloc_cache - cachesize)) {

		/*
		 * Keep the cache intact.  Notice that the '>' above guarantees that
		 * the pf will always have at least one page afterwards.
		 */
		if (munmap((char *) pf->page + (malloc_cache - cachesize),
		    pf->size - (malloc_cache - cachesize)) != 0)
			goto not_return;
		tail = (caddr_t)pf->page + pf->size;
		lidx = ptr2index(tail) - 1;
		pf->size = malloc_cache - cachesize;

		index = ptr2index((caddr_t)pf->page + pf->size);

		pidx = PI_IDX(index);
		if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
			prev_dir = NULL;	/* Will be wiped out below ! */

		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;

		spi = pi;
		if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
			pd = pi->base;

			for (i = index; i <= lidx;) {
				if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
					pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef MALLOC_EXTRA_SANITY
					if (!PD_OFF(pi->dirnum)) {
						wrterror("(ES): pages directory underflow");
						errno = EFAULT;
						return;
					}
#endif /* MALLOC_EXTRA_SANITY */
					pi->dirnum--;
				}
#ifdef MALLOC_EXTRA_SANITY
				else
					wrtwarning("(ES): page already unmapped");
#endif /* MALLOC_EXTRA_SANITY */
				i++;
				if (!PI_OFF(i)) {
					/*
					 * If no page in that dir, free
					 * directory page.
					 */
					if (!PD_OFF(pi->dirnum)) {
						/* Remove from list. */
						if (spi == pi)
							spi = pi->prev;
						if (pi->prev != NULL)
							pi->prev->next = pi->next;
						if (pi->next != NULL)
							pi->next->prev = pi->prev;
						pi = pi->next;
						munmap(pd, malloc_pagesize);
					} else
						pi = pi->next;
					if (pi == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(i))
						break;
					pd = pi->base;
				}
			}
			if (pi && !PD_OFF(pi->dirnum)) {
				/* Resulting page dir is now empty. */
				/* Remove from list. */
				if (spi == pi)	/* Update spi only if first. */
					spi = pi->prev;
				if (pi->prev != NULL)
					pi->prev->next = pi->next;
				if (pi->next != NULL)
					pi->next->prev = pi->prev;
				pi = pi->next;
				munmap(pd, malloc_pagesize);
			}
		}
		if (pi == NULL && malloc_brk == tail) {
			/* Resize down the malloc upper boundary. */
			last_index = index - 1;
			malloc_brk = index2ptr(index);
		}

		/* XXX: We could realloc/shrink the pagedir here I guess. */
		if (pf->size == 0) {	/* Remove from free-list as well. */
			if (px)
				put_pgfree(px);
			if ((px = pf->prev) != &free_list) {
				if (pi == NULL && last_index == (index - 1)) {
					if (spi == NULL) {
						malloc_brk = NULL;
						i = 11;
					} else {
						pd = spi->base;
						if (PD_IDX(spi->dirnum) < pidx)
							index =
							    ((PD_IDX(spi->dirnum) + 1) *
							    pdi_mod) - 1;
						for (pi = spi, i = index;
						    pd[PI_OFF(i)] == MALLOC_NOT_MINE;
						    i--) {
#ifdef MALLOC_EXTRA_SANITY
							if (!PI_OFF(i)) {
								pi = pi->prev;
								if (pi == NULL || i == 0)
									break;
								pd = pi->base;
								i = (PD_IDX(pi->dirnum) + 1) * pdi_mod;
							}
#endif /* MALLOC_EXTRA_SANITY */
						}
						malloc_brk = index2ptr(i + 1);
					}
					last_index = i;
				}
				if ((px->next = pf->next) != NULL)
					px->next->prev = px;
			} else {
				if ((free_list.next = pf->next) != NULL)
					free_list.next->prev = &free_list;
			}
			px = pf;
			last_dir = prev_dir;
			prev_dir = NULL;
		}
	}
not_return:
	if (pt != NULL)
		put_pgfree(pt);
}
a977 2

/* ARGSUSED */
d979 1
a979 1
free_bytes(void *ptr)
d981 2
a982 32
	u_int8_t __arc4_getbyte(void);
	struct pginfo	**mp, **pd, *info;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
	u_long		index;
	void		*vp;
	long		i;
	void *tmpptr;
	unsigned int tmpidx;
	/* pointers that we will want to free at some future time */
	static void *chunk_buffer[16];

	
	/* delay return, returning a random something from before instead */
	tmpidx = __arc4_getbyte() % 16;
	tmpptr = chunk_buffer[tmpidx];
	chunk_buffer[tmpidx] = ptr;
	ptr = tmpptr;
	if (!ptr)
		return;
	
	index = ptr2index(ptr);

	pdir_lookup(index, &pi);
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];
d984 3
d989 1
a989 1
	i = ((u_long) ptr & malloc_pagemask) >> info->shift;
d991 1
a991 1
	if ((u_long) ptr & ((1UL << (info->shift)) - 1)) {
a998 2
	if (malloc_junk && info->size != 0)
		memset(ptr, SOME_JUNK, (size_t)info->size);
d1004 1
a1004 1
		mp = page_dir + info->shift;
d1006 1
a1006 1
		mp = page_dir;
a1024 1
#ifdef MALLOC_EXTRA_SANITY
d1026 1
a1026 1
			wrterror("(ES): Not on queue");
a1029 1
#endif /* MALLOC_EXTRA_SANITY */
d1033 6
a1038 24
	/* Free the page & the info structure if need be */
	pdir_lookup(ptr2index(info->page), &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(ptr2index(info->page));
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;

	/* If the page was mprotected, unprotect it before releasing it */
	if (info->size == 0)
		mprotect(info->page, malloc_pagesize, PROT_READ | PROT_WRITE);

	vp = info->page;
	put_pginfo(info);
	ifree(vp);
a1040 9
static void
ifree(void *ptr)
{
	struct pginfo	*info, **pd;
	u_long		index;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
	struct pdinfo	*pi;
a1041 7
	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return;
	}
	/* If we're already sinking, don't make matters any worse. */
	if (suicide)
		return;
d1043 5
a1047 2
	if (malloc_ptrguard && PTR_ALIGNED(ptr))
		ptr = (char *)ptr - PTR_GAP;
d1049 25
a1073 1
	index = ptr2index(ptr);
d1075 12
a1086 9
	if (index < malloc_pageshift) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too low to make sense");
		return;
	}
	if (index > last_index) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too high to make sense");
		return;
d1089 1
a1089 26
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(index);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];

	if (info < MALLOC_MAGIC)
		free_pages(ptr, index, info);
	else
		free_bytes(ptr);

	/* does not matter if alloc_pgfree fails */
	if (px == NULL)
		px = alloc_pgfree();

	return;
d1097 1
a1097 1
static void
d1100 1
a1100 1
	static int	noprint;
a1110 3
/*
 * These are the public exported interface routines.
 */
d1114 1
a1114 1
	void		*r;
d1118 9
d1129 1
a1129 1
		return (NULL);
d1131 1
a1131 2
	r = imalloc(size);
	UTRACE(0, size, r);
d1134 1
a1134 1
	if (malloc_xmalloc && r == NULL) {
d1138 63
a1200 1
	return (r);
d1206 1
a1206 1
	/* This is legal. XXX quick path */
d1211 1
a1211 1
	malloc_func = " in free():";
d1216 1
a1216 2
	ifree(ptr);
	UTRACE(ptr, 0, 0);
d1219 74
a1292 1
	return;
d1298 2
a1299 2
	void		*r;

d1301 10
a1310 1
	malloc_func = " in realloc():";
d1313 1
a1313 1
		return (NULL);
d1316 2
a1317 6
	if (ptr == NULL)
		r = imalloc(size);
	else
		r = irealloc(ptr, size);

	UTRACE(ptr, size, r);
d1320 1
a1320 1
	if (malloc_xmalloc && r == NULL) {
d1324 1
a1324 1
	return (r);
@


1.90
log
@remove recalloc(3); it is buggy and impossible to repair without big
costs; ok jmc@@ for the man page bits; ok millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.89 2008/04/13 00:22:16 djm Exp $	*/
a657 2

	_MALLOC_LOCK_INIT();
@


1.89
log
@Use arc4random_buf() when requesting more than a single word of output

Use arc4random_uniform() when the desired random number upper bound
is not a power of two

ok deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.88 2008/02/20 18:31:34 otto Exp $	*/
d253 1
a253 1
static void	*imalloc(size_t size, int zero_fill);
d255 1
a255 1
static void	*irealloc(void *ptr, size_t size, int zero_fill);
d1191 1
a1191 1
imalloc(size_t size, int zero_fill)
d1221 1
a1221 1
	if ((malloc_zero || zero_fill) && result != NULL)
d1233 1
a1233 1
irealloc(void *ptr, size_t size, int zero_fill)
d1256 1
a1256 1
		p = imalloc(size, zero_fill);
d1318 1
a1318 3
			if (zero_fill)
				memset((char *)ptr + size, 0, osize - size);
			else if (malloc_junk)
d1341 1
a1341 3
			if (zero_fill)
				memset((char *) ptr + size, 0, osize - size);
			else if (malloc_junk)
d1350 1
a1350 1
	p = imalloc(size, zero_fill);
d1879 1
a1879 1
	r = imalloc(size, 0);
d1910 2
a1911 2
static void *
_realloc(void *ptr, size_t size, int zero_fill)
d1923 1
a1923 1
		r = imalloc(size, zero_fill);
d1925 1
a1925 1
		r = irealloc(ptr, size, zero_fill);
a1934 16
}

void *
realloc(void *ptr, size_t size)
{
	return (_realloc(ptr, size, 0));
}

void *
recalloc(void *ptr, size_t nmemb, size_t size)
{
	if (nmemb && SIZE_MAX / nmemb < size) {
		errno = ENOMEM;
		return (NULL);
	}
	return (_realloc(ptr, nmemb * size, 1));
@


1.88
log
@use pgfree pool like other code does to reserve free list slots.
prevents a few "cannot free mem because i need mem to free mem"
scenarios (one found by weingart@@). ok weingart@@ millert@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.87 2007/09/03 14:37:02 millert Exp $	*/
d1143 1
a1143 1
		i = arc4random() % bp->free;
@


1.87
log
@add recaloc(3)
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.86 2007/02/12 20:00:14 otto Exp $	*/
d263 1
a263 1
alloc_pgfree()
d284 1
a284 1
alloc_pginfo()
d1202 1
a1202 1
	/* does not matter if malloc_bytes fails */
d1204 1
a1204 1
		px = malloc_bytes(sizeof *px);
d1843 1
a1843 1
	/* does not matter if malloc_bytes fails */
@


1.86
log
@get cheaper random bytes, less waste and no getpid() calls, which are
done by arc4random(); ok millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.85 2006/12/19 13:00:50 otto Exp $	*/
d253 1
a253 1
static void	*imalloc(size_t size);
d255 1
a255 1
static void	*irealloc(void *ptr, size_t size);
d1191 1
a1191 1
imalloc(size_t size)
d1221 1
a1221 1
	if (malloc_zero && result != NULL)
d1233 1
a1233 1
irealloc(void *ptr, size_t size)
d1256 1
a1256 1
		p = imalloc(size);
d1318 3
a1320 1
			if (malloc_junk)
d1343 3
a1345 1
			if (malloc_junk)
d1354 1
a1354 1
	p = imalloc(size);
d1883 1
a1883 1
	r = imalloc(size);
d1914 2
a1915 2
void *
realloc(void *ptr, size_t size)
d1927 1
a1927 1
		r = imalloc(size);
d1929 1
a1929 1
		r = irealloc(ptr, size);
d1939 16
@


1.85
log
@a failed mmap returns MAP_FAILED, not NULL. found while exercising pax
in low-mem conditions; ok dim@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.84 2006/10/24 04:35:30 tedu Exp $	*/
d1675 1
d1691 1
a1691 1
	tmpidx = arc4random() % 16;
@


1.84
log
@respond to ben hawkes's ruxcon presentation.
create special allocators for pginfo and pgfree structs instead of imalloc.
this keeps them separated from application memory.
for chunks, to prevent deterministic reuse, keep a small array
and swizzle the to be freed chunk with a random previously freed chunk.
this last bit only for chunks because keeping arbitrarily large regions
of pages around may cause out of memory issues (and pages are, to some
extent, returned in random order).
all changes enabled by default.
thanks to ben for pointing out these issues.
ok tech@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.83 2006/05/14 19:53:40 otto Exp $	*/
d270 1
a270 1
		if (!p)
d291 1
a291 1
		if (!p)
@


1.83
log
@Fix the second malloc_ulimit regression: maintaining the free list
requires memory; try to make sure we have it. If all fails, leak
instead of crash. Test case originally found by cloder@@, fix tested
by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.82 2006/04/24 19:23:42 otto Exp $	*/
d78 12
d102 1
a102 1
	u_long		bits[1];/* Which chunks are free */
a104 3
/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))

a124 8
#ifndef malloc_minsize
#define malloc_minsize			16UL
#endif

#if !defined(malloc_pagesize)
#define malloc_pagesize			(1UL<<malloc_pageshift)
#endif

d258 60
d828 1
a828 1
	void		*p, *delay_free = NULL, *tp;
d833 1
a833 1
	struct pgfree	*pf;
d1000 1
a1000 1
			ifree(delay_free);
d1009 1
a1009 1
static __inline__ int
a1018 1
	size_t		l;
d1021 1
a1021 1
	pp = malloc_pages((size_t) malloc_pagesize);
a1025 3
	l = sizeof *bp - sizeof(u_long);
	l += sizeof(u_long) *
	    (((malloc_pagesize >> bits) + MALLOC_BITS - 1) / MALLOC_BITS);
d1035 4
a1038 8
	if (bits != 0 && (1UL << (bits)) <= l + l) {
		bp = (struct pginfo *) pp;
	} else {
		bp = (struct pginfo *) imalloc(l);
		if (bp == NULL) {
			ifree(pp);
			return (0);
		}
d1054 1
a1054 1
			ifree(bp);
a1074 12
	k = (long)l;
	if (bp == bp->page) {
		/* Mark the ones we stole for ourselves */
		for (i = 0; k > 0; i++) {
			bp->bits[i / MALLOC_BITS] &= ~(1UL << (i % MALLOC_BITS));
			bp->free--;
			bp->total--;
			k -= (1 << bits);
		}
	}
	/* MALLOC_LOCK */

d1369 1
a1369 1
static __inline__ void
d1453 2
a1454 2
	if (px == NULL && (px = malloc_bytes(sizeof *px)) == NULL)
			goto not_return;
d1622 1
a1622 1
				ifree(px);
d1636 1
a1636 1
						    i--)
d1645 1
a1645 2
#else /* !MALLOC_EXTRA_SANITY */
						{
a1646 1
#endif /* MALLOC_EXTRA_SANITY */
d1664 1
a1664 1
		ifree(pt);
d1672 2
a1673 2
static __inline__ void
free_bytes(void *ptr, u_long index, struct pginfo * info)
d1675 1
a1675 1
	struct pginfo	**mp, **pd;
d1680 1
d1683 24
d1778 2
a1779 3
	vp = info->page;	/* Order is important ! */
	if (vp != (void *) info)
		ifree(info);
d1802 1
a1802 1
		ptr = (char *) ptr - PTR_GAP;
d1816 1
d1836 1
a1836 1
		free_bytes(ptr, index, info);
d1840 1
a1840 1
		px = malloc_bytes(sizeof *px);
@


1.82
log
@Do not leave an hole in the directory list if allocation of the
region succeeds, but allocation a required page dir failed. This
can happen if we're really close to ulimit after allocation the
region of the size requested.  See malloc_ulimit1 regress test.
Tested by many; thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.81 2006/04/18 18:26:13 otto Exp $	*/
d1161 4
d1412 2
a1413 2
	if (px == NULL)
		px = imalloc(sizeof *px);	/* This cannot fail... */
d1773 5
@


1.81
log
@delint; original from deraadt@@ with fixes from tdeval@@ and me;
tested by quite a few developers. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.80 2006/02/14 11:14:11 espie Exp $	*/
d490 2
a495 1
		errno = ENOMEM;
d499 1
d515 3
d519 14
a532 1
	pdir_lookup(index, &pi);
d534 1
d537 3
a539 8
			if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
				errno = ENOMEM;		/* XXX */
				munmap(result, (size_t)(tail - result));
#ifdef MALLOC_EXTRA_SANITY
				wrtwarning("(ES): map_pages fails");
#endif /* MALLOC_EXTRA_SANITY */
				return (NULL);
			}
d578 7
@


1.80
log
@quick path for free(0)
`looks to be safe' millert, okay tedu.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.79 2005/10/10 12:00:52 espie Exp $	*/
d117 1
a117 1
#define malloc_minsize			16U
d192 2
a193 2
static int	malloc_guard = 0;
static int	malloc_guarded;
d234 1
a234 1
static void	*malloc_brk;
d313 1
a313 1
	int		j;
d320 1
a320 1
		snprintf(buf, sizeof buf, "%08lx %5d ", j << malloc_pageshift, j);
d333 1
a333 1
			snprintf(buf, sizeof buf, ".. %5d not mine\n", j);
d346 1
a346 1
			snprintf(buf, sizeof buf, ".. %5d free\n", j);
d359 1
a359 1
			snprintf(buf, sizeof buf, ".. %5d in use\n", j);
d381 1
a381 1
		    pf, pf->page, pf->page + pf->size,
d392 1
a392 1
	snprintf(buf, sizeof buf, "Minsize\t%d\n", malloc_minsize);
d394 1
a394 1
	snprintf(buf, sizeof buf, "Maxsize\t%d\n", malloc_maxsize);
d396 1
a396 1
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", (u_long) malloc_pagesize);
d398 1
a398 1
	snprintf(buf, sizeof buf, "Pageshift\t%d\n", malloc_pageshift);
d488 1
a488 1
	void		*result, *tail;
d520 1
a520 1
				munmap(result, tail - result);
d531 2
a532 1
			pi->dirnum = idx * (malloc_pagesize / sizeof(struct pginfo *));
d542 2
a543 1
				pi->dirnum += (tail - result) >> malloc_pageshift;
d932 1
d934 1
d936 2
a937 1
	int		i, k, l;
d995 1
a995 1
	for (; k - i >= MALLOC_BITS; i += MALLOC_BITS)
d1001 1
d1004 1
a1004 1
		for (i = 0; l > 0; i++) {
d1008 1
a1008 1
			l -= (1 << bits);
a1012 1
	pidx = PI_IDX(ptr2index(pp));
d1015 1
d1042 2
a1043 1
	int		i, j, k;
d1113 1
a1113 1
		memset((char *) bp->page + k, SOME_JUNK, bp->size);
d1170 2
a1171 1
	u_long		osize, index, i;
d1175 1
d1177 1
a1205 1
	pidx = PI_IDX(index);
d1208 1
d1306 2
a1307 1
	u_long		i, l, cachesize = 0, pidx, lidx;
d1311 1
a1311 1
	void		*tail;
d1378 1
a1378 1
	tail = (char *) ptr + l;
d1408 2
a1409 1
		    pf->page + pf->size < ptr && pf->next != NULL;
d1420 1
a1420 1
		if (pf->page > tail) {
d1428 1
a1428 1
		} else if ((pf->page + pf->size) == ptr) {
d1433 1
a1433 1
			    pf->page + pf->size == pf->next->page) {
d1476 1
a1476 1
		tail = pf->page + pf->size;
d1480 1
a1480 1
		index = ptr2index(pf->page + pf->size);
d1609 1
a1609 1
free_bytes(void *ptr, int index, struct pginfo * info)
d1613 1
d1615 1
d1617 1
a1617 1
	int		i;
d1631 1
a1631 1
		memset(ptr, SOME_JUNK, info->size);
a1668 1
	pidx = PI_IDX(ptr2index(info->page));
d1671 1
d1699 4
a1702 1
	u_long		pidx, index;
a1727 1
	pidx = PI_IDX(index);
d1730 1
@


1.79
log
@Remove a few warnings. Those were not apparent thanks to a bug in gcc 2.95.

Patch by Leonardo Chiquitto Filho <leonardo@@iken.com.br>
Thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.78 2005/10/05 18:38:10 deraadt Exp $	*/
a1687 4
	/* This is legal */
	if (ptr == NULL)
		return;

d1781 4
@


1.78
log
@further knf and cleaning; ok tdeval
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d51 1
d745 1
a745 2
	int		i, m;
	struct rlimit	rl;
@


1.77
log
@first KNF (no binary diffs)
@
text
@d1 2
a2 1
/* $OpenBSD: malloc.c,v 1.76 2005/08/08 08:05:36 espie Exp $ */
d69 7
a75 3
#if defined(__OpenBSD__) && defined(__sparc__)
#define    malloc_pageshift	13U
#endif				/* __OpenBSD__ */
a81 1

d83 7
a89 7
	struct pginfo  *next;	/* next on the free list */
	void           *page;	/* Pointer to the page */
	u_short         size;	/* size of this page's chunks */
	u_short         shift;	/* How far to shift for this size chunks */
	u_short         free;	/* How many free chunks */
	u_short         total;	/* How many chunk */
	u_long          bits[1];/* Which chunks are free */
d92 3
a97 1

d99 5
a103 5
	struct pgfree  *next;	/* next run of free pages */
	struct pgfree  *prev;	/* prev run of free pages */
	void           *page;	/* pointer to free pages */
	void           *pdir;	/* pointer to the base page's dir */
	size_t          size;	/* number of bytes free */
a106 6
 * How many bits per u_long in the bitmap.
 * Change only if not 8 bits/byte
 */
#define	MALLOC_BITS	(8*sizeof(u_long))

/*
a114 4
#ifndef malloc_pageshift
#define malloc_pageshift		(PGSHIFT)
#endif

a118 4
#ifndef malloc_pageshift
#error	"malloc_pageshift undefined"
#endif

d131 1
a131 1
/* A mask for the offset inside a page.  */
a137 12
/* fd of /dev/zero */
#ifdef USE_DEV_ZERO
static int      fdzero;
#define	MMAP_FD	fdzero
#define INIT_MMAP() \
	{ if ((fdzero=open("/dev/zero", O_RDWR, 0000)) == -1) \
	    wrterror("open of /dev/zero\n"); }
#else
#define MMAP_FD (-1)
#define INIT_MMAP()
#endif

d139 1
a139 1
static unsigned int malloc_started;
d142 1
a142 1
static unsigned int malloc_cache = 16;
d146 4
a149 4
	struct pginfo **base;
	struct pdinfo  *prev;
	struct pdinfo  *next;
	u_long          dirnum;
d152 1
a152 1
static struct pdinfo *prev_dir;	/* referenced directory pages.     */
d154 2
a155 2
static size_t   pdi_off;
static u_long   pdi_mod;
d162 1
a162 1
static u_long   last_index;
d170 2
a171 2
/* Abort(), user doesn't handle problems.  */
static int      malloc_abort = 2;
d173 2
a174 2
/* Are we trying to die ?  */
static int      suicide;
d176 1
a176 1
#ifdef	MALLOC_STATS
d178 1
a178 1
static int      malloc_stats;
d181 2
a182 2
/* avoid outputting warnings?  */
static int      malloc_silent;
d184 2
a185 2
/* always realloc ?  */
static int      malloc_realloc;
d188 1
a188 1
static int      malloc_freeprot;
d191 2
a192 2
static int      malloc_guard = 0;
static int      malloc_guarded;
d194 1
a194 1
static int      malloc_ptrguard;
d196 1
a196 4
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
/* pass the kernel a hint on free pages ?  */
static int      malloc_hint;
#endif
d198 2
a199 2
/* xmalloc behaviour ?  */
static int      malloc_xmalloc;
d201 2
a202 2
/* zero fill ?  */
static int      malloc_zero;
d204 2
a205 2
/* junk fill ?  */
static int      malloc_junk;
d208 2
a209 2
/* utrace ?  */
static int      malloc_utrace;
d212 3
a214 3
	void           *p;
	size_t          s;
	void           *r;
d217 1
a217 1
void            utrace(struct ut *, int);
d227 1
a227 1
static int      malloc_active;
d230 1
a230 1
static size_t   malloc_used;
d233 1
a233 1
static void    *malloc_brk;
d239 1
a239 1
char           *malloc_options;
d242 1
a242 1
static char    *malloc_func;
a243 1
/* Macro for mmap. */
d246 1
a246 1
	    MMAP_FD, (off_t)0)
d251 4
a254 5
static void    *imalloc(size_t size);
static void     ifree(void *ptr);
static void    *irealloc(void *ptr, size_t size);
static void    *malloc_bytes(size_t size);

d262 2
a263 2
	struct pdinfo  *spi;
	u_long          pidx = PI_IDX(index);
d271 2
a272 1
		    (PD_IDX(last_dir->dirnum) - pidx) : (pidx - PD_IDX(last_dir->dirnum))
d274 2
a275 1
		    (PD_IDX(prev_dir->dirnum) - pidx) : (pidx - PD_IDX(prev_dir->dirnum)))
d281 3
a283 2
			for (spi = (*pdi)->prev; spi != NULL && PD_IDX(spi->dirnum) > pidx;
			     spi = spi->prev)
d288 3
a290 2
			for (spi = (*pdi)->next; spi != NULL && PD_IDX(spi->dirnum) <= pidx;
			     spi = spi->next)
d294 3
a296 1
		for (spi = *pdi; spi != NULL && PD_IDX(spi->dirnum) <= pidx; spi = spi->next)
d300 2
a301 1
	return ((PD_IDX((*pdi)->dirnum) == pidx) ? 0 : (PD_IDX((*pdi)->dirnum) > pidx) ? 1 : -1);
d304 1
a304 2

#ifdef	MALLOC_STATS
d306 1
a306 1
malloc_dump(FILE * fd)
d308 5
a312 4
	struct pginfo **pd;
	struct pgfree  *pf;
	struct pdinfo  *pi;
	int             j;
d319 2
a320 1
		fprintf(fd, "%08lx %5d ", j << malloc_pageshift, j);
d332 2
a333 1
			fprintf(fd, ".. %5d not mine\n", j);
d345 2
a346 1
			fprintf(fd, ".. %5d free\n", j);
d358 2
a359 1
			fprintf(fd, ".. %5d in use\n", j);
d361 2
a362 1
			fprintf(fd, "(%p)\n", pd[PI_OFF(j)]);
d364 5
a368 3
			fprintf(fd, "%p %d (of %d) x %d @@ %p --> %p\n",
				pd[PI_OFF(j)], pd[PI_OFF(j)]->free, pd[PI_OFF(j)]->total,
				pd[PI_OFF(j)]->size, pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
d379 4
a382 3
		fprintf(fd, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
			pf, pf->page, pf->page + pf->size, pf->size,
			pf->prev, pf->next);
d384 2
a385 1
			fprintf(fd, "Free_list loops\n");
d391 12
a402 6
	fprintf(fd, "Minsize\t%d\n", malloc_minsize);
	fprintf(fd, "Maxsize\t%d\n", malloc_maxsize);
	fprintf(fd, "Pagesize\t%lu\n", (u_long) malloc_pagesize);
	fprintf(fd, "Pageshift\t%d\n", malloc_pageshift);
	fprintf(fd, "In use\t%lu\n", (u_long) malloc_used);
	fprintf(fd, "Guarded\t%lu\n", (u_long) malloc_guarded);
d404 1
a404 1
#endif				/* MALLOC_STATS */
d406 1
a406 1
extern char    *__progname;
d411 2
a412 2
	char           *q = " error: ";
	struct iovec    iov[4];
d422 3
a424 1
	writev(STDERR_FILENO, iov, 4);
d427 1
a427 1
#ifdef	MALLOC_STATS
d429 2
a430 2
		malloc_dump(stderr);
#endif				/* MALLOC_STATS */
d439 2
a440 2
	char           *q = " warning: ";
	struct iovec    iov[4];
d455 4
a458 1
	writev(STDERR_FILENO, iov, 4);
d461 1
a461 1
#ifdef	MALLOC_STATS
d465 5
a469 3
	FILE           *fd = fopen("malloc.out", "a");
	char           *q = "malloc() warning: Couldn't dump stats\n";
	if (fd != NULL) {
d471 1
a471 1
		fclose(fd);
d474 1
d476 1
a476 2
#endif				/* MALLOC_STATS */

d481 1
a481 1
static void    *
d484 5
a488 5
	struct pdinfo  *pi, *spi;
	struct pginfo **pd;
	u_long          idx, pidx, lidx;
	void           *result, *tail;
	u_long          index, lindex;
d494 3
a496 3
#ifdef	MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails\n");
#endif				/* MALLOC_EXTRA_SANITY */
d518 1
a518 1
				errno = ENOMEM;
d520 3
a522 3
#ifdef	MALLOC_EXTRA_SANITY
				wrtwarning("(ES): map_pages fails\n");
#endif				/* MALLOC_EXTRA_SANITY */
d547 1
a547 1
#ifdef	MALLOC_EXTRA_SANITY
d549 1
a549 1
			wrterror("(ES): pages directory overflow\n");
d553 1
a553 1
#endif				/* MALLOC_EXTRA_SANITY */
a564 1

d571 2
a572 3
	char           *p, b[64];
	int             i, j;
	int             save_errno = errno;
d576 1
a576 3
	INIT_MMAP();

#ifdef	MALLOC_EXTRA_SANITY
d578 1
a578 1
#endif				/* MALLOC_EXTRA_SANITY */
a588 1

a594 1

a597 1

d601 1
d616 1
a616 1
#ifdef	MALLOC_STATS
d623 1
a623 1
#endif				/* MALLOC_STATS */
a635 1
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
a641 1
#endif				/* __FreeBSD__ */
d673 1
a673 1
#endif				/* __FreeBSD__ */
d689 1
a689 1
				wrtwarning("unknown char in MALLOC_OPTIONS\n");
d699 3
a701 3
         * We want junk in the entire allocation, and zero only in the part
         * the user asked for.
         */
d705 1
a705 1
#ifdef	MALLOC_STATS
d707 3
a709 2
		wrtwarning("atexit(2) failed.  Will not be able to dump malloc stats on exit\n");
#endif				/* MALLOC_STATS */
d712 1
a712 1
	page_dir = (struct pginfo **) MMAP(malloc_pagesize);
d715 1
a715 1
		wrterror("mmap(2) failed, check limits\n");
a730 1

a732 1

a733 1

d740 1
a740 1
static void    *
d743 7
a749 10
	void           *p, *delay_free = NULL;
	int             i;
	struct rlimit   rl;
	struct pginfo **pd;
	struct pdinfo  *pi;
	u_long          pidx;
	void           *tp;
	struct pgfree  *pf;
	u_long          index;
	int             m;
d757 1
a757 1
#ifdef	MALLOC_EXTRA_SANITY
d759 1
a759 1
			wrterror("(ES): junk length entry on free_list\n");
d764 1
a764 1
			wrterror("(ES): zero length entry on free_list\n");
d769 1
a769 1
			wrterror("(ES): sick entry on free_list\n");
d774 1
a774 1
			wrterror("(ES): invalid page directory on free-list\n");
d779 1
a779 1
			wrterror("(ES): directory index mismatch on free-list\n");
d785 1
a785 1
			wrterror("(ES): non-free first page on free-list\n");
d790 3
a792 1
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx; pi = pi->next);
d794 1
a794 1
			wrterror("(ES): last page not referenced in page directory\n");
d800 1
a800 1
			wrterror("(ES): non-free last page on free-list\n");
d804 1
a804 1
#endif				/* MALLOC_EXTRA_SANITY */
d822 3
a824 1
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx; pi = pi->next);
d826 1
a826 1
			wrterror("(ES): hole in directories\n");
d838 1
a838 1
#ifdef	MALLOC_EXTRA_SANITY
d844 1
a844 1
		wrterror("(ES): allocated non-free page on free-list\n");
d848 1
a848 1
#endif				/* MALLOC_EXTRA_SANITY */
a859 1

d863 1
a863 1
#ifdef	MALLOC_EXTRA_SANITY
d865 1
a865 1
			wrterror("(ES): mapped pages not found in directory\n");
d869 1
a869 1
#endif				/* MALLOC_EXTRA_SANITY */
d880 1
a880 1
#ifdef	MALLOC_EXTRA_SANITY
d882 1
a882 1
					wrterror("(ES): hole in mapped pages directory\n");
d886 1
a886 1
#endif				/* MALLOC_EXTRA_SANITY */
d895 1
a895 1
#ifdef	MALLOC_EXTRA_SANITY
d897 1
a897 1
					wrterror("(ES): hole in mapped pages directory\n");
d901 1
a901 1
#endif				/* MALLOC_EXTRA_SANITY */
d928 5
a932 6
	struct pginfo  *bp;
	struct pginfo **pd;
	struct pdinfo  *pi;
	u_long          pidx;
	void           *pp;
	int             i, k, l;
d942 1
a942 1
		(((malloc_pagesize >> bits) + MALLOC_BITS - 1) / MALLOC_BITS);
d945 1
d947 5
a951 5
         * If we are to allocate a memory protected page for the malloc(0)
         * case (when bits=0), it must be from a different page than the
         * pginfo page.
         * --> Treat it like the big chunk alloc, get a second data page.
         */
a963 1

d1009 1
a1009 1
#ifdef	MALLOC_EXTRA_SANITY
d1011 1
a1011 1
		wrterror("(ES): mapped pages not found in directory\n");
d1015 1
a1015 1
#endif				/* MALLOC_EXTRA_SANITY */
a1026 1

d1033 1
a1033 1
static void    *
d1036 3
a1038 5
	int             i, j;
	u_long          u;
	struct pginfo  *bp;
	int             k;
	u_long         *lp;
d1083 1
a1083 1
#ifdef	MALLOC_EXTRA_SANITY
d1085 1
a1085 1
				wrterror("chunk overflow\n");
d1089 1
a1089 1
#endif				/* MALLOC_EXTRA_SANITY */
d1121 1
a1121 1
static void    *
d1124 2
a1125 2
	void           *result;
	int             ptralloc = 0;
d1146 1
a1146 1
		wrterror("allocation failed\n");
d1159 1
a1159 1
static void    *
d1162 6
a1167 6
	void           *p;
	u_long          osize, index, i;
	struct pginfo **mp;
	struct pginfo **pd;
	struct pdinfo  *pi;
	u_long          pidx;
d1173 1
a1173 1
		wrtwarning("malloc() has never been called\n");
d1177 1
a1177 1
		if (size <= PTR_SIZE) {
d1179 6
a1184 7
		} else {
			p = imalloc(size);
			if (p)
				memcpy(p, ptr, PTR_SIZE);
			ifree(ptr);
			return (p);
		}
d1189 1
a1189 1
		wrtwarning("junk pointer, too low to make sense\n");
d1193 1
a1193 1
		wrtwarning("junk pointer, too high to make sense\n");
d1198 1
a1198 1
#ifdef	MALLOC_EXTRA_SANITY
d1200 1
a1200 1
		wrterror("(ES): mapped pages not found in directory\n");
d1204 1
a1204 1
#endif				/* MALLOC_EXTRA_SANITY */
d1216 1
a1216 1
			wrtwarning("modified (page-) pointer\n");
d1229 1
a1229 1
		     pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
d1240 2
a1241 4
		if (!malloc_realloc &&	/* Unless we have to, */
		    size <= osize &&	/* .. or are too small, */
		    size > (osize - malloc_pagesize)) {	/* .. or can free a
							 * page, */
d1243 1
a1243 1
				memset((char *) ptr + size, SOME_JUNK, osize - size);
d1250 1
a1250 1
			wrtwarning("modified (chunk-) pointer\n");
d1258 1
a1258 1
			wrtwarning("chunk is already free\n");
d1263 2
a1264 5
		if (!malloc_realloc &&	/* Unless we have to, */
		    size <= osize &&	/* ..or are too small, */
		    (size > osize / 2 ||	/* ..or could use a smaller
						 * size, */
		     osize == malloc_minsize)) {	/* ..(if there is one) */
d1270 1
a1270 1
		wrtwarning("irealloc: pointer to wrong page\n");
a1292 1

d1296 5
a1300 6
	u_long          i, l, cachesize = 0;
	struct pginfo **pd;
	struct pdinfo  *pi, *spi;
	u_long          pidx, lidx;
	struct pgfree  *pf, *pt = NULL;
	void           *tail;
d1303 1
a1303 1
		wrtwarning("page is already free\n");
d1307 1
a1307 1
		wrtwarning("free_pages: pointer to wrong page\n");
d1311 1
a1311 1
		wrtwarning("modified (page-) pointer\n");
d1317 1
a1317 1
#ifdef	MALLOC_EXTRA_SANITY
d1319 1
a1319 1
		wrterror("(ES): mapped pages not found in directory\n");
d1323 1
a1323 1
#endif				/* MALLOC_EXTRA_SANITY */
d1341 2
a1342 1
			if ((pi = pi->next) == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i))
d1357 1
a1357 1
#ifdef	MALLOC_EXTRA_SANITY
d1359 1
a1359 1
			wrterror("(ES): hole in mapped pages directory\n");
d1363 1
a1363 1
#endif				/* MALLOC_EXTRA_SANITY */
a1368 1
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
a1370 1
#endif
a1382 1

a1388 1

a1389 1

d1397 2
a1398 2
		     (pf->page + pf->size) < ptr && pf->next != NULL;
		     pf = pf->next)
d1420 2
a1421 1
			if (pf->next != NULL && (pf->page + pf->size) == pf->next->page) {
d1443 1
a1443 1
			wrterror("freelist is destroyed\n");
d1453 1
d1462 1
a1462 1
			   pf->size - (malloc_cache - cachesize)) != 0)
d1474 3
a1476 1
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx; pi = pi->next);
d1485 1
a1485 1
#ifdef	MALLOC_EXTRA_SANITY
d1487 1
a1487 1
						wrterror("(ES): pages directory underflow\n");
d1491 1
a1491 1
#endif				/* MALLOC_EXTRA_SANITY */
d1494 1
a1494 1
#ifdef	MALLOC_EXTRA_SANITY
d1496 2
a1497 2
					wrtwarning("(ES): page already unmapped\n");
#endif				/* MALLOC_EXTRA_SANITY */
d1506 1
a1506 2
						if (spi == pi)	/* Update spi only if
								 * first. */
d1516 2
a1517 1
					if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(i))
d1540 1
d1553 8
a1560 5
							index = ((PD_IDX(spi->dirnum) + 1) * pdi_mod) - 1;
						for (pi = spi, i = index; pd[PI_OFF(i)] == MALLOC_NOT_MINE; i--)
#ifdef	MALLOC_EXTRA_SANITY
							if (!PI_OFF(i)) {	/* Should never enter
										 * here. */
d1567 1
a1567 1
#else				/* !MALLOC_EXTRA_SANITY */
d1570 1
a1570 1
#endif				/* MALLOC_EXTRA_SANITY */
d1599 5
a1603 6
	int             i;
	struct pginfo **mp;
	struct pginfo **pd;
	struct pdinfo  *pi;
	u_long          pidx;
	void           *vp;
d1609 1
a1609 1
		wrtwarning("modified (chunk-) pointer\n");
d1613 1
a1613 1
		wrtwarning("chunk is already free\n");
a1627 1

d1632 1
a1632 1
		       (*mp)->next->page < info->page)
d1644 1
a1644 1
#ifdef	MALLOC_EXTRA_SANITY
d1646 1
a1646 1
			wrterror("(ES): Not on queue\n");
d1650 1
a1650 1
#endif				/* MALLOC_EXTRA_SANITY */
d1657 1
a1657 1
#ifdef	MALLOC_EXTRA_SANITY
d1659 1
a1659 1
		wrterror("(ES): mapped pages not found in directory\n");
d1663 1
a1663 1
#endif				/* MALLOC_EXTRA_SANITY */
d1672 1
a1672 1
	if (info->size == 0) {
d1674 1
a1674 2
		/* Do we have to care if mprotect succeeds here ? */
	}
d1684 3
a1686 5
	struct pginfo  *info;
	struct pginfo **pd;
	struct pdinfo  *pi;
	u_long          pidx;
	u_long          index;
d1693 1
a1693 1
		wrtwarning("malloc() has never been called\n");
d1707 1
a1707 1
		wrtwarning("ifree: junk pointer, too low to make sense\n");
d1712 1
a1712 1
		wrtwarning("ifree: junk pointer, too high to make sense\n");
d1717 1
a1717 1
#ifdef	MALLOC_EXTRA_SANITY
d1719 1
a1719 1
		wrterror("(ES): mapped pages not found in directory\n");
d1723 1
a1723 1
#endif				/* MALLOC_EXTRA_SANITY */
d1746 1
a1746 1
	static int      noprint;
d1750 1
a1750 1
		wrtwarning("recursive call\n");
d1760 1
a1760 1
void           *
d1763 1
a1763 1
	void           *r;
d1776 1
a1776 1
		wrterror("out of memory\n");
d1798 1
a1798 1
void           *
d1801 1
a1801 1
	void           *r;
d1809 2
a1810 1
	if (ptr == NULL) {
d1812 1
a1812 1
	} else {
d1814 1
a1814 1
	}
d1819 1
a1819 1
		wrterror("out of memory\n");
@


1.76
log
@zap remaining rcsid.

Kill old files that are no longer compiled.

okay theo
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d35 1
a35 1
#define SOME_JUNK	0xd0		/* as in "Duh" :-) */
d69 2
a70 2
#   define    malloc_pageshift	13U
#endif /* __OpenBSD__ */
d79 7
a85 7
    struct pginfo	*next;	/* next on the free list */
    void		*page;	/* Pointer to the page */
    u_short		 size;	/* size of this page's chunks */
    u_short		 shift;	/* How far to shift for this size chunks */
    u_short		 free;	/* How many free chunks */
    u_short		 total;	/* How many chunk */
    u_long		 bits[1]; /* Which chunks are free */
d93 5
a97 5
    struct pgfree	*next;	/* next run of free pages */
    struct pgfree	*prev;	/* prev run of free pages */
    void		*page;	/* pointer to free pages */
    void		*pdir;	/* pointer to the base page's dir */
    size_t		 size;	/* number of bytes free */
d148 1
a148 1
static int fdzero;
d166 4
a169 4
    struct pginfo	**base;
    struct pdinfo	 *prev;
    struct pdinfo	 *next;
    u_long		  dirnum;
d171 2
a172 2
static struct	pdinfo  *last_dir;	/* Caches to the last and previous */
static struct	pdinfo  *prev_dir;	/* referenced directory pages.     */
d174 2
a175 2
static size_t		pdi_off;
static u_long		pdi_mod;
d182 1
a182 1
static u_long last_index;
d185 1
a185 1
static struct	pginfo **page_dir;
d191 1
a191 1
static int malloc_abort = 2;
d194 1
a194 1
static int suicide;
d198 1
a198 1
static int malloc_stats;
d202 1
a202 1
static int malloc_silent;
d205 1
a205 1
static int malloc_realloc;
d208 1
a208 1
static int malloc_freeprot;
d211 2
a212 2
static int malloc_guard = 0;
static int malloc_guarded;
d214 1
a214 1
static int malloc_ptrguard;
d218 1
a218 1
static int malloc_hint;
d222 1
a222 1
static int malloc_xmalloc;
d225 1
a225 1
static int malloc_zero;
d228 1
a228 1
static int malloc_junk;
d232 1
a232 1
static int malloc_utrace;
d234 5
a238 1
struct ut { void *p; size_t s; void *r; };
d240 1
a240 1
void utrace(struct ut *, int);
d245 1
a245 1
#else /* !__FreeBSD__ */
d250 1
a250 1
static int malloc_active;
d253 1
a253 1
static size_t malloc_used;
d256 1
a256 1
static void *malloc_brk;
d262 1
a262 1
char *malloc_options;
d265 1
a265 1
static char *malloc_func;
d275 4
a278 4
static void *imalloc(size_t size);
static void ifree(void *ptr);
static void *irealloc(void *ptr, size_t size);
static void *malloc_bytes(size_t size);
d285 1
a285 1
pdir_lookup(u_long index, struct pdinfo **pdi)
d287 2
a288 2
    struct pdinfo *spi;
    u_long pidx = PI_IDX(index);
d290 12
a301 12
    if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
	    *pdi = last_dir;
    else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
	    *pdi = prev_dir;
    else if (last_dir != NULL && prev_dir != NULL) {
	if ((PD_IDX(last_dir->dirnum) > pidx) ?
	  (PD_IDX(last_dir->dirnum) - pidx):(pidx - PD_IDX(last_dir->dirnum))
	  < (PD_IDX(prev_dir->dirnum) > pidx) ?
	  (PD_IDX(prev_dir->dirnum) - pidx):(pidx - PD_IDX(prev_dir->dirnum)))
	    *pdi = last_dir;
	else
	    *pdi = prev_dir;
d303 15
a317 15
	if (PD_IDX((*pdi)->dirnum) > pidx) {
	    for (spi=(*pdi)->prev;spi!=NULL && PD_IDX(spi->dirnum)>pidx;
		 spi=spi->prev)
		*pdi = spi;
	    if (spi != NULL)
		*pdi = spi;
	} else
	    for (spi=(*pdi)->next;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;
		 spi=spi->next)
		*pdi = spi;
    } else {
	*pdi = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
	for (spi=*pdi;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;spi=spi->next)
	    *pdi = spi;
    }
d319 1
a319 1
    return ((PD_IDX((*pdi)->dirnum) == pidx)?0:(PD_IDX((*pdi)->dirnum) > pidx)?1:-1);
d325 1
a325 1
malloc_dump(FILE *fd)
d327 54
a380 13
    struct pginfo **pd;
    struct pgfree *pf;
    struct pdinfo *pi;
    int j;

    pd = page_dir;
    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);

    /* print out all the pages */
    for(j=0;j<=last_index;) {
	fprintf(fd, "%08lx %5d ", j << malloc_pageshift, j);
	if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
d382 5
a386 42
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
	    j--;
	    fprintf(fd, ".. %5d not mine\n",	j);
	} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
	    j--;
	    fprintf(fd, ".. %5d free\n", j);
	} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
	    j--;
	    fprintf(fd, ".. %5d in use\n", j);
	} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[PI_OFF(j)]);
	} else {
	    fprintf(fd, "%p %d (of %d) x %d @@ %p --> %p\n",
		pd[PI_OFF(j)], pd[PI_OFF(j)]->free, pd[PI_OFF(j)]->total,
		pd[PI_OFF(j)]->size, pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
	}
	if (!PI_OFF(++j)) {
	    if ((pi = pi->next) == NULL)
		break;
	    pd = pi->base;
	    j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
a387 1
    }
d389 8
a396 7
    for(pf=free_list.next; pf; pf=pf->next) {
	fprintf(fd, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		pf, pf->page, pf->page + pf->size, pf->size,
		pf->prev, pf->next);
	if (pf == pf->next) {
		fprintf(fd, "Free_list loops\n");
		break;
a397 1
    }
d399 7
a405 7
    /* print out various info */
    fprintf(fd, "Minsize\t%d\n", malloc_minsize);
    fprintf(fd, "Maxsize\t%d\n", malloc_maxsize);
    fprintf(fd, "Pagesize\t%lu\n", (u_long)malloc_pagesize);
    fprintf(fd, "Pageshift\t%d\n", malloc_pageshift);
    fprintf(fd, "In use\t%lu\n", (u_long)malloc_used);
    fprintf(fd, "Guarded\t%lu\n", (u_long)malloc_guarded);
d407 1
a407 1
#endif	/* MALLOC_STATS */
d409 1
a409 1
extern char *__progname;
d414 2
a415 2
    char *q = " error: ";
    struct iovec iov[4];
d417 9
a425 9
    iov[0].iov_base = __progname;
    iov[0].iov_len = strlen(__progname);
    iov[1].iov_base = malloc_func;
    iov[1].iov_len = strlen(malloc_func);
    iov[2].iov_base = q;
    iov[2].iov_len = strlen(q);
    iov[3].iov_base = p;
    iov[3].iov_len = strlen(p);
    writev(STDERR_FILENO, iov, 4);
d427 1
a427 1
    suicide = 1;
d429 6
a434 6
    if (malloc_stats)
	malloc_dump(stderr);
#endif	/* MALLOC_STATS */
    malloc_active--;
    if (malloc_abort)
	abort();
d440 2
a441 7
    char *q = " warning: ";
    struct iovec iov[4];

    if (malloc_abort)
	wrterror(p);
    else if (malloc_silent)
	return;
d443 14
a456 9
    iov[0].iov_base = __progname;
    iov[0].iov_len = strlen(__progname);
    iov[1].iov_base = malloc_func;
    iov[1].iov_len = strlen(malloc_func);
    iov[2].iov_base = q;
    iov[2].iov_len = strlen(q);
    iov[3].iov_base = p;
    iov[3].iov_len = strlen(p);
    writev(STDERR_FILENO, iov, 4);
d463 7
a469 7
    FILE *fd = fopen("malloc.out", "a");
    char *q = "malloc() warning: Couldn't dump stats\n";
    if (fd != NULL) {
        malloc_dump(fd);
        fclose(fd);
    } else
        write(STDERR_FILENO, q, strlen(q));
d471 1
a471 1
#endif	/* MALLOC_STATS */
d477 1
a477 1
static void *
d480 9
a488 35
    struct pdinfo *pi, *spi;
    struct pginfo **pd;
    u_long idx, pidx, lidx;
    void *result, *tail;
    u_long index, lindex;

    pages <<= malloc_pageshift;
    result = MMAP(pages + malloc_guard);
    if (result == MAP_FAILED) {
	errno = ENOMEM;
#ifdef	MALLOC_EXTRA_SANITY
	wrtwarning("(ES): map_pages fails\n");
#endif	/* MALLOC_EXTRA_SANITY */
	return (NULL);
    }
    index = ptr2index(result);
    tail = result + pages + malloc_guard;
    lindex = ptr2index(tail) - 1;
    if (malloc_guard)
	mprotect(result + pages, malloc_guard, PROT_NONE);

    pidx = PI_IDX(index);
    lidx = PI_IDX(lindex);

    if (tail > malloc_brk) {
	malloc_brk = tail;
	last_index = lindex;
    }

    /* Insert directory pages, if needed. */
    pdir_lookup(index, &pi);

    for (idx=pidx,spi=pi;idx<=lidx;idx++) {
	if (pi == NULL || PD_IDX(pi->dirnum) != idx) {
	    if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
a489 1
		munmap(result, tail - result);
d492 1
a492 1
#endif	/* MALLOC_EXTRA_SANITY */
a493 22
	    }
	    memset(pd, 0, malloc_pagesize);
	    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);
	    pi->base = pd;
	    pi->prev = spi;
	    pi->next = spi->next;
	    pi->dirnum = idx * (malloc_pagesize/sizeof(struct pginfo *));

	    if (spi->next != NULL)
		spi->next->prev = pi;
	    spi->next = pi;
	}
        if (idx > pidx && idx < lidx) {
	    pi->dirnum += pdi_mod;
	} else if (idx == pidx) {
	    if (pidx == lidx) {
		pi->dirnum += (tail - result) >> malloc_pageshift;
	    } else {
		pi->dirnum += pdi_mod - PI_OFF(index);
	    }
	} else {
	    pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
d495 48
d544 13
a556 13
	if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > idx) {
	    wrterror("(ES): pages directory overflow\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */
	if (idx == pidx && pi != last_dir) {
	   prev_dir = last_dir;
	   last_dir = pi;
	}
	spi = pi;
	pi = spi->next;
    }
d558 1
a558 1
    return (result);
d568 3
a570 3
    char *p, b[64];
    int i, j;
    int save_errno = errno;
d572 1
a572 1
    _MALLOC_LOCK_INIT();
d574 1
a574 1
    INIT_MMAP();
d577 23
a599 2
    malloc_junk = 1;
#endif	/* MALLOC_EXTRA_SANITY */
d601 17
a617 29
    for (i = 0; i < 3; i++) {
	switch (i) {
	case 0:
	    j = readlink("/etc/malloc.conf", b, sizeof b - 1);
	    if (j <= 0)
		continue;
	    b[j] = '\0';
	    p = b;
	    break;

	case 1:
	    if (issetugid() == 0)
		p = getenv("MALLOC_OPTIONS");
	    else
		continue;
	    break;

	case 2:
	    p = malloc_options;
	    break;

	default: p = NULL;
	}
	for (; p != NULL && *p != '\0'; p++) {
	    switch (*p) {
		case '>': malloc_cache   <<= 1; break;
		case '<': malloc_cache   >>= 1; break;
		case 'a': malloc_abort   = 0; break;
		case 'A': malloc_abort   = 1; break;
d619 19
a637 7
		case 'd': malloc_stats   = 0; break;
		case 'D': malloc_stats   = 1; break;
#endif	/* MALLOC_STATS */
		case 'f': malloc_freeprot = 0; break;
		case 'F': malloc_freeprot = 1; break;
		case 'g': malloc_guard = 0; break;
		case 'G': malloc_guard = malloc_pagesize; break;
d639 31
a669 11
		case 'h': malloc_hint    = 0; break;
		case 'H': malloc_hint    = 1; break;
#endif /* __FreeBSD__ */
		case 'j': malloc_junk    = 0; break;
		case 'J': malloc_junk    = 1; break;
		case 'n': malloc_silent  = 0; break;
		case 'N': malloc_silent  = 1; break;
		case 'p': malloc_ptrguard = 0; break;
		case 'P': malloc_ptrguard = 1; break;
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d671 37
a707 25
		case 'u': malloc_utrace  = 0; break;
		case 'U': malloc_utrace  = 1; break;
#endif /* __FreeBSD__ */
		case 'x': malloc_xmalloc = 0; break;
		case 'X': malloc_xmalloc = 1; break;
		case 'z': malloc_zero    = 0; break;
		case 'Z': malloc_zero    = 1; break;
		default:
		    j = malloc_abort;
		    malloc_abort = 0;
		    wrtwarning("unknown char in MALLOC_OPTIONS\n");
		    malloc_abort = j;
		    break;
	    }
	}
    }

    UTRACE(0, 0, 0);

    /*
     * We want junk in the entire allocation, and zero only in the part
     * the user asked for.
     */
    if (malloc_zero)
	malloc_junk=1;
d710 1
a710 1
    if (malloc_stats && (atexit(malloc_exit) == -1))
d712 1
a712 1
#endif	/* MALLOC_STATS */
d714 2
a715 2
    /* Allocate one page for the page directory. */
    page_dir = (struct pginfo **) MMAP(malloc_pagesize);
d717 7
a723 8
    if (page_dir == MAP_FAILED) {
	wrterror("mmap(2) failed, check limits\n");
	errno = ENOMEM;
	return;
    }

    pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
    pdi_mod = pdi_off / sizeof(struct pginfo *);
d725 4
a728 4
    last_dir = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
    last_dir->base = page_dir;
    last_dir->prev = last_dir->next = NULL;
    last_dir->dirnum = malloc_pageshift;
d730 2
a731 2
    /* Been here, done that. */
    malloc_started++;
d733 1
a733 1
    /* Recalculate the cache size in bytes, and make sure it's nonzero. */
d735 2
a736 2
    if (!malloc_cache)
	malloc_cache++;
d738 1
a738 1
    malloc_cache <<= malloc_pageshift;
d740 1
a740 1
    errno = save_errno;
d746 1
a746 1
static void *
d749 16
a764 16
    void *p, *delay_free = NULL;
    int i;
    struct rlimit rl;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    void *tp;
    struct pgfree *pf;
    u_long index;
    int m;

    size = pageround(size) + malloc_guard;

    p = NULL;
    /* Look for free pages before asking for more */
    for (pf = free_list.next; pf; pf = pf->next) {
d767 72
a838 24
	if (pf->size & malloc_pagemask) {
	    wrterror("(ES): junk length entry on free_list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if (!pf->size) {
	    wrterror("(ES): zero length entry on free_list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if (pf->page > (pf->page + pf->size)) {
	    wrterror("(ES): sick entry on free_list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pi = pf->pdir) == NULL) {
	    wrterror("(ES): invalid page directory on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
	    wrterror("(ES): directory index mismatch on free-list\n");
	    errno = EFAULT;
	    return (NULL);
d840 7
a846 5
	pd = pi->base;
	if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
	    wrterror("(ES): non-free first page on free-list\n");
	    errno = EFAULT;
	    return (NULL);
d848 4
a851 6
	pidx = PI_IDX(ptr2index((pf->page)+(pf->size))-1);
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): last page not referenced in page directory\n");
	    errno = EFAULT;
	    return (NULL);
d853 4
a856 36
	pd = pi->base;
	if (pd[PI_OFF(ptr2index((pf->page)+(pf->size))-1)] != MALLOC_FREE) {
	    wrterror("(ES): non-free last page on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */

	if (pf->size < size)
	    continue;

	if (pf->size == size) {
	    p = pf->page;
	    pi = pf->pdir;
	    if (pf->next != NULL)
		    pf->next->prev = pf->prev;
	    pf->prev->next = pf->next;
	    delay_free = pf;
	    break;
	}

	p = pf->page;
	pf->page = (char *)pf->page + size;
	pf->size -= size;
	pidx = PI_IDX(ptr2index(pf->page));
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): hole in directories\n");
	    errno = EFAULT;
	    return (NULL);
	}
	tp = pf->pdir;
	pf->pdir = pi;
	pi = tp;
	break;
    }
d858 1
a858 1
    size -= malloc_guard;
d860 3
a862 20
#ifdef	MALLOC_EXTRA_SANITY
    if (p != NULL && pi != NULL) {
	pidx = PD_IDX(pi->dirnum);
	pd = pi->base;
    }
    if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
	wrterror("(ES): allocated non-free page on free-list\n");
	errno = EFAULT;
	return (NULL);
    }
#endif	/* MALLOC_EXTRA_SANITY */

    if (p != NULL && (malloc_guard || malloc_freeprot))
	mprotect(p, size, PROT_READ|PROT_WRITE);

    size >>= malloc_pageshift;

    /* Map new pages */
    if (p == NULL)
	p = map_pages(size);
d864 1
a864 1
    if (p != NULL) {
d866 3
a868 20
	index = ptr2index(p);
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): mapped pages not found in directory\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
	    prev_dir = last_dir;
	    last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FIRST;
	for (i=1;i<size;i++) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
d871 8
a878 3
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
a879 1
#endif	/* MALLOC_EXTRA_SANITY */
d881 20
a900 7
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FOLLOW;
	}
	if (malloc_guard) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
d902 9
a910 4
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
d912 11
a922 4
#endif	/* MALLOC_EXTRA_SANITY */
		pd = pi->base;
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FIRST;
d924 1
a924 16

	malloc_used += size << malloc_pageshift;
	malloc_guarded += malloc_guard;

	if (malloc_junk)
	    memset(p, SOME_JUNK, size << malloc_pageshift);
    }

    if (delay_free) {
	if (px == NULL)
	    px = delay_free;
	else
	    ifree(delay_free);
    }

    return (p);
d934 32
a965 76
    struct pginfo *bp;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    void *pp;
    int i, k, l;

    /* Allocate a new bucket */
    pp = malloc_pages((size_t)malloc_pagesize);
    if (pp == NULL)
	return (0);

    /* Find length of admin structure */
    l = sizeof *bp - sizeof(u_long);
    l += sizeof(u_long) *
	(((malloc_pagesize >> bits)+MALLOC_BITS-1) / MALLOC_BITS);

    /* Don't waste more than two chunks on this */
    /*
     * If we are to allocate a memory protected page for the malloc(0)
     * case (when bits=0), it must be from a different page than the
     * pginfo page.
     * --> Treat it like the big chunk alloc, get a second data page.
     */
    if (bits != 0 && (1UL<<(bits)) <= l+l) {
	bp = (struct  pginfo *)pp;
    } else {
	bp = (struct  pginfo *)imalloc(l);
	if (bp == NULL) {
	    ifree(pp);
	    return (0);
	}
    }

    /* memory protect the page allocated in the malloc(0) case */
    if (bits == 0) {

	bp->size = 0;
	bp->shift = 1;
	i = malloc_minsize-1;
	while (i >>= 1)
	    bp->shift++;
	bp->total = bp->free = malloc_pagesize >> bp->shift;
	bp->page = pp;

	k = mprotect(pp, malloc_pagesize, PROT_NONE);
	if (k < 0) {
	    ifree(pp);
	    ifree(bp);
	    return (0);
	}
    } else {
	bp->size = (1UL<<bits);
	bp->shift = bits;
	bp->total = bp->free = malloc_pagesize >> bits;
	bp->page = pp;
    }

    /* set all valid bits in the bitmap */
    k = bp->total;
    i = 0;

    /* Do a bunch at a time */
    for(;k-i >= MALLOC_BITS; i += MALLOC_BITS)
	bp->bits[i / MALLOC_BITS] = ~0UL;

    for(; i < k; i++)
        bp->bits[i/MALLOC_BITS] |= 1UL<<(i%MALLOC_BITS);

    if (bp == bp->page) {
	/* Mark the ones we stole for ourselves */
	for(i=0;l > 0;i++) {
	    bp->bits[i/MALLOC_BITS] &= ~(1UL<<(i%MALLOC_BITS));
	    bp->free--;
	    bp->total--;
	    l -= (1 << bits);
a966 1
    }
d968 45
a1012 1
    /* MALLOC_LOCK */
d1014 2
a1015 2
    pidx = PI_IDX(ptr2index(pp));
    pdir_lookup(ptr2index(pp), &pi);
d1017 12
a1028 12
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (0);
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }
    pd = pi->base;
    pd[PI_OFF(ptr2index(pp))] = bp;
d1030 2
a1031 2
    bp->next = page_dir[bits];
    page_dir[bits] = bp;
d1033 1
a1033 1
    /* MALLOC_UNLOCK */
d1035 1
a1035 1
    return (1);
d1041 1
a1041 1
static void *
d1044 49
a1092 50
    int i,j;
    u_long u;
    struct  pginfo *bp;
    int k;
    u_long *lp;

    /* Don't bother with anything less than this */
    /* unless we have a malloc(0) requests */
    if (size != 0 && size < malloc_minsize)
	size = malloc_minsize;

    /* Find the right bucket */
    if (size == 0)
	j=0;
    else {
	j = 1;
	i = size-1;
	while (i >>= 1)
	    j++;
    }

    /* If it's empty, make a page more of that size chunks */
    if (page_dir[j] == NULL && !malloc_make_chunks(j))
	return (NULL);

    bp = page_dir[j];

    /* Find first word of bitmap which isn't empty */
    for (lp = bp->bits; !*lp; lp++)
	;

    /* Find that bit, and tweak it */
    u = 1;
    k = 0;
    while (!(*lp & u)) {
	u += u;
	k++;
    }

    if (malloc_guard) {
	/* Walk to a random position. */
	i = arc4random() % bp->free;
	while (i > 0) {
	    u += u;
	    k++;
	    if (k >= MALLOC_BITS) {
		lp++;
		u = 1;
		k = 0;
	    }
d1094 20
a1113 21
	    if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
		wrterror("chunk overflow\n");
		errno = EFAULT;
		return (NULL);
	    }
#endif	/* MALLOC_EXTRA_SANITY */
	    if (*lp & u)
		i--;
	}
    }
    *lp ^= u;

    /* If there are no more free, remove from free-list */
    if (!--bp->free) {
	page_dir[j] = bp->next;
	bp->next = NULL;
    }

    /* Adjust to the real offset of that chunk */
    k += (lp-bp->bits)*MALLOC_BITS;
    k <<= bp->shift;
d1115 2
a1116 2
    if (malloc_junk && bp->size != 0)
	memset((char *)bp->page + k, SOME_JUNK, bp->size);
d1118 1
a1118 1
    return ((u_char *)bp->page + k);
d1131 1
a1131 1
static void *
d1134 2
a1135 2
    void *result;
    int ptralloc = 0;
d1137 2
a1138 2
    if (!malloc_started)
	malloc_init();
d1140 14
a1153 2
    if (suicide)
	abort();
d1155 9
a1163 23
    if (malloc_ptrguard && size == PTR_SIZE) {
	ptralloc = 1;
	size = malloc_pagesize;
    }

    if ((size + malloc_pagesize) < size) {     /* Check for overflow */
	result = NULL;
	errno = ENOMEM;
    }
    else if (size <= malloc_maxsize)
	result =  malloc_bytes(size);
    else
	result =  malloc_pages(size);

    if (malloc_abort == 1 && result == NULL)
	wrterror("allocation failed\n");

    if (malloc_zero && result != NULL)
	memset(result, 0, size);

    if (result && ptralloc)
	return ((char *)result + PTR_GAP);
    return (result);
d1169 1
a1169 1
static void *
d1172 9
a1180 38
    void *p;
    u_long osize, index, i;
    struct pginfo **mp;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;

    if (suicide)
	abort();

    if (!malloc_started) {
	wrtwarning("malloc() has never been called\n");
	return (NULL);
    }

    if (malloc_ptrguard && PTR_ALIGNED(ptr)) {
	if (size <= PTR_SIZE) {
	    return (ptr);
	} else {
	    p = imalloc(size);
	    if (p)
		memcpy(p, ptr, PTR_SIZE);
	    ifree(ptr);
	    return (p);
	}
    }

    index = ptr2index(ptr);

    if (index < malloc_pageshift) {
	wrtwarning("junk pointer, too low to make sense\n");
	return (NULL);
    }

    if (index > last_index) {
	wrtwarning("junk pointer, too high to make sense\n");
	return (NULL);
    }
d1182 3
a1184 33
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (NULL);
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    mp = &pd[PI_OFF(index)];

    if (*mp == MALLOC_FIRST) {			/* Page allocation */

	/* Check the pointer */
	if ((u_long)ptr & malloc_pagemask) {
	    wrtwarning("modified (page-) pointer\n");
	    return (NULL);
	}

	/* Find the size in bytes */
	i = index;
	if (!PI_OFF(++i)) {
	    pi = pi->next;
	    if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		pi = NULL;
	    if (pi != NULL)
		pd = pi->base;
d1186 10
a1195 10
	for (osize = malloc_pagesize;
	     pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
	    osize += malloc_pagesize;
	    if (!PI_OFF(++i)) {
		pi = pi->next;
		if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		    pi = NULL;
		if (pi != NULL)
		    pd = pi->base;
	    }
d1197 1
d1199 3
a1201 6
        if (!malloc_realloc &&			/* Unless we have to, */
	  size <= osize &&			/* .. or are too small, */
	  size > (osize - malloc_pagesize)) {	/* .. or can free a page, */
	    if (malloc_junk)
		memset((char *)ptr + size, SOME_JUNK, osize-size);
	    return (ptr);			/* ..don't do anything else. */
d1203 19
d1223 1
a1223 1
    } else if (*mp >= MALLOC_MAGIC) {		/* Chunk allocation */
d1225 25
a1249 5
	/* Check the pointer for sane values */
	if ((u_long)ptr & ((1UL<<((*mp)->shift))-1)) {
	    wrtwarning("modified (chunk-) pointer\n");
	    return (NULL);
	}
d1251 9
a1259 2
	/* Find the chunk index in the page */
	i = ((u_long)ptr & malloc_pagemask) >> (*mp)->shift;
d1261 7
a1267 5
	/* Verify that it isn't a free chunk already */
        if ((*mp)->bits[i/MALLOC_BITS] & (1UL<<(i%MALLOC_BITS))) {
	    wrtwarning("chunk is already free\n");
	    return (NULL);
	}
d1269 6
a1274 1
	osize = (*mp)->size;
d1276 12
a1287 7
	if (!malloc_realloc &&		/* Unless we have to, */
	  size <= osize &&		/* ..or are too small, */
	  (size > osize/2 ||		/* ..or could use a smaller size, */
	  osize == malloc_minsize)) {	/* ..(if there is one) */
	    if (malloc_junk)
		memset((char *)ptr + size, SOME_JUNK, osize-size);
	    return (ptr);		/* ..don't do anything else. */
d1290 1
a1290 4
    } else {
	wrtwarning("irealloc: pointer to wrong page\n");
	return (NULL);
    }
d1292 10
a1301 10
    p = imalloc(size);

    if (p != NULL) {
	/* copy the lesser of the two sizes, and free the old one */
	/* Don't move from/to 0 sized region !!! */
	if (osize != 0 && size != 0) {
	    if (osize < size)
		memcpy(p, ptr, osize);
	    else
		memcpy(p, ptr, size);
d1303 1
a1303 4
	ifree(ptr);
    }

    return (p);
d1311 1
a1311 1
free_pages(void *ptr, u_long index, struct pginfo *info)
d1313 22
a1334 25
    u_long i, l, cachesize = 0;
    struct pginfo **pd;
    struct pdinfo *pi, *spi;
    u_long pidx, lidx;
    struct pgfree *pf, *pt=NULL;
    void *tail;

    if (info == MALLOC_FREE) {
	wrtwarning("page is already free\n");
	return;
    }

    if (info != MALLOC_FIRST) {
	wrtwarning("free_pages: pointer to wrong page\n");
	return;
    }

    if ((u_long)ptr & malloc_pagemask) {
	wrtwarning("modified (page-) pointer\n");
	return;
    }

    /* Count how many pages and mark them free at the same time */
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
d1336 6
a1341 6
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */
d1343 1
a1343 1
    spi = pi;		/* Save page index for start of region. */
d1345 19
a1363 18
    pd = pi->base;
    pd[PI_OFF(index)] = MALLOC_FREE;
    i = 1;
    if (!PI_OFF(index+i)) {
	pi = pi->next;
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
	    pi = NULL;
	else
	    pd = pi->base;
    }
    while (pi != NULL && pd[PI_OFF(index+i)] == MALLOC_FOLLOW) {
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	i++;
	if (!PI_OFF(index+i)) {
	    if ((pi=pi->next) == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
		pi = NULL;
	    else
		pd = pi->base;
a1364 1
    }
d1366 1
a1366 1
    l = i << malloc_pageshift;
d1368 2
a1369 2
    if (malloc_junk)
	memset(ptr, SOME_JUNK, l);
d1371 3
a1373 3
    malloc_used -= l;
    malloc_guarded -= malloc_guard;
    if (malloc_guard) {
d1375 10
a1384 10
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i)) {
	    wrterror("(ES): hole in mapped pages directory\n");
	    errno = EFAULT;
	    return;
	}
#endif	/* MALLOC_EXTRA_SANITY */
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr + l;
d1387 2
a1388 2
    if (malloc_hint)
	madvise(ptr, l, MADV_FREE);
d1391 18
a1408 2
    if (malloc_freeprot)
	mprotect(ptr, l, PROT_NONE);
a1409 66
    /* Add to free-list. */
    if (px == NULL)
	px = imalloc(sizeof *px);	/* This cannot fail... */
    px->page = ptr;
    px->pdir = spi;
    px->size = l;

    if (free_list.next == NULL) {

	/* Nothing on free list, put this at head. */
	px->next = NULL;
	px->prev = &free_list;
	free_list.next = px;
	pf = px;
	px = NULL;

    } else {

	/* Find the right spot, leave pf pointing to the modified entry. */

	/* Race ahead here, while calculating cache size. */
	for (pf = free_list.next;
	     (pf->page + pf->size) < ptr && pf->next != NULL;
	     pf = pf->next)
		cachesize += pf->size;

	/* Finish cache size calculation. */
	pt = pf;
	while (pt) {
	    cachesize += pt->size;
	    pt = pt->next;
	}

	if (pf->page > tail) {
	    /* Insert before entry */
	    px->next = pf;
	    px->prev = pf->prev;
	    pf->prev = px;
	    px->prev->next = px;
	    pf = px;
	    px = NULL;
	} else if ((pf->page + pf->size) == ptr ) {
	    /* Append to the previous entry. */
	    cachesize -= pf->size;
	    pf->size += l;
	    if (pf->next != NULL && (pf->page + pf->size) == pf->next->page ) {
		/* And collapse the next too. */
		pt = pf->next;
		pf->size += pt->size;
		pf->next = pt->next;
		if (pf->next != NULL)
		    pf->next->prev = pf;
	    }
	} else if (pf->page == tail) {
	    /* Prepend to entry. */
	    cachesize -= pf->size;
	    pf->size += l;
	    pf->page = ptr;
	    pf->pdir = spi;
	} else if (pf->next == NULL) {
	    /* Append at tail of chain. */
	    px->next = NULL;
	    px->prev = pf;
	    pf->next = px;
	    pf = px;
	    px = NULL;
a1410 10
	    wrterror("freelist is destroyed\n");
	    errno = EFAULT;
	    return;
	}
    }

    if (pf->pdir != last_dir) {
	prev_dir = last_dir;
	last_dir = pf->pdir;
    }
d1412 17
a1428 15
    /* Return something to OS ? */
    if (pf->size > (malloc_cache - cachesize)) {

	/*
	 * Keep the cache intact.  Notice that the '>' above guarantees that
	 * the pf will always have at least one page afterwards.
	 */
	if (munmap((char *)pf->page + (malloc_cache - cachesize),
		   pf->size - (malloc_cache - cachesize)) != 0)
	    goto not_return;
	tail = pf->page + pf->size;
	lidx = ptr2index(tail) - 1;
	pf->size = malloc_cache - cachesize;

	index = ptr2index(pf->page + pf->size);
d1430 35
a1464 16
	pidx = PI_IDX(index);
	if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
	    prev_dir = NULL;	/* Will be wiped out below ! */

	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);

	spi = pi;
	if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
	    pd = pi->base;

	    for(i=index;i <= lidx;) {
		if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
		    pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef	MALLOC_EXTRA_SANITY
		    if (!PD_OFF(pi->dirnum)) {
			wrterror("(ES): pages directory underflow\n");
a1466 3
		    }
#endif	/* MALLOC_EXTRA_SANITY */
		    pi->dirnum--;
d1468 44
d1513 44
a1556 21
		else
		    wrtwarning("(ES): page already unmapped\n");
#endif	/* MALLOC_EXTRA_SANITY */
		i++;
		if (!PI_OFF(i)) {
		    /* If no page in that dir, free directory page. */
		    if (!PD_OFF(pi->dirnum)) {
			/* Remove from list. */
			if (spi == pi)	/* Update spi only if first. */
			    spi = pi->prev;
			if (pi->prev != NULL)
			    pi->prev->next = pi->next;
			if (pi->next != NULL)
			    pi->next->prev = pi->prev;
			pi = pi->next;
			munmap(pd, malloc_pagesize);
		    } else
			pi = pi->next;
		    if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(i))
			break;
		    pd = pi->base;
d1558 14
a1571 35
	    }
	    if (pi && !PD_OFF(pi->dirnum)) {
		/* Resulting page dir is now empty. */
		/* Remove from list. */
		if (spi == pi)	/* Update spi only if first. */
		    spi = pi->prev;
		if (pi->prev != NULL)
		    pi->prev->next = pi->next;
		if (pi->next != NULL)
		    pi->next->prev = pi->prev;
		pi = pi->next;
		munmap(pd, malloc_pagesize);
	    }
	}

	if (pi == NULL && malloc_brk == tail) {
	    /* Resize down the malloc upper boundary. */
	    last_index = index - 1;
	    malloc_brk = index2ptr(index);
	}

	/* XXX: We could realloc/shrink the pagedir here I guess. */
	if (pf->size == 0) {	/* Remove from free-list as well. */
	    if (px)
		ifree(px);
	    if ((px = pf->prev) != &free_list) {
		if (pi == NULL && last_index == (index - 1)) {
		    if (spi == NULL) {
			malloc_brk = NULL;
			i = 11;
		    } else {
			pd = spi->base;
			if (PD_IDX(spi->dirnum) < pidx)
			    index = ((PD_IDX(spi->dirnum) + 1) * pdi_mod) - 1;
			for (pi=spi,i=index;pd[PI_OFF(i)]==MALLOC_NOT_MINE;i--)
d1573 26
a1598 23
			    if (!PI_OFF(i)) {	/* Should never enter here. */
				pi = pi->prev;
				if (pi == NULL || i == 0)
				    break;
				pd = pi->base;
				i = (PD_IDX(pi->dirnum) + 1) * pdi_mod;
			    }
#else	/* !MALLOC_EXTRA_SANITY */
			    { }
#endif	/* MALLOC_EXTRA_SANITY */
			malloc_brk = index2ptr(i + 1);
		    }
		    last_index = i;
		}
		if ((px->next = pf->next) != NULL)
		    px->next->prev = px;
	    } else {
		if ((free_list.next = pf->next) != NULL)
		    free_list.next->prev = &free_list;
	    }
	    px = pf;
	    last_dir = prev_dir;
	    prev_dir = NULL;
a1599 1
    }
d1601 2
a1602 2
    if (pt != NULL)
	ifree(pt);
d1611 1
a1611 1
free_bytes(void *ptr, int index, struct pginfo *info)
d1613 20
a1632 6
    int i;
    struct pginfo **mp;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    void *vp;
d1634 2
a1635 2
    /* Find the chunk number on the page */
    i = ((u_long)ptr & malloc_pagemask) >> info->shift;
d1637 4
a1640 4
    if ((u_long)ptr & ((1UL<<(info->shift))-1)) {
	wrtwarning("modified (chunk-) pointer\n");
	return;
    }
d1642 1
a1642 4
    if (info->bits[i/MALLOC_BITS] & (1UL<<(i%MALLOC_BITS))) {
	wrtwarning("chunk is already free\n");
	return;
    }
d1644 1
a1644 2
    if (malloc_junk && info->size != 0)
	memset(ptr, SOME_JUNK, info->size);
d1646 23
a1668 2
    info->bits[i/MALLOC_BITS] |= 1UL<<(i%MALLOC_BITS);
    info->free++;
d1670 16
a1685 4
    if (info->size != 0)
	mp = page_dir + info->shift;
    else
	mp = page_dir;
d1687 9
a1695 57
    if (info->free == 1) {

	/* Page became non-full */

	/* Insert in address order */
	while (*mp != NULL && (*mp)->next != NULL &&
	       (*mp)->next->page < info->page)
	    mp = &(*mp)->next;
	info->next = *mp;
	*mp = info;
	return;
    }

    if (info->free != info->total)
	return;

    /* Find & remove this page in the queue */
    while (*mp != info) {
	mp = &((*mp)->next);
#ifdef	MALLOC_EXTRA_SANITY
	if (!*mp) {
	    wrterror("(ES): Not on queue\n");
	    errno = EFAULT;
	    return;
	}
#endif	/* MALLOC_EXTRA_SANITY */
    }
    *mp = info->next;

    /* Free the page & the info structure if need be */
    pidx = PI_IDX(ptr2index(info->page));
    pdir_lookup(ptr2index(info->page), &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;

    /* If the page was mprotected, unprotect it before releasing it */
    if (info->size == 0) {
	mprotect(info->page, malloc_pagesize, PROT_READ|PROT_WRITE);
	/* Do we have to care if mprotect succeeds here ? */
    }

    vp = info->page;		/* Order is important ! */
    if(vp != (void*)info)
	ifree(info);
    ifree(vp);
d1701 48
a1748 5
    struct pginfo *info;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    u_long index;
d1750 4
a1753 21
    /* This is legal */
    if (ptr == NULL)
	return;

    if (!malloc_started) {
	wrtwarning("malloc() has never been called\n");
	return;
    }

    /* If we're already sinking, don't make matters any worse. */
    if (suicide)
	return;

    if (malloc_ptrguard && PTR_ALIGNED(ptr))
	ptr = (char *)ptr - PTR_GAP;

    index = ptr2index(ptr);

    if (index < malloc_pageshift) {
	warnx("(%p)", ptr);
	wrtwarning("ifree: junk pointer, too low to make sense\n");
a1754 30
    }

    if (index > last_index) {
	warnx("(%p)", ptr);
	wrtwarning("ifree: junk pointer, too high to make sense\n");
	return;
    }

    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    info = pd[PI_OFF(index)];

    if (info < MALLOC_MAGIC)
        free_pages(ptr, index, info);
    else
	free_bytes(ptr, index, info);
    return;
d1765 1
a1765 1
    static int noprint;
d1767 7
a1773 7
    if (noprint == 0) {
	noprint = 1;
	wrtwarning("recursive call\n");
    }
    malloc_active--;
    _MALLOC_UNLOCK();
    errno = EDEADLK;
d1779 1
a1779 1
void *
d1782 1
a1782 1
    void *r;
d1784 15
a1798 15
    _MALLOC_LOCK();
    malloc_func = " in malloc():";
    if (malloc_active++) {
	malloc_recurse();
	return (NULL);
    }
    r = imalloc(size);
    UTRACE(0, size, r);
    malloc_active--;
    _MALLOC_UNLOCK();
    if (malloc_xmalloc && r == NULL) {
	wrterror("out of memory\n");
	errno = ENOMEM;
    }
    return (r);
d1804 10
a1813 4
    _MALLOC_LOCK();
    malloc_func = " in free():";
    if (malloc_active++) {
	malloc_recurse();
a1814 6
    }
    ifree(ptr);
    UTRACE(ptr, 0, 0);
    malloc_active--;
    _MALLOC_UNLOCK();
    return;
d1817 1
a1817 1
void *
d1820 1
a1820 1
    void *r;
d1822 19
a1840 19
    _MALLOC_LOCK();
    malloc_func = " in realloc():";
    if (malloc_active++) {
	malloc_recurse();
	return (NULL);
    }
    if (ptr == NULL) {
	r = imalloc(size);
    } else {
        r = irealloc(ptr, size);
    }
    UTRACE(ptr, size, r);
    malloc_active--;
    _MALLOC_UNLOCK();
    if (malloc_xmalloc && r == NULL) {
	wrterror("out of memory\n");
	errno = ENOMEM;
    }
    return (r);
@


1.75
log
@Fix the unmapping of freed pages, leaving just 64k worth of cache pages.
Prodded by art@@ and fgsch@@, ok deraadt@@
@
text
@d1 1
a9 4

#if defined(LIBC_SCCS) && !defined(lint)
static char rcsid[] = "$OpenBSD: malloc.c,v 1.74 2005/06/07 04:42:42 tedu Exp $";
#endif /* LIBC_SCCS and not lint */
@


1.74
log
@adding pointer protection to 'G' was too heavyweight.  Since malloc guard
should be generally usable, split this out into option 'P'. ok deraadt
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.73 2005/05/24 16:39:05 tedu Exp $";
d145 3
a147 2
#define pageround(foo) (((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
d215 1
d401 1
d478 1
a478 1
    u_long pidx,lidx;
d480 1
a480 1
    u_long index;
d491 1
d493 1
d497 4
a500 1
    if (tail > malloc_brk)
d502 2
a503 2
    if ((index = ptr2index(tail) - 1) > last_index)
	last_index = index;
d506 1
a506 4
    pidx = PI_IDX(ptr2index(result));
    lidx = PI_IDX(index);

    pdir_lookup(ptr2index(result), &pi);
d508 2
a509 2
    for (index=pidx,spi=pi;index<=lidx;index++) {
	if (pi == NULL || PD_IDX(pi->dirnum) != index) {
d523 1
a523 1
	    pi->dirnum = index * (malloc_pagesize/sizeof(struct pginfo *));
d529 1
a529 1
        if (index > pidx && index < lidx) {
d531 1
a531 1
	} else if (index == pidx) {
d535 1
a535 1
		pi->dirnum += pdi_mod - PI_OFF(ptr2index(result));
d541 1
a541 1
	if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > index) {
d547 1
a547 1
	if (index == pidx && pi != last_dir) {
d860 1
d1074 1
a1074 1
 * magic so that malloc(sizeof(ptr)) is near the end of the page.
d1076 3
a1078 3
#define PTR_GAP (malloc_pagesize - sizeof(void *))
#define PTR_SIZE (sizeof(void *))
#define PTR_ALIGNED(p) (((unsigned long)p & malloc_pagemask) == PTR_GAP)
d1096 2
a1097 2
	    ptralloc = 1;
	    size = malloc_pagesize;
d1116 1
a1116 1
	    return ((char *)result + PTR_GAP);
d1142 9
a1150 9
	    if (size <= PTR_SIZE)
		    return (ptr);
	    else {
		    p = imalloc(size);
		    if (p)
			    memcpy(p, ptr, PTR_SIZE);
		    ifree(ptr);
		    return (p);
	    }
a1152 1

d1248 1
a1248 1
	wrtwarning("pointer to wrong page\n");
d1276 1
a1276 1
    u_long i, l;
d1289 1
a1289 1
	wrtwarning("pointer to wrong page\n");
d1338 1
d1380 12
a1391 3
	for(pf = free_list.next; (pf->page+pf->size) < ptr && pf->next != NULL;
	    pf = pf->next)
	    ; /* Race ahead here. */
d1403 1
d1415 1
d1439 1
a1439 3
    if (pf->next == NULL &&			/* If we're the last one, */
      pf->size > malloc_cache &&		/* ..and the cache is full, */
      (pf->page + pf->size) == malloc_brk) {	/* ..and none behind us, */
d1445 2
a1446 1
	if (munmap((char *)pf->page + malloc_cache, pf->size - malloc_cache)!=0)
d1450 1
a1450 1
	pf->size = malloc_cache;
d1452 1
a1452 3
	malloc_brk = pf->page + malloc_cache;

	index = ptr2index(malloc_brk);
d1456 1
a1456 1
		prev_dir = NULL;	/* Will be wiped out below ! */
d1460 1
d1464 1
a1464 1
	    for(i=index;i <= last_index;) {
d1476 4
d1485 4
a1488 1
			pi->prev->next = pi->next;
d1500 12
d1514 5
a1518 1
	last_index = index - 1;
d1521 38
d1686 1
a1686 1
	    ptr = (char *)ptr - PTR_GAP;
d1691 2
a1692 1
	wrtwarning("junk pointer, too low to make sense\n");
d1697 2
a1698 1
	wrtwarning("junk pointer, too high to make sense\n");
@


1.73
log
@handle sizeof(void *) allocations specially when using malloc guard.
they get a whole page and go right at the end of it. ok deraadt tdeval
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.72 2005/03/31 21:24:46 tdeval Exp $";
d214 2
d617 2
d1089 1
a1089 1
    if (malloc_guard && size == PTR_SIZE) {
d1135 1
a1135 1
    if (malloc_guard && PTR_ALIGNED(ptr)) {
d1609 1
a1609 1
    if (malloc_guard && PTR_ALIGNED(ptr))
@


1.72
log
@MMAP(2) malloc, here we go again.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.70 2004/08/05 21:55:21 tdeval Exp $";
d1064 7
d1085 5
d1105 2
d1131 13
d1604 3
@


1.71
log
@
Back out to brk(2) version.

The mmap(2) code is cool and it has already uncovered some bugs in other code.
But some issues remain on some archs, and we can't afford that for production.

Don't worry, it will be back soon... I'll make sure of it...
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.71 2004/04/12 09:25:11 tdeval Exp $";
d21 2
a22 2
#ifndef MALLOC_EXTRA_SANITY
#undef MALLOC_EXTRA_SANITY
d30 2
a31 2
#ifndef MALLOC_STATS
#undef MALLOC_STATS
d41 2
d84 5
a88 5
    u_short		size;	/* size of this page's chunks */
    u_short		shift;	/* How far to shift for this size chunks */
    u_short		free;	/* How many free chunks */
    u_short		total;	/* How many chunk */
    u_long		bits[1]; /* Which chunks are free */
d99 2
a100 2
    void		*end;	/* pointer to end of free pages */
    u_long		size;	/* number of bytes free */
d145 2
a146 2
#define pageround(foo) (((foo) + (malloc_pagemask))&(~(malloc_pagemask)))
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)-malloc_origo)
d166 16
a181 2
/* The offset from pagenumber to index into the page directory */
static u_long malloc_origo;
a188 3
/* How many slots in the page directory */
static size_t	malloc_ninfo;

d198 1
a198 1
#ifdef MALLOC_STATS
d247 4
a250 1
/* my last break. */
d253 1
a253 1
/* one location cache for free-list holders */
d256 1
a256 1
/* compile-time options */
d259 1
a259 1
/* Name of the current public function */
d262 1
a262 1
/* Macro for mmap */
d268 1
a268 1
 * Necessary function declarations
a269 1
static int extend_pgdir(u_long index);
d275 44
a318 1
#ifdef MALLOC_STATS
d324 1
d328 1
d331 11
a341 5
    for(j=0;j<=last_index;j++) {
	fprintf(fd, "%08lx %5d ", (j+malloc_origo) << malloc_pageshift, j);
	if (pd[j] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_NOT_MINE;j++)
		;
d344 9
a352 3
	} else if (pd[j] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FREE;j++)
		;
d355 9
a363 3
	} else if (pd[j] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FOLLOW;j++)
		;
d366 2
a367 2
	} else if (pd[j] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[j]);
d370 8
a377 2
		pd[j], pd[j]->free, pd[j]->total,
		pd[j]->size, pd[j]->page, pd[j]->next);
d383 2
a384 1
		pf, pf->page, pf->end, pf->size, pf->prev, pf->next);
d396 1
a396 4
    fprintf(fd, "FirstPage\t%ld\n", malloc_origo);
    fprintf(fd, "LastPage\t%ld %lx\n", last_index+malloc_pageshift,
	(last_index + malloc_pageshift) << malloc_pageshift);
    fprintf(fd, "Break\t%ld\n", (u_long)sbrk(0) >> malloc_pageshift);
d398 1
a398 1
#endif /* MALLOC_STATS */
d419 1
a419 1
#ifdef MALLOC_STATS
d422 1
a422 1
#endif /* MALLOC_STATS */
d424 2
a425 1
    abort();
d450 1
a450 1
#ifdef MALLOC_STATS
d462 1
a462 1
#endif /* MALLOC_STATS */
d471 5
a475 1
    caddr_t result, tail;
a476 1
    result = (caddr_t)pageround((u_long)sbrk(0));
d478 2
a479 4
    if (pages > SIZE_T_MAX - (size_t)result) {
#ifdef MALLOC_EXTRA_SANITY
	wrtwarning("(ES): overflow in map_pages fails\n");
#endif /* MALLOC_EXTRA_SANITY */
d481 3
d487 2
d490 56
a545 5
    if (brk(tail) == (char *)-1) {
#ifdef MALLOC_EXTRA_SANITY
	wrtwarning("(ES): map_pages fails\n");
#endif /* MALLOC_EXTRA_SANITY */
	return (NULL);
a546 8
    if (malloc_guard)
	mprotect(result + pages, malloc_pagesize, PROT_NONE);

    last_index = ptr2index(tail) - 1;
    malloc_brk = tail;

    if ((last_index+1) >= malloc_ninfo && !extend_pgdir(last_index))
	return (NULL);
a550 52
/*
 * Extend page directory
 */
static int
extend_pgdir(u_long index)
{
    struct  pginfo **new, **old;
    size_t i, oldlen;

    /* Make it this many pages */
    i = index * sizeof *page_dir;
    i /= malloc_pagesize;
    i += 2;

    /* remember the old mapping size */
    oldlen = malloc_ninfo * sizeof *page_dir;

    /*
     * NOTE: we allocate new pages and copy the directory rather than tempt
     * fate by trying to "grow" the region.. There is nothing to prevent
     * us from accidently re-mapping space that's been allocated by our caller
     * via dlopen() or other mmap().
     *
     * The copy problem is not too bad, as there is 4K of page index per
     * 4MB of malloc arena.
     *
     * We can totally avoid the copy if we open a file descriptor to associate
     * the anon mappings with.  Then, when we remap the pages at the new
     * address, the old pages will be "magically" remapped..  But this means
     * keeping open a "secret" file descriptor.....
     */

    /* Get new pages */
    new = (struct pginfo**) MMAP(i * malloc_pagesize);
    if (new == MAP_FAILED)
	return (0);

    /* Copy the old stuff */
    memcpy(new, page_dir,
	    malloc_ninfo * sizeof *page_dir);

    /* register the new size */
    malloc_ninfo = i * malloc_pagesize / sizeof *page_dir;

    /* swap the pointers */
    old = page_dir;
    page_dir = new;

    /* Now free the old stuff */
    munmap(old, oldlen);
    return (1);
}
d566 1
a566 1
#ifdef MALLOC_EXTRA_SANITY
d568 1
a568 1
#endif /* MALLOC_EXTRA_SANITY */
d571 2
a572 1
	if (i == 0) {
d578 3
a580 1
	} else if (i == 1) {
d585 3
a587 1
	} else if (i == 2) {
d589 3
d599 1
a599 1
#ifdef MALLOC_STATS
d602 1
a602 1
#endif /* MALLOC_STATS */
a610 2
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d615 2
d644 1
a644 1
#ifdef MALLOC_STATS
d647 1
a647 1
#endif /* MALLOC_STATS */
d649 1
a649 1
    /* Allocate one page for the page directory */
d652 1
a652 1
    if (page_dir == MAP_FAILED)
d654 3
d658 2
a659 6
    /*
     * We need a maximum of malloc_pageshift buckets, steal these from the
     * front of the page_directory;
     */
    malloc_origo = ((u_long)pageround((u_long)sbrk(0))) >> malloc_pageshift;
    malloc_origo -= malloc_pageshift;
d661 4
a664 1
    malloc_ninfo = malloc_pagesize / sizeof *page_dir;
d666 1
a666 1
    /* Been here, done that */
d669 1
a669 1
    /* Recalculate the cache size in bytes, and make sure it's nonzero */
a675 5
    /*
     * This is a nice hack from Kaleb Keithly (kaleb@@x.org).
     * We can sbrk(2) further back when we keep this on a low address.
     */
    px = (struct pgfree *) imalloc (sizeof *px);
d687 5
d694 1
d700 1
a700 1
    for(pf = free_list.next; pf; pf = pf->next) {
d702 2
a703 2
#ifdef MALLOC_EXTRA_SANITY
	if (pf->size & malloc_pagemask)
d705 4
a708 1
	if (!pf->size)
d710 4
a713 3
	if (pf->page == pf->end)
	    wrterror("(ES): zero entry on free_list\n");
	if (pf->page > pf->end)
d715 15
a729 3
	if ((void*)pf->page >= (void*)sbrk(0))
	    wrterror("(ES): entry on free_list past brk\n");
	if (page_dir[ptr2index(pf->page)] != MALLOC_FREE)
d731 12
a742 1
	if (page_dir[ptr2index(pf->end)-1] != MALLOC_FREE)
d744 4
a747 1
#endif /* MALLOC_EXTRA_SANITY */
d754 1
d765 10
d780 6
a785 2
#ifdef MALLOC_EXTRA_SANITY
    if (p != NULL && page_dir[ptr2index(p)] != MALLOC_FREE)
d787 4
a790 1
#endif /* MALLOC_EXTRA_SANITY */
d792 1
a792 1
    if ((malloc_guard || malloc_freeprot) && p != NULL)
d804 47
a850 3
	page_dir[index] = MALLOC_FIRST;
	for (i=1;i<size;i++)
	    page_dir[index+i] = MALLOC_FOLLOW;
d873 4
a876 1
    struct  pginfo *bp;
d954 15
a968 1
    page_dir[ptr2index(pp)] = bp;
d1022 1
a1022 1
    
d1035 1
a1035 1
	if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS)
d1037 3
d1041 2
a1042 2
	if (*lp & u)
	    i--;
d1070 1
d1103 1
a1103 1
    u_long osize, index;
d1105 3
a1107 1
    int i;
d1129 16
a1144 1
    mp = &page_dir[index];
d1155 10
a1164 1
	for (osize = malloc_pagesize; *(++mp) == MALLOC_FOLLOW;)
d1166 8
d1229 1
d1241 3
d1263 32
a1294 3
    page_dir[index] = MALLOC_FREE;
    for (i = 1; page_dir[index+i] == MALLOC_FOLLOW; i++)
	page_dir[index + i] = MALLOC_FREE;
d1301 14
a1319 6
    if (malloc_guard) {
	page_dir[index + i] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr+l;

d1321 1
a1321 1
	mprotect(ptr, tail - ptr, PROT_NONE);
d1323 1
a1323 1
    /* add to free-list */
d1327 1
a1327 1
    px->end = tail;
d1332 2
a1333 2
	/* Nothing on free list, put this at head */
	px->next = free_list.next;
d1343 1
a1343 1
	for(pf = free_list.next; pf->end < ptr && pf->next != NULL;
d1345 1
a1345 1
	    ; /* Race ahead here */
d1355 2
a1356 3
	} else if (pf->end == ptr ) {
	    /* Append to the previous entry */
	    pf->end = (char *)pf->end + l;
d1358 1
a1358 1
	    if (pf->next != NULL && pf->end == pf->next->page ) {
a1360 1
		pf->end = pt->end;
d1367 1
a1367 1
	    /* Prepend to entry */
d1370 1
d1372 1
a1372 1
	    /* Append at tail of chain */
d1380 2
d1385 5
d1393 1
a1393 2
      pf->end == malloc_brk &&			/* ..and none behind us, */
      malloc_brk == sbrk(0)) {			/* ..and it's OK to do... */
d1399 4
a1402 1
	pf->end = (char *)pf->page + malloc_cache;
d1405 7
a1411 2
	brk(pf->end);
	malloc_brk = pf->end;
d1413 1
a1413 1
	index = ptr2index(pf->end);
d1415 33
a1447 2
	for(i=index;i <= last_index;)
	    page_dir[i++] = MALLOC_NOT_MINE;
d1453 1
d1468 3
d1502 2
a1503 1
	while (*mp && (*mp)->next && (*mp)->next->page < info->page)
d1516 7
a1522 4
#ifdef MALLOC_EXTRA_SANITY
	if (!*mp)
		wrterror("(ES): Not on queue\n");
#endif /* MALLOC_EXTRA_SANITY */
d1527 16
a1542 1
    page_dir[ptr2index(info->page)] = MALLOC_FIRST;
d1560 3
d1590 16
a1605 1
    info = page_dir[index];
d1624 2
a1625 1
    if (noprint == 0)
d1627 1
a1627 1
    noprint = 1;
d1651 1
a1651 1
    if (malloc_xmalloc && r == NULL)
d1653 2
d1693 1
a1693 1
    if (malloc_xmalloc && r == NULL)
d1695 2
@


1.70
log
@- Remove the userland data limit check. It's mmap(2)'s job.
- When malloc_abort==0 (MALLOC_OPTIONS=a), don't abort in wrterror().

fine deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.69 2004/08/04 19:12:53 tdeval Exp $";
a40 2
#include <sys/time.h>
#include <sys/resource.h>
d82 5
a86 5
    u_short		 size;	/* size of this page's chunks */
    u_short		 shift;	/* How far to shift for this size chunks */
    u_short		 free;	/* How many free chunks */
    u_short		 total;	/* How many chunk */
    u_long		 bits[1]; /* Which chunks are free */
d97 2
a98 2
    void		*pdir;	/* pointer to the base page's dir */
    size_t		 size;	/* number of bytes free */
d143 2
a144 2
#define pageround(foo) (((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
d164 2
a165 16
/* Structure used for linking discrete directory pages. */
struct pdinfo {
    struct pginfo	**base;
    struct pdinfo	 *prev;
    struct pdinfo	 *next;
    u_long		  dirnum;
};
static struct	pdinfo  *last_dir;	/* Caches to the last and previous */
static struct	pdinfo  *prev_dir;	/* referenced directory pages.     */

static size_t		pdi_off;
static u_long		pdi_mod;
#define	PD_IDX(num)	((num) / (malloc_pagesize/sizeof(struct pginfo *)))
#define	PD_OFF(num)	((num) & ((malloc_pagesize/sizeof(struct pginfo *))-1))
#define	PI_IDX(index)	((index) / pdi_mod)
#define	PI_OFF(index)	((index) % pdi_mod)
d234 1
a234 4
/* Allocated memory. */
static size_t malloc_used;

/* My last break. */
d237 1
a237 1
/* One location cache for free-list holders. */
d240 1
a240 1
/* Compile-time options. */
d243 1
a243 1
/* Name of the current public function. */
d246 1
a246 1
/* Macro for mmap. */
d252 1
a252 1
 * Necessary function declarations.
d254 1
a259 43

/*
 * Function for page directory lookup.
 */
static int
pdir_lookup(u_long index, struct pdinfo **pdi)
{
    struct pdinfo *spi;
    u_long pidx = PI_IDX(index);

    if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
	    *pdi = last_dir;
    else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
	    *pdi = prev_dir;
    else if (last_dir != NULL && prev_dir != NULL) {
	if ((PD_IDX(last_dir->dirnum) > pidx) ?
	  (PD_IDX(last_dir->dirnum) - pidx):(pidx - PD_IDX(last_dir->dirnum))
	  < (PD_IDX(prev_dir->dirnum) > pidx) ?
	  (PD_IDX(prev_dir->dirnum) - pidx):(pidx - PD_IDX(prev_dir->dirnum)))
	    *pdi = last_dir;
	else
	    *pdi = prev_dir;

	if (PD_IDX((*pdi)->dirnum) > pidx) {
	    for (spi=(*pdi)->prev;spi!=NULL && PD_IDX(spi->dirnum)>pidx;
		 spi=spi->prev)
		*pdi = spi;
	    if (spi != NULL)
		*pdi = spi;
	} else
	    for (spi=(*pdi)->next;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;
		 spi=spi->next)
		*pdi = spi;
    } else {
	*pdi = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
	for (spi=*pdi;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;spi=spi->next)
	    *pdi = spi;
    }

    return ((PD_IDX((*pdi)->dirnum) == pidx)?0:(PD_IDX((*pdi)->dirnum) > pidx)?1:-1);
}


a265 1
    struct pdinfo *pi;
a268 1
    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);
d271 5
a275 11
    for(j=0;j<=last_index;) {
	fprintf(fd, "%08lx %5d ", j << malloc_pageshift, j);
	if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d278 3
a280 9
	} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d283 3
a285 9
	} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d288 2
a289 2
	} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[PI_OFF(j)]);
d292 2
a293 8
		pd[PI_OFF(j)], pd[PI_OFF(j)]->free, pd[PI_OFF(j)]->total,
		pd[PI_OFF(j)]->size, pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
	}
	if (!PI_OFF(++j)) {
	    if ((pi = pi->next) == NULL)
		break;
	    pd = pi->base;
	    j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
d299 1
a299 2
		pf, pf->page, pf->page + pf->size, pf->size,
		pf->prev, pf->next);
d311 4
a314 1
    fprintf(fd, "In use\t%lu\n", (u_long)malloc_used);
d342 1
a342 2
    if (malloc_abort)
	abort();
d388 1
a388 5
    struct pdinfo *pi, *spi;
    struct pginfo **pd;
    u_long pidx,lidx;
    void *result, *tail;
    u_long index;
d390 1
d392 1
a392 3
    result = MMAP(pages + malloc_guard);
    if (result == MAP_FAILED) {
	errno = ENOMEM;
d394 1
a394 1
	wrtwarning("(ES): map_pages fails\n");
d396 1
a399 2
    if (malloc_guard)
	mprotect(result + pages, malloc_guard, PROT_NONE);
d401 1
a401 16
    if (tail > malloc_brk)
	malloc_brk = tail;
    if ((index = ptr2index(tail) - 1) > last_index)
	last_index = index;

    /* Insert directory pages, if needed. */
    pidx = PI_IDX(ptr2index(result));
    lidx = PI_IDX(index);

    pdir_lookup(ptr2index(result), &pi);

    for (index=pidx,spi=pi;index<=lidx;index++) {
	if (pi == NULL || PD_IDX(pi->dirnum) != index) {
	    if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
		errno = ENOMEM;
		munmap(result, tail - result);
d403 1
a403 1
		wrtwarning("(ES): map_pages fails\n");
d405 1
a405 37
		return (NULL);
	    }
	    memset(pd, 0, malloc_pagesize);
	    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);
	    pi->base = pd;
	    pi->prev = spi;
	    pi->next = spi->next;
	    pi->dirnum = index * (malloc_pagesize/sizeof(struct pginfo *));

	    if (spi->next != NULL)
		spi->next->prev = pi;
	    spi->next = pi;
	}
        if (index > pidx && index < lidx) {
	    pi->dirnum += pdi_mod;
	} else if (index == pidx) {
	    if (pidx == lidx) {
		pi->dirnum += (tail - result) >> malloc_pageshift;
	    } else {
		pi->dirnum += pdi_mod - PI_OFF(ptr2index(result));
	    }
	} else {
	    pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
	}
#ifdef	MALLOC_EXTRA_SANITY
	if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > index) {
	    wrterror("(ES): pages directory overflow\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (index == pidx && pi != last_dir) {
	   prev_dir = last_dir;
	   last_dir = pi;
	}
	spi = pi;
	pi = spi->next;
d407 8
d419 52
d523 2
a528 2
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d564 1
a564 1
    if (page_dir == MAP_FAILED) {
a565 3
	errno = ENOMEM;
	return;
    }
d567 6
a572 7
    pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
    pdi_mod = pdi_off / sizeof(struct pginfo *);

    last_dir = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
    last_dir->base = page_dir;
    last_dir->prev = last_dir->next = NULL;
    last_dir->dirnum = malloc_pageshift;
d574 1
a574 1
    malloc_ninfo = pdi_mod;
d586 5
a601 5
    struct rlimit rl;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    void *tp;
a603 1
    int m;
d609 1
a609 1
    for (pf = free_list.next; pf; pf = pf->next) {
d612 1
a612 1
	if (pf->size & malloc_pagemask) {
d614 1
a614 4
	    errno = EFAULT;
	    return (NULL);
	}
	if (!pf->size) {
d616 3
a618 4
	    errno = EFAULT;
	    return (NULL);
	}
	if (pf->page > (pf->page + pf->size)) {
d620 3
a622 15
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pi = pf->pdir) == NULL) {
	    wrterror("(ES): invalid page directory on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
	    wrterror("(ES): directory index mismatch on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	pd = pi->base;
	if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
d624 1
a624 12
	    errno = EFAULT;
	    return (NULL);
	}
	pidx = PI_IDX(ptr2index((pf->page)+(pf->size))-1);
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): last page not referenced in page directory\n");
	    errno = EFAULT;
	    return (NULL);
	}
	pd = pi->base;
	if (pd[PI_OFF(ptr2index((pf->page)+(pf->size))-1)] != MALLOC_FREE) {
a625 3
	    errno = EFAULT;
	    return (NULL);
	}
a632 1
	    pi = pf->pdir;
a642 10
	pidx = PI_IDX(ptr2index(pf->page));
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): hole in directories\n");
	    errno = EFAULT;
	    return (NULL);
	}
	tp = pf->pdir;
	pf->pdir = pi;
	pi = tp;
d648 2
a649 6
#ifdef	MALLOC_EXTRA_SANITY
    if (p != NULL && pi != NULL) {
	pidx = PD_IDX(pi->dirnum);
	pd = pi->base;
    }
    if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
a650 3
	errno = EFAULT;
	return (NULL);
    }
d653 1
a653 1
    if (p != NULL && (malloc_guard || malloc_freeprot))
d665 3
a667 47
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): mapped pages not found in directory\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
	    prev_dir = last_dir;
	    last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FIRST;
	for (i=1;i<size;i++) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
#ifdef	MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		pd = pi->base;
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FOLLOW;
	}
	if (malloc_guard) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
#ifdef	MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		pd = pi->base;
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FIRST;
	}

	malloc_used += size << malloc_pageshift;
d690 1
a690 4
    struct pginfo *bp;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d768 1
a768 15
    pidx = PI_IDX(ptr2index(pp));
    pdir_lookup(ptr2index(pp), &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (0);
    }
#endif /* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }
    pd = pi->base;
    pd[PI_OFF(ptr2index(pp))] = bp;
d822 1
a822 1

d835 1
a835 1
	    if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
a836 3
		errno = EFAULT;
		return (NULL);
	    }
d838 2
a839 2
	    if (*lp & u)
		i--;
d899 1
a899 1
    u_long osize, index, i;
d901 1
a901 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d923 1
a923 16
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (NULL);
    }
#endif /* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    mp = &pd[PI_OFF(index)];
d934 1
a934 10
	i = index;
	if (!PI_OFF(++i)) {
	    pi = pi->next;
	    if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		pi = NULL;
	    if (pi != NULL)
		pd = pi->base;
	}
	for (osize = malloc_pagesize;
	     pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
a935 8
	    if (!PI_OFF(++i)) {
		pi = pi->next;
		if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		    pi = NULL;
		if (pi != NULL)
		    pd = pi->base;
	    }
	}
a990 1

a1001 3
    struct pginfo **pd;
    struct pdinfo *pi, *spi;
    u_long pidx, lidx;
d1021 3
a1023 32
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif /* MALLOC_EXTRA_SANITY */

    spi = pi;		/* Save page index for start of region. */

    pd = pi->base;
    pd[PI_OFF(index)] = MALLOC_FREE;
    i = 1;
    if (!PI_OFF(index+i)) {
	pi = pi->next;
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
	    pi = NULL;
	else
	    pd = pi->base;
    }
    while (pi != NULL && pd[PI_OFF(index+i)] == MALLOC_FOLLOW) {
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	i++;
	if (!PI_OFF(index+i)) {
	    if ((pi=pi->next) == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
		pi = NULL;
	    else
		pd = pi->base;
	}
    }
a1029 14
    malloc_used -= l;
    if (malloc_guard) {
#ifdef	MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i)) {
	    wrterror("(ES): hole in mapped pages directory\n");
	    errno = EFAULT;
	    return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr + l;

d1035 6
d1042 1
a1042 1
	mprotect(ptr, l, PROT_NONE);
d1044 1
a1044 1
    /* Add to free-list. */
d1048 1
a1048 1
    px->pdir = spi;
d1053 2
a1054 2
	/* Nothing on free list, put this at head. */
	px->next = NULL;
d1064 1
a1064 1
	for(pf = free_list.next; (pf->page+pf->size) < ptr && pf->next != NULL;
d1066 1
a1066 1
	    ; /* Race ahead here. */
d1076 3
a1078 2
	} else if ((pf->page + pf->size) == ptr ) {
	    /* Append to the previous entry. */
d1080 1
a1080 1
	    if (pf->next != NULL && (pf->page + pf->size) == pf->next->page ) {
d1083 1
d1090 1
a1090 1
	    /* Prepend to entry. */
a1092 1
	    pf->pdir = spi;
d1094 1
a1094 1
	    /* Append at tail of chain. */
a1101 2
	    errno = EFAULT;
	    return;
a1104 5
    if (pf->pdir != last_dir) {
	prev_dir = last_dir;
	last_dir = pf->pdir;
    }

d1108 2
a1109 1
      (pf->page + pf->size) == malloc_brk) {	/* ..and none behind us, */
d1115 1
a1115 4
	if (munmap((char *)pf->page + malloc_cache, pf->size - malloc_cache)!=0)
	    goto not_return;
	tail = pf->page + pf->size;
	lidx = ptr2index(tail) - 1;
d1118 2
a1119 7
	malloc_brk = pf->page + malloc_cache;

	index = ptr2index(malloc_brk);

	pidx = PI_IDX(index);
	if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
		prev_dir = NULL;	/* Will be wiped out below ! */
d1121 1
a1121 1
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
d1123 2
a1124 33
	if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
	    pd = pi->base;

	    for(i=index;i <= last_index;) {
		if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
		    pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef	MALLOC_EXTRA_SANITY
		    if (!PD_OFF(pi->dirnum)) {
			wrterror("(ES): pages directory underflow\n");
			errno = EFAULT;
			return;
		    }
#endif /* MALLOC_EXTRA_SANITY */
		    pi->dirnum--;
		}
		i++;
		if (!PI_OFF(i)) {
		    /* If no page in that dir, free directory page. */
		    if (!PD_OFF(pi->dirnum)) {
			/* Remove from list. */
			pi->prev->next = pi->next;
			if (pi->next != NULL)
			    pi->next->prev = pi->prev;
			pi = pi->next;
			munmap(pd, malloc_pagesize);
		    } else
			pi = pi->next;
		    if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(i))
			break;
		    pd = pi->base;
		}
	    }
	}
a1129 1
not_return:
a1143 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d1175 1
a1175 2
	while (*mp != NULL && (*mp)->next != NULL &&
	       (*mp)->next->page < info->page)
d1189 2
a1190 5
	if (!*mp) {
	    wrterror("(ES): Not on queue\n");
	    errno = EFAULT;
	    return;
	}
d1196 1
a1196 16
    pidx = PI_IDX(ptr2index(info->page));
    pdir_lookup(ptr2index(info->page), &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif /* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;
a1213 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d1241 1
a1241 16
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif /* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    info = pd[PI_OFF(index)];
d1260 1
a1260 2
    if (noprint == 0) {
	noprint = 1;
d1262 1
a1262 1
    }
d1286 1
a1286 1
    if (malloc_xmalloc && r == NULL) {
a1287 2
	errno = ENOMEM;
    }
d1326 1
a1326 1
    if (malloc_xmalloc && r == NULL) {
a1327 2
	errno = ENOMEM;
    }
@


1.69
log
@Missing check for NULL.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.68 2004/08/01 08:45:39 tdeval Exp $";
d427 2
a428 1
    abort();
d537 1
a537 1
	if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > index)
d539 3
d647 1
a647 1
    if (page_dir == MAP_FAILED)
d649 3
a694 8
    if (getrlimit(RLIMIT_DATA, &rl) == -1)
	wrterror("process limits not available\n");
    if (rl.rlim_cur != RLIM_INFINITY &&
      size > ((size_t)rl.rlim_cur - malloc_used)) {
	errno = ENOMEM;
	return (NULL);
    }

d700 1
a700 1
	if (pf->size & malloc_pagemask)
d702 4
a705 1
	if (!pf->size)
d707 4
a710 1
	if (pf->page > (pf->page + pf->size))
d712 4
a715 1
	if ((pi = pf->pdir) == NULL)
d717 4
a720 1
	if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum))
d722 3
d726 1
a726 1
	if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE)
d728 3
d733 1
a733 1
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d735 3
d739 1
a739 1
	if (pd[PI_OFF(ptr2index((pf->page)+(pf->size))-1)] != MALLOC_FREE)
d741 3
d764 1
a764 1
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d766 3
d782 1
a782 1
    if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE)
d784 3
d804 1
a804 1
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d806 3
d821 1
a821 1
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d823 3
d836 1
a836 1
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d838 3
d954 1
a954 1
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d956 3
d1032 1
a1032 1
	    if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS)
d1034 3
d1128 1
a1128 1
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d1130 3
d1262 1
a1262 1
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d1264 3
d1300 1
a1300 1
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
d1302 3
d1376 2
d1418 1
a1418 1
		    if (!PD_OFF(pi->dirnum))
d1420 3
d1513 5
a1517 2
	if (!*mp)
		wrterror("(ES): Not on queue\n");
d1526 1
a1526 1
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d1528 3
d1589 1
a1589 1
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx)
d1591 3
d1647 1
a1647 1
    if (malloc_xmalloc && r == NULL)
d1649 2
d1689 1
a1689 1
    if (malloc_xmalloc && r == NULL)
d1691 2
@


1.68
log
@After a long gestation period, here comes our custom version of malloc(3)
using mmap(2) instead of sbrk(2).
To make a long story short, using mmap(2) in malloc(3) allows us to draw
all the benefits from our mmap(2)'s randomization feature, closing the
effort we did for returning memory blocks from random addresses.

Tested for a long time by many, thanks to them.
Go for it ! deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.67 2004/04/12 09:25:11 tdeval Exp $";
d1351 1
a1351 1
	if (PD_IDX(prev_dir->dirnum) >= pidx)
@


1.67
log
@Clean up malloc_active state when aborting.
This allows for safe abort handling, without tripping into
false recursivity problems.

Ok tedu@@, deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.66 2004/02/19 23:20:53 tdeval Exp $";
d41 2
d84 5
a88 5
    u_short		size;	/* size of this page's chunks */
    u_short		shift;	/* How far to shift for this size chunks */
    u_short		free;	/* How many free chunks */
    u_short		total;	/* How many chunk */
    u_long		bits[1]; /* Which chunks are free */
d99 2
a100 2
    void		*end;	/* pointer to end of free pages */
    u_long		size;	/* number of bytes free */
d145 2
a146 2
#define pageround(foo) (((foo) + (malloc_pagemask))&(~(malloc_pagemask)))
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)-malloc_origo)
d166 16
a181 2
/* The offset from pagenumber to index into the page directory */
static u_long malloc_origo;
d250 4
a253 1
/* my last break. */
d256 1
a256 1
/* one location cache for free-list holders */
d259 1
a259 1
/* compile-time options */
d262 1
a262 1
/* Name of the current public function */
d265 1
a265 1
/* Macro for mmap */
d271 1
a271 1
 * Necessary function declarations
a272 1
static int extend_pgdir(u_long index);
d278 43
d327 1
d331 1
d334 11
a344 5
    for(j=0;j<=last_index;j++) {
	fprintf(fd, "%08lx %5d ", (j+malloc_origo) << malloc_pageshift, j);
	if (pd[j] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_NOT_MINE;j++)
		;
d347 9
a355 3
	} else if (pd[j] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FREE;j++)
		;
d358 9
a366 3
	} else if (pd[j] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FOLLOW;j++)
		;
d369 2
a370 2
	} else if (pd[j] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[j]);
d373 8
a380 2
		pd[j], pd[j]->free, pd[j]->total,
		pd[j]->size, pd[j]->page, pd[j]->next);
d386 2
a387 1
		pf, pf->page, pf->end, pf->size, pf->prev, pf->next);
d399 1
a399 4
    fprintf(fd, "FirstPage\t%ld\n", malloc_origo);
    fprintf(fd, "LastPage\t%ld %lx\n", last_index+malloc_pageshift,
	(last_index + malloc_pageshift) << malloc_pageshift);
    fprintf(fd, "Break\t%ld\n", (u_long)sbrk(0) >> malloc_pageshift);
d473 5
a477 1
    caddr_t result, tail;
a478 1
    result = (caddr_t)pageround((u_long)sbrk(0));
d480 3
a482 1
    if (pages > SIZE_T_MAX - (size_t)result) {
d484 1
a484 1
	wrtwarning("(ES): overflow in map_pages fails\n");
a485 1
	errno = ENOMEM;
d489 2
d492 16
a507 1
    if (brk(tail) == (char *)-1) {
d509 29
a537 1
	wrtwarning("(ES): map_pages fails\n");
d539 6
a544 1
	return (NULL);
a545 8
    if (malloc_guard)
	mprotect(result + pages, malloc_pagesize, PROT_NONE);

    last_index = ptr2index(tail) - 1;
    malloc_brk = tail;

    if ((last_index+1) >= malloc_ninfo && !extend_pgdir(last_index))
	return (NULL);
a549 52
/*
 * Extend page directory
 */
static int
extend_pgdir(u_long index)
{
    struct  pginfo **new, **old;
    size_t i, oldlen;

    /* Make it this many pages */
    i = index * sizeof *page_dir;
    i /= malloc_pagesize;
    i += 2;

    /* remember the old mapping size */
    oldlen = malloc_ninfo * sizeof *page_dir;

    /*
     * NOTE: we allocate new pages and copy the directory rather than tempt
     * fate by trying to "grow" the region.. There is nothing to prevent
     * us from accidently re-mapping space that's been allocated by our caller
     * via dlopen() or other mmap().
     *
     * The copy problem is not too bad, as there is 4K of page index per
     * 4MB of malloc arena.
     *
     * We can totally avoid the copy if we open a file descriptor to associate
     * the anon mappings with.  Then, when we remap the pages at the new
     * address, the old pages will be "magically" remapped..  But this means
     * keeping open a "secret" file descriptor.....
     */

    /* Get new pages */
    new = (struct pginfo**) MMAP(i * malloc_pagesize);
    if (new == MAP_FAILED)
	return (0);

    /* Copy the old stuff */
    memcpy(new, page_dir,
	    malloc_ninfo * sizeof *page_dir);

    /* register the new size */
    malloc_ninfo = i * malloc_pagesize / sizeof *page_dir;

    /* swap the pointers */
    old = page_dir;
    page_dir = new;

    /* Now free the old stuff */
    munmap(old, oldlen);
    return (1);
}
a601 2
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d606 2
d646 7
a652 6
    /*
     * We need a maximum of malloc_pageshift buckets, steal these from the
     * front of the page_directory;
     */
    malloc_origo = ((u_long)pageround((u_long)sbrk(0))) >> malloc_pageshift;
    malloc_origo -= malloc_pageshift;
d654 1
a654 1
    malloc_ninfo = malloc_pagesize / sizeof *page_dir;
a665 5
    /*
     * This is a nice hack from Kaleb Keithly (kaleb@@x.org).
     * We can sbrk(2) further back when we keep this on a low address.
     */
    px = (struct pgfree *) imalloc (sizeof *px);
d677 5
d684 1
d688 8
d698 1
a698 1
    for(pf = free_list.next; pf; pf = pf->next) {
d705 1
a705 3
	if (pf->page == pf->end)
	    wrterror("(ES): zero entry on free_list\n");
	if (pf->page > pf->end)
d707 6
a712 3
	if ((void*)pf->page >= (void*)sbrk(0))
	    wrterror("(ES): entry on free_list past brk\n");
	if (page_dir[ptr2index(pf->page)] != MALLOC_FREE)
d714 6
a719 1
	if (page_dir[ptr2index(pf->end)-1] != MALLOC_FREE)
d728 1
d739 7
d751 6
a756 2
#ifdef MALLOC_EXTRA_SANITY
    if (p != NULL && page_dir[ptr2index(p)] != MALLOC_FREE)
d760 1
a760 1
    if ((malloc_guard || malloc_freeprot) && p != NULL)
d772 38
a809 3
	page_dir[index] = MALLOC_FIRST;
	for (i=1;i<size;i++)
	    page_dir[index+i] = MALLOC_FOLLOW;
d832 4
a835 1
    struct  pginfo *bp;
d913 12
a924 1
    page_dir[ptr2index(pp)] = bp;
d978 1
a978 1
    
d991 1
a991 1
	if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS)
d994 2
a995 2
	if (*lp & u)
	    i--;
d1055 1
a1055 1
    u_long osize, index;
d1057 3
a1059 1
    int i;
d1081 13
a1093 1
    mp = &page_dir[index];
d1104 10
a1113 1
	for (osize = malloc_pagesize; *(++mp) == MALLOC_FOLLOW;)
d1115 8
d1178 1
d1190 3
d1212 29
a1240 3
    page_dir[index] = MALLOC_FREE;
    for (i = 1; page_dir[index+i] == MALLOC_FOLLOW; i++)
	page_dir[index + i] = MALLOC_FREE;
d1247 11
a1262 6
    if (malloc_guard) {
	page_dir[index + i] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr+l;

d1264 1
a1264 1
	mprotect(ptr, tail - ptr, PROT_NONE);
d1266 1
a1266 1
    /* add to free-list */
d1270 1
a1270 1
    px->end = tail;
d1275 2
a1276 2
	/* Nothing on free list, put this at head */
	px->next = free_list.next;
d1286 1
a1286 1
	for(pf = free_list.next; pf->end < ptr && pf->next != NULL;
d1288 1
a1288 1
	    ; /* Race ahead here */
d1298 2
a1299 3
	} else if (pf->end == ptr ) {
	    /* Append to the previous entry */
	    pf->end = (char *)pf->end + l;
d1301 1
a1301 1
	    if (pf->next != NULL && pf->end == pf->next->page ) {
a1303 1
		pf->end = pt->end;
d1310 1
a1310 1
	    /* Prepend to entry */
d1313 1
d1315 1
a1315 1
	    /* Append at tail of chain */
d1326 5
d1334 1
a1334 2
      pf->end == malloc_brk &&			/* ..and none behind us, */
      malloc_brk == sbrk(0)) {			/* ..and it's OK to do... */
d1340 4
a1343 1
	pf->end = (char *)pf->page + malloc_cache;
d1346 3
a1348 2
	brk(pf->end);
	malloc_brk = pf->end;
d1350 3
a1352 1
	index = ptr2index(pf->end);
d1354 32
a1385 2
	for(i=index;i <= last_index;)
	    page_dir[i++] = MALLOC_NOT_MINE;
d1391 1
d1406 3
d1440 2
a1441 1
	while (*mp && (*mp)->next && (*mp)->next->page < info->page)
d1462 13
a1474 1
    page_dir[ptr2index(info->page)] = MALLOC_FIRST;
d1492 3
d1522 13
a1534 1
    info = page_dir[index];
d1553 2
a1554 1
    if (noprint == 0)
d1556 1
a1556 1
    noprint = 1;
@


1.66
log
@Sanity fix.
reviewed by deraadt@@, tedu@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.65 2003/11/19 02:27:18 tedu Exp $";
d231 3
d341 1
a1248 2

static int malloc_active;
@


1.65
log
@only whine about recursion once, so we don't get into problems with loops.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.64 2003/10/16 17:05:05 tedu Exp $";
d1031 4
a1034 1
    l += malloc_guard;
@


1.64
log
@by popular demand, malloc guard pages.  insert an unreadable/unwriteable
page after each page size allocation to detect overrun.  this is
somewhat electric fence like, while attempting to be mostly usable in
production.  also, use tdeval's chunk randomization code.
enabled with the G option.
ok deraadt and co.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.63 2003/10/15 21:37:01 tedu Exp $";
d995 1
a995 1
free_pages(void *ptr, int index, struct pginfo *info)
d997 1
a997 1
    int i;
a998 1
    u_long l;
d1207 1
a1207 1
    int index;
d1243 2
d1246 3
a1248 1
 * These are the public exported interface routines.
d1250 4
d1255 7
a1261 1
static int malloc_active;
d1263 3
d1274 1
a1274 4
	wrtwarning("recursive call\n");
        malloc_active--;
	_MALLOC_UNLOCK();
	errno = EDEADLK;
d1292 1
a1292 4
	wrtwarning("recursive call\n");
        malloc_active--;
	_MALLOC_UNLOCK();
	errno = EDEADLK;
d1310 1
a1310 4
	wrtwarning("recursive call\n");
        malloc_active--;
	_MALLOC_UNLOCK();
	errno = EDEADLK;
@


1.63
log
@abort on errors by default.  workaround so running out of memory isn't
actually an error, A still applies full effect.
suggested by phk. ok deraadt@@ tdeval@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.62 2003/10/02 00:02:10 tedu Exp $";
d196 6
d395 1
a395 1
    tail = result + pages;
d403 2
d511 4
d601 1
a601 1
    size = pageround(size);
d642 2
d649 3
d818 20
d1032 1
d1035 3
d1042 1
a1042 1
    px->end =  tail;
@


1.62
log
@two minor fixes.  set errno on recursive calls.  ENOMEM suggested by marc@@.
lock before setting malloc_func, not after.
ok cloder@@ deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.61 2003/09/30 00:22:03 tedu Exp $";
d180 1
a180 1
static int malloc_abort;
d842 1
a842 1
    if (malloc_abort && result == NULL)
@


1.61
log
@full stop.  reverse course.  remove all periods, so as to be aligned
with error messages elsewhere.  requested ok deraadt@@ henning@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.60 2003/09/27 21:09:15 tedu Exp $";
d1214 1
a1215 1
    _MALLOC_LOCK();
d1220 1
d1235 1
a1236 1
    _MALLOC_LOCK();
d1241 1
d1256 1
a1257 1
    _MALLOC_LOCK();
d1262 1
@


1.60
log
@remove register.  end all sentences with periods.
ok deraadt@@ henning@@ millert@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.59 2003/08/04 16:51:49 jfb Exp $";
d152 1
a152 1
	    wrterror("open of /dev/zero.\n"); }
d292 1
a292 1
		fprintf(fd, "Free_list loops.\n");
d362 1
a362 1
    char *q = "malloc() warning: Couldn't dump stats.\n";
d384 1
a384 1
	wrtwarning("(ES): overflow in map_pages fails.\n");
d393 1
a393 1
	wrtwarning("(ES): map_pages fails.\n");
d524 1
a524 1
		    wrtwarning("unknown char in MALLOC_OPTIONS.\n");
d542 1
a542 1
		wrtwarning("atexit(2) failed.  Will not be able to dump malloc stats on exit.\n");
d549 1
a549 1
	wrterror("mmap(2) failed, check limits.\n");
d597 1
a597 1
	    wrterror("(ES): junk length entry on free_list.\n");
d599 1
a599 1
	    wrterror("(ES): zero length entry on free_list.\n");
d601 1
a601 1
	    wrterror("(ES): zero entry on free_list.\n");
d603 1
a603 1
	    wrterror("(ES): sick entry on free_list.\n");
d605 1
a605 1
	    wrterror("(ES): entry on free_list past brk.\n");
d607 1
a607 1
	    wrterror("(ES): non-free first page on free-list.\n");
d609 1
a609 1
	    wrterror("(ES): non-free last page on free-list.\n");
d632 1
a632 1
	wrterror("(ES): allocated non-free page on free-list.\n");
d843 1
a843 1
	wrterror("allocation failed.\n");
d866 1
a866 1
	wrtwarning("malloc() has never been called.\n");
d873 1
a873 1
	wrtwarning("junk pointer, too low to make sense.\n");
d878 1
a878 1
	wrtwarning("junk pointer, too high to make sense.\n");
d888 1
a888 1
	    wrtwarning("modified (page-) pointer.\n");
d908 1
a908 1
	    wrtwarning("modified (chunk-) pointer.\n");
d917 1
a917 1
	    wrtwarning("chunk is already free.\n");
d933 1
a933 1
	wrtwarning("pointer to wrong page.\n");
d966 1
a966 1
	wrtwarning("page is already free.\n");
d971 1
a971 1
	wrtwarning("pointer to wrong page.\n");
d976 1
a976 1
	wrtwarning("modified (page-) pointer.\n");
d1054 1
a1054 1
	    wrterror("freelist is destroyed.\n");
d1103 1
a1103 1
	wrtwarning("modified (chunk-) pointer.\n");
d1108 1
a1108 1
	wrtwarning("chunk is already free.\n");
d1143 1
a1143 1
		wrterror("(ES): Not on queue.\n");
d1174 1
a1174 1
	wrtwarning("malloc() has never been called.\n");
d1185 1
a1185 1
	wrtwarning("junk pointer, too low to make sense.\n");
d1190 1
a1190 1
	wrtwarning("junk pointer, too high to make sense.\n");
d1217 1
a1217 1
	wrtwarning("recursive call.\n");
d1227 1
a1227 1
	wrterror("out of memory.\n");
d1237 1
a1237 1
	wrtwarning("recursive call.\n");
d1257 1
a1257 1
	wrtwarning("recursive call.\n");
d1271 1
a1271 1
	wrterror("out of memory.\n");
@


1.59
log
@ansify function arguments

ok tdeval@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.58 2003/07/19 23:52:27 tdeval Exp $";
d152 1
a152 1
	    wrterror("open of /dev/zero"); }
d384 1
a384 1
	wrtwarning("(ES): overflow in map_pages fails\n");
d393 1
a393 1
	wrtwarning("(ES): map_pages fails\n");
d524 1
a524 1
		    wrtwarning("unknown char in MALLOC_OPTIONS\n");
d597 1
a597 1
	    wrterror("(ES): junk length entry on free_list\n");
d599 1
a599 1
	    wrterror("(ES): zero length entry on free_list\n");
d601 1
a601 1
	    wrterror("(ES): zero entry on free_list\n");
d603 1
a603 1
	    wrterror("(ES): sick entry on free_list\n");
d605 1
a605 1
	    wrterror("(ES): entry on free_list past brk\n");
d607 1
a607 1
	    wrterror("(ES): non-free first page on free-list\n");
d609 1
a609 1
	    wrterror("(ES): non-free last page on free-list\n");
d632 1
a632 1
	wrterror("(ES): allocated non-free page on free-list\n");
d1143 1
a1143 1
		wrterror("(ES): Not on queue\n");
d1212 1
a1212 1
    register void *r;
d1252 1
a1252 1
    register void *r;
@


1.58
log
@- just warn in case of mmap/brk failure
- extend_pgdir and malloc_make_chunks return int, not void*

ok tedu@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.57 2003/07/13 08:35:44 otto Exp $";
d253 1
a253 2
malloc_dump(fd)
    FILE *fd;
d312 1
a312 2
wrterror(p)
    char *p;
d336 1
a336 2
wrtwarning(p)
    char *p;
d359 1
a359 1
malloc_exit()
d376 1
a376 2
map_pages(pages)
    size_t pages;
d411 1
a411 2
extend_pgdir(index)
    u_long index;
d464 1
a464 1
malloc_init ()
d582 1
a582 2
malloc_pages(size)
    size_t size;
d667 1
a667 2
malloc_make_chunks(bits)
    int bits;
d761 1
a761 2
malloc_bytes(size)
    size_t size;
d823 1
a823 2
imalloc(size)
    size_t size;
d855 1
a855 3
irealloc(ptr, size)
    void *ptr;
    size_t size;
d958 1
a958 4
free_pages(ptr, index, info)
    void *ptr;
    int index;
    struct pginfo *info;
d1093 1
a1093 4
free_bytes(ptr, index, info)
    void *ptr;
    int index;
    struct pginfo *info;
d1164 1
a1164 2
ifree(ptr)
    void *ptr;
@


1.57
log
@Fix two cases where malloc() returns NULL but does not set errno to ENOMEM.
ok tdeval@@ henning@@ millert@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.56 2003/05/14 17:46:39 tdeval Exp $";
d388 1
a388 1
	wrterror("(ES): overflow in map_pages fails\n");
d397 1
a397 1
	wrterror("(ES): map_pages fails\n");
d405 1
a405 1
    if ((last_index+1) >= malloc_ninfo && extend_pgdir(last_index) == NULL)
d793 1
a793 1
    if (page_dir[j] == NULL && malloc_make_chunks(j) == NULL)
@


1.56
log
@Unbreak 64-bit archs...
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.55 2003/05/14 15:41:51 tdeval Exp $";
d390 1
d842 1
a842 1
    if ((size + malloc_pagesize) < size)       /* Check for overflow */
d844 2
@


1.55
log
@Pointer cleaning.  ok ian@@, tedu@@, krw@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.54 2003/01/14 02:27:16 millert Exp $";
d394 1
a394 1
    if ((int)brk(tail) == -1) {
@


1.54
log
@Add sanity check to prevent int oflow for very large allocations.
Also fix a signed vs. unsigned issue while I am at it.
Found by Jim Geovedi.  OK deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.53 2002/11/27 21:40:32 tdeval Exp $";
d240 1
a240 1
	    MMAP_FD, (off_t)0);
d366 1
a366 1
    if (fd) {
d390 1
a390 1
	return 0;
d394 1
a394 1
    if (brk(tail)) {
d398 1
a398 1
	return 0;
d404 2
a405 2
    if ((last_index+1) >= malloc_ninfo && !extend_pgdir(last_index))
	return 0;
d407 1
a407 1
    return result;
d446 1
a446 1
	return 0;
d461 1
a461 1
    return 1;
d497 1
a497 1
	for (; p && *p; p++) {
d589 1
a589 1
    void *p, *delay_free = 0;
d596 1
a596 1
    p = 0;
d622 1
a622 1
	    if (pf->next)
d636 1
a636 1
    if (p && page_dir[ptr2index(p)] != MALLOC_FREE)
d643 1
a643 1
    if (!p)
d646 1
a646 1
    if (p) {
d658 1
a658 1
	if (!px)
d664 1
a664 1
    return p;
d681 2
a682 2
    if (!pp)
	return 0;
d700 1
a700 1
	if (!bp) {
d702 1
a702 1
	    return 0;
d721 1
a721 1
	    return 0;
d760 1
a760 1
    return 1;
d792 2
a793 2
    if (!page_dir[j] && !malloc_make_chunks(j))
	return 0;
d813 1
a813 1
	bp->next = 0;
d823 1
a823 1
    return (u_char *)bp->page + k;
d842 1
a842 1
	result = 0;
d848 1
a848 1
    if (malloc_abort && !result)
d851 1
a851 1
    if (malloc_zero && result)
d854 1
a854 1
    return result;
d875 1
a875 1
	return 0;
d882 1
a882 1
	return 0;
d887 1
a887 1
	return 0;
d897 1
a897 1
	    return 0;
d901 1
a901 1
	for (osize = malloc_pagesize; *++mp == MALLOC_FOLLOW;)
d909 1
a909 1
	    return ptr;				/* ..don't do anything else. */
d917 1
a917 1
	    return 0;
d926 1
a926 1
	    return 0;
d937 1
a937 1
	    return ptr;			/* ..don't do anything else. */
d942 1
a942 1
	return 0;
d947 1
a947 1
    if (p) {
d958 1
a958 1
    return p;
d972 1
a972 1
    struct pgfree *pf, *pt=0;
d1009 1
a1009 1
    if (!px)
d1014 2
a1015 1
    if (!free_list.next) {
d1022 1
a1022 1
	px = 0;
a1026 1
	tail = (char *)ptr+l;
d1028 2
a1029 1
	for(pf = free_list.next; pf->end < ptr && pf->next; pf = pf->next)
d1039 1
a1039 1
	    px = 0;
d1044 1
a1044 1
	    if (pf->next && pf->end == pf->next->page ) {
d1050 1
a1050 1
		if (pf->next)
d1057 1
a1057 1
	} else if (!pf->next) {
d1059 1
a1059 1
	    px->next = 0;
d1063 1
a1063 1
	    px = 0;
d1070 1
a1070 1
    if (!pf->next &&				/* If we're the last one, */
d1094 1
a1094 1
    if (pt)
d1185 1
a1185 1
    if (!ptr)
d1235 1
a1235 1
	return (0);
d1241 1
a1241 1
    if (malloc_xmalloc && !r)
d1275 1
a1275 1
	return (0);
d1277 1
a1277 1
    if (!ptr) {
d1285 1
a1285 1
    if (malloc_xmalloc && !r)
@


1.53
log
@Honour malloc_junk ('J') with realloc(3), and fix page_dir shrink update.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.52 2002/11/25 00:06:51 cloder Exp $";
d49 1
d380 1
a380 1
    int pages;
d385 8
a392 1
    tail = result + (pages << malloc_pageshift);
@


1.52
log
@Warn if atexit(3) fails.  Change some tabs to spaces.  Use
STDERR_FILENO instead of 2.

OK millert@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.51 2002/11/05 22:19:55 marc Exp $";
d896 1
a896 1
        if (!malloc_realloc &&			/* unless we have to, */
d899 3
a901 1
	    return ptr;				/* don't do anything. */
d924 1
a924 1
	  size < osize &&		/* ..or are too small, */
d927 3
a929 1
	    return ptr;			/* ..Don't do anything */
a1076 1
	last_index = index - 1;
d1080 2
@


1.51
log
@
thread safe libc -- 2nd try.   OK miod@@, millert@@
Thanks to miod@@ for m68k and vax fixes
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.50 2002/11/03 23:58:39 marc Exp $";
d367 1
a367 1
	fclose(fd);
d369 1
a369 1
	write(2, q, strlen(q));
d537 2
a538 2
    if (malloc_stats)
	atexit(malloc_exit);
@


1.50
log
@back out previous patch.. there are still some vax/m68k issues
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.49 2002/11/03 20:36:43 marc Exp $";
d51 2
a71 33
#ifdef _THREAD_SAFE
# include "thread_private.h"
# if 0
   /* kernel threads */
#  include <pthread.h>
   static pthread_mutex_t malloc_lock;
#  define THREAD_LOCK()		pthread_mutex_lock(&malloc_lock)
#  define THREAD_UNLOCK()	pthread_mutex_unlock(&malloc_lock)
#  define THREAD_LOCK_INIT()	pthread_mutex_init(&malloc_lock, 0);
# else
   /* user threads */
#  include "spinlock.h"
   static spinlock_t malloc_lock = _SPINLOCK_INITIALIZER;
#  define THREAD_LOCK()		if (__isthreaded) _SPINLOCK(&malloc_lock)
#  define THREAD_UNLOCK()	if (__isthreaded) _SPINUNLOCK(&malloc_lock)
#  define THREAD_LOCK_INIT()
   /*
    * Malloc can't use the wrapped write() if it fails very early, so
    * we use the unwrapped syscall _thread_sys_write()
    */
#  define write _thread_sys_write
   ssize_t write(int, const void *, size_t);
#   undef malloc
#   undef realloc
#   undef free
# endif
#else
  /* no threads */
# define THREAD_LOCK()
# define THREAD_UNLOCK()
# define THREAD_LOCK_INIT()
#endif

d466 1
a466 1
    THREAD_LOCK_INIT();
d1216 1
a1216 1
    THREAD_LOCK();
d1220 1
a1220 1
	THREAD_UNLOCK();
d1226 1
a1226 1
    THREAD_UNLOCK();
d1236 1
a1236 1
    THREAD_LOCK();
d1240 1
a1240 1
	THREAD_UNLOCK();
d1246 1
a1246 1
    THREAD_UNLOCK();
d1256 1
a1256 1
    THREAD_LOCK();
d1260 1
a1260 1
	THREAD_UNLOCK();
d1270 1
a1270 1
    THREAD_UNLOCK();
@


1.49
log
@
libc changes for thread safety.  Tested on:
alpha (millert@@), i386 (marc@@), m68k (millert@@ and miod@@),
powerpc (drahn@@ and dhartmei@@), sparc (millert@@ and marc@@),
sparc64 (marc@@), and vax (millert@@ and miod@@).
Thanks to millert@@, miod@@, and mickey@@ for fixes along the way.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.48 2002/05/27 03:13:23 deraadt Exp $";
a50 2
#include "thread_private.h"

d70 33
d497 1
a497 1
    _MALLOC_LOCK_INIT();
d1247 1
a1247 1
    _MALLOC_LOCK();
d1251 1
a1251 1
	_MALLOC_UNLOCK();
d1257 1
a1257 1
    _MALLOC_UNLOCK();
d1267 1
a1267 1
    _MALLOC_LOCK();
d1271 1
a1271 1
	_MALLOC_UNLOCK();
d1277 1
a1277 1
    _MALLOC_UNLOCK();
d1287 1
a1287 1
    _MALLOC_LOCK();
d1291 1
a1291 1
	_MALLOC_UNLOCK();
d1301 1
a1301 1
    _MALLOC_UNLOCK();
@


1.48
log
@unsigned vs unsigned int
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.47 2002/02/16 21:27:24 millert Exp $";
d51 2
a71 33
#ifdef _THREAD_SAFE
# include "thread_private.h"
# if 0
   /* kernel threads */
#  include <pthread.h>
   static pthread_mutex_t malloc_lock;
#  define THREAD_LOCK()		pthread_mutex_lock(&malloc_lock)
#  define THREAD_UNLOCK()	pthread_mutex_unlock(&malloc_lock)
#  define THREAD_LOCK_INIT()	pthread_mutex_init(&malloc_lock, 0);
# else
   /* user threads */
#  include "spinlock.h"
   static spinlock_t malloc_lock = _SPINLOCK_INITIALIZER;
#  define THREAD_LOCK()		if (__isthreaded) _SPINLOCK(&malloc_lock)
#  define THREAD_UNLOCK()	if (__isthreaded) _SPINUNLOCK(&malloc_lock)
#  define THREAD_LOCK_INIT()
   /*
    * Malloc can't use the wrapped write() if it fails very early, so
    * we use the unwrapped syscall _thread_sys_write()
    */
#  define write _thread_sys_write
   ssize_t write(int, const void *, size_t);
#   undef malloc
#   undef realloc
#   undef free
# endif
#else
  /* no threads */
# define THREAD_LOCK()
# define THREAD_UNLOCK()
# define THREAD_LOCK_INIT()
#endif

d466 1
a466 1
    THREAD_LOCK_INIT();
d1216 1
a1216 1
    THREAD_LOCK();
d1220 1
a1220 1
	THREAD_UNLOCK();
d1226 1
a1226 1
    THREAD_UNLOCK();
d1236 1
a1236 1
    THREAD_LOCK();
d1240 1
a1240 1
	THREAD_UNLOCK();
d1246 1
a1246 1
    THREAD_UNLOCK();
d1256 1
a1256 1
    THREAD_LOCK();
d1260 1
a1260 1
	THREAD_UNLOCK();
d1270 1
a1270 1
    THREAD_UNLOCK();
@


1.47
log
@Part one of userland __P removal.  Done with a simple regexp with some minor hand editing to make comments line up correctly.  Another pass is forthcoming that handles the cases that could not be done automatically.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.46 2002/01/23 20:42:24 fgsch Exp $";
d189 1
a189 1
static unsigned malloc_started;
d192 1
a192 1
static unsigned malloc_cache = 16;
@


1.46
log
@THREAD_UNLOCK() on error before returning; millert@@ ok.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.45 2001/12/05 22:54:01 tdeval Exp $";
d91 1
a91 1
   ssize_t write __P((int, const void *, size_t));
d246 1
a246 1
void utrace __P((struct ut *, int));
@


1.45
log
@correct an alignment mis-conception for malloc(0) returned regions.
OK deraadt@@
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.44 2001/11/01 07:00:51 mickey Exp $";
d1251 1
d1291 1
@


1.44
log
@remove dangling spaces and tabs
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.42 2001/05/11 15:30:14 art Exp $";
a728 5
    bp->size = (1UL<<bits);
    bp->shift = bits;
    bp->total = bp->free = malloc_pagesize >> bits;
    bp->page = pp;

d731 9
d746 5
d843 1
a843 1
    if (malloc_junk && bp->shift != 0)
d936 1
a936 1
	if (((u_long)ptr & ((*mp)->size-1))) {
d969 1
a969 1
	if (osize != 1 && size != 0) {
d1133 1
a1133 1
    if (((u_long)ptr & (info->size-1))) {
d1143 1
a1143 1
    if (malloc_junk && info->shift != 0)
d1149 4
a1152 1
    mp = page_dir + info->shift;
a1157 1
	mp = page_dir + info->shift;
d1183 1
a1183 1
    if (info->shift == 0) {
@


1.43
log
@mprotect allocations sized at 0 bytes.  This will cause a fault for access
to such, permitting them to be discovered, instead of exploited as the ssh
crc insertion detector was.  Idea by theo, written by tdeval.
@
text
@d85 1
a85 1
#  define THREAD_LOCK_INIT()	
d141 1
a141 1
#define MALLOC_FREE 	((struct pginfo*) 1)
d189 1
a189 1
static unsigned malloc_started;	
d323 1
a323 1
 		fprintf(fd, "Free_list loops.\n");
d516 1
a516 1
	    	continue;
d630 1
a630 1
	if (pf->page > pf->end) 
d634 1
a634 1
	if (page_dir[ptr2index(pf->page)] != MALLOC_FREE) 
d650 1
a650 1
	} 
d918 2
a919 2
        if (!malloc_realloc && 			/* unless we have to, */
	  size <= osize && 			/* .. or are too small, */
d944 2
a945 2
	  size < osize && 		/* ..or are too small, */
	  (size > osize/2 ||	 	/* ..or could use a smaller size, */
d967 1
a967 1
    } 
d1077 1
a1077 1
    
d1178 1
a1178 1
    if(vp != (void*)info) 
@


1.42
log
@-1 -> MAP_FAILED
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.41 2001/05/10 16:14:19 art Exp $";
d66 2
a67 22
#if defined(__i386__) && defined(__FreeBSD__)
#   define malloc_pageshift		12U
#   define malloc_minsize		16U
#endif /* __i386__ && __FreeBSD__ */

#if defined(__sparc__) && !defined(__OpenBSD__)
#   define malloc_pageshift		12U
#   define malloc_minsize		16U
#   define MAP_ANON			(0)
#   define USE_DEV_ZERO
#   define MADV_FREE			MADV_DONTNEED
#endif /* __sparc__ */

/* Insert your combination here... */
#if defined(__FOOCPU__) && defined(__BAROS__)
#   define malloc_pageshift		12U
#   define malloc_minsize		16U
#endif /* __FOOCPU__ && __BAROS__ */

#if defined(__OpenBSD__) && !defined(__sparc__)
#   define	malloc_pageshift	(PGSHIFT)
#   define	malloc_minsize		16U
d147 1
a147 1
#define malloc_pageshift		12U
d713 7
a719 1
    if ((1UL<<(bits)) <= l+l) {
d734 10
d791 2
a792 1
    if (size < malloc_minsize)
d796 8
a803 4
    j = 1;
    i = size-1;
    while (i >>= 1)
	j++;
d834 1
a834 1
    if (malloc_junk)
d959 7
a965 4
	if (osize < size)
	    memcpy(p, ptr, osize);
	else
	    memcpy(p, ptr, size);
d1134 1
a1134 1
    if (malloc_junk)
d1170 7
@


1.41
log
@Use madvise(MADV_FREE) to allow the 'h' option.
(the code was already there, just not enabled).
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.40 2000/04/10 19:36:29 deraadt Exp $";
d488 1
a488 1
    if (new == (struct pginfo **)-1)
d595 1
a595 1
    if (page_dir == (struct pginfo **) -1)
@


1.40
log
@missing THREAD_UNLOCK; netch@@segfault.kiev.ua
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.39 2000/03/01 03:09:08 deraadt Exp $";
d246 1
a246 1
#ifdef __FreeBSD__
d550 1
a550 1
#ifdef __FreeBSD__
d1007 1
a1007 1
#ifdef __FreeBSD__
@


1.39
log
@typo fix; halogen@@nol.net
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.38 1999/11/10 20:12:31 millert Exp $";
d1248 1
@


1.38
log
@calloc() needs to be separate from malloc in case a user wants to have
their own malloc() implementation.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.37 1999/11/09 19:25:33 millert Exp $";
d72 1
a72 1
#   define malloc_pageshirt		12U
@


1.37
log
@Move calloc() into malloc.c and only zero out the area if malloc()
didn't do so for us.  By default, malloc() zeros out the space it
allocates but the programmer cannot rely on this as it is implementation-
specific (and configurable via /etc/malloc.conf)
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.36 1999/09/16 19:06:06 deraadt Exp $";
a1274 24
    malloc_active--;
    THREAD_UNLOCK();
    if (malloc_xmalloc && !r)
	wrterror("out of memory.\n");
    return (r);
}

void *
calloc(size_t num, size_t size)
{
    register void *r;

    malloc_func = " in calloc():";
    THREAD_LOCK();
    if (malloc_active++) {
	wrtwarning("recursive call.\n");
        malloc_active--;
	return (0);
    }
    size *= num;
    r = imalloc(size);
    if (r && !malloc_zero)
	memset(r, 0, size)
    UTRACE(0, size, r);
@


1.36
log
@use writev() where possible
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.35 1999/02/03 03:58:05 d Exp $";
d1275 24
@


1.35
log
@wrong ret type for write define (millert@@)
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.34 1999/02/01 07:58:30 d Exp $";
d40 4
a49 3
#include <sys/types.h>
#include <sys/param.h>
#include <sys/mman.h>
d367 12
a378 4
    write(2, __progname, strlen(__progname));
    write(2, malloc_func, strlen(malloc_func));
    write(2, q, strlen(q));
    write(2, p, strlen(p));
d392 2
d398 10
a407 4
    write(2, __progname, strlen(__progname));
    write(2, malloc_func, strlen(malloc_func));
    write(2, q, strlen(q));
    write(2, p, strlen(p));
@


1.34
log
@malloc can't use write() if it fails very early, so use the unwrapped syscall _thread_sys_write() if we are threaded
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.33 1998/11/20 11:18:50 d Exp $";
d110 1
a110 1
   int write __P((int, const void *, size_t));
@


1.33
log
@Add thread-safety to libc, so that libc_r will build (on i386 at least).
All POSIX libc api now there (to P1003.1c/D10)
  (more md stuff is needed for other libc/arch/*)
  (setlogin is no longer a special syscall)
Add -pthread option to gcc (that makes it use -lc_r and -D_POSIX_THREADS).
Doc some re-entrant routines
Add libc_r to intro(3)
dig() uses some libc srcs and an extra -I was needed there.
Add more md stuff to libc_r.
Update includes for the pthreads api
Update libc_r TODO
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.32 1998/08/06 16:26:32 millert Exp $";
d105 9
@


1.32
log
@Don't enumerate every arch in the #if since all OpenBSD platforms use the same values for malloc_pageshift and malloc_minsize except for sparc
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.31 1998/06/28 06:30:34 rahnds Exp $";
d90 16
a105 5
#include <pthread.h>
static pthread_mutex_t malloc_lock;
#define THREAD_LOCK()		pthread_mutex_lock(&malloc_lock)
#define THREAD_UNLOCK()		pthread_mutex_unlock(&malloc_lock)
#define THREAD_LOCK_INIT()	pthread_mutex_init(&malloc_lock, 0);
d107 4
a110 3
#define THREAD_LOCK()
#define THREAD_UNLOCK()
#define THREAD_LOCK_INIT()
@


1.31
log
@Oh fun, mucking about with files used on all archs.

This is one of many places in the source that have
#if defined("list all architectures")
Is there some possible way to eliminate, reduce these or at least
have a file that describes all occurrances so that when a new port is
done this could be addressed. like the recent hppa port, does it need to
take a look at this????
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.30 1998/01/02 05:32:49 deraadt Exp $";
d84 3
a86 7
#ifdef __OpenBSD__
#   if defined(__alpha__) || defined(__m68k__) || defined(__mips__) || \
       defined(__i386__) || defined(__m88k__) || defined(__ns32k__) || \
       defined(__vax__) || defined(__powerpc__)
#	define	malloc_pageshift	(PGSHIFT)
#	define	malloc_minsize		16U
#   endif /* __i386__ */
@


1.30
log
@make mmap() return void *, add MAP_FAILED
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.29 1997/08/23 10:43:25 pefo Exp $";
d87 1
a87 1
       defined(__vax__)
@


1.29
log
@Change realloc(foo,0) to behave like malloc(0). Both now return a pointer
to an object of size zero. This will allow testing on reallocs return value
to determine if the operation was successful or not.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.28 1997/08/22 17:06:59 deraadt Exp $";
d271 1
a271 1
	mmap((caddr_t)0, (size), PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, \
d394 1
a394 1
static caddr_t
d469 1
a469 1
    munmap((caddr_t)old, oldlen);
@


1.28
log
@malloc_init() should try to not modify errno
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.27 1997/07/02 16:26:27 millert Exp $";
a1236 3
    } else if (ptr && !size) {
	ifree(ptr);
	r = 0;
@


1.27
log
@Use MALLOC_EXTRA_SANITY consistently (EXTRA_SANITY was used in many places)
sizeof *pt -> sizeof *px (point to same type of struct but looked wrong).
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.26 1997/05/31 08:55:06 tholo Exp $";
d481 1
d588 1
a588 1

@


1.26
log
@Make it possible to not output warnings (errors causing aborts are always
output).
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.25 1997/05/31 08:47:56 tholo Exp $";
d15 3
a17 3
 * Defining EXTRA_SANITY will enable extra checks which are related
 * to internal conditions and consistency in malloc.c. This has a
 * noticeable runtime performance hit, and generally will not do you
d281 1
d404 1
a404 1
#ifdef EXTRA_SANITY
d406 1
a406 1
#endif /* EXTRA_SANITY */
d486 1
a486 1
#ifdef EXTRA_SANITY
d488 1
a488 1
#endif /* EXTRA_SANITY */
d608 1
a608 1
#ifdef EXTRA_SANITY
d623 1
a623 1
#endif /* EXTRA_SANITY */
d643 1
a643 1
#ifdef EXTRA_SANITY
d646 1
a646 1
#endif /* EXTRA_SANITY */
d981 1
a981 1
	px = imalloc(sizeof *pt);	/* This cannot fail... */
d1122 1
a1122 1
#ifdef EXTRA_SANITY
d1125 1
a1125 1
#endif /* EXTRA_SANITY */
@


1.25
log
@Add x/X option to behave like X11 xmalloc; from FreeBSD
Reduce diffs wrt. FreeBSD some
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.24 1997/04/30 05:52:50 tholo Exp $";
d222 3
d367 2
d522 2
@


1.24
log
@Be more careful with mixing types
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.23 1997/04/05 05:05:44 tholo Exp $";
a39 4
/*
 * No user serviceable parts behind this point.
 */

d44 1
d64 20
d93 12
d106 2
d178 12
d230 3
d266 5
a344 1
    suicide = 1;
d349 1
d447 1
a447 2
    new = (struct pginfo**) mmap(0, i * malloc_pagesize, PROT_READ|PROT_WRITE,
				 MAP_ANON|MAP_PRIVATE, -1, (off_t)0);
d476 4
d521 2
d550 2
a551 2
    page_dir = (struct pginfo **) mmap(0, malloc_pagesize, PROT_READ|PROT_WRITE,
				       MAP_ANON|MAP_PRIVATE, -1, (off_t)0);
a803 3
#ifdef  _THREAD_SAFE
    int     status;
#endif
a838 3
#ifdef  _THREAD_SAFE
    int     status;
#endif
d841 1
a841 1
	return 0;
a1134 3
#ifdef  _THREAD_SAFE
    int     status;
#endif
a1173 11
#ifdef _THREAD_SAFE
#include <pthread.h>
#include "pthread_private.h"
static int malloc_lock;
#define THREAD_LOCK() _thread_kern_sig_block(&malloc_lock);
#define THREAD_UNLOCK() _thread_kern_sig_unblock(malloc_lock);
#else
#define THREAD_LOCK() 
#define THREAD_UNLOCK()
#endif

d1192 2
d1237 2
@


1.23
log
@Check for overflow; from FreeBSD
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.22 1997/02/11 17:46:36 niklas Exp $";
d163 1
a163 1
static unsigned	malloc_ninfo;
d278 1
a278 1
    fprintf(fd, "Pagesize\t%d\n", malloc_pagesize);
d370 1
a370 2
    int i;
    size_t oldlen;
d624 1
a624 1
    pp = malloc_pages(malloc_pagesize);
@


1.22
log
@is we were set[ug]id an unitialized ptr bit us
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.21 1997/02/09 22:55:38 tholo Exp $";
d759 3
a761 1
    if (size <= malloc_maxsize)
@


1.21
log
@Make this 64-bit safe again
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.20 1997/01/05 22:12:48 tholo Exp $";
d438 5
a442 2
	} else if (i == 1 && issetugid() == 0) {
	    p = getenv("MALLOC_OPTIONS");
@


1.20
log
@Integrate latest malloc(3) from FreeBSD
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.19 1996/11/24 00:41:30 niklas Exp $";
d87 1
a87 1
    u_int		bits[1]; /* Which chunks are free */
d103 1
a103 1
 * How many bits per u_int in the bitmap.
d106 1
a106 1
#define	MALLOC_BITS	(8*sizeof(u_int))
d130 1
a130 1
#define malloc_pagesize			(1U<<malloc_pageshift)
d133 2
a134 2
#if ((1<<malloc_pageshift) != malloc_pagesize)
#error	"(1<<malloc_pageshift) != malloc_pagesize"
d440 1
a440 1
	} else {
d653 1
a653 1
	bp->bits[i / MALLOC_BITS] = (u_long)~0;
d656 1
a656 1
        bp->bits[i/MALLOC_BITS] |= 1<<(i%MALLOC_BITS);
d661 1
a661 1
	    bp->bits[i/MALLOC_BITS] &= ~(1<<(i%MALLOC_BITS));
d688 1
a688 1
    u_int u;
d691 1
a691 1
    u_int *lp;
d838 1
a838 1
        if ((*mp)->bits[i/MALLOC_BITS] & (1<<(i%MALLOC_BITS))) {
d1028 1
a1028 1
    if (info->bits[i/MALLOC_BITS] & (1<<(i%MALLOC_BITS))) {
d1036 1
a1036 1
    info->bits[i/MALLOC_BITS] |= 1<<(i%MALLOC_BITS);
@


1.19
log
@more 64bit fixes
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.18 1996/11/23 19:10:26 niklas Exp $";
d21 3
a23 1
#undef EXTRA_SANITY
d30 1
a31 3

#if defined(EXTRA_SANITY) && !defined(MALLOC_STATS)
# define MALLOC_STATS	/* required for EXTRA_SANITY */
d54 12
a65 2
 * If these weren't defined here, they would be calculated on the fly,
 * at a considerable cost in performance.
a70 1
#	define	malloc_pagesize		(NBPG)
a71 1
#	define	malloc_maxsize		(malloc_pagesize >> 1)
d87 1
a87 1
    u_long		bits[1]; /* Which chunks are free */
d103 1
a103 1
 * How many bits per u_long in the bitmap.
d106 1
a106 1
#define	MALLOC_BITS	(8*sizeof(u_long))
d117 3
a119 15
/*
 * The i386 architecture has some very convenient instructions.
 * We might as well use them.  There are C-language backups, but
 * they are considerably slower.
 */
#if defined(__i386__) && defined(__GNUC__)
#define ffs _ffs
static __inline int
_ffs(input)
	unsigned input;
{
	int result;
	__asm("bsfl %1, %0" : "=r" (result) : "r" (input));
	return result+1;
}
d121 3
a123 9
#define fls _fls
static __inline int
_fls(input)
	unsigned input;
{
	int result;
	__asm("bsrl %1, %0" : "=r" (result) : "r" (input));
	return result+1;
}
d125 3
a127 9
#define set_bit _set_bit
static __inline void
_set_bit(pi, bit)
	struct pginfo *pi;
	int bit;
{
	__asm("btsl %0, (%1)" :
	: "r" (bit & (MALLOC_BITS-1)), "r" (pi->bits+(bit/MALLOC_BITS)));
}
d129 3
a131 9
#define clr_bit _clr_bit
static __inline void
_clr_bit(pi, bit)
	struct pginfo *pi;
	int bit;
{
	__asm("btcl %0, (%1)" :
	: "r" (bit & (MALLOC_BITS-1)), "r" (pi->bits+(bit/MALLOC_BITS)));
}
d133 3
a135 1
#endif /* __i386__ && __GNUC__ */
d137 3
a139 13
/*
 * Set to one when malloc_init has been called
 */
static	unsigned	initialized;

/*
 * The size of a page.
 * Must be a integral multiplum of the granularity of mmap(2).
 * Your toes will curl if it isn't a power of two
 */
#ifndef malloc_pagesize
static	unsigned        malloc_pagesize;
#endif /* malloc_pagesize */
d147 2
a148 12
/* malloc_pagesize == 1 << malloc_pageshift */
#ifndef malloc_pageshift
static	unsigned	malloc_pageshift;
#endif /* malloc_pageshift */

/*
 * The smallest allocation we bother about.
 * Must be power of two
 */
#ifndef malloc_minsize
static	unsigned  malloc_minsize;
#endif /* malloc_minsize */
d150 2
a151 11
/*
 * The largest chunk we care about.
 * Must be smaller than pagesize
 * Must be power of two
 */
#ifndef malloc_maxsize
static	unsigned  malloc_maxsize;
#endif /* malloc_maxsize */

/* The minimum size (in pages) of the free page cache.  */
static	unsigned  malloc_cache = 16;
d154 1
a154 1
static	u_long  malloc_origo;
d157 1
a157 1
static	u_long  last_index;
d160 1
a160 1
static	struct	pginfo **page_dir;
d163 1
a163 1
static	unsigned	malloc_ninfo;
d166 1
a166 1
static struct pgfree	free_list;
d177 1
a177 1
#endif /* MALLOC_STATS */
d199 2
d217 3
d287 1
a287 1
static char *malloc_func;
d293 1
a293 1
    char *q = "Malloc error: ";
d295 2
a297 1
    write(2, malloc_func, strlen(malloc_func));
d310 1
a310 1
    char *q = "Malloc warning: ";
d313 2
a315 1
    write(2, malloc_func, strlen(malloc_func));
d319 1
a319 1
#ifdef EXTRA_SANITY
d331 1
a331 1
#endif /* EXTRA_SANITY */
a362 77
 * Set a bit in the bitmap
 */
#ifndef set_bit
static __inline void
set_bit(pi, bit)
    struct pginfo *pi;
    int bit;
{
    pi->bits[bit/MALLOC_BITS] |= 1UL<<(bit%MALLOC_BITS);
}
#endif /* set_bit */

/*
 * Clear a bit in the bitmap
 */
#ifndef clr_bit
static __inline void
clr_bit(pi, bit)
    struct pginfo *pi;
    int bit;
{
    pi->bits[bit/MALLOC_BITS] &= ~(1UL<<(bit%MALLOC_BITS));
}
#endif /* clr_bit */

#ifndef tst_bit
/*
 * Test a bit in the bitmap
 */
static __inline int
tst_bit(pi, bit)
    struct pginfo *pi;
    int bit;
{
    return pi->bits[bit/MALLOC_BITS] & (1UL<<(bit%MALLOC_BITS));
}
#endif /* tst_bit */

/*
 * Find last bit
 */
#ifndef fls
static __inline int
fls(size)
    size_t size;
{
    int i = 1;
    while (size >>= 1)
	i++;
    return i;
}
#endif /* fls */

#if LONG_BIT == WORD_BIT
#define ffs_ul ffs
#else
static __inline int
ffs_ul(u_long ul)
{
    u_int n;
    int i;
    int k;

    for (i = 0; i < sizeof (u_long) / sizeof (u_int); i++) {
	n = ul & UINT_MAX;
	k = ffs (n);
	if (k)
	    break;
	ul >>= (sizeof (u_int) * 8);
    }
    if (k)
	k += i * sizeof (u_int) * 8;
    return k;
}
#endif

/*
d440 1
a440 1
	} else if (i == 2) {
a442 2
	else
	    p = NULL;
d486 1
a486 1
#ifdef EXTRA_SANITY
d489 1
a489 44
#endif /* EXTRA_SANITY */

#ifndef malloc_pagesize
    /* determine our pagesize */
    malloc_pagesize = getpagesize();
#endif /* malloc_pagesize */

#ifndef malloc_maxsize
    malloc_maxsize = malloc_pagesize >> 1;
#endif /* malloc_maxsize */

#ifndef malloc_pageshift
    {
    int i;
    /* determine how much we shift by to get there */
    for (i = malloc_pagesize; i > 1; i >>= 1)
	malloc_pageshift++;
    }
#endif /* malloc_pageshift */

#ifndef malloc_minsize
    {
    int i;
    /*
     * find the smallest size allocation we will bother about.
     * this is determined as the smallest allocation that can hold
     * it's own pginfo;
     */
    i = 2;
    for(;;) {
	int j;

	/* Figure out the size of the bits */
	j = malloc_pagesize/i;
	j /= 8;
	if (j < sizeof(u_long))
		j = sizeof (u_long);
	if (sizeof(struct pginfo) + j - sizeof (u_long) <= i)
		break;
	i += i;
    }
    malloc_minsize = i;
    }
#endif /* malloc_minsize */
d507 8
a514 1
    initialized++;
a521 4
    if (!malloc_cache)
	malloc_cache++;

    malloc_cache <<= malloc_pageshift;
d527 1
a527 1
void *
d613 1
a613 1
static __inline int
d636 2
a637 1
	if (!bp)
d639 1
d647 1
a647 6
    page_dir[ptr2index(pp)] = bp;

    bp->next = page_dir[bits];
    page_dir[bits] = bp;

    /* set all valid bits in the bits */
d656 1
a656 1
	set_bit(bp, i);
d661 1
a661 1
	    clr_bit(bp, i);
d668 9
d687 2
a688 1
    int j;
d691 1
a691 1
    u_long *lp;
d698 4
a701 1
    j = fls((size)-1);
d714 7
a720 2
    k = ffs_ul(*lp) - 1;
    *lp ^= 1UL<<k;
d735 1
a735 1
    return (void *)((char *)bp->page + k);
d750 1
a750 1
    if (!initialized)
d764 1
a764 1
    if (malloc_zero)
d789 1
a789 1
    if (!initialized) {
d838 1
a838 1
	if (tst_bit(*mp, i)) {
d874 1
a874 1
static __inline void
d907 3
d1010 1
a1010 1
static __inline void
d1028 1
a1028 1
    if (tst_bit(info, i)) {
d1033 4
a1036 1
    set_bit(info, i);
d1089 1
a1089 1
    if (!initialized) {
d1128 1
a1128 1
#define THREAD_UNLOCK() _thread_kern_sig_unblock(&malloc_lock);
d1141 1
a1141 1
    malloc_func = "malloc():";
d1158 1
a1158 1
    malloc_func = "free():";
d1177 1
a1177 1
    malloc_func = "realloc():";
@


1.18
log
@64 bit clean
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.17 1996/11/22 16:15:23 kstailey Exp $";
d399 1
a399 1
	return 0;;
d413 1
a413 1
    pi->bits[bit/MALLOC_BITS] |= 1<<(bit%MALLOC_BITS);
d426 1
a426 1
    pi->bits[bit/MALLOC_BITS] &= ~(1<<(bit%MALLOC_BITS));
d439 1
a439 1
    return pi->bits[bit/MALLOC_BITS] & (1<<(bit%MALLOC_BITS));
d449 1
a449 1
    int size;
d458 1
a458 1
#if LONG_BITS == WORD_BITS
d477 1
d489 2
a490 1
    int i, oldlen;
d793 1
a793 1
    if ((1<<(bits)) <= l+l) {
d801 1
a801 1
    bp->size = (1<<bits);
@


1.17
log
@removed plus sign from start of line
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.16 1996/09/26 15:22:19 tholo Exp $";
d458 22
d863 2
a864 2
    k = ffs((unsigned)*lp) - 1;
    *lp ^= 1<<k;
@


1.16
log
@Make sure we don't dereference stray pointer when running suid or sgid
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.15 1996/09/26 04:49:56 tholo Exp $";
d314 1
a314 1
+ 		fprintf(fd, "Free_list loops.\n");
@


1.15
log
@Restore check for suid / sgid
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.14 1996/09/26 04:19:42 tholo Exp $";
d538 2
@


1.14
log
@Latest changes from FreeBSD
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.13 1996/09/19 20:38:48 tholo Exp $";
d533 1
a533 1
	} else if (i == 1) {
@


1.13
log
@From FreeBSD:
> Fix a very rare error condition:  The code to free VM back to the kernel
> as done after a quasi-recursive call to free() had modified what we
> thought we knew about the last chunk of pages.
> This bug manifested itself when I did a "make obj" from src/usr.sbin/lpr,
> then make would coredump in the lpd directory.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.12 1996/09/16 05:43:40 tholo Exp $";
d15 5
a19 2
 * Defining EXTRA_SANITY will enable some checks which are related
 * to internal conditions and consistency in malloc.c
d35 2
a36 1
 * What to use for Junk
d40 4
d46 1
a47 1
#include <memory.h>
a48 1
#include <err.h>
a51 4
#ifdef _THREAD_SAFE
#include <pthread.h>
#include "pthread_private.h"
#endif
d114 1
a114 1
#ifdef __i386__
d121 1
a121 1
	__asm("bsfl %1,%0" : "=r" (result) : "r" (input));
d131 1
a131 1
	__asm("bsrl %1,%0" : "=r" (result) : "r" (input));
d141 1
a141 1
	__asm("btsl %0,(%1)" :
d151 1
a151 1
	__asm("btcl %0,(%1)" :
d155 1
a155 1
#endif /* __i386__ */
d171 1
a171 3
/*
 * A mask for the offset inside a page.
 */
d177 1
a177 3
/*
 * malloc_pagesize == 1 << malloc_pageshift
 */
d199 2
a200 6
/*
 * The minimum size (in bytes) of the free page cache.
 */
#ifndef malloc_cache
static	unsigned  malloc_cache;
#endif /* malloc_cache */
d202 1
a202 3
/*
 * The offset from pagenumber to index into the page directory
 */
d205 1
a205 3
/*
 * The last index in the page directory we care about
 */
d208 1
a208 4
/*
 * Pointer to page directory.
 * Allocated "as if with" malloc
 */
d211 1
a211 3
/*
 * How many slots in the page directory
 */
d214 1
a214 3
/*
 * Free pages line up here
 */
d217 1
a217 3
/*
 * Abort() if we fail to get VM ?
 */
d220 1
a220 3
/*
 * Are we trying to die ?
 */
d224 1
a224 3
/*
 * dump statistics
 */
d228 1
a228 3
/*
 * always realloc ?
 */
d231 6
a236 3
/*
 * zero fill ?
 */
d239 1
a239 3
/*
 * junk fill ?
 */
d242 14
a255 3
/*
 * my last break.
 */
d258 1
a258 3
/*
 * one location cache for free-list holders
 */
d261 3
d268 3
d285 1
a285 1
	fprintf(fd,"%08lx %5d ",(j+malloc_origo) << malloc_pageshift,j);
d290 1
a290 1
	    fprintf(fd,".. %5d not mine\n",	j);
d295 1
a295 1
	    fprintf(fd,".. %5d free\n", j);
d300 1
a300 1
	    fprintf(fd,".. %5d in use\n", j);
d302 1
a302 1
	    fprintf(fd,"(%p)\n", pd[j]);
d304 2
a305 2
	    fprintf(fd,"%p %d (of %d) x %d @@ %p --> %p\n",
		pd[j],pd[j]->free, pd[j]->total,
d311 2
a312 2
	fprintf(fd,"Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		pf,pf->page,pf->end,pf->size,pf->prev,pf->next);
d314 1
a314 1
		fprintf(fd,"Free_list loops.\n");
d320 6
a325 6
    fprintf(fd,"Minsize\t%d\n",malloc_minsize);
    fprintf(fd,"Maxsize\t%d\n",malloc_maxsize);
    fprintf(fd,"Pagesize\t%d\n",malloc_pagesize);
    fprintf(fd,"Pageshift\t%d\n",malloc_pageshift);
    fprintf(fd,"FirstPage\t%ld\n",malloc_origo);
    fprintf(fd,"LastPage\t%ld %lx\n",last_index+malloc_pageshift,
d327 1
a327 1
    fprintf(fd,"Break\t%ld\n",(u_long)sbrk(0) >> malloc_pageshift);
d331 2
d339 3
a341 2
    write(2,q,strlen(q));
    write(2,p,strlen(p));
d356 3
a358 2
    write(2,q,strlen(q));
    write(2,p,strlen(p));
d365 1
a365 1
    FILE *fd = fopen("malloc.out","a");
d371 1
a371 1
	write(2,q,strlen(q));
d383 1
a383 1
    caddr_t result,tail;
d390 1
a390 1
	wrterror("(internal): map_pages fails\n");
d465 1
a465 1
    struct  pginfo **new,**old;
d519 2
a520 1
    char *p;
d526 13
a538 2
    if (issetugid() == 0) {
        for (p=getenv("MALLOC_OPTIONS"); p && *p; p++) {
d540 4
a543 2
	    case 'a': malloc_abort   = 0; break;
	    case 'A': malloc_abort   = 1; break;
d545 2
a546 2
	    case 'd': malloc_stats   = 0; break;
	    case 'D': malloc_stats   = 1; break;
d548 20
a567 10
	    case 'r': malloc_realloc = 0; break;
	    case 'R': malloc_realloc = 1; break;
	    case 'j': malloc_junk    = 0; break;
	    case 'J': malloc_junk    = 1; break;
	    case 'z': malloc_zero    = 0; break;
	    case 'Z': malloc_zero    = 1; break;
	    default:
		wrtwarning("(Init): Unknown char in MALLOC_OPTIONS\n");
		p = 0;
		break;
d572 2
a603 4
#ifndef malloc_cache
    malloc_cache = 100 << malloc_pageshift;
#endif /* malloc_cache */

d633 1
a633 1
	wrterror("(Init) my first mmap failed.  (check limits ?)\n");
d651 6
a656 1
    px = (struct pgfree *) malloc (sizeof *px);
d666 1
a666 1
    void *p,*delay_free = 0;
d731 1
a731 1
	    memset(p, SOME_JUNK,size << malloc_pageshift);
d738 1
a738 1
	    free(delay_free);
d754 1
a754 1
    int i,k,l;
d770 1
a770 1
	bp = (struct  pginfo *)malloc(l);
d794 1
a794 1
	set_bit(bp,i);
d799 1
a799 1
	    clr_bit(bp,i);
d861 2
a862 2
void *
malloc(size)
a875 3
#ifdef  _THREAD_SAFE
    _thread_kern_sig_block(&status);
#endif
d882 1
a882 1
	wrterror("malloc(): returns NULL\n");
d885 1
a885 1
	memset(result,0,size);
a886 3
#ifdef  _THREAD_SAFE
    _thread_kern_sig_unblock(status);
#endif
d893 2
a894 2
void *
realloc(ptr, size)
d899 1
a899 1
    u_long osize,index;
a908 3
    if (!ptr)				/* Bounce to malloc() */
	return malloc(size);

d910 1
a910 6
	wrtwarning("realloc(): malloc() never got called.\n");
	return 0;
    }

    if (ptr && !size) {			/* Bounce to free() */
	free(ptr);
a913 3
#ifdef  _THREAD_SAFE
    _thread_kern_sig_block(&status);
#endif
d917 1
a917 4
	wrtwarning("realloc(): junk pointer (too low)\n");
#ifdef  _THREAD_SAFE
	_thread_kern_sig_unblock(status);
#endif
d922 1
a922 4
	wrtwarning("realloc(): junk pointer (too high)\n");
#ifdef  _THREAD_SAFE
	_thread_kern_sig_unblock(status);
#endif
d932 1
a932 4
	    wrtwarning("realloc(): modified page pointer.\n");
#ifdef  _THREAD_SAFE
	    _thread_kern_sig_unblock(status);
#endif
a942 3
#ifdef  _THREAD_SAFE
	    _thread_kern_sig_unblock(status);
#endif
d950 1
a950 4
	    wrtwarning("realloc(): modified chunk pointer.\n");
#ifdef  _THREAD_SAFE
	    _thread_kern_sig_unblock(status);
#endif
d958 2
a959 5
	if (tst_bit(*mp,i)) {
	    wrtwarning("realloc(): already free chunk.\n");
#ifdef  _THREAD_SAFE
	    _thread_kern_sig_unblock(status);
#endif
a968 3
#ifdef  _THREAD_SAFE
	    _thread_kern_sig_unblock(status);
#endif
d973 1
a973 4
	wrtwarning("realloc(): wrong page pointer.\n");
#ifdef  _THREAD_SAFE
	_thread_kern_sig_unblock(status);
#endif
d977 1
a977 1
    p = malloc(size);
d982 1
a982 1
	    memcpy(p,ptr,osize);
d984 2
a985 2
	    memcpy(p,ptr,size);
	free(ptr);
a986 3
#ifdef  _THREAD_SAFE
    _thread_kern_sig_unblock(status);
#endif
d1001 1
a1001 1
    struct pgfree *pf,*pt=0;
d1006 1
a1006 1
	wrtwarning("free(): already free page.\n");
d1011 1
a1011 1
	wrtwarning("free(): freeing wrong page.\n");
d1016 1
a1016 1
	wrtwarning("free(): modified page pointer.\n");
d1027 6
a1032 1
    tail = (char *)ptr + l;
d1036 1
a1036 1
	px = malloc(sizeof *pt);	/* This cannot fail... */
d1052 1
a1052 1
	tail = (char *)ptr + l;
d1090 1
a1090 1
	    wrterror("messed up free list");
d1119 1
a1119 1
	free(pt);
d1141 1
a1141 1
	wrtwarning("free(): modified pointer.\n");
d1145 2
a1146 2
    if (tst_bit(info,i)) {
	wrtwarning("free(): already free chunk.\n");
d1150 1
a1150 1
    set_bit(info,i);
d1185 2
a1186 2
	free(info);
    free(vp);
d1189 2
a1190 2
void
free(ptr)
d1204 1
a1204 1
	wrtwarning("free(): malloc() never got called.\n");
a1211 3
#ifdef  _THREAD_SAFE
    _thread_kern_sig_block(&status);
#endif
d1215 1
a1215 4
	wrtwarning("free(): junk pointer (too low)\n");
#ifdef  _THREAD_SAFE
	_thread_kern_sig_unblock(status);
#endif
d1220 1
a1220 4
	wrtwarning("free(): junk pointer (too high)\n");
#ifdef  _THREAD_SAFE
	_thread_kern_sig_unblock(status);
#endif
d1227 1
a1227 1
        free_pages(ptr,index,info);
d1229 17
a1245 3
	free_bytes(ptr,index,info);
#ifdef  _THREAD_SAFE
    _thread_kern_sig_unblock(status);
d1247 36
d1284 26
@


1.12
log
@Avoid pulling in stdio
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.11 1996/09/15 09:31:49 tholo Exp $";
d1022 1
a1022 1
    struct pgfree *pf,*pt;
a1092 1
		free(pt);
d1134 2
@


1.11
log
@Remove dead code
Remove unused variables
Silence some warnings
lint(1) is your friend
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.10 1996/09/11 03:04:43 deraadt Exp $";
d23 1
a23 1
 * It has no run-time performance hit.
d25 1
a25 1
#define MALLOC_STATS
@


1.10
log
@only support MALLOC_OPTIONS for non-setuid
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.9 1996/09/06 16:14:36 tholo Exp $";
d816 1
a816 1
    k = ffs(*lp) - 1;
@


1.9
log
@asm -> __asm, clean lint(1) warnings
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.8 1996/08/21 03:47:22 tholo Exp $";
d529 3
a531 2
    for (p=getenv("MALLOC_OPTIONS"); p && *p; p++) {
	switch (*p) {
d548 1
@


1.8
log
@Move cfree(3) weak symbol into a seperate file
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.7 1996/08/20 17:56:52 tholo Exp $";
d118 1
a118 1
	asm("bsfl %1,%0" : "=r" (result) : "r" (input));
d128 1
a128 1
	asm("bsrl %1,%0" : "=r" (result) : "r" (input));
d138 1
a138 1
	asm("btsl %0,(%1)" :
d148 1
a148 1
	asm("btcl %0,(%1)" :
d497 1
a497 1
				 MAP_ANON|MAP_PRIVATE, -1, 0);
d611 1
a611 1
				       MAP_ANON|MAP_PRIVATE, -1, 0);
d682 1
a682 1
	pf->page += size;
d766 1
a766 1
	bp->bits[i / MALLOC_BITS] = ~0;
d828 1
a828 1
	memset(bp->page + k, SOME_JUNK, bp->size);
d830 1
a830 1
    return bp->page + k;
d1046 1
a1046 1
    tail = ptr+l;
d1066 1
a1066 1
	tail = ptr+l;
d1081 1
a1081 1
	    pf->end += l;
d1119 1
a1119 1
	pf->end = pf->page + malloc_cache;
d1139 1
@


1.7
log
@Make the binding cfree() -> free() weak if possible
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.6 1996/08/20 17:30:49 downsj Exp $";
a26 5
/*
 * Defining CFREE_STUB will include a cfree() stub that just calls free().
 */
#define CFREE_STUB

a1255 16

#ifdef CFREE_STUB

#ifdef __indr_reference
__indr_reference(free, cfree);
#else /* __indr_reference */

void
cfree(p)
	void *p;
{
    free(p);
}
#endif /* not __indr_reference */

#endif /* CFREE_STUB */
@


1.6
log
@Remove ANSI function delcarations and add a cfree() stub function.
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.5 1996/08/19 08:33:37 tholo Exp $";
d1263 5
d1269 2
a1270 2
cfree(ptr)
    void *ptr;
d1272 1
a1272 1
    free(ptr);
d1274 3
a1276 1
#endif
@


1.5
log
@Fix RCS ids
Make sure everything uses {SYS,}LIBC_SCCS properly
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD$";
d27 5
d119 2
a120 1
_ffs(unsigned input)
d129 2
a130 1
_fls(unsigned input)
d139 3
a141 1
_set_bit(struct pginfo *pi, int bit)
d149 3
a151 1
_clr_bit(struct pginfo *pi, int bit)
d287 2
a288 1
malloc_dump(FILE *fd)
d345 2
a346 1
wrterror(char *p)
d360 2
a361 1
wrtwarning(char *p)
d389 2
a390 1
map_pages(int pages)
d418 3
a420 1
set_bit(struct pginfo *pi, int bit)
d431 3
a433 1
clr_bit(struct pginfo *pi, int bit)
d444 3
a446 1
tst_bit(struct pginfo *pi, int bit)
d457 2
a458 1
fls(int size)
d471 2
a472 1
extend_pgdir(u_long index)
d643 2
a644 1
malloc_pages(size_t size)
d729 2
a730 1
malloc_make_chunks(int bits)
d793 2
a794 1
malloc_bytes(size_t size)
d842 2
a843 1
malloc(size_t size)
d880 3
a882 1
realloc(void *ptr, size_t size)
d1019 4
a1022 1
free_pages(void *ptr, int index, struct pginfo *info)
d1145 4
a1148 1
free_bytes(void *ptr, int index, struct pginfo *info)
d1207 2
a1208 1
free(void *ptr)
d1261 9
@


1.4
log
@malloc(3) implementation from FreeBSD; uses mmap(2) to get memory
@
text
@a7 3
 *
 * $Id: malloc.c,v 1.10 1996/01/22 00:01:44 julian Exp $
 *
d9 4
@


1.3
log
@Add prototypes for internal functions
Change inline to __inline
@
text
@a0 2
/*	$NetBSD: malloc.c,v 1.6 1996/01/17 02:45:25 jtc Exp $	*/

d2 6
a7 2
 * Copyright (c) 1983 Regents of the University of California.
 * All rights reserved.
d9 1
a9 15
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
d11 17
a27 18
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#if defined(LIBC_SCCS) && !defined(lint)
#if 0
static char *sccsid = "from: @@(#)malloc.c	5.11 (Berkeley) 2/23/91";
#else
static char *rcsid = "$NetBSD: malloc.c,v 1.6 1996/01/17 02:45:25 jtc Exp $";
a28 1
#endif /* LIBC_SCCS and not lint */
d31 1
a31 8
 * malloc.c (Caltech) 2/21/82
 * Chris Kingsley, kingsley@@cit-20.
 *
 * This is a very fast storage allocator.  It allocates blocks of a small 
 * number of different sizes, and keeps free lists of each size.  Blocks that
 * don't exactly fit are passed up to the next larger size.  In this 
 * implementation, the available sizes are 2^n-4 (or 2^n-10) bytes long.
 * This is designed for use in a virtual memory environment.
d33 1
d35 1
a35 1
#include <sys/types.h>
a36 1
#include <string.h>
d38 10
d49 14
a62 1
#define	NULL 0
d65 11
a75 23
 * The overhead on a block is at least 4 bytes.  When free, this space
 * contains a pointer to the next free block, and the bottom two bits must
 * be zero.  When in use, the first byte is set to MAGIC, and the second
 * byte is the size index.  The remaining bytes are for alignment.
 * If range checking is enabled then a second word holds the size of the
 * requested block, less 1, rounded up to a multiple of sizeof(RMAGIC).
 * The order of elements is critical: ov_magic must overlay the low order
 * bits of ov_next, and ov_magic can not be a valid ov_next bit pattern.
 */
union	overhead {
	union	overhead *ov_next;	/* when free */
	struct {
		u_char	ovu_magic;	/* magic number */
		u_char	ovu_index;	/* bucket # */
#ifdef RCHECK
		u_short	ovu_rmagic;	/* range magic number */
		u_long	ovu_size;	/* actual block size */
#endif
	} ovu;
#define	ov_magic	ovu.ovu_magic
#define	ov_index	ovu.ovu_index
#define	ov_rmagic	ovu.ovu_rmagic
#define	ov_size		ovu.ovu_size
d78 3
a80 2
static void morecore __P((int));
static int findbucket __P((union overhead *, int));
d82 7
a88 2
#define	MAGIC		0xef		/* magic # on accounting info */
#define RMAGIC		0x5555		/* magic # on range info */
d90 5
a94 5
#ifdef RCHECK
#define	RSLOP		sizeof (u_short)
#else
#define	RSLOP		0
#endif
d97 1
a97 3
 * nextf[i] is the pointer to the next free block of size 2^(i+3).  The
 * smallest allocatable block is 8 bytes.  The overhead information
 * precedes the data area returned to the user.
d99 5
a103 6
#define	NBUCKETS 30
static	union overhead *nextf[NBUCKETS];
extern	char *sbrk();

static	int pagesz;			/* page size */
static	int pagebucket;			/* page size bucket */
a104 1
#ifdef MSTATS
d106 3
a108 2
 * nmalloc[i] is the difference between the number of mallocs and frees
 * for a given block size.
d110 18
a127 3
static	u_int nmalloc[NBUCKETS];
#include <stdio.h>
#endif
d129 3
a131 6
#if defined(DEBUG) || defined(RCHECK)
#define	ASSERT(p)   if (!(p)) botch("p")
#include <stdio.h>
static
botch(s)
	char *s;
d133 2
a134 3
	fprintf(stderr, "\r\nassertion botched: %s\r\n", s);
 	(void) fflush(stderr);		/* just in case user buffered it */
	abort();
a135 3
#else
#define	ASSERT(p)
#endif
d137 3
a139 3
void *
malloc(nbytes)
	size_t nbytes;
d141 74
a214 3
  	register union overhead *op;
  	register long bucket, n;
	register unsigned amt;
d216 87
a302 36
	/*
	 * First time malloc is called, setup page size and
	 * align break pointer so all data will be page aligned.
	 */
	if (pagesz == 0) {
		pagesz = n = getpagesize();
		op = (union overhead *)sbrk(0);
  		n = n - sizeof (*op) - ((long)op & (n - 1));
		if (n < 0)
			n += pagesz;
  		if (n) {
  			if (sbrk(n) == (char *)-1)
				return (NULL);
		}
		bucket = 0;
		amt = 8;
		while (pagesz > amt) {
			amt <<= 1;
			bucket++;
		}
		pagebucket = bucket;
	}
	/*
	 * Convert amount of memory requested into closest block size
	 * stored in hash buckets which satisfies request.
	 * Account for space used per block for accounting.
	 */
	if (nbytes <= (n = pagesz - sizeof (*op) - RSLOP)) {
#ifndef RCHECK
		amt = 8;	/* size of first bucket */
		bucket = 0;
#else
		amt = 16;	/* size of first bucket */
		bucket = 1;
#endif
		n = -((long)sizeof (*op) + RSLOP);
d304 3
a306 2
		amt = pagesz;
		bucket = pagebucket;
d308 8
a315 5
	while (nbytes > amt + n) {
		amt <<= 1;
		if (amt == 0)
			return (NULL);
		bucket++;
d317 211
a527 8
	/*
	 * If nothing in hash bucket right now,
	 * request more memory from the system.
	 */
  	if ((op = nextf[bucket]) == NULL) {
  		morecore(bucket);
  		if ((op = nextf[bucket]) == NULL)
  			return (NULL);
d529 84
a612 17
	/* remove from linked list */
  	nextf[bucket] = op->ov_next;
	op->ov_magic = MAGIC;
	op->ov_index = bucket;
#ifdef MSTATS
  	nmalloc[bucket]++;
#endif
#ifdef RCHECK
	/*
	 * Record allocated size of block and
	 * bound space with magic numbers.
	 */
	op->ov_size = (nbytes + RSLOP - 1) & ~(RSLOP - 1);
	op->ov_rmagic = RMAGIC;
  	*(u_short *)((caddr_t)(op + 1) + op->ov_size) = RMAGIC;
#endif
  	return ((char *)(op + 1));
d616 1
a616 1
 * Allocate more memory to the indicated bucket.
d618 2
a619 3
static void
morecore(bucket)
	int bucket;
d621 74
a694 4
  	register union overhead *op;
	register long sz;		/* size of desired block */
  	long amt;			/* amount to allocate */
  	int nblks;			/* how many blocks we get */
d696 61
a756 17
	/*
	 * sbrk_size <= 0 only for big, FLUFFY, requests (about
	 * 2^30 bytes on a VAX, I think) or for a negative arg.
	 */
	sz = 1 << (bucket + 3);
#ifdef DEBUG
	ASSERT(sz > 0);
#else
	if (sz <= 0)
		return;
#endif
	if (sz < pagesz) {
		amt = pagesz;
  		nblks = amt / sz;
	} else {
		amt = sz + pagesz;
		nblks = 1;
d758 51
a808 13
	op = (union overhead *)sbrk(amt);
	/* no more room! */
  	if ((long)op == -1)
  		return;
	/*
	 * Add new memory allocated to that on
	 * free list for this hash bucket.
	 */
  	nextf[bucket] = op;
  	while (--nblks > 0) {
		op->ov_next = (union overhead *)((caddr_t)op + sz);
		op = (union overhead *)((caddr_t)op + sz);
  	}
d811 33
a843 26
void
free(cp)
	void *cp;
{   
  	register long size;
	register union overhead *op;

  	if (cp == NULL)
  		return;
	op = (union overhead *)((caddr_t)cp - sizeof (union overhead));
#ifdef DEBUG
  	ASSERT(op->ov_magic == MAGIC);		/* make sure it was in use */
#else
	if (op->ov_magic != MAGIC)
		return;				/* sanity */
#endif
#ifdef RCHECK
  	ASSERT(op->ov_rmagic == RMAGIC);
	ASSERT(*(u_short *)((caddr_t)(op + 1) + op->ov_size) == RMAGIC);
#endif
  	size = op->ov_index;
  	ASSERT(size < NBUCKETS);
	op->ov_next = nextf[size];	/* also clobbers ov_magic */
  	nextf[size] = op;
#ifdef MSTATS
  	nmalloc[size]--;
d845 1
d849 1
a849 9
 * When a program attempts "storage compaction" as mentioned in the
 * old malloc man page, it realloc's an already freed block.  Usually
 * this is the last block it freed; occasionally it might be farther
 * back.  We have to search all the free lists for the block in order
 * to determine its bucket: 1st we make one pass thru the lists
 * checking only the first block in each; if that fails we search
 * ``realloc_srchlen'' blocks in each list for a match (the variable
 * is extern so the caller can modify it).  If that fails we just copy
 * however many bytes was given to realloc() and hope it's not huge.
d851 51
a901 1
int realloc_srchlen = 4;	/* 4 should be plenty, -1 =>'s whole list */
d903 43
a945 16
void *
realloc(cp, nbytes)
	void *cp; 
	size_t nbytes;
{   
  	register u_long onb;
	register long i;
	union overhead *op;
  	char *res;
	int was_alloced = 0;

  	if (cp == NULL)
  		return (malloc(nbytes));
	if (nbytes == 0) {
		free (cp);
		return NULL;
d947 11
a957 22
	op = (union overhead *)((caddr_t)cp - sizeof (union overhead));
	if (op->ov_magic == MAGIC) {
		was_alloced++;
		i = op->ov_index;
	} else {
		/*
		 * Already free, doing "compaction".
		 *
		 * Search for the old block of memory on the
		 * free list.  First, check the most common
		 * case (last element free'd), then (this failing)
		 * the last ``realloc_srchlen'' items free'd.
		 * If all lookups fail, then assume the size of
		 * the memory block being realloc'd is the
		 * largest possible (so that all "nbytes" of new
		 * memory are copied into).  Note that this could cause
		 * a memory fault if the old area was tiny, and the moon
		 * is gibbous.  However, that is very unlikely.
		 */
		if ((i = findbucket(op, 1)) < 0 &&
		    (i = findbucket(op, realloc_srchlen)) < 0)
			i = NBUCKETS;
d959 15
a973 3
	onb = 1 << (i + 3);
	if (onb < pagesz)
		onb -= sizeof (*op) + RSLOP;
d975 7
a981 24
		onb += pagesz - sizeof (*op) - RSLOP;
	/* avoid the copy if same size block */
	if (was_alloced) {
		if (i) {
			i = 1 << (i + 2);
			if (i < pagesz)
				i -= sizeof (*op) + RSLOP;
			else
				i += pagesz - sizeof (*op) - RSLOP;
		}
		if (nbytes <= onb && nbytes > i) {
#ifdef RCHECK
			op->ov_size = (nbytes + RSLOP - 1) & ~(RSLOP - 1);
			*(u_short *)((caddr_t)(op + 1) + op->ov_size) = RMAGIC;
#endif
			return(cp);
		} else
			free(cp);
	}
  	if ((res = malloc(nbytes)) == NULL)
  		return (NULL);
  	if (cp != res)		/* common optimization if "compacting" */
		bcopy(cp, res, (nbytes < onb) ? nbytes : onb);
  	return (res);
d985 93
a1077 19
 * Search ``srchlen'' elements of each free list for a block whose
 * header starts at ``freep''.  If srchlen is -1 search the whole list.
 * Return bucket number, or -1 if not found.
 */
static
findbucket(freep, srchlen)
	union overhead *freep;
	int srchlen;
{
	register union overhead *p;
	register int i, j;

	for (i = 0; i < NBUCKETS; i++) {
		j = 0;
		for (p = nextf[i]; p && j != srchlen; p = p->ov_next) {
			if (p == freep)
				return (i);
			j++;
		}
d1079 26
a1104 1
	return (-1);
a1106 1
#ifdef MSTATS
d1108 60
a1167 28
 * mstats - print out statistics about malloc
 * 
 * Prints two lines of numbers, one showing the length of the free list
 * for each size category, the second showing the number of mallocs -
 * frees for each size category.
 */
mstats(s)
	char *s;
{
  	register int i, j;
  	register union overhead *p;
  	int totfree = 0,
  	totused = 0;

  	fprintf(stderr, "Memory allocation statistics %s\nfree:\t", s);
  	for (i = 0; i < NBUCKETS; i++) {
  		for (j = 0, p = nextf[i]; p; p = p->ov_next, j++)
  			;
  		fprintf(stderr, " %d", j);
  		totfree += j * (1 << (i + 3));
  	}
  	fprintf(stderr, "\nused:\t");
  	for (i = 0; i < NBUCKETS; i++) {
  		fprintf(stderr, " %d", nmalloc[i]);
  		totused += nmalloc[i] * (1 << (i + 3));
  	}
  	fprintf(stderr, "\n\tTotal in use: %d, total free: %d\n",
	    totused, totfree);
d1169 25
d1195 29
@


1.2
log
@realloc(ptr, 0) does not free; from seebs@@taniemarie.solon.com;
netbsd pr#1806
@
text
@a61 3
static void morecore();
static int findbucket();

d87 3
@


1.1
log
@Initial revision
@
text
@d1 2
d37 5
a41 2
/*static char *sccsid = "from: @@(#)malloc.c	5.11 (Berkeley) 2/23/91";*/
static char *rcsid = "$Id: malloc.c,v 1.4 1994/10/19 03:06:34 cgd Exp $";
d318 4
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
