head	1.7;
access;
symbols
	OPENBSD_6_1:1.7.0.6
	OPENBSD_6_1_BASE:1.7
	OPENBSD_6_0:1.7.0.4
	OPENBSD_6_0_BASE:1.7
	OPENBSD_5_9:1.7.0.2
	OPENBSD_5_9_BASE:1.7
	OPENBSD_5_8:1.5.0.4
	OPENBSD_5_8_BASE:1.5
	OPENBSD_5_7:1.4.0.2
	OPENBSD_5_7_BASE:1.4;
locks; strict;
comment	@# @;


1.7
date	2015.11.14.21.53.03;	author guenther;	state Exp;
branches;
next	1.6;
commitid	FaP6OlPRXoQ3iZYo;

1.6
date	2015.08.31.02.53.56;	author guenther;	state Exp;
branches;
next	1.5;
commitid	lTMF8Y3C9fQGd6jQ;

1.5
date	2015.05.29.07.21.09;	author uebayasi;	state Exp;
branches;
next	1.4;
commitid	17hzypZR1YJG5Hjn;

1.4
date	2014.12.09.15.10.39;	author reyk;	state Exp;
branches;
next	1.3;
commitid	OF0WjxvqrbANKgYq;

1.3
date	2014.11.30.19.43.56;	author deraadt;	state Exp;
branches;
next	1.2;
commitid	1sP0uXsb2s1HOugs;

1.2
date	2014.11.20.21.51.02;	author deraadt;	state dead;
branches;
next	1.1;
commitid	WqDifXPHiY3WoZKX;

1.1
date	2014.11.20.14.33.00;	author reyk;	state Exp;
branches;
next	;
commitid	ektKQd6FW4Enn5pg;


desc
@@


1.7
log
@Split the non-syscall ASM bits from SYS.h into DEFS.h and use that in the
non-syscall .S source

ok millert@@ miod@@
@
text
@/*	$OpenBSD: strcmp.S,v 1.6 2015/08/31 02:53:56 guenther Exp $	*/
/*	$NetBSD: strcmp.S,v 1.2 2014/03/22 19:16:34 jakllsch Exp $	*/

/*
 * Written by J.T. Conklin <jtc@@acorntoolworks.com>
 * Public domain.
 */

#include "DEFS.h"

ENTRY(strcmp)
	/*
	 * Align s1 to word boundary.
	 * Consider unrolling loop?
	 */
.Ls1align:
	testb	$7,%dil
	je	.Ls1aligned
	movb	(%rdi),%al
	incq	%rdi
	movb	(%rsi),%dl
	incq	%rsi
	testb	%al,%al
	je	.Ldone
	cmpb	%al,%dl
	je	.Ls1align
	jmp	.Ldone

	/*
	 * Check whether s2 is aligned to a word boundary.  If it is, we
	 * can compare by words.  Otherwise we have to compare by bytes.
	 */
.Ls1aligned:
	testb	$7,%sil
	jne	.Lbyte_loop

	movabsq	$0x0101010101010101,%r8
	subq	$8,%rdi
	movabsq	$0x8080808080808080,%r9
	subq	$8,%rsi

	_ALIGN_TEXT
.Lword_loop:
	movq	8(%rdi),%rax
	addq	$8,%rdi
	movq	8(%rsi),%rdx
	addq	$8,%rsi
	cmpq	%rax,%rdx
	jne	.Lbyte_loop
	subq	%r8,%rdx
	notq	%rax
	andq	%rax,%rdx
	testq	%r9,%rdx
	je	.Lword_loop

	_ALIGN_TEXT
.Lbyte_loop:
	movb	(%rdi),%al
	incq	%rdi
	movb	(%rsi),%dl
	incq	%rsi
	testb	%al,%al
	je	.Ldone
	cmpb	%al,%dl
	je	.Lbyte_loop

.Ldone:
	movzbq	%al,%rax
	movzbq	%dl,%rdx
	subq	%rdx,%rax
	ret
END_STRONG(strcmp)
@


1.6
log
@Add framework for resolving (pun intended) libc namespace issues, using
wrapper .h files and asm labels to let internal calls resolve directly and
not be overridable or use the PLT.  Then, apply that framework to most of
the functions in stdio.h, string.h, err.h, and wchar.h.  Delete the
should-have-been-hidden-all-along _v?(err|warn)[cx]? symbols while here.

tests clean on i386, amd64, sparc64, powerpc, and mips64

naming feedback from kettenis@@ and millert@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: strcmp.S,v 1.5 2015/05/29 07:21:09 uebayasi Exp $	*/
d9 1
a9 1
#include "SYS.h"
@


1.5
log
@Sprinkle END() in some straightforward *.S files that have ENTRY().  The
resulting *.o have "FUNC" symbols with size set.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d9 1
a9 1
#include <machine/asm.h>
d72 1
a72 1
END(strcmp)
@


1.4
log
@Import new amd64 assembly versions of strchr/index, strrchr/rindex,
and strlen that provide a significantly faster performance than our
previous .c or .S implementations.  Based on NetBSD's code.

Tested with different amd64 CPUs.

ok deraadt@@ mikeb@@
@
text
@d72 1
@


1.3
log
@restructure libc/string + libc/arch/*/string coperation regarding
(potentially) MD versions (function dependent, not filename dependent)
split out memcpy/memmove/bcopy and strchr/index/strrchr/rindex
Bring back amd64 .S versions

And the final touch: switch all architectures temporarily to MI
memcpy.c, which contains syslog + abort for overlapping copies.  A nice
harsh undefined behaviour.  We will clean the entire userland of the
remaining issues in this catagory, then switch to the optimised memcpy
which skips the memmove check.

I tried to cut this change into pieces, but testing each sub-step on
every architecture is too time consuming and mindnumbing.
ok miod
@
text
@d1 3
d5 1
a5 1
 * Written by J.T. Conklin <jtc@@netbsd.org>.
a6 1
 * Adapted for NetBSD/x86_64 by Frank van der Linden <fvdl@@wasabisystems.com>
a10 6
/*
 * NOTE: I've unrolled the loop eight times: large enough to make a
 * significant difference, and small enough not to totally trash the
 * cache.
 */

d12 59
a70 68
	jmp	L2			/* Jump into the loop. */

L1:	incq	%rdi
	incq	%rsi
L2:	movb	(%rdi),%cl
	testb	%cl,%cl			/* null terminator */
	jz	L3
	cmpb	%cl,(%rsi)		/* chars match */
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	jne	L3

	incq	%rdi
	incq	%rsi
	movb	(%rdi),%cl
	testb	%cl,%cl
	jz	L3
	cmpb	%cl,(%rsi)
	je	L1
L3:	movzbl	(%rdi),%eax		/* unsigned comparison */
	movzbl	(%rsi),%edx
	subl	%edx,%eax
@


1.2
log
@One of these optimized stubs creates some incredibly subtle damage,
causing as(1) to create a wrong nop-sled for text segment aligns.
Revert, until it is found and fixed.
@
text
@@


1.1
log
@Add amd64 assembler versions of some standard functions to libc.  The
code is already used in the kernel and the files are unmodified copies
from src/sys/lib/libkern/arch/amd64/.  Depending on the function, this
gives us some speedup in userland.

ok deraadt@@, no objections from miod@@
@
text
@@

