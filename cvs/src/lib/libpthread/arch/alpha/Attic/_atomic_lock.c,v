head	1.8;
access;
symbols
	OPENBSD_3_4:1.7.0.4
	OPENBSD_3_4_BASE:1.7
	OPENBSD_3_3:1.7.0.2
	OPENBSD_3_3_BASE:1.7;
locks; strict;
comment	@ * @;


1.8
date	2004.02.06.01.52.00;	author brad;	state dead;
branches;
next	1.7;

1.7
date	2002.10.11.19.08.41;	author marc;	state Exp;
branches;
next	1.6;

1.6
date	2002.06.06.15.43.04;	author art;	state Exp;
branches;
next	1.5;

1.5
date	99.05.26.00.11.27;	author d;	state Exp;
branches;
next	1.4;

1.4
date	98.12.21.13.03.43;	author d;	state Exp;
branches;
next	1.3;

1.3
date	98.12.21.07.36.59;	author d;	state Exp;
branches;
next	1.2;

1.2
date	98.12.18.05.59.17;	author d;	state Exp;
branches;
next	1.1;

1.1
date	98.11.20.11.15.35;	author d;	state Exp;
branches;
next	;


desc
@@


1.8
log
@remove silly stub file for alpha and only try to compile _atomic_lock.c
if it actually exists.

ok marc@@
@
text
@/*	$OpenBSD: _atomic_lock.c,v 1.7 2002/10/11 19:08:41 marc Exp $	*/
/*
 * Atomi lock for alpha.
 */

/* _atomic lock is implemented in assembler. */
@


1.7
log
@remove unused/unreferenced code.
ok fgs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.6 2002/06/06 15:43:04 art Exp $	*/
@


1.6
log
@Implement _atomic_lock in assembler to make it more readable.
plus some various assembler improvements.
Tested by various people a few months ago and in my tree for almost
6 months now.
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.5 1999/05/26 00:11:27 d Exp $	*/
a5 2
#include "spinlock.h"

a6 7

int
_atomic_is_locked(volatile _spinlock_lock_t * lock)
{
	
	return (*lock != _SPINLOCK_UNLOCKED);
}
@


1.5
log
@add mb (memory barrier) instruction. from FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.4 1998/12/21 13:03:43 d Exp $	*/
d3 1
a3 1
 * Atomic lock for alpha
d8 1
a8 32
int
_atomic_lock(volatile _spinlock_lock_t * lock)
{
	_spinlock_lock_t old;
	_spinlock_lock_t new;
	int success;

	do {
		/* load the value of the thread-lock (lock mem on load) */
		__asm__( "ldq_l %0, %1" : "=r"(old) : "m"(*lock) );
		if (old) 
			new = old;	/* locked: no change */
		else
			new = _SPINLOCK_LOCKED;	/* unlocked: grab it */

		success = 0;
		/* store the new value of the thrd-lock (unlock mem on store) */
		/*
		 * XXX may need to add *large* branch forward for main line
		 * branch prediction to be right :( [this note from linux]
		 */
		__asm__( "stq_c	%2, %0\n"
			 "beq	%2, 1f\n"
			 "mb\n"		
			 "mov	1, %1\n"
			 "1:"
			: "=m"(*lock), "=r"(success)
			: "r"(new) );
	} while (!success);

	return (old != _SPINLOCK_UNLOCKED);
}
@


1.4
log
@more spinlock
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.3 1998/12/21 07:36:59 d Exp $	*/
d26 2
a27 2
		 * XXX may need to add large branch forward for main line
		 * branch prediction to be right :( [note from linux]
d29 7
a35 3
		__asm__( "stq_c %2, %0; beq %2, 1f; mov 1,%1; 1:"
					: "=m"(*lock), "=r"(success)
					: "r"(new) );
@


1.3
log
@md spinlock
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.2 1998/12/18 05:59:17 d Exp $	*/
d38 1
a38 1
_atomic_is_locked(volatile register_t * lock)
@


1.2
log
@add md _atomic_is_locked; clean
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.1 1998/11/20 11:15:35 d Exp $	*/
d8 2
a9 2
register_t
_atomic_lock(volatile register_t * lock)
d11 2
a12 2
	register_t old;
	register_t new;
d19 1
a19 1
			new = old;	/* in-use: put it back */
d21 1
a21 1
			new = 1;	/* free: store a 1 in the lock */
d27 1
a27 1
		 * branch prediction to be right :(
d34 1
a34 1
	return old;
d41 1
a41 1
	return *lock;
@


1.1
log
@Move atomic_lock code from asm to C with inline asm;
Add m68k, mips and sparc. (needs more careful checking)
Add 'slow_atomic_lock' for crippled archs.
@
text
@d1 4
a4 2
/*	$OpenBSD$	*/
/* Atomic lock for alpha */
d35 7
@

