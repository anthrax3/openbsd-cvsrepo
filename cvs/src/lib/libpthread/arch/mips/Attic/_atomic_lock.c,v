head	1.8;
access;
symbols
	OPENBSD_3_5:1.7.0.6
	OPENBSD_3_5_BASE:1.7
	OPENBSD_3_4:1.7.0.4
	OPENBSD_3_4_BASE:1.7
	OPENBSD_3_3:1.7.0.2
	OPENBSD_3_3_BASE:1.7;
locks; strict;
comment	@ * @;


1.8
date	2004.08.11.17.41.34;	author pefo;	state dead;
branches;
next	1.7;

1.7
date	2002.10.11.19.08.41;	author marc;	state Exp;
branches;
next	1.6;

1.6
date	99.02.02.01.36.00;	author imp;	state Exp;
branches;
next	1.5;

1.5
date	99.01.10.23.00.02;	author d;	state Exp;
branches;
next	1.4;

1.4
date	98.12.21.13.03.45;	author d;	state Exp;
branches;
next	1.3;

1.3
date	98.12.21.07.37.00;	author d;	state Exp;
branches;
next	1.2;

1.2
date	98.12.18.05.59.18;	author d;	state Exp;
branches;
next	1.1;

1.1
date	98.11.20.11.15.37;	author d;	state Exp;
branches;
next	;


desc
@@


1.8
log
@mips->mips64
@
text
@/*	$OpenBSD: _atomic_lock.c,v 1.7 2002/10/11 19:08:41 marc Exp $	*/
/*
 * Atomic lock for mips
 */

#include "pthread.h"
#include "pthread_private.h"
#include "spinlock.h"
#include <signal.h>

/*
 * uthread atomic lock: 
 * 	attempt to acquire a lock (by giving it a non-zero value).
 *	Return zero on success, or the lock's value on failure
 */
int
_atomic_lock(volatile _spinlock_lock_t *lock)
{
#if __mips >= 2
	_spinlock_lock_t old;
	_spinlock_lock_t temp;

	do {
		/*
		 * On a mips2 machine and above, we can use ll/sc.
		 * Read the lock and tag the cache line with a 'load linked'
		 * instruction. (Register 17 (LLAddr) will hold the 
		 * physical address of lock for diagnostic purposes);
		 * (Under pathologically heavy swapping, the physaddr may 
		 * change! XXX)
		 */
		__asm__("ll %0, %1" : "=r"(old) : "m"(*lock));
		if (old != _SPINLOCK_UNLOCKED) 
			break; /* already locked */
		/*
		 * Try and store a 1 at the tagged lock address.  If
		 * anyone else has since written it, the tag on the cache
		 * line will have been wiped, and temp will be set to zero
		 * by the 'store conditional' instruction.
		 */
		temp = _SPINLOCK_LOCKED;
		__asm__("sc  %0, %1" : "=r"(temp), "=m"(*lock)
				     : "0"(temp));
	} while (temp == 0);

	return (old != _SPINLOCK_UNLOCKED);
#else
	/*
	 * Older MIPS cpus have no way of doing an atomic lock
	 * without some kind of shift to supervisor mode.
	 */

	return (_thread_slow_atomic_lock(lock));
#endif
}
@


1.7
log
@remove unused/unreferenced code.
ok fgs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.6 1999/02/02 01:36:00 imp Exp $	*/
@


1.6
log
@arc fixes for libc_r
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.5 1999/01/10 23:00:02 d Exp $	*/
a53 11
#endif
}

int
_atomic_is_locked(volatile _spinlock_lock_t *lock)
{
	
#if __mips >= 2
	return (*lock != _SPINLOCK_UNLOCKED);
#else
	return (_thread_slow_atomic_is_locked(lock));
@


1.5
log
@note pathological problem
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.4 1998/12/21 13:03:45 d Exp $	*/
d6 1
@


1.4
log
@more spinlock
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.3 1998/12/21 07:37:00 d Exp $	*/
d28 2
@


1.3
log
@md spinlock
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.2 1998/12/18 05:59:18 d Exp $	*/
d55 1
a55 1
_atomic_is_locked(volatile register_t *lock)
@


1.2
log
@add md _atomic_is_locked; clean
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.1 1998/11/20 11:15:37 d Exp $	*/
d15 2
a16 2
register_t
_atomic_lock(volatile register_t *lock)
a17 1
	register_t old;
d19 2
a20 1
	register_t temp;
d30 1
a30 1
		if (old) 
d38 1
a38 1
		temp = 1;
d42 2
d50 1
a50 2
	old = _thread_slow_atomic_lock(lock);

a51 1
	return old;
d55 1
a55 1
_atomic_is_locked(volatile register_t * lock)
d58 5
a62 1
	return *lock;
@


1.1
log
@Move atomic_lock code from asm to C with inline asm;
Add m68k, mips and sparc. (needs more careful checking)
Add 'slow_atomic_lock' for crippled archs.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d5 1
d52 7
@

