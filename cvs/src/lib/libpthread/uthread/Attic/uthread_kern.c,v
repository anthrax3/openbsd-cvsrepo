head	1.42;
access;
symbols
	OPENBSD_5_2:1.41.0.4
	OPENBSD_5_2_BASE:1.41
	OPENBSD_5_1_BASE:1.41
	OPENBSD_5_1:1.41.0.2
	OPENBSD_5_0:1.38.0.2
	OPENBSD_5_0_BASE:1.38
	OPENBSD_4_9:1.37.0.2
	OPENBSD_4_9_BASE:1.37
	OPENBSD_4_8:1.36.0.14
	OPENBSD_4_8_BASE:1.36
	OPENBSD_4_7:1.36.0.10
	OPENBSD_4_7_BASE:1.36
	OPENBSD_4_6:1.36.0.12
	OPENBSD_4_6_BASE:1.36
	OPENBSD_4_5:1.36.0.8
	OPENBSD_4_5_BASE:1.36
	OPENBSD_4_4:1.36.0.6
	OPENBSD_4_4_BASE:1.36
	OPENBSD_4_3:1.36.0.4
	OPENBSD_4_3_BASE:1.36
	OPENBSD_4_2:1.36.0.2
	OPENBSD_4_2_BASE:1.36
	OPENBSD_4_1:1.33.0.2
	OPENBSD_4_1_BASE:1.33
	OPENBSD_4_0:1.31.0.4
	OPENBSD_4_0_BASE:1.31
	OPENBSD_3_9:1.31.0.2
	OPENBSD_3_9_BASE:1.31
	OPENBSD_3_8:1.30.0.4
	OPENBSD_3_8_BASE:1.30
	OPENBSD_3_7:1.30.0.2
	OPENBSD_3_7_BASE:1.30
	OPENBSD_3_6:1.29.0.2
	OPENBSD_3_6_BASE:1.29
	OPENBSD_3_5:1.27.0.4
	OPENBSD_3_5_BASE:1.27
	OPENBSD_3_4:1.27.0.2
	OPENBSD_3_4_BASE:1.27
	OPENBSD_3_3:1.26.0.2
	OPENBSD_3_3_BASE:1.26;
locks; strict;
comment	@ * @;


1.42
date	2012.09.01.00.32.23;	author guenther;	state dead;
branches;
next	1.41;

1.41
date	2011.10.07.08.59.43;	author fgsch;	state Exp;
branches;
next	1.40;

1.40
date	2011.09.13.23.56.00;	author fgsch;	state Exp;
branches;
next	1.39;

1.39
date	2011.09.05.21.24.24;	author guenther;	state Exp;
branches;
next	1.38;

1.38
date	2011.07.07.09.25.16;	author guenther;	state Exp;
branches;
next	1.37;

1.37
date	2011.01.25.22.55.14;	author stsp;	state Exp;
branches;
next	1.36;

1.36
date	2007.05.21.16.50.36;	author kurt;	state Exp;
branches;
next	1.35;

1.35
date	2007.05.18.19.28.50;	author kurt;	state Exp;
branches;
next	1.34;

1.34
date	2007.04.27.12.59.24;	author kurt;	state Exp;
branches;
next	1.33;

1.33
date	2006.10.25.14.32.04;	author kurt;	state Exp;
branches;
next	1.32;

1.32
date	2006.10.03.02.59.36;	author kurt;	state Exp;
branches;
next	1.31;

1.31
date	2005.10.30.02.45.09;	author krw;	state Exp;
branches;
next	1.30;

1.30
date	2005.01.28.20.35.49;	author marc;	state Exp;
branches;
next	1.29;

1.29
date	2004.04.13.01.09.55;	author marc;	state Exp;
branches;
next	1.28;

1.28
date	2004.04.09.23.57.17;	author brad;	state Exp;
branches;
next	1.27;

1.27
date	2003.05.13.16.49.32;	author marc;	state Exp;
branches
	1.27.4.1;
next	1.26;

1.26
date	2003.01.31.04.46.17;	author marc;	state Exp;
branches;
next	1.25;

1.25
date	2003.01.27.22.22.30;	author marc;	state Exp;
branches;
next	1.24;

1.24
date	2003.01.24.21.03.15;	author marc;	state Exp;
branches;
next	1.23;

1.23
date	2002.11.04.21.28.49;	author marc;	state Exp;
branches;
next	1.22;

1.22
date	2002.10.30.19.11.56;	author marc;	state Exp;
branches;
next	1.21;

1.21
date	2002.02.21.20.57.41;	author fgsch;	state Exp;
branches;
next	1.20;

1.20
date	2002.01.04.03.39.09;	author fgsch;	state Exp;
branches;
next	1.19;

1.19
date	2001.12.31.18.23.15;	author fgsch;	state Exp;
branches;
next	1.18;

1.18
date	2001.12.18.03.47.52;	author marc;	state Exp;
branches;
next	1.17;

1.17
date	2001.12.08.14.51.36;	author fgsch;	state Exp;
branches;
next	1.16;

1.16
date	2001.09.04.22.17.45;	author fgsch;	state Exp;
branches;
next	1.15;

1.15
date	2001.08.30.17.47.57;	author todd;	state Exp;
branches;
next	1.14;

1.14
date	2001.08.30.07.40.47;	author fgsch;	state Exp;
branches;
next	1.13;

1.13
date	2001.08.21.19.24.53;	author fgsch;	state Exp;
branches;
next	1.12;

1.12
date	2001.08.15.23.53.17;	author fgsch;	state Exp;
branches;
next	1.11;

1.11
date	2001.01.16.04.51.07;	author d;	state Exp;
branches;
next	1.10;

1.10
date	2000.10.04.05.55.35;	author d;	state Exp;
branches
	1.10.2.1;
next	1.9;

1.9
date	99.11.25.07.01.37;	author d;	state Exp;
branches;
next	1.8;

1.8
date	99.05.26.00.18.24;	author d;	state Exp;
branches;
next	1.7;

1.7
date	99.02.01.08.23.46;	author d;	state Exp;
branches;
next	1.6;

1.6
date	99.01.17.23.49.49;	author d;	state Exp;
branches;
next	1.5;

1.5
date	99.01.10.23.16.35;	author d;	state Exp;
branches;
next	1.4;

1.4
date	98.12.21.07.42.03;	author d;	state Exp;
branches;
next	1.3;

1.3
date	98.11.20.12.13.32;	author d;	state Exp;
branches;
next	1.2;

1.2
date	98.11.09.03.13.20;	author d;	state Exp;
branches;
next	1.1;

1.1
date	98.08.27.09.01.08;	author d;	state Exp;
branches;
next	;

1.10.2.1
date	2001.06.06.21.58.00;	author miod;	state Exp;
branches;
next	;

1.27.4.1
date	2004.05.03.01.16.27;	author brad;	state Exp;
branches;
next	;


desc
@@


1.42
log
@   So passes uthreads
Like autumn leaves on water
   don't fear the tedu@@
@
text
@/*	$OpenBSD: uthread_kern.c,v 1.41 2011/10/07 08:59:43 fgsch Exp $	*/
/*
 * Copyright (c) 1995-1998 John Birrell <jb@@cimlogic.com.au>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by John Birrell.
 * 4. Neither the name of the author nor the names of any co-contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY JOHN BIRRELL AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: uthread_kern.c,v 1.23 1999/09/29 15:18:39 marcel Exp $
 *
 */
#include <errno.h>
#include <poll.h>
#include <stdlib.h>
#include <stdarg.h>
#include <string.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/time.h>
#include <sys/socket.h>
#include <sys/uio.h>
#include <sys/syscall.h>
#include <fcntl.h>
#ifdef _THREAD_SAFE
#include <pthread.h>
#include "pthread_private.h"

/*
 * local functions.   Do NOT make these static... we want so see them in
 * crash dumps.
 */
void		_thread_kern_poll(int);
void		_dequeue_signals(void);
inline void	_thread_run_switch_hook(pthread_t, pthread_t);

/* Static variables: */
static unsigned int	last_tick = 0;

void
_thread_kern_sched(struct sigcontext * scp)
{
	struct timespec	ts;
	struct timeval	tv;
	struct pthread	*curthread = _get_curthread();
	pthread_t       pthread, pthread_h;
	unsigned int	current_tick;
	int		add_to_prioq;
	pthread_t	old_thread_run;

	/*
	 * Flag the pthread kernel as executing scheduler code
	 * to avoid a scheduler signal from interrupting this
	 * execution and calling the scheduler again.
	 */
	_thread_kern_in_sched = 1;

	/* Check if this function was called from the signal handler: */
	if (scp != NULL) {
		/*
		 * The signal handler should have saved the state of
		 * the current thread. Restore the process signal
		 * mask.
		 */
		if (_thread_sys_sigprocmask(SIG_SETMASK,
		    &_process_sigmask, NULL) != 0)
			PANIC("Unable to restore process mask after signal");

		/*
		 * Copy the signal context to the current thread's jump
		 * buffer:
		 */
		memcpy(&curthread->saved_sigcontext, scp,
		    sizeof(curthread->saved_sigcontext));

		/* Flag the signal context as the last state saved: */
		curthread->sig_saved = 1;
	} else
		/* Flag the jump buffer was the last state saved: */
		curthread->sig_saved = 0;

	/* If the currently running thread is a user thread, save it: */
	if ((curthread->flags & PTHREAD_FLAGS_PRIVATE) == 0)
		_last_user_thread = curthread;

	/* Save floating point state. */
	_thread_machdep_save_float_state(&curthread->_machdep);

	/* Save errno. */
	curthread->error = errno;

	/* Save the current thread to switch from */
	old_thread_run = curthread;

	/*
	 * Enter a scheduling loop that finds the next thread that is
	 * ready to run. This loop completes when there are no more threads
	 * in the global list or when a thread has its state restored by
	 * either a sigreturn (if the state was saved as a sigcontext) or a
	 * switch.
	 */
	while (!(TAILQ_EMPTY(&_thread_list))) {
		/* Get the current time of day: */
		GET_CURRENT_TOD(tv);
		TIMEVAL_TO_TIMESPEC(&tv, &ts);
		current_tick = _sched_ticks;

		/*
		 * Protect the scheduling queues from access by the signal
		 * handler.
		 */
		_queue_signals = 1;
		add_to_prioq = 0;

		if (curthread != &_thread_kern_thread) {
			/*
			 * This thread no longer needs to yield the CPU.
			 */
			curthread->yield_on_sig_undefer = 0;
	
			if (curthread->state != PS_RUNNING) {
				/*
				 * Save the current time as the time that the
				 * thread became inactive:
				 */
				curthread->last_inactive = (long)current_tick;
				if (curthread->last_inactive <
				    curthread->last_active) {
					/* Account for a rollover: */
					curthread->last_inactive =+
					    UINT_MAX + 1;
				}
			}

			/*
			 * Place the currently running thread into the
			 * appropriate queue(s).
			 */
			switch (curthread->state) {
			case PS_DEAD:
			case PS_STATE_MAX: /* to silence -Wall */
			case PS_SUSPENDED:
				/*
				 * Dead and suspended threads are not placed
				 * in any queue:
				 */
				break;

			case PS_RUNNING:
				/*
				 * Runnable threads can't be placed in the
				 * priority queue until after waiting threads
				 * are polled (to preserve round-robin
				 * scheduling).
				 */
				add_to_prioq = 1;
				break;

			/*
			 * States which do not depend on file descriptor I/O
			 * operations or timeouts:
			 */
			case PS_DEADLOCK:
			case PS_FDLR_WAIT:
			case PS_FDLW_WAIT:
			case PS_FILE_WAIT:
			case PS_JOIN:
			case PS_MUTEX_WAIT:
			case PS_SIGSUSPEND:
			case PS_SIGTHREAD:
			case PS_SIGWAIT:
			case PS_WAIT_WAIT:
				/* No timeouts for these states: */
				curthread->wakeup_time.tv_sec = -1;
				curthread->wakeup_time.tv_nsec = -1;

				/* Restart the time slice: */
				curthread->slice_usec = -1;

				/* Insert into the waiting queue: */
				PTHREAD_WAITQ_INSERT(curthread);
				break;

			/* States which can timeout: */
			case PS_COND_WAIT:
			case PS_SLEEP_WAIT:
				/* Restart the time slice: */
				curthread->slice_usec = -1;

				/* Insert into the waiting queue: */
				PTHREAD_WAITQ_INSERT(curthread);
				break;
	
			/* States that require periodic work: */
			case PS_SPINBLOCK:
				/* No timeouts for this state: */
				curthread->wakeup_time.tv_sec = -1;
				curthread->wakeup_time.tv_nsec = -1;

				/* Increment spinblock count: */
				_spinblock_count++;

				/* FALLTHROUGH */
			case PS_CONNECT_WAIT:
			case PS_FDR_WAIT:
			case PS_FDW_WAIT:
			case PS_KEVENT_WAIT:
			case PS_POLL_WAIT:
			case PS_SELECT_WAIT:
				/* Restart the time slice: */
				curthread->slice_usec = -1;
	
				/* Insert into the waiting queue: */
				PTHREAD_WAITQ_INSERT(curthread);
	
				/* Insert into the work queue: */
				PTHREAD_WORKQ_INSERT(curthread);
				break;
			}
		}

		/*
		 * Avoid polling file descriptors if there are none
		 * waiting:
		 */
		if (TAILQ_EMPTY(&_workq) != 0) {
		}
		/*
		 * Poll file descriptors only if a new scheduling signal
		 * has occurred or if we have no more runnable threads.
		 */
		else if (((current_tick = _sched_ticks) != last_tick) ||
		    ((curthread->state != PS_RUNNING) &&
		    (PTHREAD_PRIOQ_FIRST() == NULL))) {
			/* Unprotect the scheduling queues: */
			_queue_signals = 0;

			/*
			 * Poll file descriptors to update the state of threads
			 * waiting on file I/O where data may be available:
			 */
			_thread_kern_poll(0);

			/* Protect the scheduling queues: */
			_queue_signals = 1;
		}
		last_tick = current_tick;

		/*
		 * Wake up threads that have timedout.  This has to be
		 * done after polling in case a thread does a poll or
		 * select with zero time.
		 */
		PTHREAD_WAITQ_SETACTIVE();
		while (((pthread = TAILQ_FIRST(&_waitingq)) != NULL) &&
		       (pthread->wakeup_time.tv_sec != -1) &&
		       (((pthread->wakeup_time.tv_sec == 0) &&
			 (pthread->wakeup_time.tv_nsec == 0)) ||
			(pthread->wakeup_time.tv_sec < ts.tv_sec) ||
			((pthread->wakeup_time.tv_sec == ts.tv_sec) &&
			 (pthread->wakeup_time.tv_nsec <= ts.tv_nsec)))) {
			switch (pthread->state) {
			case PS_POLL_WAIT:
			case PS_SELECT_WAIT:
				/* Return zero file descriptors ready: */
				pthread->data.poll_data->nfds = 0;
				/* fall through */
			default:
				/*
				 * Remove this thread from the waiting queue
				 * (and work queue if necessary) and place it
				 * in the ready queue.
				 */
				PTHREAD_WAITQ_CLEARACTIVE();
				if (pthread->flags & PTHREAD_FLAGS_IN_WORKQ)
					PTHREAD_WORKQ_REMOVE(pthread);
				PTHREAD_NEW_STATE(pthread, PS_RUNNING);
				PTHREAD_WAITQ_SETACTIVE();
				break;
			}
			/*
			 * Flag the timeout in the thread structure:
			 */
			pthread->timeout = 1;
		}
		PTHREAD_WAITQ_CLEARACTIVE();

		/*
		 * Check to see if the current thread needs to be added
		 * to the priority queue:
		 */
		if (add_to_prioq != 0) {
			/*
			 * Save the current time as the time that the
			 * thread became inactive:
			 */
			current_tick = _sched_ticks;
			curthread->last_inactive = (long)current_tick;
			if (curthread->last_inactive <
			    curthread->last_active) {
				/* Account for a rollover: */
				curthread->last_inactive =+ UINT_MAX + 1;
			}

			if ((curthread->slice_usec != -1) &&
			   (curthread->attr.sched_policy != SCHED_FIFO)) {
				/*
				 * Accumulate the number of microseconds for
				 * which the current thread has run:
				 */
				curthread->slice_usec +=
				    (curthread->last_inactive -
				    curthread->last_active) *
				    (long)_clock_res_usec;
				/* Check for time quantum exceeded: */
				if (curthread->slice_usec > TIMESLICE_USEC)
					curthread->slice_usec = -1;
			}

			if (curthread->slice_usec == -1) {
				/*
				 * The thread exceeded its time
				 * quantum or it yielded the CPU;
				 * place it at the tail of the
				 * queue for its priority.
				 */
				PTHREAD_PRIOQ_INSERT_TAIL(curthread);
			} else {
				/*
				 * The thread hasn't exceeded its
				 * interval.  Place it at the head
				 * of the queue for its priority.
				 */
				PTHREAD_PRIOQ_INSERT_HEAD(curthread);
			}
		}

		/*
		 * Get the highest priority thread in the ready queue.
		 */
		pthread_h = PTHREAD_PRIOQ_FIRST();

		/* Check if there are no threads ready to run: */
		if (pthread_h == NULL) {
			/*
			 * Lock the pthread kernel by changing the pointer to
			 * the running thread to point to the global kernel
			 * thread structure:
			 */
			_set_curthread(&_thread_kern_thread);
			curthread = &_thread_kern_thread;

			/* Unprotect the scheduling queues: */
			_queue_signals = 0;

			/*
			 * There are no threads ready to run, so wait until
			 * something happens that changes this condition:
			 */
			_thread_kern_poll(1);

			/*
			 * This process' usage will likely be very small
			 * while waiting in a poll.  Since the scheduling
			 * clock is based on the profiling timer, it is
			 * unlikely that the profiling timer will fire
			 * and update the time of day.  To account for this,
			 * get the time of day after polling with a timeout.
			 */
			gettimeofday((struct timeval *) &_sched_tod, NULL);

			/* Check once more for a runnable thread: */
			_queue_signals = 1;
			pthread_h = PTHREAD_PRIOQ_FIRST();
			_queue_signals = 0;
		}

		if (pthread_h != NULL) {
			/* Remove the thread from the ready queue: */
			PTHREAD_PRIOQ_REMOVE(pthread_h);

			/* Unprotect the scheduling queues: */
			_queue_signals = 0;

			/*
			 * Check for signals queued while the scheduling
			 * queues were protected:
			 */
			while (_sigq_check_reqd != 0) {
				/* Clear before handling queued signals: */
				_sigq_check_reqd = 0;

				/* Protect the scheduling queues again: */
				_queue_signals = 1;

				_dequeue_signals();

				/*
				 * Check for a higher priority thread that
				 * became runnable due to signal handling.
				 */
				if (((pthread = PTHREAD_PRIOQ_FIRST()) != NULL) &&
				    (pthread->active_priority > pthread_h->active_priority)) {
					/* Remove the thread from the ready queue: */
					PTHREAD_PRIOQ_REMOVE(pthread);

					/*
					 * Insert the lower priority thread
					 * at the head of its priority list:
					 */
					PTHREAD_PRIOQ_INSERT_HEAD(pthread_h);

					/* There's a new thread in town: */
					pthread_h = pthread;
				}

				/* Unprotect the scheduling queues: */
				_queue_signals = 0;
			}

			/*
			 * Prevent the signal handler from fiddling with this
			 * thread before its state is set.
			 */
			_queue_signals = 1;

			/* Make the selected thread the current thread: */
			_set_curthread(pthread_h);
			curthread = pthread_h;

			/*
			 * Save the current time as the time that the thread
			 * became active:
			 */
			current_tick = _sched_ticks;
			curthread->last_active = (long) current_tick;

			/*
			 * Check if this thread is running for the first time
			 * or running again after using its full time slice
			 * allocation:
			 */
			if (curthread->slice_usec == -1) {
				/* Reset the accumulated time slice period: */
				curthread->slice_usec = 0;
			}

			/* Restore errno. */
			errno = curthread->error;

			/*
			 * If we're 'switching' to the current thread,
			 * then don't bother with the save/restore
			 */
			if (curthread == old_thread_run)
				goto after_switch;

			/* Restore floating point state. */
			_thread_machdep_restore_float_state(&curthread->_machdep);

			/* Restore the new thread, saving current. */
			_thread_machdep_switch(&curthread->_machdep,
					       &old_thread_run->_machdep);

			/*
			 * DANGER WILL ROBINSON
			 * All stack local variables now contain the values
			 * they had when this thread was last running.  In
			 * particular, curthread is NOT pointing to the
			 * current thread.   Make it point to the current
			 * before use.
			 */
			curthread = _get_curthread();

		  after_switch:

			/* Allow signals again. */
			_queue_signals = 0;

			/* Done with scheduling. */
			_thread_kern_in_sched = 0;

			/* run any installed switch-hooks */
			if ((_sched_switch_hook != NULL) &&
			    (_last_user_thread != curthread)) {
				_thread_run_switch_hook(_last_user_thread,
							curthread);
			}

			/* check for thread cancellation */
			if (((curthread->cancelflags &
			      PTHREAD_AT_CANCEL_POINT) == 0) &&
			    ((curthread->cancelflags &
			      PTHREAD_CANCEL_ASYNCHRONOUS) != 0))
				pthread_testcancel();

			/* dispatch any pending signals if possible */
			if (curthread->sig_defer_count == 0)
				_dispatch_signals(scp);

			/* Check if a signal context was saved: */
			if (curthread->sig_saved == 1) {
				/* return to signal handler.   This code
				   should be:
				   _thread_sys_sigreturn(&curthread->saved_sigcontext);
				   but that doesn't currently work on the
				   sparc */
				return;
			} else {
				/* This is the normal way out */
				return;
			}

			/* This point should not be reached. */
			PANIC("Thread has returned from sigreturn or switch");
		}
	}

	/* There are no more threads, so exit this process: */
	exit(0);
}

void
_thread_kern_sched_state(enum pthread_state state, const char *fname,
			 int lineno)
{
	struct pthread	*curthread = _get_curthread();

	/*
	 * Flag the pthread kernel as executing scheduler code
	 * to avoid a scheduler signal from interrupting this
	 * execution and calling the scheduler again.
	 */
	_thread_kern_in_sched = 1;

	/*
	 * Prevent the signal handler from fiddling with this thread
	 * before its state is set and is placed into the proper queue.
	 */
	_queue_signals = 1;

	/* Change the state of the current thread: */
	curthread->state = state;
	curthread->fname = fname;
	curthread->lineno = lineno;

	/* Schedule the next thread that is ready: */
	_thread_kern_sched(NULL);
}

void
_thread_kern_sched_state_unlock(enum pthread_state state, spinlock_t *lock,
				const char *fname, int lineno)
{
	struct pthread	*curthread = _get_curthread();

	/*
	 * Flag the pthread kernel as executing scheduler code
	 * to avoid a scheduler signal from interrupting this
	 * execution and calling the scheduler again.
	 */
	_thread_kern_in_sched = 1;

	/*
	 * Prevent the signal handler from fiddling with this thread
	 * before its state is set and it is placed into the proper
	 * queue(s).
	 */
	_queue_signals = 1;

	/* Change the state of the current thread: */
	curthread->state = state;
	curthread->fname = fname;
	curthread->lineno = lineno;

	_SPINUNLOCK(lock);

	/* Schedule the next thread that is ready: */
	_thread_kern_sched(NULL);
}

void
_thread_kern_poll(int wait_reqd)
{
	int             count = 0;
	int		kern_pipe_added = 0;
	nfds_t		i, found, nfds = 0;
	int		timeout_ms = 0;
	struct pthread	*pthread, *next;
	struct timespec ts;
	struct timeval  tv;

	/* Check if the caller wants to wait: */
	if (wait_reqd == 0) {
		timeout_ms = 0;
	}
	else {
		/* Get the current time of day: */
		GET_CURRENT_TOD(tv);
		TIMEVAL_TO_TIMESPEC(&tv, &ts);

		_queue_signals = 1;
		pthread = TAILQ_FIRST(&_waitingq);
		_queue_signals = 0;

		if ((pthread == NULL) || (pthread->wakeup_time.tv_sec == -1)) {
			/*
			 * Either there are no threads in the waiting queue,
			 * or there are no threads that can timeout.
			 */
			timeout_ms = INFTIM;
		}
		else if (pthread->wakeup_time.tv_sec - ts.tv_sec > 60000)
			/* Limit maximum timeout to prevent rollover. */
			timeout_ms = 60000;
		else {
			/*
			 * Calculate the time left for the next thread to
			 * timeout:
			 */
			timeout_ms = ((pthread->wakeup_time.tv_sec - ts.tv_sec) *
			    1000) + (time_t)((pthread->wakeup_time.tv_nsec - ts.tv_nsec) /
			    1000000);
			/*
			 * Don't allow negative timeouts:
			 */
			if (timeout_ms < 0)
				timeout_ms = 0;
		}
	}
			
	/* Protect the scheduling queues: */
	_queue_signals = 1;

	/*
	 * Check to see if the signal queue needs to be walked to look
	 * for threads awoken by a signal while in the scheduler.
	 */
	if (_sigq_check_reqd != 0) {
		/* Reset flag before handling queued signals: */
		_sigq_check_reqd = 0;
		_dequeue_signals();
	}

	/*
	 * Check for a thread that became runnable due to a signal:
	 */
	if (PTHREAD_PRIOQ_FIRST() != NULL) {
		/*
		 * Since there is at least one runnable thread,
		 * disable the wait.
		 */
		timeout_ms = 0;
	}

	/*
	 * Form the poll table:
	 */
	nfds = 0;
	if (timeout_ms != 0) {
		/* Add the kernel pipe to the poll table: */
		_thread_pfd_table[nfds].fd = _thread_kern_pipe[0];
		_thread_pfd_table[nfds].events = POLLRDNORM;
		_thread_pfd_table[nfds].revents = 0;
		nfds++;
		kern_pipe_added = 1;
	}

	PTHREAD_WAITQ_SETACTIVE();
	for (pthread = TAILQ_FIRST(&_workq); pthread != NULL; pthread = next) {
		next = TAILQ_NEXT(pthread, qe);
		switch (pthread->state) {
		case PS_SPINBLOCK:
			/*
			 * If the lock is available, let the thread run.
			 */
			if (pthread->data.spinlock->access_lock ==
			    _SPINLOCK_UNLOCKED) {
				PTHREAD_WAITQ_CLEARACTIVE();
				PTHREAD_WORKQ_REMOVE(pthread);
				PTHREAD_NEW_STATE(pthread,PS_RUNNING);
				PTHREAD_WAITQ_SETACTIVE();
				/* One less thread in a spinblock state: */
				_spinblock_count--;
				/*
				 * Since there is at least one runnable
				 * thread, disable the wait.
				 */
				timeout_ms = 0;
			}
			break;

		/* File descriptor read wait: */
		case PS_FDR_WAIT:
		case PS_KEVENT_WAIT:
			/* if fd is closing then reschedule this thread */
			if (_thread_fd_table[pthread->data.fd.fd]->state == FD_ENTRY_CLOSING) {
				pthread->closing_fd = 1;
				PTHREAD_WAITQ_CLEARACTIVE();
				PTHREAD_WORKQ_REMOVE(pthread);
				PTHREAD_NEW_STATE(pthread,PS_RUNNING);
				PTHREAD_WAITQ_SETACTIVE();
			} else {
				/* Limit number of polled files to table size: */
				if (nfds < _thread_max_pfdtsize) {
					_thread_pfd_table[nfds].events = POLLRDNORM;
					_thread_pfd_table[nfds].fd = pthread->data.fd.fd;
					nfds++;
				}
			}
			break;

		/* File descriptor write wait: */
		case PS_CONNECT_WAIT:
		case PS_FDW_WAIT:
			/* if fd is closing then reschedule this thread */
			if (_thread_fd_table[pthread->data.fd.fd]->state == FD_ENTRY_CLOSING) {
				pthread->closing_fd = 1;
				PTHREAD_WAITQ_CLEARACTIVE();
				PTHREAD_WORKQ_REMOVE(pthread);
				PTHREAD_NEW_STATE(pthread,PS_RUNNING);
				PTHREAD_WAITQ_SETACTIVE();
			} else {
				/* Limit number of polled files to table size: */
				if (nfds < _thread_max_pfdtsize) {
					_thread_pfd_table[nfds].events = POLLWRNORM;
					_thread_pfd_table[nfds].fd = pthread->data.fd.fd;
					nfds++;
				}
			}
			break;

		/* File descriptor poll or select wait: */
		case PS_POLL_WAIT:
		case PS_SELECT_WAIT:
			/* Limit number of polled files to table size: */
			if (pthread->data.poll_data->nfds + nfds <
			    _thread_max_pfdtsize) {
				for (i = 0; i < pthread->data.poll_data->nfds; i++) {
					_thread_pfd_table[nfds + i].fd =
					    pthread->data.poll_data->fds[i].fd;
					_thread_pfd_table[nfds + i].events =
					    pthread->data.poll_data->fds[i].events;
				}
				nfds += pthread->data.poll_data->nfds;
			}
			break;

		/* Other states do not depend on file I/O. */
		default:
			break;
		}
	}
	PTHREAD_WAITQ_CLEARACTIVE();

	/*
	 * Wait for a file descriptor to be ready for read, write, or
	 * an exception, or a timeout to occur:
	 */
	count = _thread_sys_poll(_thread_pfd_table, nfds, timeout_ms);

	if (kern_pipe_added != 0)
		/*
		 * Remove the pthread kernel pipe file descriptor
		 * from the pollfd table:
		 */
		nfds = 1;
	else
		nfds = 0;

	/*
	 * Check if it is possible that there are bytes in the kernel
	 * read pipe waiting to be read:
	 */
	if (count < 0 || ((kern_pipe_added != 0) &&
	    (_thread_pfd_table[0].revents & POLLRDNORM))) {
		/*
		 * If the kernel read pipe was included in the
		 * count:
		 */
		if (count > 0) {
			/* Decrement the count of file descriptors: */
			count--;
		}

		if (_sigq_check_reqd != 0) {
			/* Reset flag before handling signals: */
			_sigq_check_reqd = 0;
			_dequeue_signals();
		}
	}

	/*
	 * Check if any file descriptors are ready:
	 */
	if (count > 0) {
		/*
		 * Enter a loop to look for threads waiting on file
		 * descriptors that are flagged as available by the
		 * _poll syscall:
		 */
		PTHREAD_WAITQ_SETACTIVE();
		for (pthread = TAILQ_FIRST(&_workq); pthread != NULL;
		    pthread = next) {
			next = TAILQ_NEXT(pthread, qe);
			switch (pthread->state) {
			case PS_SPINBLOCK:
				/*
				 * If the lock is available, let the thread run.
				 */
				if (pthread->data.spinlock->access_lock ==
				    _SPINLOCK_UNLOCKED) {
					PTHREAD_WAITQ_CLEARACTIVE();
					PTHREAD_WORKQ_REMOVE(pthread);
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
					PTHREAD_WAITQ_SETACTIVE();

					/*
					 * One less thread in a spinblock state:
					 */
					_spinblock_count--;
				}
				break;

			/* File descriptor read wait: */
			case PS_FDR_WAIT:
			case PS_KEVENT_WAIT:
				if ((nfds < _thread_max_pfdtsize) &&
				    (_thread_pfd_table[nfds].revents
				       & (POLLRDNORM|POLLERR|POLLHUP|POLLNVAL))
				      != 0) {
					PTHREAD_WAITQ_CLEARACTIVE();
					PTHREAD_WORKQ_REMOVE(pthread);
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
					PTHREAD_WAITQ_SETACTIVE();
				}
				nfds++;
				break;

			/* File descriptor write wait: */
			case PS_CONNECT_WAIT:
			case PS_FDW_WAIT:
				if ((nfds < _thread_max_pfdtsize) &&
				    (_thread_pfd_table[nfds].revents
				       & (POLLWRNORM|POLLERR|POLLHUP|POLLNVAL))
				      != 0) {
					PTHREAD_WAITQ_CLEARACTIVE();
					PTHREAD_WORKQ_REMOVE(pthread);
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
					PTHREAD_WAITQ_SETACTIVE();
				}
				nfds++;
				break;

			/* File descriptor poll or select wait: */
			case PS_POLL_WAIT:
			case PS_SELECT_WAIT:
				if (pthread->data.poll_data->nfds + nfds <
				    _thread_max_pfdtsize) {
					/*
					 * Enter a loop looking for I/O
					 * readiness:
					 */
					found = 0;
					for (i = 0; i < pthread->data.poll_data->nfds; i++) {
						if (_thread_pfd_table[nfds + i].revents != 0) {
							pthread->data.poll_data->fds[i].revents =
							    _thread_pfd_table[nfds + i].revents;
							found++;
						}
					}

					/* Increment before destroying: */
					nfds += pthread->data.poll_data->nfds;

					if (found != 0) {
						pthread->data.poll_data->nfds = found;
						PTHREAD_WAITQ_CLEARACTIVE();
						PTHREAD_WORKQ_REMOVE(pthread);
						PTHREAD_NEW_STATE(pthread,PS_RUNNING);
						PTHREAD_WAITQ_SETACTIVE();
					}
				}
				else
					nfds += pthread->data.poll_data->nfds;
				break;

			/* Other states do not depend on file I/O. */
			default:
				break;
			}
		}
		PTHREAD_WAITQ_CLEARACTIVE();
	}
	else if (_spinblock_count != 0) {
		/*
		 * Enter a loop to look for threads waiting on a spinlock
		 * that is now available.
		 */
		PTHREAD_WAITQ_SETACTIVE();
		for (pthread = TAILQ_FIRST(&_workq); pthread != NULL;
		    pthread = next) {
			next = TAILQ_NEXT(pthread, qe);
			if (pthread->state == PS_SPINBLOCK) {
				/*
				 * If the lock is available, let the thread run.
				 */
				if (pthread->data.spinlock->access_lock ==
				    _SPINLOCK_UNLOCKED) {
					PTHREAD_WAITQ_CLEARACTIVE();
					PTHREAD_WORKQ_REMOVE(pthread);
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
					PTHREAD_WAITQ_SETACTIVE();

					/*
					 * One less thread in a spinblock state:
					 */
					_spinblock_count--;
				}
			}
		}
		PTHREAD_WAITQ_CLEARACTIVE();
	}

	/* Unprotect the scheduling queues: */
	_queue_signals = 0;

	while (_sigq_check_reqd != 0) {
		/* Handle queued signals: */
		_sigq_check_reqd = 0;

		/* Protect the scheduling queues: */
		_queue_signals = 1;
		_dequeue_signals();
		_queue_signals = 0;
	}
}

void
_thread_kern_set_timeout(const struct timespec * timeout)
{
	struct pthread	*curthread = _get_curthread();
	struct timespec current_time;
	struct timeval  tv;

	/* Reset the timeout flag for the running thread: */
	curthread->timeout = 0;

	/* Check if the thread is to wait forever: */
	if (timeout == NULL) {
		/*
		 * Set the wakeup time to something that can be recognised as
		 * different to an actual time of day:
		 */
		curthread->wakeup_time.tv_sec = -1;
		curthread->wakeup_time.tv_nsec = -1;
	}
	/* Check if no waiting is required: */
	else if (timeout->tv_sec == 0 && timeout->tv_nsec == 0) {
		/* Set the wake up time to 'immediately': */
		curthread->wakeup_time.tv_sec = 0;
		curthread->wakeup_time.tv_nsec = 0;
	} else {
		gettimeofday((struct timeval *) &_sched_tod, NULL);
		GET_CURRENT_TOD(tv);
		TIMEVAL_TO_TIMESPEC(&tv, &current_time);
		timespecadd(&current_time, timeout, &curthread->wakeup_time);
	}
}

/*
 * Function registered with dlctl to lock/unlock the kernel for
 * threade safe dlopen calls.
 *	which == 0:	defer signals (stops scheduler)
 *	which != 0:	undefer signals and process any queued sigs
 */
void
_thread_kern_lock(int which)
{
	if (which == 0)
		_thread_kern_sig_defer();
	else
		_thread_kern_sig_undefer();
}


void
_thread_kern_sig_defer(void)
{
	struct pthread	*curthread = _get_curthread();

	/* Allow signal deferral to be recursive. */
	curthread->sig_defer_count++;
}

void
_thread_kern_sig_undefer(void)
{
	struct pthread	*curthread = _get_curthread();

	/*
	 * Perform checks to yield only if we are about to undefer
	 * signals.
	 */
	if (curthread->sig_defer_count > 1) {
		/* Decrement the signal deferral count. */
		curthread->sig_defer_count--;
	}
	else if (curthread->sig_defer_count == 1) {
		/* Reenable signals: */
		curthread->sig_defer_count = 0;

		/*
		 * Check if there are queued signals:
		 */
		if (_sigq_check_reqd != 0)
			_thread_kern_sched(NULL);

		/*
		 * Check for asynchronous cancellation before delivering any
		 * pending signals:
		 */
		if (((curthread->cancelflags & PTHREAD_AT_CANCEL_POINT) == 0) &&
		    ((curthread->cancelflags & PTHREAD_CANCEL_ASYNCHRONOUS) != 0))
			pthread_testcancel();

		/*
		 * If there are pending signals or this thread has
		 * to yield the CPU, call the kernel scheduler:
		 *
		 * XXX - Come back and revisit the pending signal problem
		 */
		if ((curthread->yield_on_sig_undefer != 0) ||
		    curthread->sigpend != 0) {
			curthread->yield_on_sig_undefer = 0;
			_thread_kern_sched(NULL);
		}
	}
}

void
_dequeue_signals(void)
{
	char	bufr[128];
	int	i;
	ssize_t num;

	/*
	 * Enter a loop to read and handle queued signals from the
	 * pthread kernel pipe:
	 */
	while (((num = _thread_sys_read(_thread_kern_pipe[0], bufr,
	    sizeof(bufr))) > 0) || (num == -1 && errno == EINTR)) {
		/*
		 * The buffer read contains one byte per signal and
		 * each byte is the signal number.
		 */
		for (i = 0; i < num; i++) {
			if ((int) bufr[i] != _SCHED_SIGNAL)
				_thread_sig_handle((int) bufr[i], NULL);
		}
	}
	if ((num < 0) && (errno != EAGAIN)) {
		/*
		 * The only error we should expect is if there is
		 * no data to read.
		 */
		PANIC("Unable to read from thread kernel pipe");
	}
}

inline void
_thread_run_switch_hook(pthread_t thread_out, pthread_t thread_in)
{
	pthread_t tid_out = thread_out;
	pthread_t tid_in = thread_in;

	if ((tid_out != NULL) &&
	    (tid_out->flags & PTHREAD_FLAGS_PRIVATE) != 0)
		tid_out = NULL;
	if ((tid_in != NULL) &&
	    (tid_in->flags & PTHREAD_FLAGS_PRIVATE) != 0)
		tid_in = NULL;

	if ((_sched_switch_hook != NULL) && (tid_out != tid_in)) {
		/* Run the scheduler switch hook: */
		_sched_switch_hook(tid_out, tid_in);
	}
}

struct pthread *
_get_curthread(void)
{
	if (_thread_initial == NULL)
		_thread_init();

	return (_thread_run);
}

void
_set_curthread(struct pthread *newthread)
{
	_thread_run = newthread;
}
#endif
@


1.41
log
@threads waiting on PS_FDW_WAIT state should not be interruptible if
SA_RESTART is set, with connect(2) being the exception thus getting its
own state.
as pointed by kurt, threads on this and PS_FDR_WAIT states need to be
set to PS_RUNNING since the current signal dispatching code only looks
at the current thread.
ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.40 2011/09/13 23:56:00 fgsch Exp $	*/
@


1.40
log
@For threads in PS_FDR_WAIT state, check SA_RESTART before marking it as
interrupted, thus simulating the system call restart behaviour in the
non-pthreads case.
Add a state for kevent since it shouldn't be restarted regardless of
SA_RESTART being present.
guenther@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.39 2011/09/05 21:24:24 guenther Exp $	*/
d228 1
d738 1
d865 1
@


1.39
log
@The scheduling loop can change errno, so we need to restore it even
when not switching threads; issue observed by fgsch@@

ok marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.38 2011/07/07 09:25:16 guenther Exp $	*/
d230 1
d718 1
d849 1
@


1.38
log
@When context switching, if the 'new' thread is the same as the 'old'
thread, then the save and restore of errno, FPU, and regs is unnecessary
and can be skipped.

"looks reasonable" marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.37 2011/01/25 22:55:14 stsp Exp $	*/
d470 3
a478 3

			/* Restore errno. */
			errno = curthread->error;
@


1.37
log
@Make the pthread scheduler block signals while restoring a newly
selected thread's state. Fixes random qemu crashes.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.36 2007/05/21 16:50:36 kurt Exp $	*/
d470 7
d496 2
@


1.36
log
@clean up lint warnings related to the nfds_t type. okay marc@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.35 2007/05/18 19:28:50 kurt Exp $	*/
d443 6
d489 5
@


1.35
log
@Eliminate many lint warnings by either: using the appropriate type,
casting when safe or adding ARGSUSED where needed. Reviewed and
improvements from millert@@ and marc@@. okay marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.34 2007/04/27 12:59:24 kurt Exp $	*/
a587 1
	int             i, found;
d589 1
a589 1
	int             nfds = 0;
d706 1
a706 1
				if (nfds < _thread_max_fdtsize) {
d725 1
a725 1
				if (nfds < _thread_max_fdtsize) {
d738 1
a738 1
			    _thread_max_fdtsize) {
d827 1
a827 1
				if ((nfds < _thread_max_fdtsize) &&
d841 1
a841 1
				if ((nfds < _thread_max_fdtsize) &&
d857 1
a857 1
				    _thread_max_fdtsize) {
@


1.34
log
@Use rlimit nofiles max to size fd/fdp tables instead of cur. Fixes
applications that increase nofiles using setrlimit(2). ok marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.33 2006/10/25 14:32:04 kurt Exp $	*/
d625 1
a625 1
			    1000) + ((pthread->wakeup_time.tv_nsec - ts.tv_nsec) /
d1044 2
a1045 1
	int	i, num;
@


1.33
log
@select() and poll() don't _FD_LOCK their file descriptors, so there's no
need to bail from _thread_kern_poll() when a file descriptor is in
closing state. corrects segfault reported by ckuethe@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.32 2006/10/03 02:59:36 kurt Exp $	*/
d707 1
a707 1
				if (nfds < _thread_dtablesize) {
d726 1
a726 1
				if (nfds < _thread_dtablesize) {
d739 1
a739 1
			    _thread_dtablesize) {
d828 1
a828 1
				if ((nfds < _thread_dtablesize) &&
d842 1
a842 1
				if ((nfds < _thread_dtablesize) &&
d858 1
a858 1
				    _thread_dtablesize) {
@


1.32
log
@Last Part of file descriptor race and deadlock corrections.

When a fd enters the closing state prevent any threads from
polling the fd and reschedule the thread with the closing_fd
flag set. This fixes a class of deadlocks where a thread is
blocked waiting for data (that may never arrive) and a later
thread calls close() or dup2() on the fd. okay brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.31 2005/10/30 02:45:09 krw Exp $	*/
d737 8
a744 23
			/* if fd is closing then reschedule this thread */
			for (i = 0; i < pthread->data.poll_data->nfds; i++) {
				if (_thread_fd_table[pthread->data.poll_data->fds[i].fd]->state == FD_ENTRY_CLOSING) {
					pthread->closing_fd = 1;
					PTHREAD_WAITQ_CLEARACTIVE();
					PTHREAD_WORKQ_REMOVE(pthread);
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
					PTHREAD_WAITQ_SETACTIVE();
					break;
				}
			}
			/* if the thread was not rescheduled by a closing fd then add files */
			if (pthread->closing_fd == 0) {
				/* Limit number of polled files to table size: */
				if (pthread->data.poll_data->nfds + nfds <
				    _thread_dtablesize) {
					for (i = 0; i < pthread->data.poll_data->nfds; i++) {
						_thread_pfd_table[nfds + i].fd =
						    pthread->data.poll_data->fds[i].fd;
						_thread_pfd_table[nfds + i].events =
						    pthread->data.poll_data->fds[i].events;
					}
					nfds += pthread->data.poll_data->nfds;
d746 1
@


1.31
log
@Don't use TAILQ_NEXT() on an element that has been removed. Similar to
otto@@'s diff for uvm_aobj.c.

Identical to a diff canacar@@ developed independantly.

ok brad@@ 'looks correct' fgsch@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.30 2005/01/28 20:35:49 marc Exp $	*/
d698 14
a711 5
			/* Limit number of polled files to table size: */
			if (nfds < _thread_dtablesize) {
				_thread_pfd_table[nfds].events = POLLRDNORM;
				_thread_pfd_table[nfds].fd = pthread->data.fd.fd;
				nfds++;
d717 14
a730 5
			/* Limit number of polled files to table size: */
			if (nfds < _thread_dtablesize) {
				_thread_pfd_table[nfds].events = POLLWRNORM;
				_thread_pfd_table[nfds].fd = pthread->data.fd.fd;
				nfds++;
d737 23
a759 8
			/* Limit number of polled files to table size: */
			if (pthread->data.poll_data->nfds + nfds <
			    _thread_dtablesize) {
				for (i = 0; i < pthread->data.poll_data->nfds; i++) {
					_thread_pfd_table[nfds + i].fd =
					    pthread->data.poll_data->fds[i].fd;
					_thread_pfd_table[nfds + i].events =
					    pthread->data.poll_data->fds[i].events;
a760 1
				nfds += pthread->data.poll_data->nfds;
@


1.30
log
@Grab the current time before calculating thread wake-up time.
Solves a problem where select/poll calls would return early
without indicating any error.   Tested by otto@@ and kurt@@ -- thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.29 2004/04/13 01:09:55 marc Exp $	*/
d592 1
a592 1
	struct pthread	*pthread;
d673 2
a674 1
	TAILQ_FOREACH(pthread, &_workq, qe) {
d786 3
a788 1
		TAILQ_FOREACH(pthread, &_workq, qe) {
d882 3
a884 1
		TAILQ_FOREACH(pthread, &_workq, qe) {
@


1.29
log
@
refresh curthread after a thread switch so it points to the real
current thread.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.28 2004/04/09 23:57:17 brad Exp $	*/
d278 6
a283 6
		    (pthread->wakeup_time.tv_sec != -1) &&
		    (((pthread->wakeup_time.tv_sec == 0) &&
		    (pthread->wakeup_time.tv_nsec == 0)) ||
		    (pthread->wakeup_time.tv_sec < ts.tv_sec) ||
		    ((pthread->wakeup_time.tv_sec == ts.tv_sec) &&
		    (pthread->wakeup_time.tv_nsec <= ts.tv_nsec)))) {
d940 1
a940 1
		/* Get the current time: */
d943 1
a943 11

		/* Calculate the time for the current thread to wake up: */
		curthread->wakeup_time.tv_sec = current_time.tv_sec + timeout->tv_sec;
		curthread->wakeup_time.tv_nsec = current_time.tv_nsec + timeout->tv_nsec;

		/* Check if the nanosecond field needs to wrap: */
		if (curthread->wakeup_time.tv_nsec >= 1000000000) {
			/* Wrap the nanosecond field: */
			curthread->wakeup_time.tv_sec += 1;
			curthread->wakeup_time.tv_nsec -= 1000000000;
		}
@


1.28
log
@When poll(2)'ing for readability or writability of a file descriptor
on behalf of a thread, we should check the POLLERR, POLLHUP, and
POLLNVAL flags as well to wake up the thread in these cases.

From: FreeBSD's libc_r

ok marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.27 2003/05/13 16:49:32 marc Exp $	*/
d473 10
@


1.27
log
@
Add support for blocking thread switches during dlopen and other
non-thread-safe dl functions.   Only enabled for ELF architectures
at this time as needed dlxxx support has not yet been added to the
a.out run time loader.

'doesn't break xmms at least' tedu@@.   Tested by others with no comment
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.26 2003/01/31 04:46:17 marc Exp $	*/
d798 3
a800 1
				    (_thread_pfd_table[nfds].revents & POLLRDNORM)) {
d812 3
a814 1
				    (_thread_pfd_table[nfds].revents & POLLWRNORM)) {
@


1.27.4.1
log
@MFC:
Fix by marc@@

refresh curthread after a thread switch so it points to the real
current thread.

ok deraadt@@ marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.27 2003/05/13 16:49:32 marc Exp $	*/
a472 10

			/*
			 * DANGER WILL ROBINSON
			 * All stack local variables now contain the values
			 * they had when this thread was last running.  In
			 * particular, curthread is NOT pointing to the
			 * current thread.   Make it point to the current
			 * before use.
			 */
			curthread = _get_curthread();
@


1.26
log
@
Create a siginfo_t for thread-to-thread kill.
Clean up (compiler warning elimination).   Compile check options added
but commented out as they have not been checked on all architectures, yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.25 2003/01/27 22:22:30 marc Exp $	*/
d942 16
@


1.25
log
@pthreads signal handling improvements.   With these changes all
of the thread regressions tests pass on i386, sparc, sparc64
(save the siginfo test on sparc64 due to a kernel issue) and alpha.
The tests should also pass on ppc.

In addition, it fixes the problems with the "mysql torture test"
provided by one of our users.   The python port also appears to
work correctly with these changes.

Summary of changes:

* check_pending removed from thread structure, no longer used.
* unused elements of sigstatus structure removed.   The sigstatus
  structure is now used to keep track of siginfo data instead of
  defining a local array in uthread_sig.c.
* _thread_kern_sched_sig removed
* _thread_sig_process goes away -- can't have a lock active when
  signal handlers are called.   Functions now call _thread_sig_handle
  directly.
* _thread_clear_pending now used lib wide to clear pending flags.
  It was named _clean_pending_flag and only used in uthread_sig.c.
  The function clears both per thread signals, and per process signals.
* _thread_sig_handle now returns a value.
* unused debugging macros removed from the thread kernel
* dispatch pending signals after switching to a thread providing
  that signal handling hasn't been deferred by the thread.
* process thread switchhooks before dispatching pending sigs
* check for thread cancellation before dispatching pending sigs
* re-wrote pthread-kill to do the correct thing.   It now does
  minimal thread-kill-specific processing and then calls the
  existing code in uthread_sig to process the generated signal.
* shut the compiler up when compiling uthread_mutex.c
* no more "signal_lock".   It does more harm than good.
* keep track of "per-process" signals.
* don't bother saving siginfo_t data for the scheduling signal.
* per posix: SIGSTOP cleared when SIGCONT received and vice versa.
* add _dispatch_signal to properly dispatch a signal to a thread.
  It sets the appropriate signal mask, something that was missing
  in the previous implementation.   This fixes several bugs.
  The previous method held a lock.  If the signal handler longjmp-ed
  the lock was never cleared and no more signals were processed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.24 2003/01/24 21:03:15 marc Exp $	*/
d62 1
a62 1
static int	last_tick = 0;
d516 2
a517 1
_thread_kern_sched_state(enum pthread_state state, char *fname, int lineno)
d544 2
a545 2
_thread_kern_sched_state_unlock(enum pthread_state state,
    spinlock_t *lock, char *fname, int lineno)
@


1.24
log
@
save and restore fp state when switching threads.   This, with
an arch/i386 patch previously commited and arch/sparc64 patches
from jason@@ make the preemption_float test pass on those two
architectures.

Do not run signal handlers for a thread until the thread has
been made current, ensuring the proper context.   Solves several
(if not all) of the '_pq_insert_tail: Already in priority queue'
problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.23 2002/11/04 21:28:49 marc Exp $	*/
a52 6
#if defined(PTHREAD_TRACE_KERN)
#define PTHREAD_TRACE(x)	_thread_sys_write(-1, (void*) x, 0)
#else
#define PTHREAD_TRACE(x)
#endif

a64 10
_thread_kern_sched_sig(void)
{
	struct pthread	*curthread = _get_curthread();

	PTHREAD_TRACE(1);
	curthread->check_pending = 1;
	_thread_kern_sched(NULL);
}

void
a74 2
	PTHREAD_TRACE(2);

a241 17

			/*
			 * Are there pending signals for this thread?
			 *
			 * This check has to be performed after the thread
			 * has been placed in the queue(s) appropriate for
			 * its state.  The process of adding pending signals
			 * can change a threads state, which in turn will
			 * attempt to add or remove the thread from any
			 * scheduling queue to which it belongs.
			 */
#ifdef notyet
			if (curthread->check_pending != 0) {
				curthread->check_pending = 0;
				_thread_sig_check_pending(curthread);
			}
#endif
d470 1
a470 3
			/*
			 * Restore the new thread, saving current.
			 */
a472 5

			/*
			 * Process any pending signals for the thread we
			 * just switched to.
			 */
a473 1
			_dispatch_signals(scp);
d489 4
a519 1
	PTHREAD_TRACE(3);
a547 2
	PTHREAD_TRACE(4);

a584 2
	PTHREAD_TRACE(5);

a906 2
	PTHREAD_TRACE(6);

a946 2
	PTHREAD_TRACE(7);

a955 2
	PTHREAD_TRACE(8);

a1001 2
	PTHREAD_TRACE(9);

d1013 2
a1014 10
			if ((int) bufr[i] == _SCHED_SIGNAL) {
				/*
				 * Scheduling signals shouldn't ever be
				 * queued; just ignore it for now.
				 */
			}
			else {
				/* Handle this signal: */
				_thread_sig_process((int) bufr[i], NULL);
			}
a1030 2

	PTHREAD_TRACE(10);
@


1.23
log
@
test locks against _SPINLOCK_UNLOCKED, not 0.   _SPINLOCK_UNLOCKED is
not zero on all arches
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.22 2002/10/30 19:11:56 marc Exp $	*/
a117 5
		/*
		 * Save floating point state.
		 */
		_thread_machdep_save_float_state(&curthread->_machdep);

d128 3
d502 3
d509 1
a509 1
			    &old_thread_run->_machdep);
d511 6
a516 6
			/* Check if a signal context was saved: */
			if (curthread->sig_saved == 1) {
				/*
				 * Restore floating point state.
				 */
				_thread_machdep_restore_float_state(&curthread->_machdep);
d518 6
a523 5
				/*
				 * Do a sigreturn to restart the thread that
				 * was interrupted by a signal:
				 */
				_thread_kern_in_sched = 0;
d525 6
a530 15
				/*
				 * If we had a context switch, run any
				 * installed switch hooks.
				 */
				if ((_sched_switch_hook != NULL) &&
				    (_last_user_thread != curthread)) {
					_thread_run_switch_hook(_last_user_thread,
					    curthread);
				}

				if (((curthread->cancelflags &
				    PTHREAD_AT_CANCEL_POINT) == 0) &&
				    ((curthread->cancelflags &
				     PTHREAD_CANCEL_ASYNCHRONOUS) != 0))
					pthread_testcancel();
d532 2
d541 1
a541 16
				/*
				 * This is the normal way out of the scheduler.
				 */
				_thread_kern_in_sched = 0;

				if (_sched_switch_hook != NULL) {
					/* Run the installed switch hook: */
					_thread_run_switch_hook(_last_user_thread,
					    curthread);
				}

				if (((curthread->cancelflags &
				    PTHREAD_AT_CANCEL_POINT) == 0) &&
				    ((curthread->cancelflags &
				     PTHREAD_CANCEL_ASYNCHRONOUS) != 0))
					pthread_testcancel();
@


1.22
log
@signal handling changes.   This corrects several signal
handling errors in the threads library.   Most of the libc_r regression
tests are now ok.   thread specific kill semantics are still not correct.
No negative comments after posting to tech@@ a week or so ago.
siginfo test fails on sparc64 due to sparc64 oddity.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.21 2002/02/21 20:57:41 fgsch Exp $	*/
d732 2
a733 1
			if (pthread->data.spinlock->access_lock == 0) {
d844 2
a845 1
				if (pthread->data.spinlock->access_lock == 0) {
d933 2
a934 1
				if (pthread->data.spinlock->access_lock == 0) {
@


1.21
log
@account for the process signal mask when dealing with signals; tested
a while ago by marc@@ and brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.20 2002/01/04 03:39:09 fgsch Exp $	*/
d53 5
a57 3
/* Static function prototype definitions: */
static void
thread_kern_poll(int wait_reqd);
d59 7
a65 5
static void
dequeue_signals(void);

static inline void
thread_run_switch_hook(pthread_t thread_out, pthread_t thread_in);
d75 1
d91 2
d301 1
a301 1
			thread_kern_poll(0);
d419 1
a419 1
			thread_kern_poll(1);
d455 1
a455 1
				dequeue_signals();
d529 1
a529 1
					thread_run_switch_hook(_last_user_thread,
d553 1
a553 1
					thread_run_switch_hook(_last_user_thread,
d579 1
d608 2
d635 2
a636 2
static void
thread_kern_poll(int wait_reqd)
d647 2
d698 1
a698 2

		dequeue_signals();
d823 1
a823 2

			dequeue_signals();
d956 1
a956 4

		dequeue_signals();

		/* Unprotect the scheduling queues: */
d968 2
d1010 2
d1021 2
d1063 2
a1064 2
static void
dequeue_signals(void)
d1069 2
d1090 1
a1090 1
				_thread_sig_handle((int) bufr[i], NULL);
d1103 2
a1104 2
static inline void
thread_run_switch_hook(pthread_t thread_out, pthread_t thread_in)
d1108 2
@


1.20
log
@comment out the pending signals check by now; it was not suppose to be
there yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.19 2001/12/31 18:23:15 fgsch Exp $	*/
d67 9
d95 9
@


1.19
log
@More changes from FreeBSD, including:

o Only poll file descriptors when needed.
o Change the way timing is achieved, counting scheduling ticks
  instead of calculating the elapsed time via gettimeofday().
o Prevent an overflow when polling.
o Use curthread instead of _thread_run.
o Remove extra spaces; indent.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.18 2001/12/18 03:47:52 marc Exp $	*/
d248 1
d253 1
@


1.18
log
@
Allow the scheduler to return to the signal handler and the signal
handler to return instead of calling sigreturn directly.   This works
around an apparent bug in sparc sigreturn handling. ok fgs@@ and noone
else has bitched
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.17 2001/12/08 14:51:36 fgsch Exp $	*/
d37 1
a40 1
#include <poll.h>
d54 2
a55 2
static void 
_thread_kern_poll(int wait_reqd);
d63 3
d69 2
d72 3
a74 1
	pthread_t       pthread, pthread_h = NULL;
a75 4
	struct itimerval itimer;
	struct timespec ts, ts1;
	struct timeval  tv, tv1;
	int		set_timer = 0;
d88 1
a88 1
		 * buffer: 
d90 2
a91 1
		memcpy(&_thread_run->saved_sigcontext, scp, sizeof(_thread_run->saved_sigcontext));
d96 1
a96 1
		_thread_machdep_save_float_state(&_thread_run->_machdep);
d99 1
a99 1
		_thread_run->sig_saved = 1;
d102 1
a102 1
		_thread_run->sig_saved = 0;
d105 2
a106 2
	if ((_thread_run->flags & PTHREAD_FLAGS_PRIVATE) == 0)
		_last_user_thread = _thread_run;
d109 1
a109 1
	_thread_run->error = errno;
d112 1
a112 1
	old_thread_run = _thread_run;
d123 1
a123 1
		gettimeofday(&tv, NULL);
d125 1
d132 1
d134 1
a134 2
		if (_thread_run != &_thread_kern_thread) {

d138 1
a138 8
			_thread_run->yield_on_sig_undefer = 0;
	
			/*
			 * Save the current time as the time that the thread
			 * became inactive: 
			 */
			_thread_run->last_inactive.tv_sec = tv.tv_sec;
			_thread_run->last_inactive.tv_usec = tv.tv_usec;
d140 14
d158 1
a158 1
			switch (_thread_run->state) {
d160 2
d163 2
a164 1
				 * Dead threads are not placed in any queue:
d175 1
a175 16
				if ((_thread_run->slice_usec != -1) &&
				    (_thread_run->attr.sched_policy != SCHED_FIFO)) {
					/*
					 * Accumulate the number of microseconds that
					 * this thread has run for:
					 */
					_thread_run->slice_usec +=
					    (_thread_run->last_inactive.tv_sec -
					    _thread_run->last_active.tv_sec) * 1000000 +
					    _thread_run->last_inactive.tv_usec -
					    _thread_run->last_active.tv_usec;
	
					/* Check for time quantum exceeded: */
					if (_thread_run->slice_usec > TIMESLICE_USEC)
						_thread_run->slice_usec = -1;
				}
d180 1
a180 1
			 * operations or timeouts: 
a190 1
			case PS_SUSPENDED:
d193 2
a194 2
				_thread_run->wakeup_time.tv_sec = -1;
				_thread_run->wakeup_time.tv_nsec = -1;
d197 1
a197 1
				_thread_run->slice_usec = -1;
d200 1
a200 1
				PTHREAD_WAITQ_INSERT(_thread_run);
d207 1
a207 1
				_thread_run->slice_usec = -1;
d210 1
a210 1
				PTHREAD_WAITQ_INSERT(_thread_run);
d216 2
a217 2
				_thread_run->wakeup_time.tv_sec = -1;
				_thread_run->wakeup_time.tv_nsec = -1;
d222 1
a222 1
				/* fall through */
d228 1
a228 1
				_thread_run->slice_usec = -1;
d231 1
a231 1
				PTHREAD_WAITQ_INSERT(_thread_run);
d234 17
a250 1
				PTHREAD_WORKQ_INSERT(_thread_run);
a253 3
		/* Unprotect the scheduling queues: */
		_queue_signals = 0;

d255 8
a262 2
		 * Poll file descriptors to update the state of threads
		 * waiting on file I/O where data may be available: 
d264 11
a274 1
		_thread_kern_poll(0);
d276 4
a279 2
		/* Protect the scheduling queues: */
		_queue_signals = 1;
d321 2
a322 2
		 * Check if there is a current runnable thread that isn't
		 * already in the ready queue:
d324 29
a352 4
		if ((_thread_run != &_thread_kern_thread) &&
		    (_thread_run->state == PS_RUNNING) &&
		    ((_thread_run->flags & PTHREAD_FLAGS_IN_PRIOQ) == 0)) {
			if (_thread_run->slice_usec == -1) {
d359 1
a359 1
				PTHREAD_PRIOQ_INSERT_TAIL(_thread_run);
d366 1
a366 1
				PTHREAD_PRIOQ_INSERT_HEAD(_thread_run);
d380 1
a380 1
			 * thread structure: 
d390 1
a390 1
			 * something happens that changes this condition: 
d392 16
a407 1
			_thread_kern_poll(1);
d409 2
a410 1
		else {
a413 10
			/* Get first thread on the waiting list: */
			pthread = TAILQ_FIRST(&_waitingq);

			/* Check to see if there is more than one thread: */
			if (pthread_h != TAILQ_FIRST(&_thread_list) ||
			    TAILQ_NEXT(pthread_h, tle) != NULL)
				set_timer = 1;
			else
				set_timer = 0;

d436 3
a444 3
					/* Remove the thread from the ready queue: */
					PTHREAD_PRIOQ_REMOVE(pthread);

a448 13
				/* Get first thread on the waiting list: */
				pthread = TAILQ_FIRST(&_waitingq);

				/*
				 * Check to see if there is more than one
				 * thread:
				 */
				if (pthread_h != TAILQ_FIRST(&_thread_list) ||
				    TAILQ_NEXT(pthread_h, tle) != NULL)
					set_timer = 1;
				else
					set_timer = 0;

d459 1
a459 16
			 * became active: 
			 */
			curthread->last_active.tv_sec = tv.tv_sec;
			curthread->last_active.tv_usec = tv.tv_usec;

			/*
			 * Define the maximum time before a scheduling signal
			 * is required: 
			 */
			itimer.it_value.tv_sec = 0;
			itimer.it_value.tv_usec = TIMESLICE_USEC;

			/*
			 * The interval timer is not reloaded when it
			 * times out. The interval time needs to be
			 * calculated every time. 
d461 2
a462 57
			itimer.it_interval.tv_sec = 0;
			itimer.it_interval.tv_usec = 0;

			/* Get first thread on the waiting list: */
			if ((pthread != NULL) &&
			    (pthread->wakeup_time.tv_sec != -1)) {
				/*
				 * Calculate the time until this thread
				 * is ready, allowing for the clock
				 * resolution: 
				 */
				ts1.tv_sec = pthread->wakeup_time.tv_sec
				    - ts.tv_sec;
				ts1.tv_nsec = pthread->wakeup_time.tv_nsec
				    - ts.tv_nsec + _clock_res_nsec;

				/*
				 * Check for underflow of the nanosecond field:
				 */
				while (ts1.tv_nsec < 0) {
					/*
					 * Allow for the underflow of the
					 * nanosecond field: 
					 */
					ts1.tv_sec--;
					ts1.tv_nsec += 1000000000;
				}
				/*
				 * Check for overflow of the nanosecond field: 
				 */
				while (ts1.tv_nsec >= 1000000000) {
					/*
					 * Allow for the overflow of the
					 * nanosecond field: 
					 */
					ts1.tv_sec++;
					ts1.tv_nsec -= 1000000000;
				}
				/*
				 * Convert the timespec structure to a
				 * timeval structure: 
				 */
				TIMESPEC_TO_TIMEVAL(&tv1, &ts1);

				/*
				 * Check if the thread will be ready
				 * sooner than the earliest ones found
				 * so far: 
				 */
				if (timercmp(&tv1, &itimer.it_value, <)) {
					/*
					 * Update the time value: 
					 */
					itimer.it_value.tv_sec = tv1.tv_sec;
					itimer.it_value.tv_usec = tv1.tv_usec;
				}
			}
d467 1
a467 1
			 * allocation: 
d469 1
a469 1
			if (_thread_run->slice_usec == -1) {
d471 1
a471 16
				_thread_run->slice_usec = 0;
			}

			/* Check if there is more than one thread: */
			if (set_timer != 0) {
				/*
				 * Start the interval timer for the
				 * calculated time interval: 
				 */
				if (setitimer(_ITIMER_SCHED_TIMER, &itimer, NULL) != 0) {
					/*
					 * Cannot initialise the timer, so
					 * abort this process: 
					 */
					PANIC("Cannot set scheduling timer");
				}
d475 1
a475 1
			errno = _thread_run->error;
d480 1
a480 1
			_thread_machdep_switch(&_thread_run->_machdep,
d484 1
a484 1
			if (_thread_run->sig_saved == 1) {
d488 1
a488 1
				_thread_machdep_restore_float_state(&_thread_run->_machdep);
d492 1
a492 1
				 * was interrupted by a signal: 
d501 1
a501 1
				    (_last_user_thread != _thread_run)) {
d503 1
a503 1
					    _thread_run);
d514 1
a514 1
				   _thread_sys_sigreturn(&_thread_run->saved_sigcontext);
d527 1
a527 1
					    _thread_run);
d606 1
a606 1
_thread_kern_poll(int wait_reqd)
d623 1
a623 1
		gettimeofday(&tv, NULL);
d635 1
a635 1
			timeout_ms = -1;
d637 3
d643 1
a643 1
			 * timeout allowing for the clock resolution:
d646 2
a647 2
			    1000) + ((pthread->wakeup_time.tv_nsec - ts.tv_nsec +
			    _clock_res_nsec) / 1000000);
d761 1
a761 1
	 * an exception, or a timeout to occur: 
d768 1
a768 1
		 * from the pollfd table: 
d782 1
a782 1
		 * count: 
d804 1
a804 1
		 * _poll syscall: 
a931 3

	/* Nothing to return. */
	return;
d948 1
a948 1
		 * different to an actual time of day: 
d960 1
a960 1
		gettimeofday(&tv, NULL);
a973 1
	return;
d988 1
a988 3
        struct pthread  *curthread = _get_curthread();
	pthread_t pthread;
	int need_resched = 0;
d1005 2
a1006 24
		while (_sigq_check_reqd != 0) {
			/* Defer scheduling while we process queued signals: */
			curthread->sig_defer_count = 1;

			/* Clear the flag before checking the signal queue: */
			_sigq_check_reqd = 0;

			/* Dequeue and handle signals: */
			dequeue_signals();

			/*
			 * Avoiding an unnecessary check to reschedule, check
			 * to see if signal handling caused a higher priority
			 * thread to become ready.
			 */
			if ((need_resched == 0) &&
			    (((pthread = PTHREAD_PRIOQ_FIRST()) != NULL) &&
			    (pthread->active_priority > curthread->active_priority))) {
				need_resched = 1;
			}

			/* Reenable signals: */
			curthread->sig_defer_count = 0;
		}
d1016 8
a1023 2
		/* Yield the CPU if necessary: */
		if (need_resched || curthread->yield_on_sig_undefer != 0) {
d1038 1
a1038 1
	 * pthread kernel pipe: 
d1075 1
a1075 1
	    ((tid_out->flags & PTHREAD_FLAGS_PRIVATE) != 0))
d1078 1
a1078 1
	    ((tid_in->flags & PTHREAD_FLAGS_PRIVATE) != 0))
@


1.17
log
@Partially sync with FreeBSD; mostly pthread_cancel(3) related changes.
make includes is needed in case you want to play.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.16 2001/09/04 22:17:45 fgsch Exp $	*/
d549 6
a554 1
				_thread_sys_sigreturn(&_thread_run->saved_sigcontext);
@


1.16
log
@put changes back, this time ALL the files.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.14 2001/08/30 07:40:47 fgsch Exp $	*/
a62 12
static void
_thread_check_cancel()
{
	if (!(_thread_run->flags & PTHREAD_FLAGS_CANCELPT) &&
	    (_thread_run->canceltype == PTHREAD_CANCEL_ASYNCHRONOUS))
		/*
		 * Check if an async-cancellable thread
		 * has been cancelled.
		 */
		_thread_cancellation_point();
}

d542 7
a548 1
				_thread_check_cancel();
d562 5
a566 1
				_thread_check_cancel();
d1064 8
@


1.15
log
@Back out fgsch@@'s tree breaking commits.
Test next time, ok?
@
text
@d582 1
a582 1
_thread_kern_sched_state(enum pthread_state state, const char *fname, int lineno)
@


1.14
log
@fix some const warnings.
more sync with freebsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.13 2001/08/21 19:24:53 fgsch Exp $	*/
d582 1
a582 1
_thread_kern_sched_state(enum pthread_state state, char *fname, int lineno)
@


1.13
log
@Start syncing with FreeBSD:

o Implement _get_curthread() and _set_curthread(). Use it where possible.
o Add missing _thread_[enter|leave]_cancellation_point().
o Add a couple of not yet used vars to pthread_private.h.
o Remove return's from void functions.

This is by no means complete, but instead of doing a big commit, i'll
split it in small ones, minimizing diffs.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.12 2001/08/15 23:53:17 fgsch Exp $	*/
d582 1
a582 1
_thread_kern_sched_state(enum pthread_state state, const char *fname, int lineno)
@


1.12
log
@ops, _thread_kern_set_timeout() to const.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.11 2001/01/16 04:51:07 d Exp $	*/
d78 1
d339 2
a340 1
			_thread_run = &_thread_kern_thread;
d418 2
a419 1
			_thread_run = pthread_h;
d425 2
a426 2
			_thread_run->last_active.tv_sec = tv.tv_sec;
			_thread_run->last_active.tv_usec = tv.tv_usec;
d584 2
d600 3
a602 3
	_thread_run->state = state;
	_thread_run->fname = fname;
	_thread_run->lineno = lineno;
a605 1
	return;
d612 2
d629 3
a631 3
	_thread_run->state = state;
	_thread_run->fname = fname;
	_thread_run->lineno = lineno;
a636 1
	return;
d971 1
d976 1
a976 1
	_thread_run->timeout = 0;
d984 2
a985 2
		_thread_run->wakeup_time.tv_sec = -1;
		_thread_run->wakeup_time.tv_nsec = -1;
d990 2
a991 2
		_thread_run->wakeup_time.tv_sec = 0;
		_thread_run->wakeup_time.tv_nsec = 0;
d998 2
a999 2
		_thread_run->wakeup_time.tv_sec = current_time.tv_sec + timeout->tv_sec;
		_thread_run->wakeup_time.tv_nsec = current_time.tv_nsec + timeout->tv_nsec;
d1002 1
a1002 1
		if (_thread_run->wakeup_time.tv_nsec >= 1000000000) {
d1004 2
a1005 2
			_thread_run->wakeup_time.tv_sec += 1;
			_thread_run->wakeup_time.tv_nsec -= 1000000000;
d1014 2
d1017 1
a1017 1
	_thread_run->sig_defer_count++;
d1023 1
d1031 1
a1031 1
	if (_thread_run->sig_defer_count > 1) {
d1033 1
a1033 1
		_thread_run->sig_defer_count--;
d1035 1
a1035 1
	else if (_thread_run->sig_defer_count == 1) {
d1037 1
a1037 1
		_thread_run->sig_defer_count = 0;
d1044 1
a1044 1
			_thread_run->sig_defer_count = 1;
d1059 1
a1059 1
			    (pthread->active_priority > _thread_run->active_priority))) {
d1064 1
a1064 1
			_thread_run->sig_defer_count = 0;
d1068 2
a1069 2
		if (need_resched || _thread_run->yield_on_sig_undefer != 0) {
			_thread_run->yield_on_sig_undefer = 0;
d1130 15
@


1.11
log
@switch stacks even when restoring a preempted thread. (credit to pefo@@)
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.10 2000/10/04 05:55:35 d Exp $	*/
d964 1
a964 1
_thread_kern_set_timeout(struct timespec * timeout)
@


1.10
log
@switch to _machdep_switch() instead of setjmp/longjmp. For some reason this fixes sparc threads.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.9 1999/11/25 07:01:37 d Exp $	*/
d523 6
d551 1
a565 10

				/*
				 * Resume thread _thread_run.
				 */
				_thread_machdep_switch(&_thread_run->_machdep,
					&old_thread_run->_machdep);
				/*
				 * This thread is now the new _thread_run
				 * again.
				 */
a566 1

@


1.10.2.1
log
@Pull in patch from current, requested by jasoni@@ and d@@
Fix(d):
switch stacks even when restoring a preempted thread. (credit to pefo@@)
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_kern.c,v 1.10 2000/10/04 05:55:35 d Exp $	*/
a522 6
			/*
			 * Restore the new thread, saving current.
			 */
			_thread_machdep_switch(&_thread_run->_machdep,
			    &old_thread_run->_machdep);

a544 1
				_thread_check_cancel();
d559 10
d570 1
@


1.9
log
@sync with FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a41 1
#include <setjmp.h>
d79 1
d103 1
a103 1
		_thread_machdep_save_float_state(_thread_run);
a106 19
	}
	/* Save the state of the current thread: */
	else if (_thread_machdep_setjmp(_thread_run->saved_jmp_buf) != 0) {
		/*
		 * This point is reached when a longjmp() is called to
		 * restore the state of a thread. 
		 *
		 * This is the normal way out of the scheduler.
		 */
		_thread_kern_in_sched = 0;

		if (_sched_switch_hook != NULL) {
			/* Run the installed switch hook: */
			thread_run_switch_hook(_last_user_thread, _thread_run);
		}

		_thread_check_cancel();

		return;
d118 3
d126 1
a126 1
	 * longjmp (if the state was saved by a setjmp). 
d528 1
a528 1
				_thread_machdep_restore_float_state(_thread_run);
d548 1
a548 3
				 * Do a longjmp to restart the thread that
				 * was context switched out (by a longjmp to
				 * a different thread): 
d550 21
a570 1
				_thread_machdep_longjmp(_thread_run->saved_jmp_buf, 1);
d574 1
a574 1
			PANIC("Thread has returned from sigreturn or longjmp");
@


1.8
log
@sync with FreeBSD
@
text
@d33 1
a33 1
 * $FreeBSD: uthread_kern.c,v 1.18 1999/05/08 07:50:05 jasone Exp $
d40 1
a46 3
#ifdef _THREAD_RUSAGE
#include <sys/resource.h>
#endif
d56 4
a59 1
_thread_kern_select(int wait_reqd);
d64 12
d79 1
a79 3
	pthread_t       pthread;
	pthread_t       pthread_h = NULL;
	pthread_t	last_thread = NULL;
d81 3
a83 8
	struct timespec ts;
	struct timespec ts1;
	struct timeval  tv;
	struct timeval  tv1;
#ifdef _THREAD_RUSAGE
	struct rusage	ru;
	static struct rusage ru_prev;
#endif
d100 3
a102 1
		/* Save the floating point data: */
d123 1
a123 19
		if (!(_thread_run->flags & PTHREAD_AT_CANCEL_POINT) &&
		    (_thread_run->canceltype == PTHREAD_CANCEL_ASYNCHRONOUS)) {
			/* 
			 * Cancelations override signals.
			 *
			 * Stick a cancellation point at the start of
			 * each async-cancellable thread's resumption.
			 *
			 * We allow threads woken at cancel points to do their
			 * own checks.
			 */
			_thread_cancellation_point();
		}

		/*
		 * There might be pending signals for this thread, so
		 * dispatch any that aren't blocked:
		 */
		_dispatch_signals();
a136 12
#ifdef _THREAD_RUSAGE
	/* Accumulate time spent */
	if (getrusage(RUSAGE_SELF, &ru))
		PANIC("Cannot get resource usage");
	timersub(&ru.ru_utime, &ru_prev.ru_utime, &tv);
	timeradd(&tv, &_thread_run->ru_utime, &_thread_run->ru_utime);
	timersub(&ru.ru_stime, &ru_prev.ru_stime, &tv);
	timeradd(&tv, &_thread_run->ru_stime, &_thread_run->ru_stime);
	memcpy(&ru_prev.ru_utime, &ru.ru_utime, sizeof ru_prev.ru_utime);
	memcpy(&ru_prev.ru_stime, &ru.ru_stime, sizeof ru_prev.ru_stime);
#endif /* _THREAD_RUSAGE */

d144 1
a144 1
	while (_thread_link_list != NULL) {
d150 2
a151 2
		 * Poll file descriptors to update the state of threads
		 * waiting on file I/O where data may be available: 
d153 1
a153 1
		_thread_kern_select(0);
d155 1
a155 27
		/*
		 * Define the maximum time before a scheduling signal
		 * is required: 
		 */
		itimer.it_value.tv_sec = 0;
		itimer.it_value.tv_usec = TIMESLICE_USEC;

		/*
		 * The interval timer is not reloaded when it
		 * times out. The interval time needs to be
		 * calculated every time. 
		 */
		itimer.it_interval.tv_sec = 0;
		itimer.it_interval.tv_usec = 0;

		/*
		 * Enter a loop to look for sleeping threads that are ready
		 * or timedout.  While we're at it, also find the smallest
		 * timeout value for threads waiting for a time.
		 */
		_waitingq_check_reqd = 0;	/* reset flag before loop */
		TAILQ_FOREACH(pthread, &_waitingq, pqe) {
			/* Check if this thread is ready: */
			if (pthread->state == PS_RUNNING) {
				PTHREAD_WAITQ_REMOVE(pthread);
				PTHREAD_PRIOQ_INSERT_TAIL(pthread);
			}
d158 14
a171 2
			 * Check if this thread is blocked by an
			 * atomic lock:
d173 2
a174 1
			else if (pthread->state == PS_SPINBLOCK) {
d176 1
a176 2
				 * If the lock is available, let
				 * the thread run.
d178 24
a201 2
				if (pthread->data.spinlock->access_lock == 0) {
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
d203 1
d205 21
a225 39
			/* Check if this thread is to timeout: */
			} else if (pthread->state == PS_COND_WAIT ||
			    pthread->state == PS_SLEEP_WAIT ||
			    pthread->state == PS_FDR_WAIT ||
			    pthread->state == PS_FDW_WAIT ||
			    pthread->state == PS_SELECT_WAIT) {
				/* Check if this thread is to wait forever: */
				if (pthread->wakeup_time.tv_sec == -1) {
				}
				/*
				 * Check if this thread is to wakeup
				 * immediately or if it is past its wakeup
				 * time: 
				 */
				else if ((pthread->wakeup_time.tv_sec == 0 &&
					pthread->wakeup_time.tv_nsec == 0) ||
					 (ts.tv_sec > pthread->wakeup_time.tv_sec) ||
					 ((ts.tv_sec == pthread->wakeup_time.tv_sec) &&
					  (ts.tv_nsec >= pthread->wakeup_time.tv_nsec))) {
					/*
					 * Check if this thread is waiting on
					 * select: 
					 */
					if (pthread->state == PS_SELECT_WAIT) {
						/*
						 * The select has timed out, so
						 * zero the file descriptor
						 * sets: 
						 */
						FD_ZERO(&pthread->data.select_data->readfds);
						FD_ZERO(&pthread->data.select_data->writefds);
						FD_ZERO(&pthread->data.select_data->exceptfds);
						pthread->data.select_data->nfds = 0;
					}
					/*
					 * Return an error as an interrupted
					 * wait: 
					 */
					_thread_seterrno(pthread, EINTR);
d227 3
a229 5
					/*
					 * Flag the timeout in the thread
					 * structure: 
					 */
					pthread->timeout = 1;
d231 5
a235 15
					/*
					 * Change the threads state to allow
					 * it to be restarted: 
					 */
					PTHREAD_NEW_STATE(pthread,PS_RUNNING);
				} else {
					/*
					 * Calculate the time until this thread
					 * is ready, allowing for the clock
					 * resolution: 
					 */
					ts1.tv_sec = pthread->wakeup_time.tv_sec
					    - ts.tv_sec;
					ts1.tv_nsec = pthread->wakeup_time.tv_nsec
					    - ts.tv_nsec + CLOCK_RES_NSEC;
d237 9
a245 29
					/*
					 * Check for underflow of the
					 * nanosecond field: 
					 */
					if (ts1.tv_nsec < 0) {
						/*
						 * Allow for the underflow
						 * of the nanosecond field: 
						 */
						ts1.tv_sec--;
						ts1.tv_nsec += 1000000000;
					}
					/*
					 * Check for overflow of the nanosecond
					 * field: 
					 */
					if (ts1.tv_nsec >= 1000000000) {
						/*
						 * Allow for the overflow of
						 * the nanosecond field: 
						 */
						ts1.tv_sec++;
						ts1.tv_nsec -= 1000000000;
					}
					/*
					 * Convert the timespec structure
					 * to a timeval structure: 
					 */
					TIMESPEC_TO_TIMEVAL(&tv1, &ts1);
d247 2
a248 13
					/*
					 * Check if the thread will be ready
					 * sooner than the earliest ones found
					 * so far: 
					 */
					if (timercmp(&tv1, &itimer.it_value, <)) {
						/*
						 * Update the time value: 
						 */
						itimer.it_value.tv_sec = tv1.tv_sec;
						itimer.it_value.tv_usec = tv1.tv_usec;
					}
				}
d250 13
d266 11
a276 6
		/* Check if there is a current thread: */
		if (_thread_run != _thread_kern_threadp) {
			/*
			 * This thread no longer needs to yield the CPU.
			 */
			_thread_run->yield_on_sched_undefer = 0;
d278 32
d311 1
a311 2
			 * Save the current time as the time that the thread
			 * became inactive: 
d313 3
a315 2
			_thread_run->last_inactive.tv_sec = tv.tv_sec;
			_thread_run->last_inactive.tv_usec = tv.tv_usec;
d317 8
a324 35
			/*
			 * Accumulate the number of microseconds that this
			 * thread has run for:
			 */
			if ((_thread_run->slice_usec != -1) &&
			    (_thread_run->attr.sched_policy != SCHED_FIFO)) {
				_thread_run->slice_usec +=
				    (_thread_run->last_inactive.tv_sec -
				    _thread_run->last_active.tv_sec) * 1000000 +
				    _thread_run->last_inactive.tv_usec -
				    _thread_run->last_active.tv_usec;

				/* Check for time quantum exceeded: */
				if (_thread_run->slice_usec > TIMESLICE_USEC)
					_thread_run->slice_usec = -1;
			}
			if (_thread_run->state == PS_RUNNING) {
				if (_thread_run->slice_usec == -1) {
					/*
					 * The thread exceeded its time
					 * quantum or it yielded the CPU;
					 * place it at the tail of the
					 * queue for its priority.
					 */
					PTHREAD_PRIOQ_INSERT_TAIL(_thread_run);
				} else {
					/*
					 * The thread hasn't exceeded its
					 * interval.  Place it at the head
					 * of the queue for its priority.
					 */
					PTHREAD_PRIOQ_INSERT_HEAD(_thread_run);
				}
			}
			else if (_thread_run->state == PS_DEAD) {
d326 4
a329 3
				 * Don't add dead threads to the waiting
				 * queue, because when they're reaped, it
				 * will corrupt the queue.
d331 2
a332 2
			}
			else {
d334 3
a336 2
				 * This thread has changed state and needs
				 * to be placed in the waiting queue.
d338 1
a338 4
				PTHREAD_WAITQ_INSERT(_thread_run);

				/* Restart the time slice: */
				_thread_run->slice_usec = -1;
d345 1
a345 1
		pthread_h = PTHREAD_PRIOQ_FIRST;
d354 4
a357 1
			_thread_run = _thread_kern_threadp;
d363 68
a430 2
			_thread_kern_select(1);
		} else {
a433 3
			/* Remove the thread from the ready queue. */
			PTHREAD_PRIOQ_REMOVE(_thread_run);

d442 70
d522 1
a522 1
			if (_thread_run != _thread_link_list || _thread_run->nxt != NULL) {
d541 3
a543 2

				/* Restore the floating point state: */
d562 1
a562 1
			} else
d569 1
d583 13
d610 14
d637 1
a637 1
_thread_kern_select(int wait_reqd)
a638 4
	char            bufr[128];
	fd_set          fd_set_except;
	fd_set          fd_set_read;
	fd_set          fd_set_write;
d640 5
a644 7
	int             count_dec;
	int             found_one;
	int             i;
	int             nfds = -1;
	int             settimeout;
	pthread_t       pthread;
	ssize_t         num;
a645 2
	struct timespec ts1;
	struct timeval *p_tv;
a646 6
	struct timeval  tv1;

	/* Zero the file descriptor sets: */
	FD_ZERO(&fd_set_read);
	FD_ZERO(&fd_set_write);
	FD_ZERO(&fd_set_except);
d649 4
a652 8
	if (wait_reqd) {
		/*
		 * Add the pthread kernel pipe file descriptor to the read
		 * set: 
		 */
		FD_SET(_thread_kern_pipe[0], &fd_set_read);
		nfds = _thread_kern_pipe[0];

d656 26
d683 3
a685 3
	/* Initialise the time value structure: */
	tv.tv_sec = 0;
	tv.tv_usec = 0;
d688 2
a689 2
	 * Enter a loop to process threads waiting on either file descriptors
	 * or times: 
d691 6
a696 4
	_waitingq_check_reqd = 0;	/* reset flag before loop */
	TAILQ_FOREACH (pthread, &_waitingq, pqe) {
		/* Assume that this state does not time out: */
		settimeout = 0;
d698 4
a701 2
		/* Process according to thread state: */
		switch (pthread->state) {
d703 2
a704 2
		 * States which do not depend on file descriptor I/O
		 * operations or timeouts: 
d706 15
a720 15
		case PS_DEAD:
		case PS_DEADLOCK:
		case PS_FDLR_WAIT:
		case PS_FDLW_WAIT:
		case PS_FILE_WAIT:
		case PS_JOIN:
		case PS_MUTEX_WAIT:
		case PS_SIGTHREAD:
		case PS_SIGWAIT:
		case PS_STATE_MAX:
		case PS_WAIT_WAIT:
		case PS_SUSPENDED:
		case PS_SIGSUSPEND:
			/* Nothing to do here. */
			break;
d722 4
a725 1
		case PS_RUNNING:
d727 1
a727 3
			 * A signal occurred and made this thread ready
			 * while in the scheduler or while the scheduling
			 * queues were protected.
d729 13
a741 2
			PTHREAD_WAITQ_REMOVE(pthread);
			PTHREAD_PRIOQ_INSERT_TAIL(pthread);
d746 5
a750 10
			/* Add the file descriptor to the read set: */
			FD_SET(pthread->data.fd.fd, &fd_set_read);

			/*
			 * Check if this file descriptor is greater than any
			 * of those seen so far: 
			 */
			if (pthread->data.fd.fd > nfds) {
				/* Remember this file descriptor: */
				nfds = pthread->data.fd.fd;
a751 5
			/* Increment the file descriptor count: */
			count++;

			/* This state can time out: */
			settimeout = 1;
d756 5
a760 10
			/* Add the file descriptor to the write set: */
			FD_SET(pthread->data.fd.fd, &fd_set_write);

			/*
			 * Check if this file descriptor is greater than any
			 * of those seen so far: 
			 */
			if (pthread->data.fd.fd > nfds) {
				/* Remember this file descriptor: */
				nfds = pthread->data.fd.fd;
a761 12
			/* Increment the file descriptor count: */
			count++;

			/* This state can time out: */
			settimeout = 1;
			break;

		/* States that time out: */
		case PS_SLEEP_WAIT:
		case PS_COND_WAIT:
			/* Flag a timeout as required: */
			settimeout = 1;
d764 2
a765 1
		/* Select wait: */
d767 8
a774 94
			/*
			 * Enter a loop to process each file descriptor in
			 * the thread-specific file descriptor sets: 
			 */
			for (i = 0; i < pthread->data.select_data->nfds; i++) {
				/*
				 * Check if this file descriptor is set for
				 * exceptions: 
				 */
				if (FD_ISSET(i, &pthread->data.select_data->exceptfds)) {
					/*
					 * Add the file descriptor to the
					 * exception set: 
					 */
					FD_SET(i, &fd_set_except);

					/*
					 * Increment the file descriptor
					 * count: 
					 */
					count++;

					/*
					 * Check if this file descriptor is
					 * greater than any of those seen so
					 * far: 
					 */
					if (i > nfds) {
						/*
						 * Remember this file
						 * descriptor: 
						 */
						nfds = i;
					}
				}
				/*
				 * Check if this file descriptor is set for
				 * write: 
				 */
				if (FD_ISSET(i, &pthread->data.select_data->writefds)) {
					/*
					 * Add the file descriptor to the
					 * write set: 
					 */
					FD_SET(i, &fd_set_write);

					/*
					 * Increment the file descriptor
					 * count: 
					 */
					count++;

					/*
					 * Check if this file descriptor is
					 * greater than any of those seen so
					 * far: 
					 */
					if (i > nfds) {
						/*
						 * Remember this file
						 * descriptor: 
						 */
						nfds = i;
					}
				}
				/*
				 * Check if this file descriptor is set for
				 * read: 
				 */
				if (FD_ISSET(i, &pthread->data.select_data->readfds)) {
					/*
					 * Add the file descriptor to the
					 * read set: 
					 */
					FD_SET(i, &fd_set_read);

					/*
					 * Increment the file descriptor
					 * count: 
					 */
					count++;

					/*
					 * Check if this file descriptor is
					 * greater than any of those seen so
					 * far: 
					 */
					if (i > nfds) {
						/*
						 * Remember this file
						 * descriptor: 
						 */
						nfds = i;
					}
d776 1
d778 1
d780 2
a781 2
			/* This state can time out: */
			settimeout = 1;
a783 65

		/*
		 * Check if the caller wants to wait and if the thread state
		 * is one that times out: 
		 */
		if (wait_reqd && settimeout) {
			/* Check if this thread wants to wait forever: */
			if (pthread->wakeup_time.tv_sec == -1) {
			}
			/* Check if this thread doesn't want to wait at all: */
			else if (pthread->wakeup_time.tv_sec == 0 &&
				 pthread->wakeup_time.tv_nsec == 0) {
				/* Override the caller's request to wait: */
				wait_reqd = 0;
			} else {
				/*
				 * Calculate the time until this thread is
				 * ready, allowing for the clock resolution: 
				 */
				ts1.tv_sec = pthread->wakeup_time.tv_sec - ts.tv_sec;
				ts1.tv_nsec = pthread->wakeup_time.tv_nsec - ts.tv_nsec +
					CLOCK_RES_NSEC;

				/*
				 * Check for underflow of the nanosecond
				 * field: 
				 */
				if (ts1.tv_nsec < 0) {
					/*
					 * Allow for the underflow of the
					 * nanosecond field: 
					 */
					ts1.tv_sec--;
					ts1.tv_nsec += 1000000000;
				}
				/*
				 * Check for overflow of the nanosecond
				 * field: 
				 */
				if (ts1.tv_nsec >= 1000000000) {
					/*
					 * Allow for the overflow of the
					 * nanosecond field: 
					 */
					ts1.tv_sec++;
					ts1.tv_nsec -= 1000000000;
				}
				/*
				 * Convert the timespec structure to a
				 * timeval structure: 
				 */
				TIMESPEC_TO_TIMEVAL(&tv1, &ts1);

				/*
				 * Check if no time value has been found yet,
				 * or if the thread will be ready sooner that
				 * the earliest one found so far: 
				 */
				if ((tv.tv_sec == 0 && tv.tv_usec == 0) || timercmp(&tv1, &tv, <)) {
					/* Update the time value: */
					tv.tv_sec = tv1.tv_sec;
					tv.tv_usec = tv1.tv_usec;
				}
			}
		}
d785 1
d787 5
a791 13
	/* Check if the caller wants to wait: */
	if (wait_reqd) {
		/* Check if no threads were found with timeouts: */
		if (tv.tv_sec == 0 && tv.tv_usec == 0) {
			/* Wait forever: */
			p_tv = NULL;
		} else {
			/*
			 * Point to the time value structure which contains
			 * the earliest time that a thread will be ready: 
			 */
			p_tv = &tv;
		}
d793 1
d795 2
a796 3
		 * Flag the pthread kernel as in a select. This is to avoid
		 * the window between the next statement that unblocks
		 * signals and the select statement which follows. 
d798 3
a800 1
		_thread_kern_in_select = 1;
d802 6
d809 2
a810 2
		 * Wait for a file descriptor to be ready for read, write, or
		 * an exception, or a timeout to occur: 
d812 4
a815 1
		count = _thread_sys_select(nfds + 1, &fd_set_read, &fd_set_write, &fd_set_except, p_tv);
d817 3
a819 2
		/* Reset the kernel in select flag: */
		_thread_kern_in_select = 0;
d821 1
a821 36
		/*
		 * Check if it is possible that there are bytes in the kernel
		 * read pipe waiting to be read: 
		 */
		if (count < 0 || FD_ISSET(_thread_kern_pipe[0], &fd_set_read)) {
			/*
			 * Check if the kernel read pipe was included in the
			 * count: 
			 */
			if (count > 0) {
				/*
				 * Remove the kernel read pipe from the
				 * count: 
				 */
				FD_CLR(_thread_kern_pipe[0], &fd_set_read);

				/* Decrement the count of file descriptors: */
				count--;
			}
			/*
			 * Enter a loop to read (and trash) bytes from the
			 * pthread kernel pipe: 
			 */
			while ((num = _thread_sys_read(_thread_kern_pipe[0], bufr, sizeof(bufr))) > 0) {
				/*
				 * The buffer read contains one byte per
				 * signal and each byte is the signal number.
				 * This data is not used, but the fact that
				 * the signal handler wrote to the pipe *is*
				 * used to cause the _select call
				 * to complete if the signal occurred between
				 * the time when signals were unblocked and
				 * the _select select call being
				 * made. 
				 */
			}
a823 11
	/* Check if there are file descriptors to poll: */
	else if (count > 0) {
		/*
		 * Point to the time value structure which has been zeroed so
		 * that the call to _select will not wait: 
		 */
		p_tv = &tv;

		/* Poll file descrptors without wait: */
		count = _thread_sys_select(nfds + 1, &fd_set_read, &fd_set_write, &fd_set_except, p_tv);
	}
d832 1
a832 1
		 * _select syscall: 
d834 2
a835 2
		TAILQ_FOREACH (pthread, &_waitingq, pqe) {
			/* Process according to thread state: */
d837 1
a837 23
			/*
			 * States which do not depend on file
			 * descriptor I/O operations: 
			 */
			case PS_COND_WAIT:
			case PS_DEAD:
			case PS_DEADLOCK:
			case PS_FDLR_WAIT:
			case PS_FDLW_WAIT:
			case PS_FILE_WAIT:
			case PS_JOIN:
			case PS_MUTEX_WAIT:
			case PS_SIGWAIT:
			case PS_SLEEP_WAIT:
			case PS_WAIT_WAIT:
			case PS_SIGTHREAD:
			case PS_STATE_MAX:
			case PS_SUSPENDED:
			case PS_SIGSUSPEND:
				/* Nothing to do here. */
				break;

			case PS_RUNNING:
d839 1
a839 2
				 * A signal occurred and made this thread
				 * ready while in the scheduler.
d841 11
a851 2
				PTHREAD_WAITQ_REMOVE(pthread);
				PTHREAD_PRIOQ_INSERT_TAIL(pthread);
d856 6
a861 18
				/*
				 * Check if the file descriptor is available
				 * for read: 
				 */
				if (FD_ISSET(pthread->data.fd.fd, &fd_set_read)) {
					/*
					 * Change the thread state to allow
					 * it to read from the file when it
					 * is scheduled next: 
					 */
					pthread->state = PS_RUNNING;

					/*
					 * Remove it from the waiting queue
					 * and add it to the ready queue:
					 */
					PTHREAD_WAITQ_REMOVE(pthread);
					PTHREAD_PRIOQ_INSERT_TAIL(pthread);
d863 1
d868 6
a873 18
				/*
				 * Check if the file descriptor is available
				 * for write: 
				 */
				if (FD_ISSET(pthread->data.fd.fd, &fd_set_write)) {
					/*
					 * Change the thread state to allow
					 * it to write to the file when it is
					 * scheduled next: 
					 */
					pthread->state = PS_RUNNING;

					/*
					 * Remove it from the waiting queue
					 * and add it to the ready queue:
					 */
					PTHREAD_WAITQ_REMOVE(pthread);
					PTHREAD_PRIOQ_INSERT_TAIL(pthread);
d875 1
d878 2
a879 1
			/* Select wait: */
d881 2
a882 13
				/*
				 * Reset the flag that indicates if a file
				 * descriptor is ready for some type of
				 * operation: 
				 */
				count_dec = 0;

				/*
				 * Enter a loop to search though the
				 * thread-specific select file descriptors
				 * for the first descriptor that is ready: 
				 */
				for (i = 0; i < pthread->data.select_data->nfds && count_dec == 0; i++) {
d884 2
a885 2
					 * Check if this file descriptor does
					 * not have an exception: 
d887 7
a893 6
					if (FD_ISSET(i, &pthread->data.select_data->exceptfds) && FD_ISSET(i, &fd_set_except)) {
						/*
						 * Flag this file descriptor
						 * as ready: 
						 */
						count_dec = 1;
d895 10
a904 21
					/*
					 * Check if this file descriptor is
					 * not ready for write: 
					 */
					if (FD_ISSET(i, &pthread->data.select_data->writefds) && FD_ISSET(i, &fd_set_write)) {
						/*
						 * Flag this file descriptor
						 * as ready: 
						 */
						count_dec = 1;
					}
					/*
					 * Check if this file descriptor is
					 * not ready for read: 
					 */
					if (FD_ISSET(i, &pthread->data.select_data->readfds) && FD_ISSET(i, &fd_set_read)) {
						/*
						 * Flag this file descriptor
						 * as ready: 
						 */
						count_dec = 1;
d907 3
d911 15
d927 1
a927 2
				 * Check if any file descriptors are ready
				 * for the current thread: 
d929 6
a934 1
				if (count_dec) {
d936 1
a936 3
					 * Reset the count of file
					 * descriptors that are ready for
					 * this thread: 
d938 6
a943 1
					found_one = 0;
d945 6
a950 129
					/*
					 * Enter a loop to search though the
					 * thread-specific select file
					 * descriptors: 
					 */
					for (i = 0; i < pthread->data.select_data->nfds; i++) {
						/*
						 * Reset the count of
						 * operations for which the
						 * current file descriptor is
						 * ready: 
						 */
						count_dec = 0;

						/*
						 * Check if this file
						 * descriptor is selected for
						 * exceptions: 
						 */
						if (FD_ISSET(i, &pthread->data.select_data->exceptfds)) {
							/*
							 * Check if this file
							 * descriptor has an
							 * exception: 
							 */
							if (FD_ISSET(i, &fd_set_except)) {
								/*
								 * Increment
								 * the count
								 * for this
								 * file: 
								 */
								count_dec++;
							} else {
								/*
								 * Clear the
								 * file
								 * descriptor
								 * in the
								 * thread-spec
								 * ific file
								 * descriptor
								 * set: 
								 */
								FD_CLR(i, &pthread->data.select_data->exceptfds);
							}
						}
						/*
						 * Check if this file
						 * descriptor is selected for
						 * write: 
						 */
						if (FD_ISSET(i, &pthread->data.select_data->writefds)) {
							/*
							 * Check if this file
							 * descriptor is
							 * ready for write: 
							 */
							if (FD_ISSET(i, &fd_set_write)) {
								/*
								 * Increment
								 * the count
								 * for this
								 * file: 
								 */
								count_dec++;
							} else {
								/*
								 * Clear the
								 * file
								 * descriptor
								 * in the
								 * thread-spec
								 * ific file
								 * descriptor
								 * set: 
								 */
								FD_CLR(i, &pthread->data.select_data->writefds);
							}
						}
						/*
						 * Check if this file
						 * descriptor is selected for
						 * read: 
						 */
						if (FD_ISSET(i, &pthread->data.select_data->readfds)) {
							/*
							 * Check if this file
							 * descriptor is
							 * ready for read: 
							 */
							if (FD_ISSET(i, &fd_set_read)) {
								/*
								 * Increment
								 * the count
								 * for this
								 * file: 
								 */
								count_dec++;
							} else {
								/*
								 * Clear the
								 * file
								 * descriptor
								 * in the
								 * thread-spec
								 * ific file
								 * descriptor
								 * set: 
								 */
								FD_CLR(i, &pthread->data.select_data->readfds);
							}
						}
						/*
						 * Check if the current file
						 * descriptor is ready for
						 * any one of the operations: 
						 */
						if (count_dec > 0) {
							/*
							 * Increment the
							 * count of file
							 * descriptors that
							 * are ready for the
							 * current thread: 
							 */
							found_one++;
						}
					}
d952 2
a953 5
					/*
					 * Return the number of file
					 * descriptors that are ready: 
					 */
					pthread->data.select_data->nfds = found_one;
d955 1
a955 5
					/*
					 * Change the state of the current
					 * thread to run: 
					 */
					pthread->state = PS_RUNNING;
d957 2
a958 10
					/*
					 * Remove it from the waiting queue
					 * and add it to the ready queue:
					 */
					PTHREAD_WAITQ_REMOVE(pthread);
					PTHREAD_PRIOQ_INSERT_TAIL(pthread);
				}
				break;
			}
		}
d1008 1
a1008 1
_thread_kern_sched_defer(void)
d1010 2
a1011 2
	/* Allow scheduling deferral to be recursive. */
	_thread_run->sched_defer_count++;
d1015 1
a1015 1
_thread_kern_sched_undefer(void)
d1022 1
a1022 1
	 * scheduling.
d1024 8
a1031 1
	if (_thread_run->sched_defer_count == 1) {
d1033 1
a1033 2
		 * Check if the waiting queue needs to be examined for
		 * threads that are now ready:
d1035 19
a1053 9
		while (_waitingq_check_reqd != 0) {
			/* Clear the flag before checking the waiting queue: */
			_waitingq_check_reqd = 0;

			TAILQ_FOREACH(pthread, &_waitingq, pqe) {
				if (pthread->state == PS_RUNNING) {
					PTHREAD_WAITQ_REMOVE(pthread);
					PTHREAD_PRIOQ_INSERT_TAIL(pthread);
				}
d1055 3
d1060 20
d1081 2
a1082 3
		 * We need to yield if a thread change of state caused a
		 * higher priority thread to become ready, or if a
		 * scheduling signal occurred while preemption was disabled.
d1084 11
a1094 5
		if ((((pthread = PTHREAD_PRIOQ_FIRST) != NULL) &&
		   (pthread->active_priority > _thread_run->active_priority)) ||
		   (_thread_run->yield_on_sched_undefer != 0)) {
			_thread_run->yield_on_sched_undefer = 0;
			need_resched = 1;
d1097 6
a1102 8

	if (_thread_run->sched_defer_count > 0) {
		/* Decrement the scheduling deferral count. */
		_thread_run->sched_defer_count--;

		/* Yield the CPU if necessary: */
		if (need_resched)
			_thread_kern_sched(NULL);
d1113 1
a1113 1
	    (tid_out->flags & PTHREAD_FLAGS_PRIVATE != 0))
d1116 1
a1116 1
	    (tid_in->flags & PTHREAD_FLAGS_PRIVATE != 0))
@


1.7
log
@don't compute resource usage. this speeds things up a lot
@
text
@d1 1
d24 1
a24 1
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
d33 1
a33 2
 * $FreeBSD: uthread_kern.c,v 1.15 1998/11/15 09:58:26 jb Exp $
 * $OpenBSD: uthread_kern.c,v 1.6 1999/01/17 23:49:49 d Exp $
d46 1
d48 1
d60 3
a65 1
	int             prio = -1;
d68 1
a68 1
	pthread_t       pthread_s = NULL;
d110 5
d134 1
d140 4
d160 1
a160 1
	 * Enter a the scheduling loop that finds the next thread that is
d178 18
a195 1
		 * Enter a loop to look for sleeping threads that are ready:
d197 21
a217 2
		for (pthread = _thread_link_list; pthread != NULL;
		    pthread = pthread->nxt) {
d219 1
a219 1
			if (pthread->state == PS_COND_WAIT ||
d243 3
a245 3
						 * The select has timed out,
						 * so zero the file
						 * descriptor sets: 
d269 53
d323 1
d330 5
d343 1
a343 1
			 * thread has run for: 
d345 10
a354 12
			if (_thread_run->slice_usec != -1) {
				if (timerisset(&_thread_run->last_active)) {
					struct timeval s;

					timersub(&_thread_run->last_inactive,
					    &_thread_run->last_active,
					    &s);
					_thread_run->slice_usec = 
					    s.tv_usec + 1000000 * s.tv_sec;
					if (_thread_run->slice_usec < 0)
						PANIC("slice_usec");
				} else
a355 12
                        }

			/*
			 * Check if this thread has reached its allocated
			 * time slice period: 
			 */
			if (_thread_run->slice_usec > TIMESLICE_USEC) {
				/*
				 * Flag the allocated time slice period as
				 * up: 
				 */
				_thread_run->slice_usec = -1;
d357 14
a370 26
		}
		/* Check if an incremental priority update is required: */
		if (((tv.tv_sec - kern_inc_prio_time.tv_sec) * 1000000 +
		 tv.tv_usec - kern_inc_prio_time.tv_usec) > INC_PRIO_USEC) {
			/*
			 * Enter a loop to look for run-enabled threads that
			 * have not run since the last time that an
			 * incremental priority update was performed: 
			 */
			for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
				/* Check if this thread is unable to run: */
				if (pthread->state != PS_RUNNING) {
				}
				/*
				 * Check if the last time that this thread
				 * was run (as indicated by the last time it
				 * became inactive) is before the time that
				 * the last incremental priority check was
				 * made: 
				 */
				else if (timercmp(&pthread->last_inactive, &kern_inc_prio_time, <)) {
					/*
					 * Increment the incremental priority
					 * for this thread in the hope that
					 * it will eventually get a chance to
					 * run: 
d372 1
a372 1
					(pthread->inc_prio)++;
d375 1
a375 19

			/* Save the new incremental priority update time: */
			kern_inc_prio_time.tv_sec = tv.tv_sec;
			kern_inc_prio_time.tv_usec = tv.tv_usec;
		}
		/*
		 * Enter a loop to look for the first thread of the highest
		 * priority that is ready to run: 
		 */
		for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
			/* Check if the current thread is unable to run: */
			if (pthread->state != PS_RUNNING) {
			}
			/*
			 * Check if no run-enabled thread has been seen or if
			 * the current thread has a priority higher than the
			 * highest seen so far: 
			 */
			else if (pthread_h == NULL || (pthread->pthread_priority + pthread->inc_prio) > prio) {
d377 3
a379 2
				 * Save this thread as the highest priority
				 * thread seen so far: 
a380 15
				pthread_h = pthread;
				prio = pthread->pthread_priority + pthread->inc_prio;
			}
		}

		/*
		 * Enter a loop to look for a thread that: 1. Is run-enabled.
		 * 2. Has the required agregate priority. 3. Has not been
		 * allocated its allocated time slice. 4. Became inactive
		 * least recently. 
		 */
		for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
			/* Check if the current thread is unable to run: */
			if (pthread->state != PS_RUNNING) {
				/* Ignore threads that are not ready to run. */
d382 1
a382 7

			/*
			 * Check if the current thread as an agregate
			 * priority not equal to the highest priority found
			 * above: 
			 */
			else if ((pthread->pthread_priority + pthread->inc_prio) != prio) {
d384 2
a385 2
				 * Ignore threads which have lower agregate
				 * priority. 
d387 1
a387 1
			}
d389 2
a390 26
			/*
			 * Check if the current thread reached its time slice
			 * allocation last time it ran (or if it has not run
			 * yet): 
			 */
			else if (pthread->slice_usec == -1) {
			}

			/*
			 * Check if an eligible thread has not been found
			 * yet, or if the current thread has an inactive time
			 * earlier than the last one seen: 
			 */
			else if (pthread_s == NULL || timercmp(&pthread->last_inactive, &tv1, <)) {
				/*
				 * Save the pointer to the current thread as
				 * the most eligible thread seen so far: 
				 */
				pthread_s = pthread;

				/*
				 * Save the time that the selected thread
				 * became inactive: 
				 */
				tv1.tv_sec = pthread->last_inactive.tv_sec;
				tv1.tv_usec = pthread->last_inactive.tv_usec;
d395 1
a395 2
		 * Check if no thread was selected according to incomplete
		 * time slice allocation: 
d397 1
a397 41
		if (pthread_s == NULL) {
			/*
			 * Enter a loop to look for any other thread that: 1.
			 * Is run-enabled. 2. Has the required agregate
			 * priority. 3. Became inactive least recently. 
			 */
			for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
				/*
				 * Check if the current thread is unable to
				 * run: 
				 */
				if (pthread->state != PS_RUNNING) {
					/*
					 * Ignore threads that are not ready
					 * to run. 
					 */
				}
				/*
				 * Check if the current thread as an agregate
				 * priority not equal to the highest priority
				 * found above: 
				 */
				else if ((pthread->pthread_priority + pthread->inc_prio) != prio) {
					/*
					 * Ignore threads which have lower
					 * agregate priority.   
					 */
				}
				/*
				 * Check if an eligible thread has not been
				 * found yet, or if the current thread has an
				 * inactive time earlier than the last one
				 * seen: 
				 */
				else if (pthread_s == NULL || timercmp(&pthread->last_inactive, &tv1, <)) {
					/*
					 * Save the pointer to the current
					 * thread as the most eligible thread
					 * seen so far: 
					 */
					pthread_s = pthread;
a398 9
					/*
					 * Save the time that the selected
					 * thread became inactive: 
					 */
					tv1.tv_sec = pthread->last_inactive.tv_sec;
					tv1.tv_usec = pthread->last_inactive.tv_usec;
				}
			}
		}
d400 1
a400 1
		if (pthread_s == NULL) {
d415 4
a418 1
			_thread_run = pthread_s;
a435 5
			/*
			 * Reset the incremental priority now that this
			 * thread has been given the chance to run: 
			 */
			_thread_run->inc_prio = 0;
a439 92
				 * Define the maximum time before a SIGVTALRM
				 * is required: 
				 */
				itimer.it_value.tv_sec = 0;
				itimer.it_value.tv_usec = TIMESLICE_USEC;

				/*
				 * The interval timer is not reloaded when it
				 * times out. The interval time needs to be
				 * calculated every time. 
				 */
				timerclear(&itimer.it_interval);

				/*
				 * Enter a loop to look for threads waiting
				 * for a time: 
				 */
				for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
					/*
					 * Check if this thread is to
					 * timeout: 
					 */
					if (pthread->state == PS_COND_WAIT ||
					  pthread->state == PS_SLEEP_WAIT ||
					    pthread->state == PS_FDR_WAIT ||
					    pthread->state == PS_FDW_WAIT ||
					 pthread->state == PS_SELECT_WAIT) {
						/*
						 * Check if this thread is to
						 * wait forever: 
						 */
						if (pthread->wakeup_time.tv_sec == -1) {
						}
						/*
						 * Check if this thread is to
						 * wakeup immediately: 
						 */
						else if (pthread->wakeup_time.tv_sec == 0 &&
							 pthread->wakeup_time.tv_nsec == 0) {
						}
						/*
						 * Check if the current time
						 * is after the wakeup time: 
						 */
						else if (timespeccmp(&ts,
						    &pthread->wakeup_time, > )){
						} else {
							/*
							 * Calculate the time
							 * until this thread
							 * is ready, allowing
							 * for the clock
							 * resolution: 
							 */
							struct timespec
							 clock_res
							  = {0,CLOCK_RES_NSEC};
							timespecsub(
							  &pthread->wakeup_time,
							  &ts, &ts1);
							timespecadd(
							  &ts1, &clock_res,
							  &ts1);
							/*
							 * Convert the
							 * timespec structure
							 * to a timeval
							 * structure: 
							 */
							TIMESPEC_TO_TIMEVAL(&tv, &ts1);

							/*
							 * Check if the
							 * thread will be
							 * ready sooner than
							 * the earliest one
							 * found so far: 
							 */
							if (timercmp(&tv, &itimer.it_value, <)) {
								/*
								 * Update the
								 * time
								 * value: 
								 */
								itimer.it_value.tv_sec = tv.tv_sec;
								itimer.it_value.tv_usec = tv.tv_usec;
							}
						}
					}
				}

				/*
d443 1
a443 1
				if (setitimer(ITIMER_VIRTUAL, &itimer, NULL) != 0) {
d448 1
a448 1
					PANIC("Cannot set virtual timer");
d465 11
a475 1
		                _thread_kern_in_sched = 0;
d570 2
a571 1
	for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
d582 1
a587 1
		case PS_RUNNING:
d597 10
d913 1
a913 1
		for (pthread = _thread_link_list; pthread != NULL; pthread = pthread->nxt) {
a919 1
			case PS_RUNNING:
d922 1
d938 9
d960 7
d983 7
d1196 7
d1253 76
@


1.6
log
@mi+md jmp_buf; save i386s fs and gs for WINE (csapuntz@@stanford.edu)
@
text
@d33 1
a33 1
 * $OpenBSD: uthread_kern.c,v 1.5 1999/01/10 23:16:35 d Exp $
d70 1
d73 1
d133 1
d143 1
@


1.5
log
@clean up timer calculations; dont use PTHREAD_NOFLOAT; keep freebsd ident
@
text
@d33 1
a33 1
 * $OpenBSD: uthread_kern.c,v 1.4 1998/12/21 07:42:03 d Exp $
d103 14
@


1.4
log
@resource accounting; initial timeslice bug fix
@
text
@d32 2
a33 2
 * $Id: uthread_kern.c,v 1.3 1998/11/20 12:13:32 d Exp $
 * $OpenBSD: uthread_kern.c,v 1.3 1998/11/20 12:13:32 d Exp $
d89 1
a89 2
		if (_thread_run->attr.flags & PTHREAD_NOFLOAT == 0)
			_thread_machdep_save_float_state(_thread_run);
d206 1
a206 1
		if (_thread_run != &_thread_kern_thread) {
d417 1
a417 1
			_thread_run = &_thread_kern_thread;
d464 1
a464 2
				itimer.it_interval.tv_sec = 0;
				itimer.it_interval.tv_usec = 0;
d497 2
a498 3
						else if ((ts.tv_sec > pthread->wakeup_time.tv_sec) ||
							 ((ts.tv_sec == pthread->wakeup_time.tv_sec) &&
							  (ts.tv_nsec > pthread->wakeup_time.tv_nsec))) {
d507 9
a515 38
							ts1.tv_sec = pthread->wakeup_time.tv_sec - ts.tv_sec;
							ts1.tv_nsec = pthread->wakeup_time.tv_nsec - ts.tv_nsec +
								CLOCK_RES_NSEC;

							/*
							 * Check for
							 * underflow of the
							 * nanosecond field: 
							 */
							if (ts1.tv_nsec < 0) {
								/*
								 * Allow for
								 * the
								 * underflow
								 * of the
								 * nanosecond
								 * field: 
								 */
								ts1.tv_sec--;
								ts1.tv_nsec += 1000000000;
							}
							/*
							 * Check for overflow
							 * of the nanosecond
							 * field: 
							 */
							if (ts1.tv_nsec >= 1000000000) {
								/*
								 * Allow for
								 * the
								 * overflow
								 * of the
								 * nanosecond
								 * field: 
								 */
								ts1.tv_sec++;
								ts1.tv_nsec -= 1000000000;
							}
d556 1
d559 1
d564 2
a565 2
				if (_thread_run->attr.flags & PTHREAD_NOFLOAT == 0)
					_thread_machdep_restore_float_state(_thread_run);
@


1.3
log
@sync with freebsd
@
text
@d32 2
a33 2
 * $Id: uthread_kern.c,v 1.2 1998/11/09 03:13:20 d Exp $
 * $OpenBSD: uthread_kern.c,v 1.2 1998/11/09 03:13:20 d Exp $
d46 1
d70 2
d89 2
a90 1
		_thread_machdep_save_float_state(_thread_run);
d118 10
d220 12
a231 4
 			        _thread_run->slice_usec += (_thread_run->last_inactive.tv_sec -
				        _thread_run->last_active.tv_sec) * 1000000 +
				        _thread_run->last_inactive.tv_usec -
				        _thread_run->last_active.tv_usec;
d594 2
a595 1
				_thread_machdep_restore_float_state(_thread_run);
@


1.2
log
@sync with FreeBSD (rwlock, gc thread, man pages)
add (broken) mips md stuff
fix some const warnings
add sigaltstack() stub
another hash at getting shlib auto-init to work (mips/elf and i386/a.out)
@
text
@d32 2
a33 2
 * $Id: uthread_kern.c,v 1.1 1998/08/27 09:01:08 d Exp $
 * $OpenBSD: uthread_kern.c,v 1.1 1998/08/27 09:01:08 d Exp $
d577 1
d603 16
@


1.1
log
@experimental threaded libc - kernel only
@
text
@d32 2
a33 2
 * $Id: uthread_kern.c,v 1.11 1998/04/30 21:50:29 jb Exp $
 * $OpenBSD$
a62 2
	pthread_t       pthread_nxt = NULL;
	pthread_t       pthread_prv = NULL;
d92 1
a92 1
	else if (setjmp(_thread_run->saved_jmp_buf) != 0) {
d107 1
a107 1
	} else {
a109 1
	}
a113 75
	/* Point to the first dead thread (if there are any): */
	pthread = _thread_dead;

	/* There is no previous dead thread: */
	pthread_prv = NULL;

	/* Enter a loop to cleanup after dead threads: */
	while (pthread != NULL) {
		/* Save a pointer to the next thread: */
		pthread_nxt = pthread->nxt;

		/* Check if this thread is one which is running: */
		if (pthread == _thread_run || pthread == _thread_initial) {
			/*
			 * Don't destroy the running thread or the initial
			 * thread. 
			 */
			pthread_prv = pthread;
		}
		/*
		 * Check if this thread has detached:
		 */
		else if ((pthread->attr.flags & PTHREAD_DETACHED) != 0) {
			/* Check if there is no previous dead thread: */
			if (pthread_prv == NULL) {
				/*
				 * The dead thread is at the head of the
				 * list: 
				 */
				_thread_dead = pthread_nxt;
			} else {
				/*
				 * The dead thread is not at the head of the
				 * list: 
				 */
				pthread_prv->nxt = pthread->nxt;
			}

			/*
			 * Check if the stack was not specified by the caller
			 * to pthread_create and has not been destroyed yet: 
			 */
			if (pthread->attr.stackaddr_attr == NULL && pthread->stack != NULL) {
				/* Free the stack of the dead thread: */
				free(pthread->stack);
			}
			/* Free the memory allocated to the thread structure: */
			free(pthread);
		} else {
			/*
			 * This thread has not detached, so do not destroy
			 * it: 
			 */
			pthread_prv = pthread;

			/*
			 * Check if the stack was not specified by the caller
			 * to pthread_create and has not been destroyed yet: 
			 */
			if (pthread->attr.stackaddr_attr == NULL && pthread->stack != NULL) {
				/* Free the stack of the dead thread: */
				free(pthread->stack);

				/*
				 * NULL the stack pointer now that the memory
				 * has been freed: 
				 */
				pthread->stack = NULL;
			}
		}

		/* Point to the next thread: */
		pthread = pthread_nxt;
	}

d205 6
a210 4
			_thread_run->slice_usec += (_thread_run->last_inactive.tv_sec -
				_thread_run->last_active.tv_sec) * 1000000 +
				_thread_run->last_inactive.tv_usec -
				_thread_run->last_active.tv_usec;
d243 1
a243 1
				else if (timercmp(&_thread_run->last_inactive, &kern_inc_prio_time, <)) {
d584 1
a584 1
				longjmp(_thread_run->saved_jmp_buf, 1);
d596 1
a596 1
_thread_kern_sched_state(enum pthread_state state, char *fname, int lineno)
d677 1
d1008 1
@

