head	1.22;
access;
symbols
	OPENBSD_6_1:1.22.0.4
	OPENBSD_6_1_BASE:1.22;
locks; strict;
comment	@ * @;


1.22
date	2016.11.04.13.56.04;	author miod;	state Exp;
branches;
next	1.21;
commitid	TilmOttV27QFPTgF;

1.21
date	2016.09.04.14.31.29;	author jsing;	state Exp;
branches;
next	1.20;
commitid	Jv5Ac6XTUqjrS5wz;

1.20
date	2014.11.09.19.08.24;	author miod;	state Exp;
branches;
next	1.19;
commitid	FoFrpNSIJcwTcWmA;

1.19
date	2014.10.20.13.06.54;	author bcook;	state Exp;
branches;
next	1.18;
commitid	3Xsy3xjIIblEEksr;

1.18
date	2014.08.18.19.11.48;	author bcook;	state Exp;
branches;
next	1.17;
commitid	mJM4fnqu5nmkm6sJ;

1.17
date	2014.08.12.15.02.52;	author bcook;	state Exp;
branches;
next	1.16;
commitid	pohIKjTrVMk8vgOM;

1.16
date	2014.07.10.22.45.56;	author jsing;	state Exp;
branches;
next	1.15;
commitid	nzndm3zqPmFurSaK;

1.15
date	2014.06.12.15.49.27;	author deraadt;	state Exp;
branches;
next	1.14;
commitid	mJUVYpkFBZ0Zv2bG;

1.14
date	2014.04.23.19.09.48;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2014.04.20.14.32.19;	author jsing;	state Exp;
branches;
next	1.12;

1.12
date	2014.04.17.21.10.59;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2014.04.17.21.07.04;	author miod;	state Exp;
branches;
next	1.10;

1.10
date	2014.04.15.13.42.55;	author jsing;	state Exp;
branches;
next	1.9;

1.9
date	2011.11.03.02.34.32;	author djm;	state Exp;
branches;
next	1.8;

1.8
date	2010.10.01.22.58.53;	author djm;	state Exp;
branches;
next	1.7;

1.7
date	2009.01.05.21.36.38;	author djm;	state Exp;
branches;
next	1.6;

1.6
date	2008.09.06.12.17.48;	author djm;	state Exp;
branches;
next	1.5;

1.5
date	2005.04.29.05.39.16;	author djm;	state Exp;
branches;
next	1.4;

1.4
date	2003.05.12.02.18.35;	author markus;	state Exp;
branches;
next	1.3;

1.3
date	2002.05.15.02.29.10;	author beck;	state Exp;
branches;
next	1.2;

1.2
date	2000.03.19.11.08.29;	author beck;	state Exp;
branches;
next	1.1;

1.1
date	99.09.29.04.35.24;	author beck;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2002.09.05.12.49.42;	author markus;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2003.05.11.21.34.27;	author markus;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2005.04.29.05.37.01;	author djm;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2008.09.06.12.15.39;	author djm;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2009.01.09.12.13.50;	author djm;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2010.10.01.22.54.02;	author djm;	state Exp;
branches;
next	1.1.1.7;

1.1.1.7
date	2011.11.03.02.32.07;	author djm;	state Exp;
branches;
next	;


desc
@@


1.22
log
@Remove I386_ONLY define. It was only used to prefer a
faster-on-genuine-80386-but-slower-on-80486-onwards innstruction sequence in
the SHA512 code, and had not been enabled in years, if at all.

ok tom@@ bcook@@
@
text
@/* $OpenBSD: md32_common.h,v 1.21 2016/09/04 14:31:29 jsing Exp $ */
/* ====================================================================
 * Copyright (c) 1999-2007 The OpenSSL Project.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in
 *    the documentation and/or other materials provided with the
 *    distribution.
 *
 * 3. All advertising materials mentioning features or use of this
 *    software must display the following acknowledgment:
 *    "This product includes software developed by the OpenSSL Project
 *    for use in the OpenSSL Toolkit. (http://www.OpenSSL.org/)"
 *
 * 4. The names "OpenSSL Toolkit" and "OpenSSL Project" must not be used to
 *    endorse or promote products derived from this software without
 *    prior written permission. For written permission, please contact
 *    licensing@@OpenSSL.org.
 *
 * 5. Products derived from this software may not be called "OpenSSL"
 *    nor may "OpenSSL" appear in their names without prior written
 *    permission of the OpenSSL Project.
 *
 * 6. Redistributions of any form whatsoever must retain the following
 *    acknowledgment:
 *    "This product includes software developed by the OpenSSL Project
 *    for use in the OpenSSL Toolkit (http://www.OpenSSL.org/)"
 *
 * THIS SOFTWARE IS PROVIDED BY THE OpenSSL PROJECT ``AS IS'' AND ANY
 * EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE OpenSSL PROJECT OR
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 * ====================================================================
 *
 */

/*
 * This is a generic 32 bit "collector" for message digest algorithms.
 * Whenever needed it collects input character stream into chunks of
 * 32 bit values and invokes a block function that performs actual hash
 * calculations.
 *
 * Porting guide.
 *
 * Obligatory macros:
 *
 * DATA_ORDER_IS_BIG_ENDIAN or DATA_ORDER_IS_LITTLE_ENDIAN
 *	this macro defines byte order of input stream.
 * HASH_CBLOCK
 *	size of a unit chunk HASH_BLOCK operates on.
 * HASH_LONG
 *	has to be at least 32 bit wide.
 * HASH_CTX
 *	context structure that at least contains following
 *	members:
 *		typedef struct {
 *			...
 *			HASH_LONG	Nl,Nh;
 *			either {
 *			HASH_LONG	data[HASH_LBLOCK];
 *			unsigned char	data[HASH_CBLOCK];
 *			};
 *			unsigned int	num;
 *			...
 *			} HASH_CTX;
 *	data[] vector is expected to be zeroed upon first call to
 *	HASH_UPDATE.
 * HASH_UPDATE
 *	name of "Update" function, implemented here.
 * HASH_TRANSFORM
 *	name of "Transform" function, implemented here.
 * HASH_FINAL
 *	name of "Final" function, implemented here.
 * HASH_BLOCK_DATA_ORDER
 *	name of "block" function capable of treating *unaligned* input
 *	message in original (data) byte order, implemented externally.
 * HASH_MAKE_STRING
 *	macro convering context variables to an ASCII hash string.
 *
 * MD5 example:
 *
 *	#define DATA_ORDER_IS_LITTLE_ENDIAN
 *
 *	#define HASH_LONG		MD5_LONG
 *	#define HASH_CTX		MD5_CTX
 *	#define HASH_CBLOCK		MD5_CBLOCK
 *	#define HASH_UPDATE		MD5_Update
 *	#define HASH_TRANSFORM		MD5_Transform
 *	#define HASH_FINAL		MD5_Final
 *	#define HASH_BLOCK_DATA_ORDER	md5_block_data_order
 *
 *					<appro@@fy.chalmers.se>
 */

#include <stdint.h>

#include <openssl/opensslconf.h>

#if !defined(DATA_ORDER_IS_BIG_ENDIAN) && !defined(DATA_ORDER_IS_LITTLE_ENDIAN)
#error "DATA_ORDER must be defined!"
#endif

#ifndef HASH_CBLOCK
#error "HASH_CBLOCK must be defined!"
#endif
#ifndef HASH_LONG
#error "HASH_LONG must be defined!"
#endif
#ifndef HASH_CTX
#error "HASH_CTX must be defined!"
#endif

#ifndef HASH_UPDATE
#error "HASH_UPDATE must be defined!"
#endif
#ifndef HASH_TRANSFORM
#error "HASH_TRANSFORM must be defined!"
#endif
#if !defined(HASH_FINAL) && !defined(HASH_NO_FINAL)
#error "HASH_FINAL or HASH_NO_FINAL must be defined!"
#endif

#ifndef HASH_BLOCK_DATA_ORDER
#error "HASH_BLOCK_DATA_ORDER must be defined!"
#endif

/*
 * This common idiom is recognized by the compiler and turned into a
 * CPU-specific intrinsic as appropriate. 
 * e.g. GCC optimizes to roll on amd64 at -O0
 */
static inline uint32_t ROTATE(uint32_t a, uint32_t n)
{
	return (a<<n)|(a>>(32-n));
}

#if defined(DATA_ORDER_IS_BIG_ENDIAN)

#if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
# if (defined(__i386) || defined(__i386__) || \
      defined(__x86_64) || defined(__x86_64__))
    /*
     * This gives ~30-40% performance improvement in SHA-256 compiled
     * with gcc [on P4]. Well, first macro to be frank. We can pull
     * this trick on x86* platforms only, because these CPUs can fetch
     * unaligned data without raising an exception.
     */
#  define HOST_c2l(c,l)	({ unsigned int r=*((const unsigned int *)(c));	\
				   asm ("bswapl %0":"=r"(r):"0"(r));	\
				   (c)+=4; (l)=r;			})
#  define HOST_l2c(l,c)	({ unsigned int r=(l);			\
				   asm ("bswapl %0":"=r"(r):"0"(r));	\
				   *((unsigned int *)(c))=r; (c)+=4;	})
# endif
#endif

#ifndef HOST_c2l
#define HOST_c2l(c,l) do {l =(((unsigned long)(*((c)++)))<<24);	\
			  l|=(((unsigned long)(*((c)++)))<<16);	\
			  l|=(((unsigned long)(*((c)++)))<< 8);	\
			  l|=(((unsigned long)(*((c)++)))    );	\
		      } while (0)
#endif
#ifndef HOST_l2c
#define HOST_l2c(l,c) do {*((c)++)=(unsigned char)(((l)>>24)&0xff);	\
			  *((c)++)=(unsigned char)(((l)>>16)&0xff);	\
			  *((c)++)=(unsigned char)(((l)>> 8)&0xff);	\
			  *((c)++)=(unsigned char)(((l)    )&0xff);	\
		      } while (0)
#endif

#elif defined(DATA_ORDER_IS_LITTLE_ENDIAN)

#if defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)
#  define HOST_c2l(c,l)	((l)=*((const unsigned int *)(c)), (c)+=4)
#  define HOST_l2c(l,c)	(*((unsigned int *)(c))=(l), (c)+=4)
#endif

#ifndef HOST_c2l
#define HOST_c2l(c,l) do {l =(((unsigned long)(*((c)++)))    );	\
			  l|=(((unsigned long)(*((c)++)))<< 8);	\
			  l|=(((unsigned long)(*((c)++)))<<16);	\
			  l|=(((unsigned long)(*((c)++)))<<24);	\
		      } while (0)
#endif
#ifndef HOST_l2c
#define HOST_l2c(l,c) do {*((c)++)=(unsigned char)(((l)    )&0xff);	\
			  *((c)++)=(unsigned char)(((l)>> 8)&0xff);	\
			  *((c)++)=(unsigned char)(((l)>>16)&0xff);	\
			  *((c)++)=(unsigned char)(((l)>>24)&0xff);	\
		      } while (0)
#endif

#endif

/*
 * Time for some action:-)
 */

int
HASH_UPDATE(HASH_CTX *c, const void *data_, size_t len)
{
	const unsigned char *data = data_;
	unsigned char *p;
	HASH_LONG l;
	size_t n;

	if (len == 0)
		return 1;

	l = (c->Nl + (((HASH_LONG)len) << 3))&0xffffffffUL;
	/* 95-05-24 eay Fixed a bug with the overflow handling, thanks to
	 * Wei Dai <weidai@@eskimo.com> for pointing it out. */
	if (l < c->Nl) /* overflow */
		c->Nh++;
	c->Nh+=(HASH_LONG)(len>>29);	/* might cause compiler warning on 16-bit */
	c->Nl = l;

	n = c->num;
	if (n != 0) {
		p = (unsigned char *)c->data;

		if (len >= HASH_CBLOCK || len + n >= HASH_CBLOCK) {
			memcpy (p + n, data, HASH_CBLOCK - n);
			HASH_BLOCK_DATA_ORDER (c, p, 1);
			n = HASH_CBLOCK - n;
			data += n;
			len -= n;
			c->num = 0;
			memset (p,0,HASH_CBLOCK);	/* keep it zeroed */
		} else {
			memcpy (p + n, data, len);
			c->num += (unsigned int)len;
			return 1;
		}
	}

	n = len/HASH_CBLOCK;
	if (n > 0) {
		HASH_BLOCK_DATA_ORDER (c, data, n);
		n    *= HASH_CBLOCK;
		data += n;
		len -= n;
	}

	if (len != 0) {
		p = (unsigned char *)c->data;
		c->num = (unsigned int)len;
		memcpy (p, data, len);
	}
	return 1;
}


void HASH_TRANSFORM (HASH_CTX *c, const unsigned char *data)
{
	HASH_BLOCK_DATA_ORDER (c, data, 1);
}


#ifndef HASH_NO_FINAL
int HASH_FINAL (unsigned char *md, HASH_CTX *c)
{
	unsigned char *p = (unsigned char *)c->data;
	size_t n = c->num;

	p[n] = 0x80; /* there is always room for one */
	n++;

	if (n > (HASH_CBLOCK - 8)) {
		memset (p + n, 0, HASH_CBLOCK - n);
		n = 0;
		HASH_BLOCK_DATA_ORDER (c, p, 1);
	}
	memset (p + n, 0, HASH_CBLOCK - 8 - n);

	p += HASH_CBLOCK - 8;
#if   defined(DATA_ORDER_IS_BIG_ENDIAN)
	HOST_l2c(c->Nh, p);
	HOST_l2c(c->Nl, p);
#elif defined(DATA_ORDER_IS_LITTLE_ENDIAN)
	HOST_l2c(c->Nl, p);
	HOST_l2c(c->Nh, p);
#endif
	p -= HASH_CBLOCK;
	HASH_BLOCK_DATA_ORDER (c, p, 1);
	c->num = 0;
	memset (p, 0, HASH_CBLOCK);

#ifndef HASH_MAKE_STRING
#error "HASH_MAKE_STRING must be defined!"
#else
	HASH_MAKE_STRING(c, md);
#endif

	return 1;
}
#endif

#ifndef MD32_REG_T
#if defined(__alpha) || defined(__sparcv9) || defined(__mips)
#define MD32_REG_T long
/*
 * This comment was originaly written for MD5, which is why it
 * discusses A-D. But it basically applies to all 32-bit digests,
 * which is why it was moved to common header file.
 *
 * In case you wonder why A-D are declared as long and not
 * as MD5_LONG. Doing so results in slight performance
 * boost on LP64 architectures. The catch is we don't
 * really care if 32 MSBs of a 64-bit register get polluted
 * with eventual overflows as we *save* only 32 LSBs in
 * *either* case. Now declaring 'em long excuses the compiler
 * from keeping 32 MSBs zeroed resulting in 13% performance
 * improvement under SPARC Solaris7/64 and 5% under AlphaLinux.
 * Well, to be honest it should say that this *prevents*
 * performance degradation.
 *				<appro@@fy.chalmers.se>
 */
#else
/*
 * Above is not absolute and there are LP64 compilers that
 * generate better code if MD32_REG_T is defined int. The above
 * pre-processor condition reflects the circumstances under which
 * the conclusion was made and is subject to further extension.
 *				<appro@@fy.chalmers.se>
 */
#define MD32_REG_T int
#endif
#endif
@


1.21
log
@Less S390.

ok deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.20 2014/11/09 19:08:24 miod Exp $ */
d155 2
a156 2
# if ((defined(__i386) || defined(__i386__)) && !defined(I386_ONLY)) || \
      (defined(__x86_64) || defined(__x86_64__))
@


1.20
log
@Allow digest routines to provide their own HASH_FINAL routine; will be
necessary for upcoming GOST code.

From Dmitry Eremin-Solenikov
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.19 2014/10/20 13:06:54 bcook Exp $ */
a170 4
#if defined(__s390__) || defined(__s390x__)
# define HOST_c2l(c,l) ((l)=*((const unsigned int *)(c)), (c)+=4)
# define HOST_l2c(l,c) (*((unsigned int *)(c))=(l), (c)+=4)
#endif
a188 10
#if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
# if defined(__s390x__)
#  define HOST_c2l(c,l)	({ asm ("lrv	%0,%1"			\
				   :"=d"(l) :"m"(*(const unsigned int *)(c)));\
				   (c)+=4; 				})
#  define HOST_l2c(l,c)	({ asm ("strv	%1,%0"			\
				   :"=m"(*(unsigned int *)(c)) :"d"(l));\
				   (c)+=4; 				})
# endif
#endif
@


1.19
log
@digests: *_LONG_LOG2 is not used, stop talking about it.

Modified patch from Dmitry Eremin-Solenikov

leave the sole public define in ripemd.h

ok deraadt@@ miod@@
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.18 2014/08/18 19:11:48 bcook Exp $ */
d134 2
a135 2
#ifndef HASH_FINAL
#error "HASH_FINAL must be defined!"
d290 1
d327 1
@


1.18
log
@remove return value from HOST_c2l/l2c macros

These macros and asm inlines simulate a function returning a value, but
nothing ever uses this return value. Remove the pseudo-returns and
(void) casts discarding the unused values.

This, maybe unsurprisingly, speeds things up a bit. It also removes the
GCC 4.9 warnings about unused values.

ok miod@@ deraadt@@
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.17 2014/08/12 15:02:52 bcook Exp $ */
d67 1
a67 2
 *	has to be at lest 32 bit wide, if it's wider, then
 *	HASH_LONG_LOG2 *has to* be defined along
a99 1
 *	#define HASH_LONG_LOG2		MD5_LONG_LOG2
@


1.17
log
@Replace intrinsic ROTATE macros with an inline.

Without the cast/mask, the compiler is allowed to optimize this directly
to the correct CPU intrinsic for rotate.
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.16 2014/07/10 22:45:56 jsing Exp $ */
d170 1
a170 1
				   *((unsigned int *)(c))=r; (c)+=4; r;	})
d174 2
a175 2
# define HOST_c2l(c,l) ((l)=*((const unsigned int *)(c)), (c)+=4, (l))
# define HOST_l2c(l,c) (*((unsigned int *)(c))=(l), (c)+=4, (l))
d179 5
a183 5
#define HOST_c2l(c,l)	(l =(((unsigned long)(*((c)++)))<<24),		\
			 l|=(((unsigned long)(*((c)++)))<<16),		\
			 l|=(((unsigned long)(*((c)++)))<< 8),		\
			 l|=(((unsigned long)(*((c)++)))    ),		\
			 l)
d186 5
a190 5
#define HOST_l2c(l,c)	(*((c)++)=(unsigned char)(((l)>>24)&0xff),	\
			 *((c)++)=(unsigned char)(((l)>>16)&0xff),	\
			 *((c)++)=(unsigned char)(((l)>> 8)&0xff),	\
			 *((c)++)=(unsigned char)(((l)    )&0xff),	\
			 l)
d199 1
a199 1
				   (c)+=4; (l);				})
d202 1
a202 1
				   (c)+=4; (l);				})
d206 2
a207 2
#  define HOST_c2l(c,l)	((l)=*((const unsigned int *)(c)), (c)+=4, l)
#  define HOST_l2c(l,c)	(*((unsigned int *)(c))=(l), (c)+=4, l)
d211 5
a215 5
#define HOST_c2l(c,l)	(l =(((unsigned long)(*((c)++)))    ),		\
			 l|=(((unsigned long)(*((c)++)))<< 8),		\
			 l|=(((unsigned long)(*((c)++)))<<16),		\
			 l|=(((unsigned long)(*((c)++)))<<24),		\
			 l)
d218 5
a222 5
#define HOST_l2c(l,c)	(*((c)++)=(unsigned char)(((l)    )&0xff),	\
			 *((c)++)=(unsigned char)(((l)>> 8)&0xff),	\
			 *((c)++)=(unsigned char)(((l)>>16)&0xff),	\
			 *((c)++)=(unsigned char)(((l)>>24)&0xff),	\
			 l)
d309 2
a310 2
	(void)HOST_l2c(c->Nh, p);
	(void)HOST_l2c(c->Nl, p);
d312 2
a313 2
	(void)HOST_l2c(c->Nl, p);
	(void)HOST_l2c(c->Nh, p);
@


1.16
log
@Explicitly include <openssl/opensslconf.h> in every file that references
an OPENSSL_NO_* define. This avoids relying on something else pulling it
in for us, plus it fixes several cases where the #ifndef OPENSSL_NO_XYZ is
never going to do anything, since OPENSSL_NO_XYZ will never defined, due
to the fact that opensslconf.h has not been included.

This also includes some miscellaneous sorting/tidying of headers.
@
text
@d1 1
a1 1
/* $OpenBSD: md32_common.h,v 1.15 2014/06/12 15:49:27 deraadt Exp $ */
d112 2
d145 3
a147 1
 * Engage compiler specific rotate intrinsic function if available.
d149 4
a152 39
#undef ROTATE
#if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
  /*
   * Some GNU C inline assembler templates. Note that these are
   * rotates by *constant* number of bits! But that's exactly
   * what we need here...
   * 					<appro@@fy.chalmers.se>
   */
# if defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)
#  define ROTATE(a,n)	({ register unsigned int ret;	\
				asm (			\
				"roll %1,%0"		\
				: "=r"(ret)		\
				: "I"(n), "0"((unsigned int)(a))	\
				: "cc");		\
			   ret;				\
			})
# elif defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
	defined(__powerpc) || defined(__ppc__) || defined(__powerpc64__)
#  define ROTATE(a,n)	({ register unsigned int ret;	\
				asm (			\
				"rlwinm %0,%1,%2,0,31"	\
				: "=r"(ret)		\
				: "r"(a), "I"(n));	\
			   ret;				\
			})
# elif defined(__s390x__)
#  define ROTATE(a,n) ({ register unsigned int ret;	\
				asm ("rll %0,%1,%2"	\
				: "=r"(ret)		\
				: "r"(a), "I"(n));	\
			  ret;				\
			})
# endif
#endif

#ifndef ROTATE
#define ROTATE(a,n)     (((a)<<(n))|(((a)&0xffffffff)>>(32-(n))))
#endif
@


1.15
log
@tags as requested by miod and tedu
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d111 2
@


1.14
log
@Unifdef -UPEDANTIC. ok beck@@ tedu@@
@
text
@d1 1
a1 1
/* crypto/md32_common.h */
@


1.13
log
@More KNF.
@
text
@d144 1
a144 2
#ifndef PEDANTIC
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
d151 2
a152 2
#  if defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)
#   define ROTATE(a,n)	({ register unsigned int ret;	\
d160 1
a160 1
#  elif defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
d162 1
a162 1
#   define ROTATE(a,n)	({ register unsigned int ret;	\
d169 2
a170 2
#  elif defined(__s390x__)
#   define ROTATE(a,n) ({ register unsigned int ret;	\
a175 1
#  endif
d177 1
a177 1
#endif /* PEDANTIC */
d185 2
a186 3
#ifndef PEDANTIC
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
#  if ((defined(__i386) || defined(__i386__)) && !defined(I386_ONLY)) || \
d194 1
a194 1
#   define HOST_c2l(c,l)	({ unsigned int r=*((const unsigned int *)(c));	\
d197 1
a197 1
#   define HOST_l2c(l,c)	({ unsigned int r=(l);			\
a199 1
#  endif
d224 3
a226 4
#ifndef PEDANTIC
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
#  if defined(__s390x__)
#   define HOST_c2l(c,l)	({ asm ("lrv	%0,%1"			\
d229 1
a229 1
#   define HOST_l2c(l,c)	({ asm ("strv	%1,%0"			\
a231 1
#  endif
@


1.12
log
@There are no plans to ever build this with the Metrojerks compiler.
@
text
@d10 1
a10 1
 *    notice, this list of conditions and the following disclaimer. 
d380 1
a380 1
 * Well, to be honest it should say that this *prevents* 
@


1.11
log
@Remove support for big-endian i386 and amd64.

Before someone suggests the OpenSSL people are junkies, here is what they
mention about this:
	/* Most will argue that x86_64 is always little-endian. Well,
	 * yes, but then we have stratus.com who has modified gcc to
	 * "emulate" big-endian on x86. Is there evidence that they
	 * [or somebody else] won't do same for x86_64? Naturally no.
	 * And this line is waiting ready for that brave soul:-) */

So, yes, they are on drugs. But they are not alone, the stratus.com people are,
too.
@
text
@d145 1
a145 12
# if defined(_MSC_VER) || defined(__ICC)
#  define ROTATE(a,n)	_lrotl(a,n)
# elif defined(__MWERKS__)
#  if defined(__POWERPC__)
#   define ROTATE(a,n)	__rlwinm(a,n,0,31)
#  elif defined(__MC68K__)
/* Motorola specific tweak. <appro@@fy.chalmers.se> */
#   define ROTATE(a,n)	( n<24 ? __rol(a,n) : __ror(a,32-n) )
#  else
#   define ROTATE(a,n)	__rol(a,n)
#  endif
# elif defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
@


1.10
log
@First pass at applying KNF to the OpenSSL code, which almost makes it
readable. This pass is whitespace only and can readily be verified using
tr and md5.
@
text
@a201 1
#   if !defined(B_ENDIAN)
a213 1
#   endif
a251 2
# ifndef B_ENDIAN
/* See comment in DATA_ORDER_IS_BIG_ENDIAN section. */
a253 1
# endif
@


1.9
log
@openssl-1.0.0e: resolve conflicts
@
text
@d151 1
a151 1
    /* Motorola specific tweak. <appro@@fy.chalmers.se> */
d255 1
a255 1
   /* See comment in DATA_ORDER_IS_BIG_ENDIAN section. */
d282 4
a285 3
int HASH_UPDATE (HASH_CTX *c, const void *data_, size_t len)
	{
	const unsigned char *data=data_;
d290 2
a291 1
	if (len==0) return 1;
d293 1
a293 1
	l=(c->Nl+(((HASH_LONG)len)<<3))&0xffffffffUL;
d299 1
a299 1
	c->Nl=l;
d302 9
a310 11
	if (n != 0)
		{
		p=(unsigned char *)c->data;

		if (len >= HASH_CBLOCK || len+n >= HASH_CBLOCK)
			{
			memcpy (p+n,data,HASH_CBLOCK-n);
			HASH_BLOCK_DATA_ORDER (c,p,1);
			n      = HASH_CBLOCK-n;
			data  += n;
			len   -= n;
d313 2
a314 4
			}
		else
			{
			memcpy (p+n,data,len);
a316 1
			}
d318 1
d321 2
a322 3
	if (n > 0)
		{
		HASH_BLOCK_DATA_ORDER (c,data,n);
d325 2
a326 2
		len  -= n;
		}
d328 1
a328 2
	if (len != 0)
		{
d331 2
a332 2
		memcpy (p,data,len);
		}
d334 1
a334 1
	}
d338 3
a340 3
	{
	HASH_BLOCK_DATA_ORDER (c,data,1);
	}
d344 1
a344 1
	{
d351 6
a356 7
	if (n > (HASH_CBLOCK-8))
		{
		memset (p+n,0,HASH_CBLOCK-n);
		n=0;
		HASH_BLOCK_DATA_ORDER (c,p,1);
		}
	memset (p+n,0,HASH_CBLOCK-8-n);
d358 1
a358 1
	p += HASH_CBLOCK-8;
d360 2
a361 2
	(void)HOST_l2c(c->Nh,p);
	(void)HOST_l2c(c->Nl,p);
d363 2
a364 2
	(void)HOST_l2c(c->Nl,p);
	(void)HOST_l2c(c->Nh,p);
d367 3
a369 3
	HASH_BLOCK_DATA_ORDER (c,p,1);
	c->num=0;
	memset (p,0,HASH_CBLOCK);
d374 1
a374 1
	HASH_MAKE_STRING(c,md);
d378 1
a378 1
	}
@


1.8
log
@resolve conflicts, fix local changes
@
text
@d168 1
a168 1
				: "I"(n), "0"(a)	\
d386 1
d404 7
a410 3
 * Apparently there're LP64 compilers that generate better
 * code if A-D are declared int. Most notably GCC-x86_64
 * generates better code.
d413 2
@


1.7
log
@update to openssl-0.9.8i; tested by several, especially krw@@
@
text
@d244 2
a245 2
#   define HOST_c2l(c,l)	({ asm ("lrv	%0,0(%1)"		\
					:"=r"(l) : "r"(c));		\
d247 2
a248 2
#   define HOST_l2c(l,c)	({ asm ("strv	%0,0(%1)"		\
					: : "r"(l),"r"(c) : "memory");	\
d296 1
a296 1
	c->Nh+=(len>>29);	/* might cause compiler warning on 16-bit */
d334 1
a334 1
		c->num = len;
@


1.6
log
@resolve conflicts
@
text
@d304 1
a304 1
		if ((n+len) >= HASH_CBLOCK)
@


1.5
log
@resolve conflicts
@
text
@d3 1
a3 1
 * Copyright (c) 1999-2002 The OpenSSL Project.  All rights reserved.
a49 4
 * This product includes cryptographic software written by Eric Young
 * (eay@@cryptsoft.com).  This product includes software written by Tim
 * Hudson (tjh@@cryptsoft.com).
 *
d75 1
d77 3
a79 1
 *			int		num;
d82 2
a89 3
 * HASH_BLOCK_HOST_ORDER
 *	name of "block" function treating *aligned* input message
 *	in host byte order, implemented externally.
d91 2
a92 4
 *	name of "block" function treating *unaligned* input message
 *	in original (data) byte order, implemented externally (it
 *	actually is optional if data and host are of the same
 *	"endianess").
a95 13
 * Optional macros:
 *
 * B_ENDIAN or L_ENDIAN
 *	defines host byte-order.
 * HASH_LONG_LOG2
 *	defaults to 2 if not states otherwise.
 * HASH_LBLOCK
 *	assumed to be HASH_CBLOCK/4 if not stated otherwise.
 * HASH_BLOCK_DATA_ORDER_ALIGNED
 *	alternative "block" function capable of treating
 *	aligned input message in original (data) order,
 *	implemented externally.
 *
a103 1
 *	#define HASH_LBLOCK		MD5_LBLOCK
a106 1
 *	#define HASH_BLOCK_HOST_ORDER	md5_block_host_order
a111 4
#include <openssl/crypto.h>
#include <openssl/fips.h>
#include <openssl/err.h>

a135 9
#ifndef HASH_BLOCK_HOST_ORDER
#error "HASH_BLOCK_HOST_ORDER must be defined!"
#endif

#if 0
/*
 * Moved below as it's required only if HASH_BLOCK_DATA_ORDER_ALIGNED
 * isn't defined.
 */
a138 9
#endif

#ifndef HASH_LBLOCK
#define HASH_LBLOCK	(HASH_CBLOCK/4)
#endif

#ifndef HASH_LONG_LOG2
#define HASH_LONG_LOG2	2
#endif
d145 1
a145 1
# if 0 /* defined(_MSC_VER) */
a160 1
   *
d172 2
a173 1
#  elif defined(__powerpc) || defined(__ppc__) || defined(__powerpc64__)
d181 6
a186 32
#  endif
# endif

/*
 * Engage compiler specific "fetch in reverse byte order"
 * intrinsic function if available.
 */
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
  /* some GNU C inline assembler templates by <appro@@fy.chalmers.se> */
#  if (defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)) && !defined(I386_ONLY)
#   define BE_FETCH32(a)	({ register unsigned int l=(a);\
				asm (			\
				"bswapl %0"		\
				: "=r"(l) : "0"(l));	\
			  l;				\
			})
#  elif defined(__powerpc)
#   define LE_FETCH32(a)	({ register unsigned int l;	\
				asm (			\
				"lwbrx %0,0,%1"		\
				: "=r"(l)		\
				: "r"(a));		\
			   l;				\
			})

#  elif defined(__sparc) && defined(OPENSSL_SYS_ULTRASPARC)
#  define LE_FETCH32(a)	({ register unsigned int l;		\
				asm (				\
				"lda [%1]#ASI_PRIMARY_LITTLE,%0"\
				: "=r"(l)			\
				: "r"(a));			\
			   l;					\
a191 32
#if HASH_LONG_LOG2==2	/* Engage only if sizeof(HASH_LONG)== 4 */
/* A nice byte order reversal from Wei Dai <weidai@@eskimo.com> */
#ifdef ROTATE
/* 5 instructions with rotate instruction, else 9 */
#define REVERSE_FETCH32(a,l)	(					\
		l=*(const HASH_LONG *)(a),				\
		((ROTATE(l,8)&0x00FF00FF)|(ROTATE((l&0x00FF00FF),24)))	\
				)
#else
/* 6 instructions with rotate instruction, else 8 */
#define REVERSE_FETCH32(a,l)	(				\
		l=*(const HASH_LONG *)(a),			\
		l=(((l>>8)&0x00FF00FF)|((l&0x00FF00FF)<<8)),	\
		ROTATE(l,16)					\
				)
/*
 * Originally the middle line started with l=(((l&0xFF00FF00)>>8)|...
 * It's rewritten as above for two reasons:
 *	- RISCs aren't good at long constants and have to explicitely
 *	  compose 'em with several (well, usually 2) instructions in a
 *	  register before performing the actual operation and (as you
 *	  already realized:-) having same constant should inspire the
 *	  compiler to permanently allocate the only register for it;
 *	- most modern CPUs have two ALUs, but usually only one has
 *	  circuitry for shifts:-( this minor tweak inspires compiler
 *	  to schedule shift instructions in a better way...
 *
 *				<appro@@fy.chalmers.se>
 */
#endif
#endif

d196 20
a215 21
/*
 * Make some obvious choices. E.g., HASH_BLOCK_DATA_ORDER_ALIGNED
 * and HASH_BLOCK_HOST_ORDER ought to be the same if input data
 * and host are of the same "endianess". It's possible to mask
 * this with blank #define HASH_BLOCK_DATA_ORDER though...
 *
 *				<appro@@fy.chalmers.se>
 */
#if defined(B_ENDIAN)
#  if defined(DATA_ORDER_IS_BIG_ENDIAN)
#    if !defined(HASH_BLOCK_DATA_ORDER_ALIGNED) && HASH_LONG_LOG2==2
#      define HASH_BLOCK_DATA_ORDER_ALIGNED	HASH_BLOCK_HOST_ORDER
#    endif
#  elif defined(DATA_ORDER_IS_LITTLE_ENDIAN)
#    ifndef HOST_FETCH32
#      ifdef LE_FETCH32
#        define HOST_FETCH32(p,l)	LE_FETCH32(p)
#      elif defined(REVERSE_FETCH32)
#        define HOST_FETCH32(p,l)	REVERSE_FETCH32(p,l)
#      endif
#    endif
d217 1
a217 19
#elif defined(L_ENDIAN)
#  if defined(DATA_ORDER_IS_LITTLE_ENDIAN)
#    if !defined(HASH_BLOCK_DATA_ORDER_ALIGNED) && HASH_LONG_LOG2==2
#      define HASH_BLOCK_DATA_ORDER_ALIGNED	HASH_BLOCK_HOST_ORDER
#    endif
#  elif defined(DATA_ORDER_IS_BIG_ENDIAN)
#    ifndef HOST_FETCH32
#      ifdef BE_FETCH32
#        define HOST_FETCH32(p,l)	BE_FETCH32(p)
#      elif defined(REVERSE_FETCH32)
#        define HOST_FETCH32(p,l)	REVERSE_FETCH32(p,l)
#      endif
#    endif
#  endif
#endif

#if !defined(HASH_BLOCK_DATA_ORDER_ALIGNED)
#ifndef HASH_BLOCK_DATA_ORDER
#error "HASH_BLOCK_DATA_ORDER must be defined!"
d219 3
d224 1
a224 2
#if defined(DATA_ORDER_IS_BIG_ENDIAN)

d230 2
a231 23
#define HOST_p_c2l(c,l,n)	{					\
			switch (n) {					\
			case 0: l =((unsigned long)(*((c)++)))<<24;	\
			case 1: l|=((unsigned long)(*((c)++)))<<16;	\
			case 2: l|=((unsigned long)(*((c)++)))<< 8;	\
			case 3: l|=((unsigned long)(*((c)++)));		\
				} }
#define HOST_p_c2l_p(c,l,sc,len) {					\
			switch (sc) {					\
			case 0: l =((unsigned long)(*((c)++)))<<24;	\
				if (--len == 0) break;			\
			case 1: l|=((unsigned long)(*((c)++)))<<16;	\
				if (--len == 0) break;			\
			case 2: l|=((unsigned long)(*((c)++)))<< 8;	\
				} }
/* NOTE the pointer is not incremented at the end of this */
#define HOST_c2l_p(c,l,n)	{					\
			l=0; (c)+=n;					\
			switch (n) {					\
			case 3: l =((unsigned long)(*(--(c))))<< 8;	\
			case 2: l|=((unsigned long)(*(--(c))))<<16;	\
			case 1: l|=((unsigned long)(*(--(c))))<<24;	\
				} }
d237 1
d241 21
d267 2
a268 23
#define HOST_p_c2l(c,l,n)	{					\
			switch (n) {					\
			case 0: l =((unsigned long)(*((c)++)));		\
			case 1: l|=((unsigned long)(*((c)++)))<< 8;	\
			case 2: l|=((unsigned long)(*((c)++)))<<16;	\
			case 3: l|=((unsigned long)(*((c)++)))<<24;	\
				} }
#define HOST_p_c2l_p(c,l,sc,len) {					\
			switch (sc) {					\
			case 0: l =((unsigned long)(*((c)++)));		\
				if (--len == 0) break;			\
			case 1: l|=((unsigned long)(*((c)++)))<< 8;	\
				if (--len == 0) break;			\
			case 2: l|=((unsigned long)(*((c)++)))<<16;	\
				} }
/* NOTE the pointer is not incremented at the end of this */
#define HOST_c2l_p(c,l,n)	{					\
			l=0; (c)+=n;					\
			switch (n) {					\
			case 3: l =((unsigned long)(*(--(c))))<<16;	\
			case 2: l|=((unsigned long)(*(--(c))))<< 8;	\
			case 1: l|=((unsigned long)(*(--(c))));		\
				} }
d274 1
d282 1
a282 1
int HASH_UPDATE (HASH_CTX *c, const void *data_, unsigned long len)
d285 3
a287 3
	register HASH_LONG * p;
	register unsigned long l;
	int sw,sc,ew,ec;
d291 1
a291 1
	l=(c->Nl+(len<<3))&0xffffffffL;
d296 1
a296 1
	c->Nh+=(len>>29);
d299 2
a300 1
	if (c->num != 0)
d302 1
a302 3
		p=c->data;
		sw=c->num>>2;
		sc=c->num&0x03;
d304 1
a304 1
		if ((c->num+len) >= HASH_CBLOCK)
d306 7
a312 9
			l=p[sw]; HOST_p_c2l(data,l,sc); p[sw++]=l;
			for (; sw<HASH_LBLOCK; sw++)
				{
				HOST_c2l(data,l); p[sw]=l;
				}
			HASH_BLOCK_HOST_ORDER (c,p,1);
			len-=(HASH_CBLOCK-c->num);
			c->num=0;
			/* drop through and do the rest */
d316 2
a317 22
			c->num+=len;
			if ((sc+len) < 4) /* ugly, add char's to a word */
				{
				l=p[sw]; HOST_p_c2l_p(data,l,sc,len); p[sw]=l;
				}
			else
				{
				ew=(c->num>>2);
				ec=(c->num&0x03);
				if (sc)
					l=p[sw];
				HOST_p_c2l(data,l,sc);
				p[sw++]=l;
				for (; sw < ew; sw++)
					{
					HOST_c2l(data,l); p[sw]=l;
					}
				if (ec)
					{
					HOST_c2l_p(data,l,ec); p[sw]=l;
					}
				}
d322 2
a323 2
	sw=len/HASH_CBLOCK;
	if (sw > 0)
d325 4
a328 32
#if defined(HASH_BLOCK_DATA_ORDER_ALIGNED)
		/*
		 * Note that HASH_BLOCK_DATA_ORDER_ALIGNED gets defined
		 * only if sizeof(HASH_LONG)==4.
		 */
		if ((((unsigned long)data)%4) == 0)
			{
			/* data is properly aligned so that we can cast it: */
			HASH_BLOCK_DATA_ORDER_ALIGNED (c,(HASH_LONG *)data,sw);
			sw*=HASH_CBLOCK;
			data+=sw;
			len-=sw;
			}
		else
#if !defined(HASH_BLOCK_DATA_ORDER)
			while (sw--)
				{
				memcpy (p=c->data,data,HASH_CBLOCK);
				HASH_BLOCK_DATA_ORDER_ALIGNED(c,p,1);
				data+=HASH_CBLOCK;
				len-=HASH_CBLOCK;
				}
#endif
#endif
#if defined(HASH_BLOCK_DATA_ORDER)
			{
			HASH_BLOCK_DATA_ORDER(c,data,sw);
			sw*=HASH_CBLOCK;
			data+=sw;
			len-=sw;
			}
#endif
d331 1
a331 1
	if (len!=0)
d333 1
a333 1
		p = c->data;
d335 1
a335 8
		ew=len>>2;	/* words to copy */
		ec=len&0x03;
		for (; ew; ew--,p++)
			{
			HOST_c2l(data,l); *p=l;
			}
		HOST_c2l_p(data,l,ec);
		*p=l;
a342 13
#if defined(HASH_BLOCK_DATA_ORDER_ALIGNED)
	if ((((unsigned long)data)%4) == 0)
		/* data is properly aligned so that we can cast it: */
		HASH_BLOCK_DATA_ORDER_ALIGNED (c,(HASH_LONG *)data,1);
	else
#if !defined(HASH_BLOCK_DATA_ORDER)
		{
		memcpy (c->data,data,HASH_CBLOCK);
		HASH_BLOCK_DATA_ORDER_ALIGNED (c,c->data,1);
		}
#endif
#endif
#if defined(HASH_BLOCK_DATA_ORDER)
a343 1
#endif
d349 5
a353 37
	register HASH_LONG *p;
	register unsigned long l;
	register int i,j;
	static const unsigned char end[4]={0x80,0x00,0x00,0x00};
	const unsigned char *cp=end;

#if 0
	if(FIPS_mode() && !FIPS_md5_allowed())
	    {
	    FIPSerr(FIPS_F_HASH_FINAL,FIPS_R_NON_FIPS_METHOD);
	    return 0;
	    }
#endif

	/* c->num should definitly have room for at least one more byte. */
	p=c->data;
	i=c->num>>2;
	j=c->num&0x03;

#if 0
	/* purify often complains about the following line as an
	 * Uninitialized Memory Read.  While this can be true, the
	 * following p_c2l macro will reset l when that case is true.
	 * This is because j&0x03 contains the number of 'valid' bytes
	 * already in p[i].  If and only if j&0x03 == 0, the UMR will
	 * occur but this is also the only time p_c2l will do
	 * l= *(cp++) instead of l|= *(cp++)
	 * Many thanks to Alex Tang <altitude@@cic.net> for pickup this
	 * 'potential bug' */
#ifdef PURIFY
	if (j==0) p[i]=0; /* Yeah, but that's not the way to fix it:-) */
#endif
	l=p[i];
#else
	l = (j==0) ? 0 : p[i];
#endif
	HOST_p_c2l(cp,l,j); p[i++]=l; /* i is the next 'undefined word' */
d355 1
a355 1
	if (i>(HASH_LBLOCK-2)) /* save room for Nl and Nh */
d357 3
a359 3
		if (i<HASH_LBLOCK) p[i]=0;
		HASH_BLOCK_HOST_ORDER (c,p,1);
		i=0;
d361 1
a361 2
	for (; i<(HASH_LBLOCK-2); i++)
		p[i]=0;
d363 1
d365 2
a366 2
	p[HASH_LBLOCK-2]=c->Nh;
	p[HASH_LBLOCK-1]=c->Nl;
d368 2
a369 2
	p[HASH_LBLOCK-2]=c->Nl;
	p[HASH_LBLOCK-1]=c->Nh;
d371 4
a374 1
	HASH_BLOCK_HOST_ORDER (c,p,1);
a381 5
	c->num=0;
	/* clear stuff, HASH_BLOCK may be leaving some stuff on the stack
	 * but I'm not worried :-)
	OPENSSL_cleanse((void *)c,sizeof(HASH_CTX));
	 */
@


1.4
log
@merge 0.9.7b with local changes; crank majors for libssl/libcrypto
@
text
@d131 4
d214 1
a214 1
#  elif defined(__powerpc) || defined(__ppc)
d561 8
@


1.3
log
@OpenSSL 0.9.7 stable 2002 05 08 merge
@
text
@d3 1
a3 1
 * Copyright (c) 1999 The OpenSSL Project.  All rights reserved.
d201 1
a201 1
#  if defined(__i386) || defined(__i386__)
d227 1
a227 1
#  if (defined(__i386) || defined(__i386__)) && !defined(I386_ONLY)
d459 4
a462 1
				l=p[sw]; HOST_p_c2l(data,l,sc); p[sw++]=l;
d609 1
a609 1
	memset((void *)c,0,sizeof(HASH_CTX));
d613 25
@


1.2
log
@OpenSSL 0.9.5 merge

*warning* this bumps shared lib minors for libssl and libcrypto from 2.1 to 2.2
if you are using the ssl26 packages for ssh and other things to work you will
need to get new ones (see ~beck/libsslsnap/<arch>) on cvs or ~beck/src-patent.tar.gz on cvs
@
text
@d182 1
a182 1
# if defined(_MSC_VER)
d193 1
a193 1
# elif defined(__GNUC__) && __GNUC__>=2 && !defined(NO_ASM) && !defined(NO_INLINE_ASM)
d201 1
a201 1
#  if defined(__i386)
d225 1
a225 1
# if defined(__GNUC__) && __GNUC__>=2 && !defined(NO_ASM) && !defined(NO_INLINE_ASM)
d227 1
a227 1
#  if defined(__i386) && !defined(I386_ONLY)
d243 1
a243 1
#  elif defined(__sparc) && defined(ULTRASPARC)
d413 1
a413 1
void HASH_UPDATE (HASH_CTX *c, const void *data_, unsigned long len)
d420 1
a420 1
	if (len==0) return;
d469 1
a469 1
			return;
d523 1
d547 1
a547 1
void HASH_FINAL (unsigned char *md, HASH_CTX *c)
d608 1
@


1.1
log
@OpenSSL 0.9.4 merge
@
text
@d97 2
d183 11
a193 2
#  define ROTATE(a,n)     _lrotl(a,n)
# elif defined(__GNUC__) && __GNUC__>=2 && !defined(NO_ASM)
d203 1
a203 1
				asm volatile (		\
d210 1
a210 1
#  elif defined(__powerpc)
d212 1
a212 1
				asm volatile (		\
d225 1
a225 1
# if defined(__GNUC__) && __GNUC__>=2 && !defined(NO_ASM)
d229 1
a229 1
				asm volatile (		\
d236 1
a236 1
				asm volatile (		\
d245 1
a245 1
				asm volatile (			\
d413 1
a413 1
void HASH_UPDATE (HASH_CTX *c, const unsigned char *data, unsigned long len)
d415 1
d596 5
a600 4
	l=c->A; HOST_l2c(l,md);
	l=c->B; HOST_l2c(l,md);
	l=c->C; HOST_l2c(l,md);
	l=c->D; HOST_l2c(l,md);
@


1.1.1.1
log
@import openssl-0.9.7-beta1
@
text
@a96 2
 * HASH_MAKE_STRING
 *	macro convering context variables to an ASCII hash string.
d180 3
a182 12
# if 0 /* defined(_MSC_VER) */
#  define ROTATE(a,n)	_lrotl(a,n)
# elif defined(__MWERKS__)
#  if defined(__POWERPC__)
#   define ROTATE(a,n)	__rlwinm(a,n,0,31)
#  elif defined(__MC68K__)
    /* Motorola specific tweak. <appro@@fy.chalmers.se> */
#   define ROTATE(a,n)	( n<24 ? __rol(a,n) : __ror(a,32-n) )
#  else
#   define ROTATE(a,n)	__rol(a,n)
#  endif
# elif defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
d190 1
a190 1
#  if defined(__i386) || defined(__i386__)
d192 1
a192 1
				asm (			\
d199 1
a199 1
#  elif defined(__powerpc) || defined(__ppc)
d201 1
a201 1
				asm (			\
d214 1
a214 1
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
d216 1
a216 1
#  if (defined(__i386) || defined(__i386__)) && !defined(I386_ONLY)
d218 1
a218 1
				asm (			\
d225 1
a225 1
				asm (			\
d232 1
a232 1
#  elif defined(__sparc) && defined(OPENSSL_SYS_ULTRASPARC)
d234 1
a234 1
				asm (				\
d402 1
a402 1
int HASH_UPDATE (HASH_CTX *c, const void *data_, unsigned long len)
a403 1
	const unsigned char *data=data_;
d408 1
a408 1
	if (len==0) return 1;
d457 1
a457 1
			return 1;
a510 1
	return 1;
d534 1
a534 1
int HASH_FINAL (unsigned char *md, HASH_CTX *c)
d584 4
a587 5
#ifndef HASH_MAKE_STRING
#error "HASH_MAKE_STRING must be defined!"
#else
	HASH_MAKE_STRING(c,md);
#endif
a593 1
	return 1;
@


1.1.1.2
log
@import 0.9.7b (without idea and rc5)
@
text
@d3 1
a3 1
 * Copyright (c) 1999-2002 The OpenSSL Project.  All rights reserved.
d201 1
a201 1
#  if defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)
d227 1
a227 1
#  if (defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)) && !defined(I386_ONLY)
d459 1
a459 4
				if (sc)
					l=p[sw];
				HOST_p_c2l(data,l,sc);
				p[sw++]=l;
d606 1
a606 1
	OPENSSL_cleanse((void *)c,sizeof(HASH_CTX));
a609 25

#ifndef MD32_REG_T
#define MD32_REG_T long
/*
 * This comment was originaly written for MD5, which is why it
 * discusses A-D. But it basically applies to all 32-bit digests,
 * which is why it was moved to common header file.
 *
 * In case you wonder why A-D are declared as long and not
 * as MD5_LONG. Doing so results in slight performance
 * boost on LP64 architectures. The catch is we don't
 * really care if 32 MSBs of a 64-bit register get polluted
 * with eventual overflows as we *save* only 32 LSBs in
 * *either* case. Now declaring 'em long excuses the compiler
 * from keeping 32 MSBs zeroed resulting in 13% performance
 * improvement under SPARC Solaris7/64 and 5% under AlphaLinux.
 * Well, to be honest it should say that this *prevents* 
 * performance degradation.
 *				<appro@@fy.chalmers.se>
 * Apparently there're LP64 compilers that generate better
 * code if A-D are declared int. Most notably GCC-x86_64
 * generates better code.
 *				<appro@@fy.chalmers.se>
 */
#endif
@


1.1.1.3
log
@import of openssl-0.9.7g; tested on platforms from alpha to zaurus, ok deraadt@@
@
text
@a130 4
#include <openssl/crypto.h>
#include <openssl/fips.h>
#include <openssl/err.h>

d210 1
a210 1
#  elif defined(__powerpc) || defined(__ppc__) || defined(__powerpc64__)
a556 8

#if 0
	if(FIPS_mode() && !FIPS_md5_allowed())
	    {
	    FIPSerr(FIPS_F_HASH_FINAL,FIPS_R_NON_FIPS_METHOD);
	    return 0;
	    }
#endif
@


1.1.1.4
log
@import of OpenSSL 0.9.8h
@
text
@d3 1
a3 1
 * Copyright (c) 1999-2007 The OpenSSL Project.  All rights reserved.
d50 4
a78 1
 *			either {
d80 1
a80 3
 *			unsigned char	data[HASH_CBLOCK];
 *			};
 *			unsigned int	num;
a82 2
 *	data[] vector is expected to be zeroed upon first call to
 *	HASH_UPDATE.
d89 3
d93 4
a96 2
 *	name of "block" function capable of treating *unaligned* input
 *	message in original (data) byte order, implemented externally.
d100 13
d121 1
d125 1
d131 4
d159 9
d171 9
d186 1
a186 1
# if defined(_MSC_VER) || defined(__ICC)
d202 1
d214 1
a214 2
#  elif defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
	defined(__powerpc) || defined(__ppc__) || defined(__powerpc64__)
d222 32
a253 6
#  elif defined(__s390x__)
#   define ROTATE(a,n) ({ register unsigned int ret;	\
				asm ("rll %0,%1,%2"	\
				: "=r"(ret)		\
				: "r"(a), "I"(n));	\
			  ret;				\
d259 32
d295 37
a331 1
#if defined(DATA_ORDER_IS_BIG_ENDIAN)
d333 3
a335 20
#ifndef PEDANTIC
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
#  if ((defined(__i386) || defined(__i386__)) && !defined(I386_ONLY)) || \
      (defined(__x86_64) || defined(__x86_64__))
#   if !defined(B_ENDIAN)
    /*
     * This gives ~30-40% performance improvement in SHA-256 compiled
     * with gcc [on P4]. Well, first macro to be frank. We can pull
     * this trick on x86* platforms only, because these CPUs can fetch
     * unaligned data without raising an exception.
     */
#   define HOST_c2l(c,l)	({ unsigned int r=*((const unsigned int *)(c));	\
				   asm ("bswapl %0":"=r"(r):"0"(r));	\
				   (c)+=4; (l)=r;			})
#   define HOST_l2c(l,c)	({ unsigned int r=(l);			\
				   asm ("bswapl %0":"=r"(r):"0"(r));	\
				   *((unsigned int *)(c))=r; (c)+=4; r;	})
#   endif
#  endif
# endif
a336 3
#if defined(__s390__) || defined(__s390x__)
# define HOST_c2l(c,l) ((l)=*((const unsigned int *)(c)), (c)+=4, (l))
# define HOST_l2c(l,c) (*((unsigned int *)(c))=(l), (c)+=4, (l))
d339 2
a340 1
#ifndef HOST_c2l
d346 23
a368 2
#endif
#ifndef HOST_l2c
a373 1
#endif
a376 21
#ifndef PEDANTIC
# if defined(__GNUC__) && __GNUC__>=2 && !defined(OPENSSL_NO_ASM) && !defined(OPENSSL_NO_INLINE_ASM)
#  if defined(__s390x__)
#   define HOST_c2l(c,l)	({ asm ("lrv	%0,0(%1)"		\
					:"=r"(l) : "r"(c));		\
				   (c)+=4; (l);				})
#   define HOST_l2c(l,c)	({ asm ("strv	%0,0(%1)"		\
					: : "r"(l),"r"(c) : "memory");	\
				   (c)+=4; (l);				})
#  endif
# endif
#endif
#if defined(__i386) || defined(__i386__) || defined(__x86_64) || defined(__x86_64__)
# ifndef B_ENDIAN
   /* See comment in DATA_ORDER_IS_BIG_ENDIAN section. */
#  define HOST_c2l(c,l)	((l)=*((const unsigned int *)(c)), (c)+=4, l)
#  define HOST_l2c(l,c)	(*((unsigned int *)(c))=(l), (c)+=4, l)
# endif
#endif

#ifndef HOST_c2l
d382 23
a404 2
#endif
#ifndef HOST_l2c
a409 1
#endif
d417 1
a417 1
int HASH_UPDATE (HASH_CTX *c, const void *data_, size_t len)
d420 3
a422 3
	unsigned char *p;
	HASH_LONG l;
	size_t n;
d426 1
a426 1
	l=(c->Nl+(((HASH_LONG)len)<<3))&0xffffffffUL;
d431 1
a431 1
	c->Nh+=(len>>29);	/* might cause compiler warning on 16-bit */
d434 1
a434 2
	n = c->num;
	if (n != 0)
d436 3
a438 1
		p=(unsigned char *)c->data;
d440 1
a440 1
		if ((n+len) >= HASH_CBLOCK)
d442 9
a450 7
			memcpy (p+n,data,HASH_CBLOCK-n);
			HASH_BLOCK_DATA_ORDER (c,p,1);
			n      = HASH_CBLOCK-n;
			data  += n;
			len   -= n;
			c->num = 0;
			memset (p,0,HASH_CBLOCK);	/* keep it zeroed */
d454 22
a475 2
			memcpy (p+n,data,len);
			c->num += (unsigned int)len;
d480 2
a481 2
	n = len/HASH_CBLOCK;
	if (n > 0)
d483 32
a514 4
		HASH_BLOCK_DATA_ORDER (c,data,n);
		n    *= HASH_CBLOCK;
		data += n;
		len  -= n;
d517 1
a517 1
	if (len != 0)
d519 1
a519 1
		p = (unsigned char *)c->data;
d521 8
a528 1
		memcpy (p,data,len);
d536 13
d550 1
d556 37
a592 5
	unsigned char *p = (unsigned char *)c->data;
	size_t n = c->num;

	p[n] = 0x80; /* there is always room for one */
	n++;
d594 1
a594 1
	if (n > (HASH_CBLOCK-8))
d596 3
a598 3
		memset (p+n,0,HASH_CBLOCK-n);
		n=0;
		HASH_BLOCK_DATA_ORDER (c,p,1);
d600 2
a601 1
	memset (p+n,0,HASH_CBLOCK-8-n);
a602 1
	p += HASH_CBLOCK-8;
d604 2
a605 2
	(void)HOST_l2c(c->Nh,p);
	(void)HOST_l2c(c->Nl,p);
d607 2
a608 2
	(void)HOST_l2c(c->Nl,p);
	(void)HOST_l2c(c->Nh,p);
d610 1
a610 4
	p -= HASH_CBLOCK;
	HASH_BLOCK_DATA_ORDER (c,p,1);
	c->num=0;
	memset (p,0,HASH_CBLOCK);
d618 5
@


1.1.1.5
log
@import openssl-0.9.8j
@
text
@d304 1
a304 1
		if (len >= HASH_CBLOCK || len+n >= HASH_CBLOCK)
@


1.1.1.6
log
@import OpenSSL-1.0.0a
@
text
@d244 2
a245 2
#   define HOST_c2l(c,l)	({ asm ("lrv	%0,%1"			\
				   :"=d"(l) :"m"(*(const unsigned int *)(c)));\
d247 2
a248 2
#   define HOST_l2c(l,c)	({ asm ("strv	%1,%0"			\
				   :"=m"(*(unsigned int *)(c)) :"d"(l));\
d296 1
a296 1
	c->Nh+=(HASH_LONG)(len>>29);	/* might cause compiler warning on 16-bit */
d334 1
a334 1
		c->num = (unsigned int)len;
@


1.1.1.7
log
@import OpenSSL 1.0.0e
@
text
@d168 1
a168 1
				: "I"(n), "0"((unsigned int)(a))	\
a385 1
#if defined(__alpha) || defined(__sparcv9) || defined(__mips)
d403 3
a405 7
 */
#else
/*
 * Above is not absolute and there are LP64 compilers that
 * generate better code if MD32_REG_T is defined int. The above
 * pre-processor condition reflects the circumstances under which
 * the conclusion was made and is subject to further extension.
a407 2
#define MD32_REG_T int
#endif
@


