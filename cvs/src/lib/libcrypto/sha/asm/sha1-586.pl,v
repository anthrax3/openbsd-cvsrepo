head	1.14;
access;
symbols
	OPENBSD_6_1:1.14.0.2
	OPENBSD_6_1_BASE:1.14;
locks; strict;
comment	@# @;


1.14
date	2016.11.04.17.30.30;	author miod;	state Exp;
branches;
next	1.13;
commitid	uVQFi30uotTYOUA9;

1.13
date	2014.08.19.16.13.24;	author miod;	state Exp;
branches;
next	1.12;
commitid	rLZoiOusNVuJOUiU;

1.12
date	2014.04.17.20.18.57;	author miod;	state Exp;
branches;
next	1.11;

1.11
date	2012.10.13.21.25.13;	author djm;	state Exp;
branches;
next	1.10;

1.10
date	2010.10.01.22.58.57;	author djm;	state Exp;
branches;
next	1.9;

1.9
date	2009.01.05.21.36.38;	author djm;	state Exp;
branches;
next	1.8;

1.8
date	2008.09.06.12.17.52;	author djm;	state Exp;
branches;
next	1.7;

1.7
date	2005.04.29.05.39.26;	author djm;	state Exp;
branches;
next	1.6;

1.6
date	2004.04.08.08.03.14;	author markus;	state Exp;
branches;
next	1.5;

1.5
date	2001.06.22.00.03.20;	author beck;	state Exp;
branches;
next	1.4;

1.4
date	2000.04.15.06.18.45;	author beck;	state Exp;
branches;
next	1.3;

1.3
date	2000.03.19.11.11.48;	author beck;	state Exp;
branches;
next	1.2;

1.2
date	99.09.29.04.36.46;	author beck;	state Exp;
branches;
next	1.1;

1.1
date	98.10.05.20.12.49;	author ryker;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	98.10.05.20.12.49;	author ryker;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2002.09.05.12.50.55;	author markus;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2004.04.07.20.41.51;	author markus;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2005.04.29.05.37.16;	author djm;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2008.09.06.12.15.47;	author djm;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2009.01.09.12.14.00;	author djm;	state Exp;
branches;
next	1.1.1.7;

1.1.1.7
date	2010.10.01.22.54.10;	author djm;	state Exp;
branches;
next	1.1.1.8;

1.1.1.8
date	2012.10.13.21.23.43;	author djm;	state Exp;
branches;
next	;


desc
@@


1.14
log
@Replace all uses of magic numbers when operating on OPENSSL_ia32_P[] by
meaningful constants in a private header file, so that reviewers can actually
get a chance to figure out what the code is attempting to do without knowing
all cpuid bits.

While there, turn it from an array of two 32-bit ints into a properly aligned
64-bit int.

Use of OPENSSL_ia32_P is now restricted to the assembler parts. C code will
now always use OPENSSL_cpu_caps() and check for the proper bits in the
whole 64-bit word it returns.

i386 tests and ok jsing@@
@
text
@#!/usr/bin/env perl

# ====================================================================
# [Re]written by Andy Polyakov <appro@@fy.chalmers.se> for the OpenSSL
# project. The module is, however, dual licensed under OpenSSL and
# CRYPTOGAMS licenses depending on where you obtain it. For further
# details see http://www.openssl.org/~appro/cryptogams/.
# ====================================================================

# "[Re]written" was achieved in two major overhauls. In 2004 BODY_*
# functions were re-implemented to address P4 performance issue [see
# commentary below], and in 2006 the rest was rewritten in order to
# gain freedom to liberate licensing terms.

# January, September 2004.
#
# It was noted that Intel IA-32 C compiler generates code which
# performs ~30% *faster* on P4 CPU than original *hand-coded*
# SHA1 assembler implementation. To address this problem (and
# prove that humans are still better than machines:-), the
# original code was overhauled, which resulted in following
# performance changes:
#
#		compared with original	compared with Intel cc
#		assembler impl.		generated code
# Pentium	-16%			+48%
# PIII/AMD	+8%			+16%
# P4		+85%(!)			+45%
#
# As you can see Pentium came out as looser:-( Yet I reckoned that
# improvement on P4 outweights the loss and incorporate this
# re-tuned code to 0.9.7 and later.
# ----------------------------------------------------------------
#					<appro@@fy.chalmers.se>

# August 2009.
#
# George Spelvin has tipped that F_40_59(b,c,d) can be rewritten as
# '(c&d) + (b&(c^d))', which allows to accumulate partial results
# and lighten "pressure" on scratch registers. This resulted in
# >12% performance improvement on contemporary AMD cores (with no
# degradation on other CPUs:-). Also, the code was revised to maximize
# "distance" between instructions producing input to 'lea' instruction
# and the 'lea' instruction itself, which is essential for Intel Atom
# core and resulted in ~15% improvement.

# October 2010.
#
# Add SSSE3, Supplemental[!] SSE3, implementation. The idea behind it
# is to offload message schedule denoted by Wt in NIST specification,
# or Xupdate in OpenSSL source, to SIMD unit. The idea is not novel,
# and in SSE2 context was first explored by Dean Gaudet in 2004, see
# http://arctic.org/~dean/crypto/sha1.html. Since then several things
# have changed that made it interesting again:
#
# a) XMM units became faster and wider;
# b) instruction set became more versatile;
# c) an important observation was made by Max Locktykhin, which made
#    it possible to reduce amount of instructions required to perform
#    the operation in question, for further details see
#    http://software.intel.com/en-us/articles/improving-the-performance-of-the-secure-hash-algorithm-1/.

# April 2011.
#
# Add AVX code path, probably most controversial... The thing is that
# switch to AVX alone improves performance by as little as 4% in
# comparison to SSSE3 code path. But below result doesn't look like
# 4% improvement... Trouble is that Sandy Bridge decodes 'ro[rl]' as
# pair of µ-ops, and it's the additional µ-ops, two per round, that
# make it run slower than Core2 and Westmere. But 'sh[rl]d' is decoded
# as single µ-op by Sandy Bridge and it's replacing 'ro[rl]' with
# equivalent 'sh[rl]d' that is responsible for the impressive 5.1
# cycles per processed byte. But 'sh[rl]d' is not something that used
# to be fast, nor does it appear to be fast in upcoming Bulldozer
# [according to its optimization manual]. Which is why AVX code path
# is guarded by *both* AVX and synthetic bit denoting Intel CPUs.
# One can argue that it's unfair to AMD, but without 'sh[rl]d' it
# makes no sense to keep the AVX code path. If somebody feels that
# strongly, it's probably more appropriate to discuss possibility of
# using vector rotate XOP on AMD...

######################################################################
# Current performance is summarized in following table. Numbers are
# CPU clock cycles spent to process single byte (less is better).
#
#		x86		SSSE3		AVX
# Pentium	15.7		-
# PIII		11.5		-
# P4		10.6		-
# AMD K8	7.1		-
# Core2		7.3		6.1/+20%	-
# Atom		12.5		9.5(*)/+32%	-
# Westmere	7.3		5.6/+30%	-
# Sandy Bridge	8.8		6.2/+40%	5.1(**)/+70%
#
# (*)	Loop is 1056 instructions long and expected result is ~8.25.
#	It remains mystery [to me] why ILP is limited to 1.7.
#
# (**)	As per above comment, the result is for AVX *plus* sh[rl]d.

$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
push(@@INC,"${dir}","${dir}../../perlasm");
require "x86asm.pl";

&asm_init($ARGV[0],"sha1-586.pl",$ARGV[$#ARGV] eq "386");

$xmm=$ymm=0;
for (@@ARGV) { $xmm=1 if (/-DOPENSSL_IA32_SSE2/); }

$ymm=1 if ($xmm &&
		`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
			=~ /GNU assembler version ([2-9]\.[0-9]+)/ &&
		$1>=2.19);	# first version supporting AVX

&external_label("OPENSSL_ia32cap_P") if ($xmm);


$A="eax";
$B="ebx";
$C="ecx";
$D="edx";
$E="edi";
$T="esi";
$tmp1="ebp";

@@V=($A,$B,$C,$D,$E,$T);

$alt=0;	# 1 denotes alternative IALU implementation, which performs
	# 8% *worse* on P4, same on Westmere and Atom, 2% better on
	# Sandy Bridge...

sub BODY_00_15
	{
	local($n,$a,$b,$c,$d,$e,$f)=@@_;

	&comment("00_15 $n");

	&mov($f,$c);			# f to hold F_00_19(b,c,d)
	 if ($n==0)  { &mov($tmp1,$a); }
	 else        { &mov($a,$tmp1); }
	&rotl($tmp1,5);			# tmp1=ROTATE(a,5)
	 &xor($f,$d);
	&add($tmp1,$e);			# tmp1+=e;
	 &mov($e,&swtmp($n%16));	# e becomes volatile and is loaded
	 				# with xi, also note that e becomes
					# f in next round...
	&and($f,$b);
	&rotr($b,2);			# b=ROTATE(b,30)
	 &xor($f,$d);			# f holds F_00_19(b,c,d)
	&lea($tmp1,&DWP(0x5a827999,$tmp1,$e));	# tmp1+=K_00_19+xi

	if ($n==15) { &mov($e,&swtmp(($n+1)%16));# pre-fetch f for next round
		      &add($f,$tmp1); }	# f+=tmp1
	else        { &add($tmp1,$f); }	# f becomes a in next round
	&mov($tmp1,$a)			if ($alt && $n==15);
	}

sub BODY_16_19
	{
	local($n,$a,$b,$c,$d,$e,$f)=@@_;

	&comment("16_19 $n");

if ($alt) {
	&xor($c,$d);
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&and($tmp1,$c);			# tmp1 to hold F_00_19(b,c,d), b&=c^d
	 &xor($f,&swtmp(($n+8)%16));
	&xor($tmp1,$d);			# tmp1=F_00_19(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &add($e,$tmp1);		# e+=F_00_19(b,c,d)
	&xor($c,$d);			# restore $c
	 &mov($tmp1,$a);		# b in next round
	&rotr($b,$n==16?2:7);		# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &lea($f,&DWP(0x5a827999,$f,$e));# f+=F_00_19(b,c,d)+e
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
} else {
	&mov($tmp1,$c);			# tmp1 to hold F_00_19(b,c,d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&and($tmp1,$b);
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &xor($tmp1,$d);		# tmp1=F_00_19(b,c,d)
	&add($e,$tmp1);			# e+=F_00_19(b,c,d)
	 &mov($tmp1,$a);
	&rotr($b,2);			# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($tmp1,5);			# ROTATE(a,5)
	 &lea($f,&DWP(0x5a827999,$f,$e));# f+=F_00_19(b,c,d)+e
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$tmp1);		# f+=ROTATE(a,5)
}
	}

sub BODY_20_39
	{
	local($n,$a,$b,$c,$d,$e,$f)=@@_;
	local $K=($n<40)?0x6ed9eba1:0xca62c1d6;

	&comment("20_39 $n");

if ($alt) {
	&xor($tmp1,$c);			# tmp1 to hold F_20_39(b,c,d), b^=c
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);			# tmp1 holds F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+8)%16));
	&add($e,$tmp1);			# e+=F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &mov($tmp1,$a);		# b in next round
	&rotr($b,7);			# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f)		if($n<77);# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &xor($b,$c)			if($n==39);# warm up for BODY_40_59
	&and($tmp1,$b)			if($n==39);
	 &lea($f,&DWP($K,$f,$e));	# f+=e+K_XX_YY
	&mov($e,&swtmp(($n+1)%16))	if($n<79);# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
	&rotr($a,5)			if ($n==79);
} else {
	&mov($tmp1,$b);			# tmp1 to hold F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$c);
	 &xor($f,&swtmp(($n+8)%16));
	&xor($tmp1,$d);			# tmp1 holds F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &add($e,$tmp1);		# e+=F_20_39(b,c,d)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &mov($tmp1,$a);
	&rotl($tmp1,5);			# ROTATE(a,5)
	 &mov(&swtmp($n%16),$f) if($n<77);# xi=f
	&lea($f,&DWP($K,$f,$e));	# f+=e+K_XX_YY
	 &mov($e,&swtmp(($n+1)%16)) if($n<79);# pre-fetch f for next round
	&add($f,$tmp1);			# f+=ROTATE(a,5)
}
	}

sub BODY_40_59
	{
	local($n,$a,$b,$c,$d,$e,$f)=@@_;

	&comment("40_59 $n");

if ($alt) {
	&add($e,$tmp1);			# e+=b&(c^d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&mov($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&xor($c,$d);			# restore $c
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &and($tmp1,$c);
	&rotr($b,7);			# b=ROTATE(b,30)
	 &add($e,$tmp1);		# e+=c&d
	&mov($tmp1,$a);			# b in next round
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &xor($b,$c)			if ($n<59);
	&and($tmp1,$b)			if ($n<59);# tmp1 to hold F_40_59(b,c,d)
	 &lea($f,&DWP(0x8f1bbcdc,$f,$e));# f+=K_40_59+e+(b&(c^d))
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
} else {
	&mov($tmp1,$c);			# tmp1 to hold F_40_59(b,c,d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&and($tmp1,$b);
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &add($tmp1,$e);		# b&(c^d)+=e
	&rotr($b,2);			# b=ROTATE(b,30)
	 &mov($e,$a);			# e becomes volatile
	&rotl($e,5);			# ROTATE(a,5)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&lea($f,&DWP(0x8f1bbcdc,$f,$tmp1));# f+=K_40_59+e+(b&(c^d))
	 &mov($tmp1,$c);
	&add($f,$e);			# f+=ROTATE(a,5)
	 &and($tmp1,$d);
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$tmp1);		# f+=c&d
}
	}

&function_begin("sha1_block_data_order");
if ($xmm) {
  &static_label("ssse3_shortcut");
  &static_label("avx_shortcut")		if ($ymm);
  &static_label("K_XX_XX");

	&call	(&label("pic_point"));	# make it PIC!
  &set_label("pic_point");
	&blindpop($tmp1);
	&picmeup($T,"OPENSSL_ia32cap_P",$tmp1,&label("pic_point"));
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));

	&mov	($A,&DWP(0,$T));
	&mov	($D,&DWP(4,$T));
	&test	($D,"\$IA32CAP_MASK1_SSSE3");	# check SSSE3 bit
	&jz	(&label("x86"));
	&test	($A,"\$IA32CAP_MASK0_FXSR");	# check FXSR bit
	&jz	(&label("x86"));
	if ($ymm) {
		&and	($D,"\$IA32CAP_MASK1_AVX");	# mask AVX bit
		&and	($A,"\$IA32CAP_MASK0_INTEL");	# mask "Intel CPU" bit
		&or	($A,$D);
		&cmp	($A,"\$(IA32CAP_MASK1_AVX | IA32CAP_MASK0_INTEL)");
		&je	(&label("avx_shortcut"));
	}
	&jmp	(&label("ssse3_shortcut"));
  &set_label("x86",16);
}
	&mov($tmp1,&wparam(0));	# SHA_CTX *c
	&mov($T,&wparam(1));	# const void *input
	&mov($A,&wparam(2));	# size_t num
	&stack_push(16+3);	# allocate X[16]
	&shl($A,6);
	&add($A,$T);
	&mov(&wparam(2),$A);	# pointer beyond the end of input
	&mov($E,&DWP(16,$tmp1));# pre-load E
	&jmp(&label("loop"));

&set_label("loop",16);

	# copy input chunk to X, but reversing byte order!
	for ($i=0; $i<16; $i+=4)
		{
		&mov($A,&DWP(4*($i+0),$T));
		&mov($B,&DWP(4*($i+1),$T));
		&mov($C,&DWP(4*($i+2),$T));
		&mov($D,&DWP(4*($i+3),$T));
		&bswap($A);
		&bswap($B);
		&bswap($C);
		&bswap($D);
		&mov(&swtmp($i+0),$A);
		&mov(&swtmp($i+1),$B);
		&mov(&swtmp($i+2),$C);
		&mov(&swtmp($i+3),$D);
		}
	&mov(&wparam(1),$T);	# redundant in 1st spin

	&mov($A,&DWP(0,$tmp1));	# load SHA_CTX
	&mov($B,&DWP(4,$tmp1));
	&mov($C,&DWP(8,$tmp1));
	&mov($D,&DWP(12,$tmp1));
	# E is pre-loaded

	for($i=0;$i<16;$i++)	{ &BODY_00_15($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<20;$i++)	{ &BODY_16_19($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<40;$i++)	{ &BODY_20_39($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<60;$i++)	{ &BODY_40_59($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<80;$i++)	{ &BODY_20_39($i,@@V); unshift(@@V,pop(@@V)); }

	(($V[5] eq $D) and ($V[0] eq $E)) or die;	# double-check

	&mov($tmp1,&wparam(0));	# re-load SHA_CTX*
	&mov($D,&wparam(1));	# D is last "T" and is discarded

	&add($E,&DWP(0,$tmp1));	# E is last "A"...
	&add($T,&DWP(4,$tmp1));
	&add($A,&DWP(8,$tmp1));
	&add($B,&DWP(12,$tmp1));
	&add($C,&DWP(16,$tmp1));

	&mov(&DWP(0,$tmp1),$E);	# update SHA_CTX
	 &add($D,64);		# advance input pointer
	&mov(&DWP(4,$tmp1),$T);
	 &cmp($D,&wparam(2));	# have we reached the end yet?
	&mov(&DWP(8,$tmp1),$A);
	 &mov($E,$C);		# C is last "E" which needs to be "pre-loaded"
	&mov(&DWP(12,$tmp1),$B);
	 &mov($T,$D);		# input pointer
	&mov(&DWP(16,$tmp1),$C);
	&jb(&label("loop"));

	&stack_pop(16+3);
&function_end("sha1_block_data_order");

if ($xmm) {
######################################################################
# The SSSE3 implementation.
#
# %xmm[0-7] are used as ring @@X[] buffer containing quadruples of last
# 32 elements of the message schedule or Xupdate outputs. First 4
# quadruples are simply byte-swapped input, next 4 are calculated
# according to method originally suggested by Dean Gaudet (modulo
# being implemented in SSSE3). Once 8 quadruples or 32 elements are
# collected, it switches to routine proposed by Max Locktyukhin.
#
# Calculations inevitably require temporary reqisters, and there are
# no %xmm registers left to spare. For this reason part of the ring
# buffer, X[2..4] to be specific, is offloaded to 3 quadriples ring
# buffer on the stack. Keep in mind that X[2] is alias X[-6], X[3] -
# X[-5], and X[4] - X[-4]...
#
# Another notable optimization is aggressive stack frame compression
# aiming to minimize amount of 9-byte instructions...
#
# Yet another notable optimization is "jumping" $B variable. It means
# that there is no register permanently allocated for $B value. This
# allowed to eliminate one instruction from body_20_39...
#
my $Xi=4;			# 4xSIMD Xupdate round, start pre-seeded
my @@X=map("xmm$_",(4..7,0..3));	# pre-seeded for $Xi=4
my @@V=($A,$B,$C,$D,$E);
my $j=0;			# hash round
my @@T=($T,$tmp1);
my $inp;

my $_rol=sub { &rol(@@_) };
my $_ror=sub { &ror(@@_) };

&function_begin("_sha1_block_data_order_ssse3");
	&call	(&label("pic_point"));	# make it PIC!
	&set_label("pic_point");
	&blindpop($tmp1);
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));
&set_label("ssse3_shortcut");

	&movdqa	(@@X[3],&QWP(0,$tmp1));		# K_00_19
	&movdqa	(@@X[4],&QWP(16,$tmp1));		# K_20_39
	&movdqa	(@@X[5],&QWP(32,$tmp1));		# K_40_59
	&movdqa	(@@X[6],&QWP(48,$tmp1));		# K_60_79
	&movdqa	(@@X[2],&QWP(64,$tmp1));		# pbswap mask

	&mov	($E,&wparam(0));		# load argument block
	&mov	($inp=@@T[1],&wparam(1));
	&mov	($D,&wparam(2));
	&mov	(@@T[0],"esp");

	# stack frame layout
	#
	# +0	X[0]+K	X[1]+K	X[2]+K	X[3]+K	# XMM->IALU xfer area
	#	X[4]+K	X[5]+K	X[6]+K	X[7]+K
	#	X[8]+K	X[9]+K	X[10]+K	X[11]+K
	#	X[12]+K	X[13]+K	X[14]+K	X[15]+K
	#
	# +64	X[0]	X[1]	X[2]	X[3]	# XMM->XMM backtrace area
	#	X[4]	X[5]	X[6]	X[7]
	#	X[8]	X[9]	X[10]	X[11]	# even borrowed for K_00_19
	#
	# +112	K_20_39	K_20_39	K_20_39	K_20_39	# constants
	#	K_40_59	K_40_59	K_40_59	K_40_59
	#	K_60_79	K_60_79	K_60_79	K_60_79
	#	K_00_19	K_00_19	K_00_19	K_00_19
	#	pbswap mask
	#
	# +192	ctx				# argument block
	# +196	inp
	# +200	end
	# +204	esp
	&sub	("esp",208);
	&and	("esp",-64);

	&movdqa	(&QWP(112+0,"esp"),@@X[4]);	# copy constants
	&movdqa	(&QWP(112+16,"esp"),@@X[5]);
	&movdqa	(&QWP(112+32,"esp"),@@X[6]);
	&shl	($D,6);				# len*64
	&movdqa	(&QWP(112+48,"esp"),@@X[3]);
	&add	($D,$inp);			# end of input
	&movdqa	(&QWP(112+64,"esp"),@@X[2]);
	&add	($inp,64);
	&mov	(&DWP(192+0,"esp"),$E);		# save argument block
	&mov	(&DWP(192+4,"esp"),$inp);
	&mov	(&DWP(192+8,"esp"),$D);
	&mov	(&DWP(192+12,"esp"),@@T[0]);	# save original %esp

	&mov	($A,&DWP(0,$E));		# load context
	&mov	($B,&DWP(4,$E));
	&mov	($C,&DWP(8,$E));
	&mov	($D,&DWP(12,$E));
	&mov	($E,&DWP(16,$E));
	&mov	(@@T[0],$B);			# magic seed

	&movdqu	(@@X[-4&7],&QWP(-64,$inp));	# load input to %xmm[0-3]
	&movdqu	(@@X[-3&7],&QWP(-48,$inp));
	&movdqu	(@@X[-2&7],&QWP(-32,$inp));
	&movdqu	(@@X[-1&7],&QWP(-16,$inp));
	&pshufb	(@@X[-4&7],@@X[2]);		# byte swap
	&pshufb	(@@X[-3&7],@@X[2]);
	&pshufb	(@@X[-2&7],@@X[2]);
	&movdqa	(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot
	&pshufb	(@@X[-1&7],@@X[2]);
	&paddd	(@@X[-4&7],@@X[3]);		# add K_00_19
	&paddd	(@@X[-3&7],@@X[3]);
	&paddd	(@@X[-2&7],@@X[3]);
	&movdqa	(&QWP(0,"esp"),@@X[-4&7]);	# X[]+K xfer to IALU
	&psubd	(@@X[-4&7],@@X[3]);		# restore X[]
	&movdqa	(&QWP(0+16,"esp"),@@X[-3&7]);
	&psubd	(@@X[-3&7],@@X[3]);
	&movdqa	(&QWP(0+32,"esp"),@@X[-2&7]);
	&psubd	(@@X[-2&7],@@X[3]);
	&movdqa	(@@X[0],@@X[-3&7]);
	&jmp	(&label("loop"));

######################################################################
# SSE instruction sequence is first broken to groups of independent
# instructions, independent in respect to their inputs and shifter
# (not all architectures have more than one). Then IALU instructions
# are "knitted in" between the SSE groups. Distance is maintained for
# SSE latency of 2 in hope that it fits better upcoming AMD Bulldozer
# [which allegedly also implements SSSE3]...
#
# Temporary registers usage. X[2] is volatile at the entry and at the
# end is restored from backtrace ring buffer. X[3] is expected to
# contain current K_XX_XX constant and is used to caclulate X[-1]+K
# from previous round, it becomes volatile the moment the value is
# saved to stack for transfer to IALU. X[4] becomes volatile whenever
# X[-4] is accumulated and offloaded to backtrace ring buffer, at the
# end it is loaded with next K_XX_XX [which becomes X[3] in next
# round]...
#
sub Xupdate_ssse3_16_31()		# recall that $Xi starts wtih 4
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 40 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&palignr(@@X[0],@@X[-4&7],8);	# compose "X[-14]" in "X[0]"
	&movdqa	(@@X[2],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &paddd	(@@X[3],@@X[-1&7]);
	  &movdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&psrldq	(@@X[2],4);		# "X[-3]", 3 dwords
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&pxor	(@@X[0],@@X[-4&7]);	# "X[0]"^="X[-16]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[2],@@X[-2&7]);	# "X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[2]);		# "X[0]"^="X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&movdqa	(@@X[4],@@X[0]);
	&movdqa	(@@X[2],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pslldq	(@@X[4],12);		# "X[0]"<<96, extract one dword
	&paddd	(@@X[0],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&psrld	(@@X[2],31);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&movdqa	(@@X[3],@@X[4]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&psrld	(@@X[4],30);
	&por	(@@X[0],@@X[2]);		# "X[0]"<<<=1
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if ($Xi>5);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pslld	(@@X[3],2);
	&pxor	(@@X[0],@@X[4]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(@@X[4],&QWP(112-16+16*(($Xi)/5),"esp"));	# K_XX_XX
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[3]);		# "X[0]"^=("X[0]"<<96)<<<2
	  &movdqa	(@@X[1],@@X[-2&7])	if ($Xi<7);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions [if any]

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xupdate_ssse3_32_79()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 to 48 instructions
  my ($a,$b,$c,$d,$e);

	&movdqa	(@@X[2],@@X[-1&7])	if ($Xi==8);
	 eval(shift(@@insns));		# body_20_39
	&pxor	(@@X[0],@@X[-4&7]);	# "X[0]"="X[-32]"^"X[-16]"
	&palignr(@@X[2],@@X[-2&7],8);	# compose "X[-6]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&pxor	(@@X[0],@@X[-7&7]);	# "X[0]"^="X[-28]"
	  &movdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);	# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 if ($Xi%5) {
	  &movdqa	(@@X[4],@@X[3]);	# "perpetuate" K_XX_XX...
	 } else {			# ... or load next one
	  &movdqa	(@@X[4],&QWP(112-16+16*($Xi/5),"esp"));
	 }
	  &paddd	(@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[2]);		# "X[0]"^="X[-6]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&movdqa	(@@X[2],@@X[0]);
	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&pslld	(@@X[0],2);
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	&psrld	(@@X[2],30);
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&por	(@@X[0],@@X[2]);		# "X[0]"<<<=2
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	  &movdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if($Xi<19);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	  &movdqa	(@@X[3],@@X[0])	if ($Xi<19);
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xuplast_ssse3_80()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	  &paddd	(@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer IALU

	 foreach (@@insns) { eval; }		# remaining instructions

	&mov	($inp=@@T[1],&DWP(192+4,"esp"));
	&cmp	($inp,&DWP(192+8,"esp"));
	&je	(&label("done"));

	&movdqa	(@@X[3],&QWP(112+48,"esp"));	# K_00_19
	&movdqa	(@@X[2],&QWP(112+64,"esp"));	# pbswap mask
	&movdqu	(@@X[-4&7],&QWP(0,$inp));	# load input
	&movdqu	(@@X[-3&7],&QWP(16,$inp));
	&movdqu	(@@X[-2&7],&QWP(32,$inp));
	&movdqu	(@@X[-1&7],&QWP(48,$inp));
	&add	($inp,64);
	&pshufb	(@@X[-4&7],@@X[2]);		# byte swap
	&mov	(&DWP(192+4,"esp"),$inp);
	&movdqa	(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot

  $Xi=0;
}

sub Xloop_ssse3()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&pshufb	(@@X[($Xi-3)&7],@@X[2]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&paddd	(@@X[($Xi-4)&7],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&movdqa	(&QWP(0+16*$Xi,"esp"),@@X[($Xi-4)&7]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&psubd	(@@X[($Xi-4)&7],@@X[3]);

	foreach (@@insns) { eval; }
  $Xi++;
}

sub Xtail_ssse3()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	foreach (@@insns) { eval; }
}

sub body_00_19 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&add	($e,&DWP(4*($j&15),"esp"));',	# X[]+K xfer
	'&xor	($c,$d);',
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&and	(@@T[0],$c);',	# ($b&($c^$d))
	'&xor	($c,$d);',	# restore $c
	'&xor	(@@T[0],$d);',
	'&add	($e,$a);',
	'&$_ror	($b,$j?7:2);',	# $b>>>2
	'&add	($e,@@T[0]);'	.'$j++; unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

sub body_20_39 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&add	($e,&DWP(4*($j++&15),"esp"));',	# X[]+K xfer
	'&xor	(@@T[0],$d);',	# ($b^$d)
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&xor	(@@T[0],$c);',	# ($b^$d^$c)
	'&add	($e,$a);',
	'&$_ror	($b,7);',	# $b>>>2
	'&add	($e,@@T[0]);'	.'unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

sub body_40_59 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&mov	(@@T[1],$c);',
	'&xor	($c,$d);',
	'&add	($e,&DWP(4*($j++&15),"esp"));',	# X[]+K xfer
	'&and	(@@T[1],$d);',
	'&and	(@@T[0],$c);',	# ($b&($c^$d))
	'&$_ror	($b,7);',	# $b>>>2
	'&add	($e,@@T[1]);',
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&add	($e,@@T[0]);',
	'&xor	($c,$d);',	# restore $c
	'&add	($e,$a);'	.'unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

&set_label("loop",16);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_32_79(\&body_00_19);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xuplast_ssse3_80(\&body_20_39);	# can jump to "done"

				$saved_j=$j; @@saved_V=@@V;

	&Xloop_ssse3(\&body_20_39);
	&Xloop_ssse3(\&body_20_39);
	&Xloop_ssse3(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	($B,@@T[0]);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);
	&movdqa	(@@X[0],@@X[-3&7]);

	&jmp	(&label("loop"));

&set_label("done",16);		$j=$saved_j; @@V=@@saved_V;

	&Xtail_ssse3(\&body_20_39);
	&Xtail_ssse3(\&body_20_39);
	&Xtail_ssse3(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&mov	("esp",&DWP(192+12,"esp"));	# restore %esp
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);

&function_end("_sha1_block_data_order_ssse3");

if ($ymm) {
my $Xi=4;			# 4xSIMD Xupdate round, start pre-seeded
my @@X=map("xmm$_",(4..7,0..3));	# pre-seeded for $Xi=4
my @@V=($A,$B,$C,$D,$E);
my $j=0;			# hash round
my @@T=($T,$tmp1);
my $inp;

my $_rol=sub { &shld(@@_[0],@@_) };
my $_ror=sub { &shrd(@@_[0],@@_) };

&function_begin("_sha1_block_data_order_avx");
	&call	(&label("pic_point"));	# make it PIC!
	&set_label("pic_point");
	&blindpop($tmp1);
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));
&set_label("avx_shortcut");
	&vzeroall();

	&vmovdqa(@@X[3],&QWP(0,$tmp1));		# K_00_19
	&vmovdqa(@@X[4],&QWP(16,$tmp1));		# K_20_39
	&vmovdqa(@@X[5],&QWP(32,$tmp1));		# K_40_59
	&vmovdqa(@@X[6],&QWP(48,$tmp1));		# K_60_79
	&vmovdqa(@@X[2],&QWP(64,$tmp1));		# pbswap mask

	&mov	($E,&wparam(0));		# load argument block
	&mov	($inp=@@T[1],&wparam(1));
	&mov	($D,&wparam(2));
	&mov	(@@T[0],"esp");

	# stack frame layout
	#
	# +0	X[0]+K	X[1]+K	X[2]+K	X[3]+K	# XMM->IALU xfer area
	#	X[4]+K	X[5]+K	X[6]+K	X[7]+K
	#	X[8]+K	X[9]+K	X[10]+K	X[11]+K
	#	X[12]+K	X[13]+K	X[14]+K	X[15]+K
	#
	# +64	X[0]	X[1]	X[2]	X[3]	# XMM->XMM backtrace area
	#	X[4]	X[5]	X[6]	X[7]
	#	X[8]	X[9]	X[10]	X[11]	# even borrowed for K_00_19
	#
	# +112	K_20_39	K_20_39	K_20_39	K_20_39	# constants
	#	K_40_59	K_40_59	K_40_59	K_40_59
	#	K_60_79	K_60_79	K_60_79	K_60_79
	#	K_00_19	K_00_19	K_00_19	K_00_19
	#	pbswap mask
	#
	# +192	ctx				# argument block
	# +196	inp
	# +200	end
	# +204	esp
	&sub	("esp",208);
	&and	("esp",-64);

	&vmovdqa(&QWP(112+0,"esp"),@@X[4]);	# copy constants
	&vmovdqa(&QWP(112+16,"esp"),@@X[5]);
	&vmovdqa(&QWP(112+32,"esp"),@@X[6]);
	&shl	($D,6);				# len*64
	&vmovdqa(&QWP(112+48,"esp"),@@X[3]);
	&add	($D,$inp);			# end of input
	&vmovdqa(&QWP(112+64,"esp"),@@X[2]);
	&add	($inp,64);
	&mov	(&DWP(192+0,"esp"),$E);		# save argument block
	&mov	(&DWP(192+4,"esp"),$inp);
	&mov	(&DWP(192+8,"esp"),$D);
	&mov	(&DWP(192+12,"esp"),@@T[0]);	# save original %esp

	&mov	($A,&DWP(0,$E));		# load context
	&mov	($B,&DWP(4,$E));
	&mov	($C,&DWP(8,$E));
	&mov	($D,&DWP(12,$E));
	&mov	($E,&DWP(16,$E));
	&mov	(@@T[0],$B);			# magic seed

	&vmovdqu(@@X[-4&7],&QWP(-64,$inp));	# load input to %xmm[0-3]
	&vmovdqu(@@X[-3&7],&QWP(-48,$inp));
	&vmovdqu(@@X[-2&7],&QWP(-32,$inp));
	&vmovdqu(@@X[-1&7],&QWP(-16,$inp));
	&vpshufb(@@X[-4&7],@@X[-4&7],@@X[2]);	# byte swap
	&vpshufb(@@X[-3&7],@@X[-3&7],@@X[2]);
	&vpshufb(@@X[-2&7],@@X[-2&7],@@X[2]);
	&vmovdqa(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot
	&vpshufb(@@X[-1&7],@@X[-1&7],@@X[2]);
	&vpaddd	(@@X[0],@@X[-4&7],@@X[3]);		# add K_00_19
	&vpaddd	(@@X[1],@@X[-3&7],@@X[3]);
	&vpaddd	(@@X[2],@@X[-2&7],@@X[3]);
	&vmovdqa(&QWP(0,"esp"),@@X[0]);		# X[]+K xfer to IALU
	&vmovdqa(&QWP(0+16,"esp"),@@X[1]);
	&vmovdqa(&QWP(0+32,"esp"),@@X[2]);
	&jmp	(&label("loop"));

sub Xupdate_avx_16_31()		# recall that $Xi starts wtih 4
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 40 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpalignr(@@X[0],@@X[-3&7],@@X[-4&7],8);	# compose "X[-14]" in "X[0]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	  &vmovdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpsrldq(@@X[2],@@X[-1&7],4);		# "X[-3]", 3 dwords
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpxor	(@@X[0],@@X[0],@@X[-4&7]);		# "X[0]"^="X[-16]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[2],@@X[2],@@X[-2&7]);		# "X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"^="X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpsrld	(@@X[2],@@X[0],31);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpslldq(@@X[4],@@X[0],12);		# "X[0]"<<96, extract one dword
	&vpaddd	(@@X[0],@@X[0],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpsrld	(@@X[3],@@X[4],30);
	&vpor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"<<<=1
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpslld	(@@X[4],@@X[4],2);
	  &vmovdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if ($Xi>5);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpxor	(@@X[0],@@X[0],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[4]);		# "X[0]"^=("X[0]"<<96)<<<2
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &vmovdqa	(@@X[4],&QWP(112-16+16*(($Xi)/5),"esp"));	# K_XX_XX
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions [if any]

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xupdate_avx_32_79()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 to 48 instructions
  my ($a,$b,$c,$d,$e);

	&vpalignr(@@X[2],@@X[-1&7],@@X[-2&7],8);	# compose "X[-6]"
	&vpxor	(@@X[0],@@X[0],@@X[-4&7]);	# "X[0]"="X[-32]"^"X[-16]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&vpxor	(@@X[0],@@X[0],@@X[-7&7]);	# "X[0]"^="X[-28]"
	  &vmovdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);	# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 if ($Xi%5) {
	  &vmovdqa	(@@X[4],@@X[3]);	# "perpetuate" K_XX_XX...
	 } else {			# ... or load next one
	  &vmovdqa	(@@X[4],&QWP(112-16+16*($Xi/5),"esp"));
	 }
	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"^="X[-6]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&vpsrld	(@@X[2],@@X[0],30);
	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpslld	(@@X[0],@@X[0],2);
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpor	(@@X[0],@@X[0],@@X[2]);	# "X[0]"<<<=2
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	  &vmovdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if($Xi<19);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xuplast_avx_80()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer IALU

	 foreach (@@insns) { eval; }		# remaining instructions

	&mov	($inp=@@T[1],&DWP(192+4,"esp"));
	&cmp	($inp,&DWP(192+8,"esp"));
	&je	(&label("done"));

	&vmovdqa(@@X[3],&QWP(112+48,"esp"));	# K_00_19
	&vmovdqa(@@X[2],&QWP(112+64,"esp"));	# pbswap mask
	&vmovdqu(@@X[-4&7],&QWP(0,$inp));	# load input
	&vmovdqu(@@X[-3&7],&QWP(16,$inp));
	&vmovdqu(@@X[-2&7],&QWP(32,$inp));
	&vmovdqu(@@X[-1&7],&QWP(48,$inp));
	&add	($inp,64);
	&vpshufb(@@X[-4&7],@@X[-4&7],@@X[2]);		# byte swap
	&mov	(&DWP(192+4,"esp"),$inp);
	&vmovdqa(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot

  $Xi=0;
}

sub Xloop_avx()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpshufb	(@@X[($Xi-3)&7],@@X[($Xi-3)&7],@@X[2]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpaddd	(@@X[$Xi&7],@@X[($Xi-4)&7],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vmovdqa	(&QWP(0+16*$Xi,"esp"),@@X[$Xi&7]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	foreach (@@insns) { eval; }
  $Xi++;
}

sub Xtail_avx()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	foreach (@@insns) { eval; }
}

&set_label("loop",16);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_32_79(\&body_00_19);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xuplast_avx_80(\&body_20_39);	# can jump to "done"

				$saved_j=$j; @@saved_V=@@V;

	&Xloop_avx(\&body_20_39);
	&Xloop_avx(\&body_20_39);
	&Xloop_avx(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	($B,@@T[0]);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);

	&jmp	(&label("loop"));

&set_label("done",16);		$j=$saved_j; @@V=@@saved_V;

	&Xtail_avx(\&body_20_39);
	&Xtail_avx(\&body_20_39);
	&Xtail_avx(\&body_20_39);

	&vzeroall();

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&mov	("esp",&DWP(192+12,"esp"));	# restore %esp
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);
&function_end("_sha1_block_data_order_avx");
}
&set_label("K_XX_XX",64);
&data_word(0x5a827999,0x5a827999,0x5a827999,0x5a827999);	# K_00_19
&data_word(0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1);	# K_20_39
&data_word(0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc);	# K_40_59
&data_word(0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6);	# K_60_79
&data_word(0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f);	# pbswap mask
}
&asciz("SHA1 block transform for x86, CRYPTOGAMS by <appro\@@openssl.org>");

&asm_finish();
@


1.13
log
@Three independent typos for `independent' or `independently'.
@
text
@d306 1
a306 1
	&test	($D,1<<9);		# check SSSE3 bit
d308 1
a308 1
	&test	($A,1<<24);		# check FXSR bit
d311 2
a312 2
		&and	($D,1<<28);		# mask AVX bit
		&and	($A,1<<30);		# mask "Intel CPU" bit
d314 1
a314 1
		&cmp	($A,1<<28|1<<30);
@


1.12
log
@No need to consider being compiled by NASM anymore.
@
text
@d505 1
a505 1
# SSE instruction sequence is first broken to groups of indepentent
@


1.11
log
@resolve conflicts
@
text
@a114 4
$ymm=1 if ($xmm && !$ymm && $ARGV[0] eq "win32n" && 
		`nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/ &&
		$1>=2.03);	# first version supporting AVX

@


1.10
log
@resolve conflicts, fix local changes
@
text
@d15 2
d36 65
d107 15
d132 4
d148 1
a148 2
	 &and($f,$b);
	&mov($e,&swtmp($n%16));		# e becomes volatile and is loaded
d151 2
d154 1
a154 2
	&rotr($b,2);			# b=ROTATE(b,30)
	 &lea($tmp1,&DWP(0x5a827999,$tmp1,$e));	# tmp1+=K_00_19+xi
d156 2
a157 1
	if ($n==15) { &add($f,$tmp1); }	# f+=tmp1
d159 1
d168 23
a190 7
	&mov($f,&swtmp($n%16));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,$c);		# tmp1 to hold F_00_19(b,c,d)
	&xor($f,&swtmp(($n+2)%16));
	 &xor($tmp1,$d);
	&xor($f,&swtmp(($n+8)%16));
	 &and($tmp1,$b);		# tmp1 holds F_00_19(b,c,d)
	&rotr($b,2);			# b=ROTATE(b,30)
d194 9
a202 6
	&mov(&swtmp($n%16),$f);		# xi=f
	&lea($f,&DWP(0x5a827999,$f,$e));# f+=K_00_19+e
	 &mov($e,$a);			# e becomes volatile
	&rotl($e,5);			# e=ROTATE(a,5)
	 &add($f,$tmp1);		# f+=F_00_19(b,c,d)
	&add($f,$e);			# f+=ROTATE(a,5)
d212 19
d232 1
a232 3
	 &mov($f,&swtmp($n%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &xor($f,&swtmp(($n+2)%16));
d238 9
a246 6
	 &add($tmp1,$e);
	&mov(&swtmp($n%16),$f);		# xi=f
	 &mov($e,$a);			# e becomes volatile
	&rotl($e,5);			# e=ROTATE(a,5)
	 &lea($f,&DWP($K,$f,$tmp1));	# f+=K_20_39+e
	&add($f,$e);			# f+=ROTATE(a,5)
d255 26
a280 8
	&mov($f,&swtmp($n%16));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,&swtmp(($n+2)%16));
	&xor($f,$tmp1);
	 &mov($tmp1,&swtmp(($n+8)%16));
	&xor($f,$tmp1);
	 &mov($tmp1,&swtmp(($n+13)%16));
	&xor($f,$tmp1);			# f holds xa^xb^xc^xd
	 &mov($tmp1,$b);		# tmp1 to hold F_40_59(b,c,d)
d282 1
a282 6
	 &or($tmp1,$c);
	&mov(&swtmp($n%16),$f);		# xi=f
	 &and($tmp1,$d);
	&lea($f,&DWP(0x8f1bbcdc,$f,$e));# f+=K_40_59+e
	 &mov($e,$b);			# e becomes volatile and is used
					# to calculate F_40_59(b,c,d)
d284 5
a288 5
	 &and($e,$c);
	&or($tmp1,$e);			# tmp1 holds F_40_59(b,c,d)		
	 &mov($e,$a);
	&rotl($e,5);			# e=ROTATE(a,5)
	 &add($f,$tmp1);		# f+=tmp1;
d290 4
d297 27
d327 1
a327 1
	&stack_push(16);	# allocate X[16]
d332 1
d334 1
a334 1
	&set_label("loop",16);
d388 1
a388 1
	&stack_pop(16);
d390 837
@


1.9
log
@update to openssl-0.9.8i; tested by several, especially krw@@
@
text
@d218 1
@


1.8
log
@resolve conflicts
@
text
@d152 1
a152 1
&function_begin("sha1_block_data_order",16);
@


1.7
log
@resolve conflicts
@
text
@d1 13
a13 1
#!/usr/local/bin/perl
d24 1
a24 1
# Pentium	-25%			+37%
a31 6
# Those who for any particular reason absolutely must score on
# Pentium can replace this module with one from 0.9.6 distribution.
# This "offer" shall be revoked the moment programming interface to
# this module is changed, in which case this paragraph should be
# removed.
# ----------------------------------------------------------------
d34 2
a35 3
$normal=0;

push(@@INC,"perlasm","../../perlasm");
d41 2
a42 2
$B="ecx";
$C="ebx";
d48 1
a48 57
$off=9*4;

@@K=(0x5a827999,0x6ed9eba1,0x8f1bbcdc,0xca62c1d6);

&sha1_block_data("sha1_block_asm_data_order");

&asm_finish();

sub Nn
	{
	local($p)=@@_;
	local(%n)=($A,$T,$B,$A,$C,$B,$D,$C,$E,$D,$T,$E);
	return($n{$p});
	}

sub Np
	{
	local($p)=@@_;
	local(%n)=($A,$T,$B,$A,$C,$B,$D,$C,$E,$D,$T,$E);
	local(%n)=($A,$B,$B,$C,$C,$D,$D,$E,$E,$T,$T,$A);
	return($n{$p});
	}

sub Na
	{
	local($n)=@@_;
	return( (($n   )&0x0f),
		(($n+ 2)&0x0f),
		(($n+ 8)&0x0f),
		(($n+13)&0x0f),
		(($n+ 1)&0x0f));
	}

sub X_expand
	{
	local($in)=@@_;

	&comment("First, load the words onto the stack in network byte order");
	for ($i=0; $i<16; $i+=2)
		{
		&mov($A,&DWP(($i+0)*4,$in,"",0));# unless $i == 0;
		 &mov($B,&DWP(($i+1)*4,$in,"",0));
		&bswap($A);
		 &bswap($B);
		&mov(&swtmp($i+0),$A);
		 &mov(&swtmp($i+1),$B);
		}

	&comment("We now have the X array on the stack");
	&comment("starting at sp-4");
	}

# Rules of engagement
# F is always trashable at the start, the running total.
# E becomes the next F so it can be trashed after it has been 'accumulated'
# F becomes A in the next round.  We don't need to access it much.
# During the X update part, the result ends up in $X[$n0].
d52 1
a52 1
	local($pos,$K,$X,$n,$a,$b,$c,$d,$e,$f)=@@_;
d56 3
a58 2
	&mov($tmp1,$a);
	 &mov($f,$c);			# f to hold F_00_19(b,c,d)
a60 2
	&and($f,$b);
	 &rotr($b,2);			# b=ROTATE(b,30)
d62 10
a71 6
	 &mov($e,&swtmp($n));		# e becomes volatile and
	 				# is loaded with xi
	&xor($f,$d);			# f holds F_00_19(b,c,d)
	 &lea($tmp1,&DWP($K,$tmp1,$e,1));# tmp1+=K_00_19+xi
	
	&add($f,$tmp1);			# f+=tmp1
d76 1
a76 2
	local($pos,$K,$X,$n,$a,$b,$c,$d,$e,$f)=@@_;
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
d80 1
a80 1
	&mov($f,&swtmp($n1));		# f to hold Xupdate(xi,xa,xb,xc,xd)
d82 1
a82 1
	&xor($f,&swtmp($n0));
d84 1
a84 1
	&xor($f,&swtmp($n2));
d86 6
a91 6
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	 &rotr($b,2);			# b=ROTATE(b,30)
	&xor($tmp1,$d);			# tmp1=F_00_19(b,c,d)
	 &rotl($f,1);			# f=ROATE(f,1)
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_00_19+e
d93 2
a94 2
	&add($f,$tmp1);			# f+=F_00_19(b,c,d)
	 &rotl($e,5);			# e=ROTATE(a,5)
d100 2
a101 1
	local($pos,$K,$X,$n,$a,$b,$c,$d,$e,$f)=@@_;
a103 1
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
d105 8
a112 8
	&mov($f,&swtmp($n0));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,$b);		# tmp1 to hold F_20_39(b,c,d)
	&xor($f,&swtmp($n1));
	 &rotr($b,2);			# b=ROTATE(b,30)
	&xor($f,&swtmp($n2));
	 &xor($tmp1,$c);
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	 &xor($tmp1,$d);		# tmp1 holds F_20_39(b,c,d)
d114 2
a115 2
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_20_39+e
d118 1
a118 1
	 &add($f,$tmp1);		# f+=F_20_39(b,c,d)
d124 1
a124 1
	local($pos,$K,$X,$n,$a,$b,$c,$d,$e,$f)=@@_;
a126 1
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
d128 7
a134 1
	&mov($f,&swtmp($n0));		# f to hold Xupdate(xi,xa,xb,xc,xd)
d136 1
a136 1
	&xor($f,&swtmp($n1));
d138 1
a138 1
	&xor($f,&swtmp($n2));
d140 1
a140 4
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_40_59+e
d148 2
a149 7
	&add($tmp1,$e);			# tmp1+=ROTATE(a,5)
	&add($f,$tmp1);			# f+=tmp1;
	}

sub BODY_60_79
	{
	&BODY_20_39(@@_);
d152 9
a160 27
sub sha1_block_host
	{
	local($name, $sclabel)=@@_;

	&function_begin_B($name,"");

	# parameter 1 is the MD5_CTX structure.
	# A	0
	# B	4
	# C	8
	# D 	12
	# E 	16

	&mov("ecx",	&wparam(2));
	 &push("esi");
	&shl("ecx",6);
	 &mov("esi",	&wparam(1));
	&push("ebp");
	 &add("ecx","esi");	# offset to leave on
	&push("ebx");
	 &mov("ebp",	&wparam(0));
	&push("edi");
	 &mov($D,	&DWP(12,"ebp","",0));
	&stack_push(18+9);
	 &mov($E,	&DWP(16,"ebp","",0));
	&mov($C,	&DWP( 8,"ebp","",0));
	 &mov(&swtmp(17),"ecx");
d162 1
a162 1
	&comment("First we need to setup the X array");
d164 2
a165 1
	for ($i=0; $i<16; $i+=2)
d167 8
a174 2
		&mov($A,&DWP(($i+0)*4,"esi","",0));# unless $i == 0;
		 &mov($B,&DWP(($i+1)*4,"esi","",0));
d176 3
a178 1
		 &mov(&swtmp($i+1),$B);
d180 1
a180 3
	&jmp($sclabel);
	&function_end_B($name);
	}
d182 33
d216 2
a217 179
sub sha1_block_data
	{
	local($name)=@@_;

	&function_begin_B($name,"");

	# parameter 1 is the MD5_CTX structure.
	# A	0
	# B	4
	# C	8
	# D 	12
	# E 	16

	&mov("ecx",	&wparam(2));
	 &push("esi");
	&shl("ecx",6);
	 &mov("esi",	&wparam(1));
	&push("ebp");
	 &add("ecx","esi");	# offset to leave on
	&push("ebx");
	 &mov("ebp",	&wparam(0));
	&push("edi");
	 &mov($D,	&DWP(12,"ebp","",0));
	&stack_push(18+9);
	 &mov($E,	&DWP(16,"ebp","",0));
	&mov($C,	&DWP( 8,"ebp","",0));
	 &mov(&swtmp(17),"ecx");

	&comment("First we need to setup the X array");

	&set_label("start") unless $normal;

	&X_expand("esi");
	 &mov(&wparam(1),"esi");

	&set_label("shortcut", 0, 1);
	&comment("");
	&comment("Start processing");

	# odd start
	&mov($A,	&DWP( 0,"ebp","",0));
	 &mov($B,	&DWP( 4,"ebp","",0));
	$X="esp";
	&BODY_00_15(-2,$K[0],$X, 0,$A,$B,$C,$D,$E,$T);
	&BODY_00_15( 0,$K[0],$X, 1,$T,$A,$B,$C,$D,$E);
	&BODY_00_15( 0,$K[0],$X, 2,$E,$T,$A,$B,$C,$D);
	&BODY_00_15( 0,$K[0],$X, 3,$D,$E,$T,$A,$B,$C);
	&BODY_00_15( 0,$K[0],$X, 4,$C,$D,$E,$T,$A,$B);
	&BODY_00_15( 0,$K[0],$X, 5,$B,$C,$D,$E,$T,$A);
	&BODY_00_15( 0,$K[0],$X, 6,$A,$B,$C,$D,$E,$T);
	&BODY_00_15( 0,$K[0],$X, 7,$T,$A,$B,$C,$D,$E);
	&BODY_00_15( 0,$K[0],$X, 8,$E,$T,$A,$B,$C,$D);
	&BODY_00_15( 0,$K[0],$X, 9,$D,$E,$T,$A,$B,$C);
	&BODY_00_15( 0,$K[0],$X,10,$C,$D,$E,$T,$A,$B);
	&BODY_00_15( 0,$K[0],$X,11,$B,$C,$D,$E,$T,$A);
	&BODY_00_15( 0,$K[0],$X,12,$A,$B,$C,$D,$E,$T);
	&BODY_00_15( 0,$K[0],$X,13,$T,$A,$B,$C,$D,$E);
	&BODY_00_15( 0,$K[0],$X,14,$E,$T,$A,$B,$C,$D);
	&BODY_00_15( 1,$K[0],$X,15,$D,$E,$T,$A,$B,$C);
	&BODY_16_19(-1,$K[0],$X,16,$C,$D,$E,$T,$A,$B);
	&BODY_16_19( 0,$K[0],$X,17,$B,$C,$D,$E,$T,$A);
	&BODY_16_19( 0,$K[0],$X,18,$A,$B,$C,$D,$E,$T);
	&BODY_16_19( 1,$K[0],$X,19,$T,$A,$B,$C,$D,$E);

	&BODY_20_39(-1,$K[1],$X,20,$E,$T,$A,$B,$C,$D);
	&BODY_20_39( 0,$K[1],$X,21,$D,$E,$T,$A,$B,$C);
	&BODY_20_39( 0,$K[1],$X,22,$C,$D,$E,$T,$A,$B);
	&BODY_20_39( 0,$K[1],$X,23,$B,$C,$D,$E,$T,$A);
	&BODY_20_39( 0,$K[1],$X,24,$A,$B,$C,$D,$E,$T);
	&BODY_20_39( 0,$K[1],$X,25,$T,$A,$B,$C,$D,$E);
	&BODY_20_39( 0,$K[1],$X,26,$E,$T,$A,$B,$C,$D);
	&BODY_20_39( 0,$K[1],$X,27,$D,$E,$T,$A,$B,$C);
	&BODY_20_39( 0,$K[1],$X,28,$C,$D,$E,$T,$A,$B);
	&BODY_20_39( 0,$K[1],$X,29,$B,$C,$D,$E,$T,$A);
	&BODY_20_39( 0,$K[1],$X,30,$A,$B,$C,$D,$E,$T);
	&BODY_20_39( 0,$K[1],$X,31,$T,$A,$B,$C,$D,$E);
	&BODY_20_39( 0,$K[1],$X,32,$E,$T,$A,$B,$C,$D);
	&BODY_20_39( 0,$K[1],$X,33,$D,$E,$T,$A,$B,$C);
	&BODY_20_39( 0,$K[1],$X,34,$C,$D,$E,$T,$A,$B);
	&BODY_20_39( 0,$K[1],$X,35,$B,$C,$D,$E,$T,$A);
	&BODY_20_39( 0,$K[1],$X,36,$A,$B,$C,$D,$E,$T);
	&BODY_20_39( 0,$K[1],$X,37,$T,$A,$B,$C,$D,$E);
	&BODY_20_39( 0,$K[1],$X,38,$E,$T,$A,$B,$C,$D);
	&BODY_20_39( 1,$K[1],$X,39,$D,$E,$T,$A,$B,$C);

	&BODY_40_59(-1,$K[2],$X,40,$C,$D,$E,$T,$A,$B);
	&BODY_40_59( 0,$K[2],$X,41,$B,$C,$D,$E,$T,$A);
	&BODY_40_59( 0,$K[2],$X,42,$A,$B,$C,$D,$E,$T);
	&BODY_40_59( 0,$K[2],$X,43,$T,$A,$B,$C,$D,$E);
	&BODY_40_59( 0,$K[2],$X,44,$E,$T,$A,$B,$C,$D);
	&BODY_40_59( 0,$K[2],$X,45,$D,$E,$T,$A,$B,$C);
	&BODY_40_59( 0,$K[2],$X,46,$C,$D,$E,$T,$A,$B);
	&BODY_40_59( 0,$K[2],$X,47,$B,$C,$D,$E,$T,$A);
	&BODY_40_59( 0,$K[2],$X,48,$A,$B,$C,$D,$E,$T);
	&BODY_40_59( 0,$K[2],$X,49,$T,$A,$B,$C,$D,$E);
	&BODY_40_59( 0,$K[2],$X,50,$E,$T,$A,$B,$C,$D);
	&BODY_40_59( 0,$K[2],$X,51,$D,$E,$T,$A,$B,$C);
	&BODY_40_59( 0,$K[2],$X,52,$C,$D,$E,$T,$A,$B);
	&BODY_40_59( 0,$K[2],$X,53,$B,$C,$D,$E,$T,$A);
	&BODY_40_59( 0,$K[2],$X,54,$A,$B,$C,$D,$E,$T);
	&BODY_40_59( 0,$K[2],$X,55,$T,$A,$B,$C,$D,$E);
	&BODY_40_59( 0,$K[2],$X,56,$E,$T,$A,$B,$C,$D);
	&BODY_40_59( 0,$K[2],$X,57,$D,$E,$T,$A,$B,$C);
	&BODY_40_59( 0,$K[2],$X,58,$C,$D,$E,$T,$A,$B);
	&BODY_40_59( 1,$K[2],$X,59,$B,$C,$D,$E,$T,$A);

	&BODY_60_79(-1,$K[3],$X,60,$A,$B,$C,$D,$E,$T);
	&BODY_60_79( 0,$K[3],$X,61,$T,$A,$B,$C,$D,$E);
	&BODY_60_79( 0,$K[3],$X,62,$E,$T,$A,$B,$C,$D);
	&BODY_60_79( 0,$K[3],$X,63,$D,$E,$T,$A,$B,$C);
	&BODY_60_79( 0,$K[3],$X,64,$C,$D,$E,$T,$A,$B);
	&BODY_60_79( 0,$K[3],$X,65,$B,$C,$D,$E,$T,$A);
	&BODY_60_79( 0,$K[3],$X,66,$A,$B,$C,$D,$E,$T);
	&BODY_60_79( 0,$K[3],$X,67,$T,$A,$B,$C,$D,$E);
	&BODY_60_79( 0,$K[3],$X,68,$E,$T,$A,$B,$C,$D);
	&BODY_60_79( 0,$K[3],$X,69,$D,$E,$T,$A,$B,$C);
	&BODY_60_79( 0,$K[3],$X,70,$C,$D,$E,$T,$A,$B);
	&BODY_60_79( 0,$K[3],$X,71,$B,$C,$D,$E,$T,$A);
	&BODY_60_79( 0,$K[3],$X,72,$A,$B,$C,$D,$E,$T);
	&BODY_60_79( 0,$K[3],$X,73,$T,$A,$B,$C,$D,$E);
	&BODY_60_79( 0,$K[3],$X,74,$E,$T,$A,$B,$C,$D);
	&BODY_60_79( 0,$K[3],$X,75,$D,$E,$T,$A,$B,$C);
	&BODY_60_79( 0,$K[3],$X,76,$C,$D,$E,$T,$A,$B);
	&BODY_60_79( 0,$K[3],$X,77,$B,$C,$D,$E,$T,$A);
	&BODY_60_79( 0,$K[3],$X,78,$A,$B,$C,$D,$E,$T);
	&BODY_60_79( 2,$K[3],$X,79,$T,$A,$B,$C,$D,$E);

	&comment("End processing");
	&comment("");
	# D is the tmp value

	# E -> A
	# T -> B
	# A -> C
	# B -> D
	# C -> E
	# D -> T

	&mov($tmp1,&wparam(0));

	 &mov($D,	&DWP(12,$tmp1,"",0));
	&add($D,$B);
	 &mov($B,	&DWP( 4,$tmp1,"",0));
	&add($B,$T);
	 &mov($T,	$A);
	&mov($A,	&DWP( 0,$tmp1,"",0));
	 &mov(&DWP(12,$tmp1,"",0),$D);

	&add($A,$E);
	 &mov($E,	&DWP(16,$tmp1,"",0));
	&add($E,$C);
	 &mov($C,	&DWP( 8,$tmp1,"",0));
	&add($C,$T);

	 &mov(&DWP( 0,$tmp1,"",0),$A);
	&mov("esi",&wparam(1));
	 &mov(&DWP( 8,$tmp1,"",0),$C);
 	&add("esi",64);
	 &mov("eax",&swtmp(17));
	&mov(&DWP(16,$tmp1,"",0),$E);
	 &cmp("esi","eax");
	&mov(&DWP( 4,$tmp1,"",0),$B);
	 &jb(&label("start"));

	&stack_pop(18+9);
	 &pop("edi");
	&pop("ebx");
	 &pop("ebp");
	&pop("esi");
	 &ret();

	# keep a note of shortcut label so it can be used outside
	# block.
	my $sclabel = &label("shortcut");

	&function_end_B($name);
	# Putting this here avoids problems with MASM in debugging mode
	&sha1_block_host("sha1_block_asm_host_order", $sclabel);
	}
d219 1
@


1.6
log
@merge 0.9.7d
@
text
@d408 1
a408 1
	 &jl(&label("start"));
@


1.5
log
@openssl-engine-0.9.6a merge
@
text
@d3 25
a104 1
return if $n & 1;
a106 2
	 &mov($f,$c);

d108 12
a119 42
	 &xor($f,$d);			# F2

	&rotl($tmp1,5);			# A2

	&and($f,$b);			# F3
	 &add($tmp1,$e);

	&rotr($b,1);			# B1	<- F
	 &mov($e,&swtmp($n));		# G1

	&rotr($b,1);			# B1	<- F
	 &xor($f,$d);			# F4

	&lea($tmp1,&DWP($K,$tmp1,$e,1));

############################
#	&BODY_40_59( 0,$K[2],$X,42,$A,$B,$C,$D,$E,$T);
#	&BODY_40_59( 0,$K[2],$X,43,$T,$A,$B,$C,$D,$E);
$n++;
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
	($b,$c,$d,$e,$f,$a)=($a,$b,$c,$d,$e,$f);

	 &mov($f,$c);

	&add($a,$tmp1);		# MOVED DOWN
	 &xor($f,$d);			# F2

	&mov($tmp1,$a);
	 &and($f,$b);			# F3

	&rotl($tmp1,5);			# A2

	&add($tmp1,$e);
	 &mov($e,&swtmp($n));		# G1

	&rotr($b,1);			# B1	<- F
	 &xor($f,$d);			# F4

	&rotr($b,1);			# B1	<- F
	 &lea($tmp1,&DWP($K,$tmp1,$e,1));

	&add($f,$tmp1);
a126 1
return if $n & 1;
d129 16
a144 57
 &nop() if ($pos < 0);
&mov($tmp1,&swtmp($n0));			# X1
 &mov($f,&swtmp($n1));			# X2
&xor($f,$tmp1);				# X3
 &mov($tmp1,&swtmp($n2));		# X4
&xor($f,$tmp1);				# X5
 &mov($tmp1,&swtmp($n3));		# X6
&xor($f,$tmp1);				# X7 - slot
 &mov($tmp1,$c);			# F1
&rotl($f,1);				# X8 - slot
 &xor($tmp1,$d);			# F2
&mov(&swtmp($n0),$f);			# X9 - anytime
 &and($tmp1,$b);			# F3
&lea($f,&DWP($K,$f,$e,1));		# tot=X+K+e
 &xor($tmp1,$d);				# F4
&mov($e,$a);				# A1
 &add($f,$tmp1);			# tot+=F();

&rotl($e,5);				# A2

&rotr($b,1);				# B1	<- F
 &add($f,$e);				# tot+=a

############################
#	&BODY_40_59( 0,$K[2],$X,42,$A,$B,$C,$D,$E,$T);
#	&BODY_40_59( 0,$K[2],$X,43,$T,$A,$B,$C,$D,$E);
$n++;
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
	($b,$c,$d,$e,$f,$a)=($a,$b,$c,$d,$e,$f);


&mov($f,&swtmp($n0));			# X1
 &mov($tmp1,&swtmp($n1));		# X2
&xor($f,$tmp1);				# X3
 &mov($tmp1,&swtmp($n2));		# X4
&xor($f,$tmp1);				# X5
 &mov($tmp1,&swtmp($n3));		# X6
&rotr($c,1); #&rotr($b,1);		# B1	<- F # MOVED DOWN
 &xor($f,$tmp1);				# X7 - slot
&rotl($f,1);				# X8 - slot
 &mov($tmp1,$c);			# F1
&xor($tmp1,$d);			# F2
 &mov(&swtmp($n0),$f);			# X9 - anytime
&and($tmp1,$b);			# F3
 &lea($f,&DWP($K,$f,$e,1));		# tot=X+K+e

&xor($tmp1,$d);				# F4
 &mov($e,$a);				# A1

&rotl($e,5);				# A2

&rotr($b,1);				# B1	<- F
 &add($f,$e);				# tot+=a

&rotr($b,1);				# B1	<- F
 &add($f,$tmp1);			# tot+=F();

d154 15
a168 36
&mov($f,&swtmp($n0));			# X1
 &mov($tmp1,&swtmp($n1));		# X2
&xor($f,$tmp1);				# X3
 &mov($tmp1,&swtmp($n2));		# X4
&xor($f,$tmp1);				# X5
 &mov($tmp1,&swtmp($n3));		# X6
&xor($f,$tmp1);				# X7 - slot
 &mov($tmp1,$b);			# F1
&rotl($f,1);				# X8 - slot
 &xor($tmp1,$c);			# F2
&mov(&swtmp($n0),$f);			# X9 - anytime
 &xor($tmp1,$d);			# F3

&lea($f,&DWP($K,$f,$e,1));		# tot=X+K+e
 &mov($e,$a);				# A1

&rotl($e,5);				# A2

if ($n != 79) # last loop	
	{
	&rotr($b,1);				# B1	<- F
	 &add($e,$tmp1);			# tmp1=F()+a

	&rotr($b,1);				# B2	<- F
	 &add($f,$e);				# tot+=tmp1;
	}
else
	{
	&add($e,$tmp1);				# tmp1=F()+a
	 &mov($tmp1,&wparam(0));

	&rotr($b,1);				# B1	<- F
	 &add($f,$e);				# tot+=tmp1;

	&rotr($b,1);				# B2	<- F
	}
a175 1
	return if $n & 1;
d178 19
a196 61
&mov($f,&swtmp($n0));			# X1
 &mov($tmp1,&swtmp($n1));		# X2
&xor($f,$tmp1);				# X3
 &mov($tmp1,&swtmp($n2));		# X4
&xor($f,$tmp1);				# X5
 &mov($tmp1,&swtmp($n3));		# X6
&xor($f,$tmp1);				# X7 - slot
 &mov($tmp1,$b);			# F1
&rotl($f,1);				# X8 - slot
 &or($tmp1,$c);				# F2
&mov(&swtmp($n0),$f);			# X9 - anytime
 &and($tmp1,$d);			# F3

&lea($f,&DWP($K,$f,$e,1));		# tot=X+K+e
 &mov($e,$b);				# F4

&rotr($b,1);				# B1	<- F
 &and($e,$c);				# F5

&or($tmp1,$e);				# F6
 &mov($e,$a);				# A1

&rotl($e,5);				# A2

&add($tmp1,$e);			# tmp1=F()+a

############################
#	&BODY_40_59( 0,$K[2],$X,42,$A,$B,$C,$D,$E,$T);
#	&BODY_40_59( 0,$K[2],$X,43,$T,$A,$B,$C,$D,$E);
$n++;
	local($n0,$n1,$n2,$n3,$np)=&Na($n);
	($b,$c,$d,$e,$f,$a)=($a,$b,$c,$d,$e,$f);

 &mov($f,&swtmp($n0));			# X1
&add($a,$tmp1);				# tot+=tmp1; # moved was add f,tmp1
 &mov($tmp1,&swtmp($n1));		# X2
&xor($f,$tmp1);				# X3
 &mov($tmp1,&swtmp($n2));		# X4
&xor($f,$tmp1);				# X5
 &mov($tmp1,&swtmp($n3));		# X6
&rotr($c,1);				# B2	<- F # moved was rotr b,1
 &xor($f,$tmp1);			# X7 - slot
&rotl($f,1);				# X8 - slot
 &mov($tmp1,$b);			# F1
&mov(&swtmp($n0),$f);			# X9 - anytime
 &or($tmp1,$c);				# F2
&lea($f,&DWP($K,$f,$e,1));		# tot=X+K+e
 &mov($e,$b);				# F4
&and($tmp1,$d);				# F3
 &and($e,$c);				# F5

&or($tmp1,$e);				# F6
 &mov($e,$a);				# A1

&rotl($e,5);				# A2

&rotr($b,1);				# B1	<- F
 &add($tmp1,$e);			# tmp1=F()+a

&rotr($b,1);				# B2	<- F
 &add($f,$tmp1);			# tot+=tmp1;
d384 1
a384 2
	# The last 2 have been moved into the last loop
	# &mov($tmp1,&wparam(0));
@


1.4
log
@OpenSSL 0.9.5a merge
@
text
@d320 1
a320 1
	local($name)=@@_;
d355 1
a355 1
	&jmp(&label("shortcut"));
d532 3
a534 3
	# it has to reside within sha1_block_asm_host_order body
	# because it calls &jmp(&label("shortcut"));
	&sha1_block_host("sha1_block_asm_host_order");
d537 2
@


1.3
log
@OpenSSL 0.9.5 merge

*warning* this bumps shared lib minors for libssl and libcrypto from 2.1 to 2.2
if you are using the ssl26 packages for ssh and other things to work you will
need to get new ones (see ~beck/libsslsnap/<arch>) on cvs or ~beck/src-patent.tar.gz on cvs
@
text
@d395 1
a395 1
	&set_label("shortcut", 1);
@


1.2
log
@OpenSSL 0.9.4 merge
@
text
@d11 2
a12 2
$B="ebx";
$C="ecx";
d22 1
a22 1
&sha1_block("sha1_block_x86");
d56 1
a56 1
	for ($i=0; $i<16; $i++)
d58 6
a63 3
		&mov("eax",&DWP(($i+0)*4,$in,"",0)) unless $i == 0;
		&bswap("eax");
		&mov(&swtmp($i+0),"eax");
d318 1
a318 1
sub sha1_block
d331 3
a333 3
	&push("esi");
	 &push("ebp");
	&mov("eax",	&wparam(2));
d335 3
a337 1
	&add("eax",	"esi");	# offset to leave on
d339 40
d380 1
a380 1
	 &sub("eax",	64);
a381 2
	 &mov($B,	&DWP( 4,"ebp","",0));
	&stack_push(18);
d383 4
a386 3
	&mov($E,	&DWP(16,"ebp","",0));
	 &mov($C,	&DWP( 8,"ebp","",0));
	&mov(&swtmp(17),"eax");
a388 1
	 &mov("eax",&DWP(0,"esi","",0)); # pulled out of X_expand
d393 1
a393 1
	 &mov(&swtmp(16),"esi");
d395 1
d401 1
d516 2
a517 2
	&mov("esi",&swtmp(16));
	 &mov(&DWP( 8,$tmp1,"",0),$C);	# This is for looping
d521 3
a523 5
	 &cmp("eax","esi");
	&mov(&DWP( 4,$tmp1,"",0),$B);	# This is for looping
	 &jl(&label("end"));
	&mov("eax",&DWP(0,"esi","",0));	# Pulled down from 
	 &jmp(&label("start"));
d525 1
a525 2
	&set_label("end");
	&stack_pop(18);
d531 5
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
#!/usr/bin/perl
d8 1
a8 1
&asm_init($ARGV[0],"sha1-586.pl");
@


1.1.1.1
log
@Import of SSLeay-0.9.0b with RSA and IDEA stubbed + OpenBSD build
functionality for shared libs.

Note that routines such as sslv2_init and friends that use RSA will
not work due to lack of RSA in this library.

Needs documentation and help from ports for easy upgrade to full
functionality where legally possible.
@
text
@@


1.1.1.2
log
@import openssl-0.9.7-beta1
@
text
@d1 1
a1 1
#!/usr/local/bin/perl
d8 1
a8 1
&asm_init($ARGV[0],"sha1-586.pl",$ARGV[$#ARGV] eq "386");
d11 2
a12 2
$B="ecx";
$C="ebx";
d22 1
a22 1
&sha1_block_data("sha1_block_asm_data_order");
d56 1
a56 1
	for ($i=0; $i<16; $i+=2)
d58 3
a60 6
		&mov($A,&DWP(($i+0)*4,$in,"",0));# unless $i == 0;
		 &mov($B,&DWP(($i+1)*4,$in,"",0));
		&bswap($A);
		 &bswap($B);
		&mov(&swtmp($i+0),$A);
		 &mov(&swtmp($i+1),$B);
d315 1
a315 1
sub sha1_block_host
d317 1
a317 1
	local($name, $sclabel)=@@_;
d328 3
a330 3
	&mov("ecx",	&wparam(2));
	 &push("esi");
	&shl("ecx",6);
d332 1
a332 3
	&push("ebp");
	 &add("ecx","esi");	# offset to leave on
	&push("ebx");
a333 40
	&push("edi");
	 &mov($D,	&DWP(12,"ebp","",0));
	&stack_push(18+9);
	 &mov($E,	&DWP(16,"ebp","",0));
	&mov($C,	&DWP( 8,"ebp","",0));
	 &mov(&swtmp(17),"ecx");

	&comment("First we need to setup the X array");

	for ($i=0; $i<16; $i+=2)
		{
		&mov($A,&DWP(($i+0)*4,"esi","",0));# unless $i == 0;
		 &mov($B,&DWP(($i+1)*4,"esi","",0));
		&mov(&swtmp($i+0),$A);
		 &mov(&swtmp($i+1),$B);
		}
	&jmp($sclabel);
	&function_end_B($name);
	}


sub sha1_block_data
	{
	local($name)=@@_;

	&function_begin_B($name,"");

	# parameter 1 is the MD5_CTX structure.
	# A	0
	# B	4
	# C	8
	# D 	12
	# E 	16

	&mov("ecx",	&wparam(2));
	 &push("esi");
	&shl("ecx",6);
	 &mov("esi",	&wparam(1));
	&push("ebp");
	 &add("ecx","esi");	# offset to leave on
d335 1
a335 1
	 &mov("ebp",	&wparam(0));
d337 2
d340 3
a342 4
	&stack_push(18+9);
	 &mov($E,	&DWP(16,"ebp","",0));
	&mov($C,	&DWP( 8,"ebp","",0));
	 &mov(&swtmp(17),"ecx");
d345 1
d350 1
a350 1
	 &mov(&wparam(1),"esi");
a351 1
	&set_label("shortcut", 0, 1);
a356 1
	 &mov($B,	&DWP( 4,"ebp","",0));
d471 2
a472 2
	&mov("esi",&wparam(1));
	 &mov(&DWP( 8,$tmp1,"",0),$C);
d476 5
a480 3
	 &cmp("esi","eax");
	&mov(&DWP( 4,$tmp1,"",0),$B);
	 &jl(&label("start"));
d482 2
a483 1
	&stack_pop(18+9);
a488 5

	# keep a note of shortcut label so it can be used outside
	# block.
	my $sclabel = &label("shortcut");

a489 2
	# Putting this here avoids problems with MASM in debugging mode
	&sha1_block_host("sha1_block_asm_host_order", $sclabel);
@


1.1.1.3
log
@import openssl-0.9.7d
@
text
@a2 25
# It was noted that Intel IA-32 C compiler generates code which
# performs ~30% *faster* on P4 CPU than original *hand-coded*
# SHA1 assembler implementation. To address this problem (and
# prove that humans are still better than machines:-), the
# original code was overhauled, which resulted in following
# performance changes:
#
#		compared with original	compared with Intel cc
#		assembler impl.		generated code
# Pentium	-25%			+37%
# PIII/AMD	+8%			+16%
# P4		+85%(!)			+45%
#
# As you can see Pentium came out as looser:-( Yet I reckoned that
# improvement on P4 outweights the loss and incorporate this
# re-tuned code to 0.9.7 and later.
# ----------------------------------------------------------------
# Those who for any particular reason absolutely must score on
# Pentium can replace this module with one from 0.9.6 distribution.
# This "offer" shall be revoked the moment programming interface to
# this module is changed, in which case this paragraph should be
# removed.
# ----------------------------------------------------------------
#					<appro@@fy.chalmers.se>

d80 1
d83 2
d86 42
a127 12
	 &mov($f,$c);			# f to hold F_00_19(b,c,d)
	&rotl($tmp1,5);			# tmp1=ROTATE(a,5)
	 &xor($f,$d);
	&and($f,$b);
	 &rotr($b,2);			# b=ROTATE(b,30)
	&add($tmp1,$e);			# tmp1+=e;
	 &mov($e,&swtmp($n));		# e becomes volatile and
	 				# is loaded with xi
	&xor($f,$d);			# f holds F_00_19(b,c,d)
	 &lea($tmp1,&DWP($K,$tmp1,$e,1));# tmp1+=K_00_19+xi
	
	&add($f,$tmp1);			# f+=tmp1
d135 1
d138 57
a194 16
	&mov($f,&swtmp($n1));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,$c);		# tmp1 to hold F_00_19(b,c,d)
	&xor($f,&swtmp($n0));
	 &xor($tmp1,$d);
	&xor($f,&swtmp($n2));
	 &and($tmp1,$b);		# tmp1 holds F_00_19(b,c,d)
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	 &rotr($b,2);			# b=ROTATE(b,30)
	&xor($tmp1,$d);			# tmp1=F_00_19(b,c,d)
	 &rotl($f,1);			# f=ROATE(f,1)
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_00_19+e
	 &mov($e,$a);			# e becomes volatile
	&add($f,$tmp1);			# f+=F_00_19(b,c,d)
	 &rotl($e,5);			# e=ROTATE(a,5)
	&add($f,$e);			# f+=ROTATE(a,5)
d204 36
a239 15
	&mov($f,&swtmp($n0));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,$b);		# tmp1 to hold F_20_39(b,c,d)
	&xor($f,&swtmp($n1));
	 &rotr($b,2);			# b=ROTATE(b,30)
	&xor($f,&swtmp($n2));
	 &xor($tmp1,$c);
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	 &xor($tmp1,$d);		# tmp1 holds F_20_39(b,c,d)
	&rotl($f,1);			# f=ROTATE(f,1)
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_20_39+e
	 &mov($e,$a);			# e becomes volatile
	&rotl($e,5);			# e=ROTATE(a,5)
	 &add($f,$tmp1);		# f+=F_20_39(b,c,d)
	&add($f,$e);			# f+=ROTATE(a,5)
d247 1
d250 61
a310 19
	&mov($f,&swtmp($n0));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,$b);		# tmp1 to hold F_40_59(b,c,d)
	&xor($f,&swtmp($n1));
	 &or($tmp1,$c);
	&xor($f,&swtmp($n2));
	 &and($tmp1,$d);
	&xor($f,&swtmp($n3));		# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	&mov(&swtmp($n0),$f);		# xi=f
	&lea($f,&DWP($K,$f,$e,1));	# f+=K_40_59+e
	 &mov($e,$b);			# e becomes volatile and is used
					# to calculate F_40_59(b,c,d)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &and($e,$c);
	&or($tmp1,$e);			# tmp1 holds F_40_59(b,c,d)		
	 &mov($e,$a);
	&rotl($e,5);			# e=ROTATE(a,5)
	&add($tmp1,$e);			# tmp1+=ROTATE(a,5)
	&add($f,$tmp1);			# f+=tmp1;
d498 2
a499 1
	&mov($tmp1,&wparam(0));
@


1.1.1.4
log
@import of openssl-0.9.7g; tested on platforms from alpha to zaurus, ok deraadt@@
@
text
@d408 1
a408 1
	 &jb(&label("start"));
@


1.1.1.5
log
@import of OpenSSL 0.9.8h
@
text
@d1 1
a1 13
#!/usr/bin/env perl

# ====================================================================
# [Re]written by Andy Polyakov <appro@@fy.chalmers.se> for the OpenSSL
# project. The module is, however, dual licensed under OpenSSL and
# CRYPTOGAMS licenses depending on where you obtain it. For further
# details see http://www.openssl.org/~appro/cryptogams/.
# ====================================================================

# "[Re]written" was achieved in two major overhauls. In 2004 BODY_*
# functions were re-implemented to address P4 performance issue [see
# commentary below], and in 2006 the rest was rewritten in order to
# gain freedom to liberate licensing terms.
d12 1
a12 1
# Pentium	-16%			+48%
d20 6
d28 3
a30 2
$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
push(@@INC,"${dir}","${dir}../../perlasm");
d36 2
a37 2
$B="ebx";
$C="ecx";
d43 57
a99 1
@@V=($A,$B,$C,$D,$E,$T);
d103 1
a103 1
	local($n,$a,$b,$c,$d,$e,$f)=@@_;
d107 2
a108 3
	&mov($f,$c);			# f to hold F_00_19(b,c,d)
	 if ($n==0)  { &mov($tmp1,$a); }
	 else        { &mov($a,$tmp1); }
d111 2
d114 6
a119 10
	 &and($f,$b);
	&mov($e,&swtmp($n%16));		# e becomes volatile and is loaded
	 				# with xi, also note that e becomes
					# f in next round...
	 &xor($f,$d);			# f holds F_00_19(b,c,d)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &lea($tmp1,&DWP(0x5a827999,$tmp1,$e));	# tmp1+=K_00_19+xi

	if ($n==15) { &add($f,$tmp1); }	# f+=tmp1
	else        { &add($tmp1,$f); }	# f becomes a in next round
d124 2
a125 1
	local($n,$a,$b,$c,$d,$e,$f)=@@_;
d129 1
a129 1
	&mov($f,&swtmp($n%16));		# f to hold Xupdate(xi,xa,xb,xc,xd)
d131 1
a131 1
	&xor($f,&swtmp(($n+2)%16));
d133 1
a133 1
	&xor($f,&swtmp(($n+8)%16));
d135 6
a140 6
	&rotr($b,2);			# b=ROTATE(b,30)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &xor($tmp1,$d);		# tmp1=F_00_19(b,c,d)
	&mov(&swtmp($n%16),$f);		# xi=f
	&lea($f,&DWP(0x5a827999,$f,$e));# f+=K_00_19+e
d142 2
a143 2
	&rotl($e,5);			# e=ROTATE(a,5)
	 &add($f,$tmp1);		# f+=F_00_19(b,c,d)
d149 1
a149 2
	local($n,$a,$b,$c,$d,$e,$f)=@@_;
	local $K=($n<40)?0x6ed9eba1:0xca62c1d6;
d152 1
d154 8
a161 8
	&mov($tmp1,$b);			# tmp1 to hold F_20_39(b,c,d)
	 &mov($f,&swtmp($n%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &xor($f,&swtmp(($n+2)%16));
	&xor($tmp1,$c);
	 &xor($f,&swtmp(($n+8)%16));
	&xor($tmp1,$d);			# tmp1 holds F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
d163 2
a164 2
	 &add($tmp1,$e);
	&mov(&swtmp($n%16),$f);		# xi=f
d167 1
a167 1
	 &lea($f,&DWP($K,$f,$tmp1));	# f+=K_20_39+e
d173 1
a173 1
	local($n,$a,$b,$c,$d,$e,$f)=@@_;
d176 1
d178 1
a178 7
	&mov($f,&swtmp($n%16));		# f to hold Xupdate(xi,xa,xb,xc,xd)
	 &mov($tmp1,&swtmp(($n+2)%16));
	&xor($f,$tmp1);
	 &mov($tmp1,&swtmp(($n+8)%16));
	&xor($f,$tmp1);
	 &mov($tmp1,&swtmp(($n+13)%16));
	&xor($f,$tmp1);			# f holds xa^xb^xc^xd
d180 1
a180 1
	&rotl($f,1);			# f=ROTATE(f,1)
d182 1
a182 1
	&mov(&swtmp($n%16),$f);		# xi=f
d184 4
a187 1
	&lea($f,&DWP(0x8f1bbcdc,$f,$e));# f+=K_40_59+e
d195 7
a201 2
	 &add($f,$tmp1);		# f+=tmp1;
	&add($f,$e);			# f+=ROTATE(a,5)
d204 27
a230 9
&function_begin("sha1_block_data_order",16);
	&mov($tmp1,&wparam(0));	# SHA_CTX *c
	&mov($T,&wparam(1));	# const void *input
	&mov($A,&wparam(2));	# size_t num
	&stack_push(16);	# allocate X[16]
	&shl($A,6);
	&add($A,$T);
	&mov(&wparam(2),$A);	# pointer beyond the end of input
	&mov($E,&DWP(16,$tmp1));# pre-load E
d232 1
a232 1
	&set_label("loop",16);
d234 1
a234 2
	# copy input chunk to X, but reversing byte order!
	for ($i=0; $i<16; $i+=4)
d236 2
a237 8
		&mov($A,&DWP(4*($i+0),$T));
		&mov($B,&DWP(4*($i+1),$T));
		&mov($C,&DWP(4*($i+2),$T));
		&mov($D,&DWP(4*($i+3),$T));
		&bswap($A);
		&bswap($B);
		&bswap($C);
		&bswap($D);
d239 1
a239 3
		&mov(&swtmp($i+1),$B);
		&mov(&swtmp($i+2),$C);
		&mov(&swtmp($i+3),$D);
d241 8
a248 1
	&mov(&wparam(1),$T);	# redundant in 1st spin
d250 1
a250 33
	&mov($A,&DWP(0,$tmp1));	# load SHA_CTX
	&mov($B,&DWP(4,$tmp1));
	&mov($C,&DWP(8,$tmp1));
	&mov($D,&DWP(12,$tmp1));
	# E is pre-loaded

	for($i=0;$i<16;$i++)	{ &BODY_00_15($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<20;$i++)	{ &BODY_16_19($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<40;$i++)	{ &BODY_20_39($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<60;$i++)	{ &BODY_40_59($i,@@V); unshift(@@V,pop(@@V)); }
	for(;$i<80;$i++)	{ &BODY_20_39($i,@@V); unshift(@@V,pop(@@V)); }

	(($V[5] eq $D) and ($V[0] eq $E)) or die;	# double-check

	&mov($tmp1,&wparam(0));	# re-load SHA_CTX*
	&mov($D,&wparam(1));	# D is last "T" and is discarded

	&add($E,&DWP(0,$tmp1));	# E is last "A"...
	&add($T,&DWP(4,$tmp1));
	&add($A,&DWP(8,$tmp1));
	&add($B,&DWP(12,$tmp1));
	&add($C,&DWP(16,$tmp1));

	&mov(&DWP(0,$tmp1),$E);	# update SHA_CTX
	 &add($D,64);		# advance input pointer
	&mov(&DWP(4,$tmp1),$T);
	 &cmp($D,&wparam(2));	# have we reached the end yet?
	&mov(&DWP(8,$tmp1),$A);
	 &mov($E,$C);		# C is last "E" which needs to be "pre-loaded"
	&mov(&DWP(12,$tmp1),$B);
	 &mov($T,$D);		# input pointer
	&mov(&DWP(16,$tmp1),$C);
	&jb(&label("loop"));
d252 173
a424 2
	&stack_pop(16);
&function_end("sha1_block_data_order");
a425 1
&asm_finish();
@


1.1.1.6
log
@import openssl-0.9.8j
@
text
@d152 1
a152 1
&function_begin("sha1_block_data_order");
@


1.1.1.7
log
@import OpenSSL-1.0.0a
@
text
@a217 1
&asciz("SHA1 block transform for x86, CRYPTOGAMS by <appro\@@openssl.org>");
@


1.1.1.8
log
@import OpenSSL-1.0.1c
@
text
@a14 2
# January, September 2004.
#
a33 65
# August 2009.
#
# George Spelvin has tipped that F_40_59(b,c,d) can be rewritten as
# '(c&d) + (b&(c^d))', which allows to accumulate partial results
# and lighten "pressure" on scratch registers. This resulted in
# >12% performance improvement on contemporary AMD cores (with no
# degradation on other CPUs:-). Also, the code was revised to maximize
# "distance" between instructions producing input to 'lea' instruction
# and the 'lea' instruction itself, which is essential for Intel Atom
# core and resulted in ~15% improvement.

# October 2010.
#
# Add SSSE3, Supplemental[!] SSE3, implementation. The idea behind it
# is to offload message schedule denoted by Wt in NIST specification,
# or Xupdate in OpenSSL source, to SIMD unit. The idea is not novel,
# and in SSE2 context was first explored by Dean Gaudet in 2004, see
# http://arctic.org/~dean/crypto/sha1.html. Since then several things
# have changed that made it interesting again:
#
# a) XMM units became faster and wider;
# b) instruction set became more versatile;
# c) an important observation was made by Max Locktykhin, which made
#    it possible to reduce amount of instructions required to perform
#    the operation in question, for further details see
#    http://software.intel.com/en-us/articles/improving-the-performance-of-the-secure-hash-algorithm-1/.

# April 2011.
#
# Add AVX code path, probably most controversial... The thing is that
# switch to AVX alone improves performance by as little as 4% in
# comparison to SSSE3 code path. But below result doesn't look like
# 4% improvement... Trouble is that Sandy Bridge decodes 'ro[rl]' as
# pair of µ-ops, and it's the additional µ-ops, two per round, that
# make it run slower than Core2 and Westmere. But 'sh[rl]d' is decoded
# as single µ-op by Sandy Bridge and it's replacing 'ro[rl]' with
# equivalent 'sh[rl]d' that is responsible for the impressive 5.1
# cycles per processed byte. But 'sh[rl]d' is not something that used
# to be fast, nor does it appear to be fast in upcoming Bulldozer
# [according to its optimization manual]. Which is why AVX code path
# is guarded by *both* AVX and synthetic bit denoting Intel CPUs.
# One can argue that it's unfair to AMD, but without 'sh[rl]d' it
# makes no sense to keep the AVX code path. If somebody feels that
# strongly, it's probably more appropriate to discuss possibility of
# using vector rotate XOP on AMD...

######################################################################
# Current performance is summarized in following table. Numbers are
# CPU clock cycles spent to process single byte (less is better).
#
#		x86		SSSE3		AVX
# Pentium	15.7		-
# PIII		11.5		-
# P4		10.6		-
# AMD K8	7.1		-
# Core2		7.3		6.1/+20%	-
# Atom		12.5		9.5(*)/+32%	-
# Westmere	7.3		5.6/+30%	-
# Sandy Bridge	8.8		6.2/+40%	5.1(**)/+70%
#
# (*)	Loop is 1056 instructions long and expected result is ~8.25.
#	It remains mystery [to me] why ILP is limited to 1.7.
#
# (**)	As per above comment, the result is for AVX *plus* sh[rl]d.

a39 15
$xmm=$ymm=0;
for (@@ARGV) { $xmm=1 if (/-DOPENSSL_IA32_SSE2/); }

$ymm=1 if ($xmm &&
		`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
			=~ /GNU assembler version ([2-9]\.[0-9]+)/ &&
		$1>=2.19);	# first version supporting AVX

$ymm=1 if ($xmm && !$ymm && $ARGV[0] eq "win32n" && 
		`nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/ &&
		$1>=2.03);	# first version supporting AVX

&external_label("OPENSSL_ia32cap_P") if ($xmm);


a49 4
$alt=0;	# 1 denotes alternative IALU implementation, which performs
	# 8% *worse* on P4, same on Westmere and Atom, 2% better on
	# Sandy Bridge...

d62 2
a63 1
	 &mov($e,&swtmp($n%16));	# e becomes volatile and is loaded
d66 1
a66 1
	&and($f,$b);
d68 1
a68 2
	 &xor($f,$d);			# f holds F_00_19(b,c,d)
	&lea($tmp1,&DWP(0x5a827999,$tmp1,$e));	# tmp1+=K_00_19+xi
d70 1
a70 2
	if ($n==15) { &mov($e,&swtmp(($n+1)%16));# pre-fetch f for next round
		      &add($f,$tmp1); }	# f+=tmp1
a71 1
	&mov($tmp1,$a)			if ($alt && $n==15);
d80 7
a86 23
if ($alt) {
	&xor($c,$d);
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&and($tmp1,$c);			# tmp1 to hold F_00_19(b,c,d), b&=c^d
	 &xor($f,&swtmp(($n+8)%16));
	&xor($tmp1,$d);			# tmp1=F_00_19(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &add($e,$tmp1);		# e+=F_00_19(b,c,d)
	&xor($c,$d);			# restore $c
	 &mov($tmp1,$a);		# b in next round
	&rotr($b,$n==16?2:7);		# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &lea($f,&DWP(0x5a827999,$f,$e));# f+=F_00_19(b,c,d)+e
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
} else {
	&mov($tmp1,$c);			# tmp1 to hold F_00_19(b,c,d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&and($tmp1,$b);
d90 6
a95 9
	&add($e,$tmp1);			# e+=F_00_19(b,c,d)
	 &mov($tmp1,$a);
	&rotr($b,2);			# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($tmp1,5);			# ROTATE(a,5)
	 &lea($f,&DWP(0x5a827999,$f,$e));# f+=F_00_19(b,c,d)+e
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$tmp1);		# f+=ROTATE(a,5)
}
a104 19
if ($alt) {
	&xor($tmp1,$c);			# tmp1 to hold F_20_39(b,c,d), b^=c
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);			# tmp1 holds F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+8)%16));
	&add($e,$tmp1);			# e+=F_20_39(b,c,d)
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &mov($tmp1,$a);		# b in next round
	&rotr($b,7);			# b=ROTATE(b,30)
	 &mov(&swtmp($n%16),$f)		if($n<77);# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &xor($b,$c)			if($n==39);# warm up for BODY_40_59
	&and($tmp1,$b)			if($n==39);
	 &lea($f,&DWP($K,$f,$e));	# f+=e+K_XX_YY
	&mov($e,&swtmp(($n+1)%16))	if($n<79);# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
	&rotr($a,5)			if ($n==79);
} else {
d106 3
a108 1
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
d114 6
a119 9
	 &add($e,$tmp1);		# e+=F_20_39(b,c,d)
	&rotr($b,2);			# b=ROTATE(b,30)
	 &mov($tmp1,$a);
	&rotl($tmp1,5);			# ROTATE(a,5)
	 &mov(&swtmp($n%16),$f) if($n<77);# xi=f
	&lea($f,&DWP($K,$f,$e));	# f+=e+K_XX_YY
	 &mov($e,&swtmp(($n+1)%16)) if($n<79);# pre-fetch f for next round
	&add($f,$tmp1);			# f+=ROTATE(a,5)
}
d128 8
a135 7
if ($alt) {
	&add($e,$tmp1);			# e+=b&(c^d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&mov($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&xor($c,$d);			# restore $c
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
d137 6
a142 20
	 &and($tmp1,$c);
	&rotr($b,7);			# b=ROTATE(b,30)
	 &add($e,$tmp1);		# e+=c&d
	&mov($tmp1,$a);			# b in next round
	 &mov(&swtmp($n%16),$f);	# xi=f
	&rotl($a,5);			# ROTATE(a,5)
	 &xor($b,$c)			if ($n<59);
	&and($tmp1,$b)			if ($n<59);# tmp1 to hold F_40_59(b,c,d)
	 &lea($f,&DWP(0x8f1bbcdc,$f,$e));# f+=K_40_59+e+(b&(c^d))
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$a);			# f+=ROTATE(a,5)
} else {
	&mov($tmp1,$c);			# tmp1 to hold F_40_59(b,c,d)
	 &xor($f,&swtmp(($n+2)%16));	# f to hold Xupdate(xi,xa,xb,xc,xd)
	&xor($tmp1,$d);
	 &xor($f,&swtmp(($n+8)%16));
	&and($tmp1,$b);
	 &xor($f,&swtmp(($n+13)%16));	# f holds xa^xb^xc^xd
	&rotl($f,1);			# f=ROTATE(f,1)
	 &add($tmp1,$e);		# b&(c^d)+=e
d144 5
a148 5
	 &mov($e,$a);			# e becomes volatile
	&rotl($e,5);			# ROTATE(a,5)
	 &mov(&swtmp($n%16),$f);	# xi=f
	&lea($f,&DWP(0x8f1bbcdc,$f,$tmp1));# f+=K_40_59+e+(b&(c^d))
	 &mov($tmp1,$c);
a149 4
	 &and($tmp1,$d);
	&mov($e,&swtmp(($n+1)%16));	# pre-fetch f for next round
	 &add($f,$tmp1);		# f+=c&d
}
a152 27
if ($xmm) {
  &static_label("ssse3_shortcut");
  &static_label("avx_shortcut")		if ($ymm);
  &static_label("K_XX_XX");

	&call	(&label("pic_point"));	# make it PIC!
  &set_label("pic_point");
	&blindpop($tmp1);
	&picmeup($T,"OPENSSL_ia32cap_P",$tmp1,&label("pic_point"));
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));

	&mov	($A,&DWP(0,$T));
	&mov	($D,&DWP(4,$T));
	&test	($D,1<<9);		# check SSSE3 bit
	&jz	(&label("x86"));
	&test	($A,1<<24);		# check FXSR bit
	&jz	(&label("x86"));
	if ($ymm) {
		&and	($D,1<<28);		# mask AVX bit
		&and	($A,1<<30);		# mask "Intel CPU" bit
		&or	($A,$D);
		&cmp	($A,1<<28|1<<30);
		&je	(&label("avx_shortcut"));
	}
	&jmp	(&label("ssse3_shortcut"));
  &set_label("x86",16);
}
d156 1
a156 1
	&stack_push(16+3);	# allocate X[16]
a160 1
	&jmp(&label("loop"));
d162 1
a162 1
&set_label("loop",16);
d216 1
a216 1
	&stack_pop(16+3);
a217 837

if ($xmm) {
######################################################################
# The SSSE3 implementation.
#
# %xmm[0-7] are used as ring @@X[] buffer containing quadruples of last
# 32 elements of the message schedule or Xupdate outputs. First 4
# quadruples are simply byte-swapped input, next 4 are calculated
# according to method originally suggested by Dean Gaudet (modulo
# being implemented in SSSE3). Once 8 quadruples or 32 elements are
# collected, it switches to routine proposed by Max Locktyukhin.
#
# Calculations inevitably require temporary reqisters, and there are
# no %xmm registers left to spare. For this reason part of the ring
# buffer, X[2..4] to be specific, is offloaded to 3 quadriples ring
# buffer on the stack. Keep in mind that X[2] is alias X[-6], X[3] -
# X[-5], and X[4] - X[-4]...
#
# Another notable optimization is aggressive stack frame compression
# aiming to minimize amount of 9-byte instructions...
#
# Yet another notable optimization is "jumping" $B variable. It means
# that there is no register permanently allocated for $B value. This
# allowed to eliminate one instruction from body_20_39...
#
my $Xi=4;			# 4xSIMD Xupdate round, start pre-seeded
my @@X=map("xmm$_",(4..7,0..3));	# pre-seeded for $Xi=4
my @@V=($A,$B,$C,$D,$E);
my $j=0;			# hash round
my @@T=($T,$tmp1);
my $inp;

my $_rol=sub { &rol(@@_) };
my $_ror=sub { &ror(@@_) };

&function_begin("_sha1_block_data_order_ssse3");
	&call	(&label("pic_point"));	# make it PIC!
	&set_label("pic_point");
	&blindpop($tmp1);
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));
&set_label("ssse3_shortcut");

	&movdqa	(@@X[3],&QWP(0,$tmp1));		# K_00_19
	&movdqa	(@@X[4],&QWP(16,$tmp1));		# K_20_39
	&movdqa	(@@X[5],&QWP(32,$tmp1));		# K_40_59
	&movdqa	(@@X[6],&QWP(48,$tmp1));		# K_60_79
	&movdqa	(@@X[2],&QWP(64,$tmp1));		# pbswap mask

	&mov	($E,&wparam(0));		# load argument block
	&mov	($inp=@@T[1],&wparam(1));
	&mov	($D,&wparam(2));
	&mov	(@@T[0],"esp");

	# stack frame layout
	#
	# +0	X[0]+K	X[1]+K	X[2]+K	X[3]+K	# XMM->IALU xfer area
	#	X[4]+K	X[5]+K	X[6]+K	X[7]+K
	#	X[8]+K	X[9]+K	X[10]+K	X[11]+K
	#	X[12]+K	X[13]+K	X[14]+K	X[15]+K
	#
	# +64	X[0]	X[1]	X[2]	X[3]	# XMM->XMM backtrace area
	#	X[4]	X[5]	X[6]	X[7]
	#	X[8]	X[9]	X[10]	X[11]	# even borrowed for K_00_19
	#
	# +112	K_20_39	K_20_39	K_20_39	K_20_39	# constants
	#	K_40_59	K_40_59	K_40_59	K_40_59
	#	K_60_79	K_60_79	K_60_79	K_60_79
	#	K_00_19	K_00_19	K_00_19	K_00_19
	#	pbswap mask
	#
	# +192	ctx				# argument block
	# +196	inp
	# +200	end
	# +204	esp
	&sub	("esp",208);
	&and	("esp",-64);

	&movdqa	(&QWP(112+0,"esp"),@@X[4]);	# copy constants
	&movdqa	(&QWP(112+16,"esp"),@@X[5]);
	&movdqa	(&QWP(112+32,"esp"),@@X[6]);
	&shl	($D,6);				# len*64
	&movdqa	(&QWP(112+48,"esp"),@@X[3]);
	&add	($D,$inp);			# end of input
	&movdqa	(&QWP(112+64,"esp"),@@X[2]);
	&add	($inp,64);
	&mov	(&DWP(192+0,"esp"),$E);		# save argument block
	&mov	(&DWP(192+4,"esp"),$inp);
	&mov	(&DWP(192+8,"esp"),$D);
	&mov	(&DWP(192+12,"esp"),@@T[0]);	# save original %esp

	&mov	($A,&DWP(0,$E));		# load context
	&mov	($B,&DWP(4,$E));
	&mov	($C,&DWP(8,$E));
	&mov	($D,&DWP(12,$E));
	&mov	($E,&DWP(16,$E));
	&mov	(@@T[0],$B);			# magic seed

	&movdqu	(@@X[-4&7],&QWP(-64,$inp));	# load input to %xmm[0-3]
	&movdqu	(@@X[-3&7],&QWP(-48,$inp));
	&movdqu	(@@X[-2&7],&QWP(-32,$inp));
	&movdqu	(@@X[-1&7],&QWP(-16,$inp));
	&pshufb	(@@X[-4&7],@@X[2]);		# byte swap
	&pshufb	(@@X[-3&7],@@X[2]);
	&pshufb	(@@X[-2&7],@@X[2]);
	&movdqa	(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot
	&pshufb	(@@X[-1&7],@@X[2]);
	&paddd	(@@X[-4&7],@@X[3]);		# add K_00_19
	&paddd	(@@X[-3&7],@@X[3]);
	&paddd	(@@X[-2&7],@@X[3]);
	&movdqa	(&QWP(0,"esp"),@@X[-4&7]);	# X[]+K xfer to IALU
	&psubd	(@@X[-4&7],@@X[3]);		# restore X[]
	&movdqa	(&QWP(0+16,"esp"),@@X[-3&7]);
	&psubd	(@@X[-3&7],@@X[3]);
	&movdqa	(&QWP(0+32,"esp"),@@X[-2&7]);
	&psubd	(@@X[-2&7],@@X[3]);
	&movdqa	(@@X[0],@@X[-3&7]);
	&jmp	(&label("loop"));

######################################################################
# SSE instruction sequence is first broken to groups of indepentent
# instructions, independent in respect to their inputs and shifter
# (not all architectures have more than one). Then IALU instructions
# are "knitted in" between the SSE groups. Distance is maintained for
# SSE latency of 2 in hope that it fits better upcoming AMD Bulldozer
# [which allegedly also implements SSSE3]...
#
# Temporary registers usage. X[2] is volatile at the entry and at the
# end is restored from backtrace ring buffer. X[3] is expected to
# contain current K_XX_XX constant and is used to caclulate X[-1]+K
# from previous round, it becomes volatile the moment the value is
# saved to stack for transfer to IALU. X[4] becomes volatile whenever
# X[-4] is accumulated and offloaded to backtrace ring buffer, at the
# end it is loaded with next K_XX_XX [which becomes X[3] in next
# round]...
#
sub Xupdate_ssse3_16_31()		# recall that $Xi starts wtih 4
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 40 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&palignr(@@X[0],@@X[-4&7],8);	# compose "X[-14]" in "X[0]"
	&movdqa	(@@X[2],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &paddd	(@@X[3],@@X[-1&7]);
	  &movdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&psrldq	(@@X[2],4);		# "X[-3]", 3 dwords
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&pxor	(@@X[0],@@X[-4&7]);	# "X[0]"^="X[-16]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[2],@@X[-2&7]);	# "X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[2]);		# "X[0]"^="X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&movdqa	(@@X[4],@@X[0]);
	&movdqa	(@@X[2],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pslldq	(@@X[4],12);		# "X[0]"<<96, extract one dword
	&paddd	(@@X[0],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&psrld	(@@X[2],31);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&movdqa	(@@X[3],@@X[4]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&psrld	(@@X[4],30);
	&por	(@@X[0],@@X[2]);		# "X[0]"<<<=1
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if ($Xi>5);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pslld	(@@X[3],2);
	&pxor	(@@X[0],@@X[4]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &movdqa	(@@X[4],&QWP(112-16+16*(($Xi)/5),"esp"));	# K_XX_XX
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[3]);		# "X[0]"^=("X[0]"<<96)<<<2
	  &movdqa	(@@X[1],@@X[-2&7])	if ($Xi<7);
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions [if any]

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xupdate_ssse3_32_79()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 to 48 instructions
  my ($a,$b,$c,$d,$e);

	&movdqa	(@@X[2],@@X[-1&7])	if ($Xi==8);
	 eval(shift(@@insns));		# body_20_39
	&pxor	(@@X[0],@@X[-4&7]);	# "X[0]"="X[-32]"^"X[-16]"
	&palignr(@@X[2],@@X[-2&7],8);	# compose "X[-6]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&pxor	(@@X[0],@@X[-7&7]);	# "X[0]"^="X[-28]"
	  &movdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);	# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 if ($Xi%5) {
	  &movdqa	(@@X[4],@@X[3]);	# "perpetuate" K_XX_XX...
	 } else {			# ... or load next one
	  &movdqa	(@@X[4],&QWP(112-16+16*($Xi/5),"esp"));
	 }
	  &paddd	(@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&pxor	(@@X[0],@@X[2]);		# "X[0]"^="X[-6]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&movdqa	(@@X[2],@@X[0]);
	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&pslld	(@@X[0],2);
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	&psrld	(@@X[2],30);
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&por	(@@X[0],@@X[2]);		# "X[0]"<<<=2
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	  &movdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if($Xi<19);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	  &movdqa	(@@X[3],@@X[0])	if ($Xi<19);
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xuplast_ssse3_80()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	  &paddd	(@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &movdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer IALU

	 foreach (@@insns) { eval; }		# remaining instructions

	&mov	($inp=@@T[1],&DWP(192+4,"esp"));
	&cmp	($inp,&DWP(192+8,"esp"));
	&je	(&label("done"));

	&movdqa	(@@X[3],&QWP(112+48,"esp"));	# K_00_19
	&movdqa	(@@X[2],&QWP(112+64,"esp"));	# pbswap mask
	&movdqu	(@@X[-4&7],&QWP(0,$inp));	# load input
	&movdqu	(@@X[-3&7],&QWP(16,$inp));
	&movdqu	(@@X[-2&7],&QWP(32,$inp));
	&movdqu	(@@X[-1&7],&QWP(48,$inp));
	&add	($inp,64);
	&pshufb	(@@X[-4&7],@@X[2]);		# byte swap
	&mov	(&DWP(192+4,"esp"),$inp);
	&movdqa	(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot

  $Xi=0;
}

sub Xloop_ssse3()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&pshufb	(@@X[($Xi-3)&7],@@X[2]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&paddd	(@@X[($Xi-4)&7],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&movdqa	(&QWP(0+16*$Xi,"esp"),@@X[($Xi-4)&7]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&psubd	(@@X[($Xi-4)&7],@@X[3]);

	foreach (@@insns) { eval; }
  $Xi++;
}

sub Xtail_ssse3()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	foreach (@@insns) { eval; }
}

sub body_00_19 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&add	($e,&DWP(4*($j&15),"esp"));',	# X[]+K xfer
	'&xor	($c,$d);',
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&and	(@@T[0],$c);',	# ($b&($c^$d))
	'&xor	($c,$d);',	# restore $c
	'&xor	(@@T[0],$d);',
	'&add	($e,$a);',
	'&$_ror	($b,$j?7:2);',	# $b>>>2
	'&add	($e,@@T[0]);'	.'$j++; unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

sub body_20_39 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&add	($e,&DWP(4*($j++&15),"esp"));',	# X[]+K xfer
	'&xor	(@@T[0],$d);',	# ($b^$d)
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&xor	(@@T[0],$c);',	# ($b^$d^$c)
	'&add	($e,$a);',
	'&$_ror	($b,7);',	# $b>>>2
	'&add	($e,@@T[0]);'	.'unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

sub body_40_59 () {
	(
	'($a,$b,$c,$d,$e)=@@V;'.
	'&mov	(@@T[1],$c);',
	'&xor	($c,$d);',
	'&add	($e,&DWP(4*($j++&15),"esp"));',	# X[]+K xfer
	'&and	(@@T[1],$d);',
	'&and	(@@T[0],$c);',	# ($b&($c^$d))
	'&$_ror	($b,7);',	# $b>>>2
	'&add	($e,@@T[1]);',
	'&mov	(@@T[1],$a);',	# $b in next round
	'&$_rol	($a,5);',
	'&add	($e,@@T[0]);',
	'&xor	($c,$d);',	# restore $c
	'&add	($e,$a);'	.'unshift(@@V,pop(@@V)); unshift(@@T,pop(@@T));'
	);
}

&set_label("loop",16);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_16_31(\&body_00_19);
	&Xupdate_ssse3_32_79(\&body_00_19);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_40_59);
	&Xupdate_ssse3_32_79(\&body_20_39);
	&Xuplast_ssse3_80(\&body_20_39);	# can jump to "done"

				$saved_j=$j; @@saved_V=@@V;

	&Xloop_ssse3(\&body_20_39);
	&Xloop_ssse3(\&body_20_39);
	&Xloop_ssse3(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	($B,@@T[0]);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);
	&movdqa	(@@X[0],@@X[-3&7]);

	&jmp	(&label("loop"));

&set_label("done",16);		$j=$saved_j; @@V=@@saved_V;

	&Xtail_ssse3(\&body_20_39);
	&Xtail_ssse3(\&body_20_39);
	&Xtail_ssse3(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&mov	("esp",&DWP(192+12,"esp"));	# restore %esp
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);

&function_end("_sha1_block_data_order_ssse3");

if ($ymm) {
my $Xi=4;			# 4xSIMD Xupdate round, start pre-seeded
my @@X=map("xmm$_",(4..7,0..3));	# pre-seeded for $Xi=4
my @@V=($A,$B,$C,$D,$E);
my $j=0;			# hash round
my @@T=($T,$tmp1);
my $inp;

my $_rol=sub { &shld(@@_[0],@@_) };
my $_ror=sub { &shrd(@@_[0],@@_) };

&function_begin("_sha1_block_data_order_avx");
	&call	(&label("pic_point"));	# make it PIC!
	&set_label("pic_point");
	&blindpop($tmp1);
	&lea	($tmp1,&DWP(&label("K_XX_XX")."-".&label("pic_point"),$tmp1));
&set_label("avx_shortcut");
	&vzeroall();

	&vmovdqa(@@X[3],&QWP(0,$tmp1));		# K_00_19
	&vmovdqa(@@X[4],&QWP(16,$tmp1));		# K_20_39
	&vmovdqa(@@X[5],&QWP(32,$tmp1));		# K_40_59
	&vmovdqa(@@X[6],&QWP(48,$tmp1));		# K_60_79
	&vmovdqa(@@X[2],&QWP(64,$tmp1));		# pbswap mask

	&mov	($E,&wparam(0));		# load argument block
	&mov	($inp=@@T[1],&wparam(1));
	&mov	($D,&wparam(2));
	&mov	(@@T[0],"esp");

	# stack frame layout
	#
	# +0	X[0]+K	X[1]+K	X[2]+K	X[3]+K	# XMM->IALU xfer area
	#	X[4]+K	X[5]+K	X[6]+K	X[7]+K
	#	X[8]+K	X[9]+K	X[10]+K	X[11]+K
	#	X[12]+K	X[13]+K	X[14]+K	X[15]+K
	#
	# +64	X[0]	X[1]	X[2]	X[3]	# XMM->XMM backtrace area
	#	X[4]	X[5]	X[6]	X[7]
	#	X[8]	X[9]	X[10]	X[11]	# even borrowed for K_00_19
	#
	# +112	K_20_39	K_20_39	K_20_39	K_20_39	# constants
	#	K_40_59	K_40_59	K_40_59	K_40_59
	#	K_60_79	K_60_79	K_60_79	K_60_79
	#	K_00_19	K_00_19	K_00_19	K_00_19
	#	pbswap mask
	#
	# +192	ctx				# argument block
	# +196	inp
	# +200	end
	# +204	esp
	&sub	("esp",208);
	&and	("esp",-64);

	&vmovdqa(&QWP(112+0,"esp"),@@X[4]);	# copy constants
	&vmovdqa(&QWP(112+16,"esp"),@@X[5]);
	&vmovdqa(&QWP(112+32,"esp"),@@X[6]);
	&shl	($D,6);				# len*64
	&vmovdqa(&QWP(112+48,"esp"),@@X[3]);
	&add	($D,$inp);			# end of input
	&vmovdqa(&QWP(112+64,"esp"),@@X[2]);
	&add	($inp,64);
	&mov	(&DWP(192+0,"esp"),$E);		# save argument block
	&mov	(&DWP(192+4,"esp"),$inp);
	&mov	(&DWP(192+8,"esp"),$D);
	&mov	(&DWP(192+12,"esp"),@@T[0]);	# save original %esp

	&mov	($A,&DWP(0,$E));		# load context
	&mov	($B,&DWP(4,$E));
	&mov	($C,&DWP(8,$E));
	&mov	($D,&DWP(12,$E));
	&mov	($E,&DWP(16,$E));
	&mov	(@@T[0],$B);			# magic seed

	&vmovdqu(@@X[-4&7],&QWP(-64,$inp));	# load input to %xmm[0-3]
	&vmovdqu(@@X[-3&7],&QWP(-48,$inp));
	&vmovdqu(@@X[-2&7],&QWP(-32,$inp));
	&vmovdqu(@@X[-1&7],&QWP(-16,$inp));
	&vpshufb(@@X[-4&7],@@X[-4&7],@@X[2]);	# byte swap
	&vpshufb(@@X[-3&7],@@X[-3&7],@@X[2]);
	&vpshufb(@@X[-2&7],@@X[-2&7],@@X[2]);
	&vmovdqa(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot
	&vpshufb(@@X[-1&7],@@X[-1&7],@@X[2]);
	&vpaddd	(@@X[0],@@X[-4&7],@@X[3]);		# add K_00_19
	&vpaddd	(@@X[1],@@X[-3&7],@@X[3]);
	&vpaddd	(@@X[2],@@X[-2&7],@@X[3]);
	&vmovdqa(&QWP(0,"esp"),@@X[0]);		# X[]+K xfer to IALU
	&vmovdqa(&QWP(0+16,"esp"),@@X[1]);
	&vmovdqa(&QWP(0+32,"esp"),@@X[2]);
	&jmp	(&label("loop"));

sub Xupdate_avx_16_31()		# recall that $Xi starts wtih 4
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 40 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpalignr(@@X[0],@@X[-3&7],@@X[-4&7],8);	# compose "X[-14]" in "X[0]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	  &vmovdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpsrldq(@@X[2],@@X[-1&7],4);		# "X[-3]", 3 dwords
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpxor	(@@X[0],@@X[0],@@X[-4&7]);		# "X[0]"^="X[-16]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[2],@@X[2],@@X[-2&7]);		# "X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"^="X[-3]"^"X[-8]"
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpsrld	(@@X[2],@@X[0],31);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpslldq(@@X[4],@@X[0],12);		# "X[0]"<<96, extract one dword
	&vpaddd	(@@X[0],@@X[0],@@X[0]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpsrld	(@@X[3],@@X[4],30);
	&vpor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"<<<=1
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpslld	(@@X[4],@@X[4],2);
	  &vmovdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if ($Xi>5);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpxor	(@@X[0],@@X[0],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[4]);		# "X[0]"^=("X[0]"<<96)<<<2
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	  &vmovdqa	(@@X[4],&QWP(112-16+16*(($Xi)/5),"esp"));	# K_XX_XX
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions [if any]

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xupdate_avx_32_79()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 to 48 instructions
  my ($a,$b,$c,$d,$e);

	&vpalignr(@@X[2],@@X[-1&7],@@X[-2&7],8);	# compose "X[-6]"
	&vpxor	(@@X[0],@@X[0],@@X[-4&7]);	# "X[0]"="X[-32]"^"X[-16]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&vpxor	(@@X[0],@@X[0],@@X[-7&7]);	# "X[0]"^="X[-28]"
	  &vmovdqa	(&QWP(64+16*(($Xi-4)%3),"esp"),@@X[-4&7]);	# save X[] to backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 if ($Xi%5) {
	  &vmovdqa	(@@X[4],@@X[3]);	# "perpetuate" K_XX_XX...
	 } else {			# ... or load next one
	  &vmovdqa	(@@X[4],&QWP(112-16+16*($Xi/5),"esp"));
	 }
	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpxor	(@@X[0],@@X[0],@@X[2]);		# "X[0]"^="X[-6]"
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol

	&vpsrld	(@@X[2],@@X[0],30);
	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpslld	(@@X[0],@@X[0],2);
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	&vpor	(@@X[0],@@X[0],@@X[2]);	# "X[0]"<<<=2
	 eval(shift(@@insns));		# body_20_39
	 eval(shift(@@insns));
	  &vmovdqa	(@@X[2],&QWP(64+16*(($Xi-6)%3),"esp")) if($Xi<19);	# restore X[] from backtrace buffer
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# rol
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));		# ror
	 eval(shift(@@insns));

	 foreach (@@insns) { eval; }	# remaining instructions

  $Xi++;	push(@@X,shift(@@X));	# "rotate" X[]
}

sub Xuplast_avx_80()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	  &vpaddd	(@@X[3],@@X[3],@@X[-1&7]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	  &vmovdqa	(&QWP(0+16*(($Xi-1)&3),"esp"),@@X[3]);	# X[]+K xfer IALU

	 foreach (@@insns) { eval; }		# remaining instructions

	&mov	($inp=@@T[1],&DWP(192+4,"esp"));
	&cmp	($inp,&DWP(192+8,"esp"));
	&je	(&label("done"));

	&vmovdqa(@@X[3],&QWP(112+48,"esp"));	# K_00_19
	&vmovdqa(@@X[2],&QWP(112+64,"esp"));	# pbswap mask
	&vmovdqu(@@X[-4&7],&QWP(0,$inp));	# load input
	&vmovdqu(@@X[-3&7],&QWP(16,$inp));
	&vmovdqu(@@X[-2&7],&QWP(32,$inp));
	&vmovdqu(@@X[-1&7],&QWP(48,$inp));
	&add	($inp,64);
	&vpshufb(@@X[-4&7],@@X[-4&7],@@X[2]);		# byte swap
	&mov	(&DWP(192+4,"esp"),$inp);
	&vmovdqa(&QWP(112-16,"esp"),@@X[3]);	# borrow last backtrace slot

  $Xi=0;
}

sub Xloop_avx()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpshufb	(@@X[($Xi-3)&7],@@X[($Xi-3)&7],@@X[2]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vpaddd	(@@X[$Xi&7],@@X[($Xi-4)&7],@@X[3]);
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	 eval(shift(@@insns));
	&vmovdqa	(&QWP(0+16*$Xi,"esp"),@@X[$Xi&7]);	# X[]+K xfer to IALU
	 eval(shift(@@insns));
	 eval(shift(@@insns));

	foreach (@@insns) { eval; }
  $Xi++;
}

sub Xtail_avx()
{ use integer;
  my $body = shift;
  my @@insns = (&$body,&$body,&$body,&$body);	# 32 instructions
  my ($a,$b,$c,$d,$e);

	foreach (@@insns) { eval; }
}

&set_label("loop",16);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_16_31(\&body_00_19);
	&Xupdate_avx_32_79(\&body_00_19);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_40_59);
	&Xupdate_avx_32_79(\&body_20_39);
	&Xuplast_avx_80(\&body_20_39);	# can jump to "done"

				$saved_j=$j; @@saved_V=@@V;

	&Xloop_avx(\&body_20_39);
	&Xloop_avx(\&body_20_39);
	&Xloop_avx(\&body_20_39);

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	($B,@@T[0]);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);

	&jmp	(&label("loop"));

&set_label("done",16);		$j=$saved_j; @@V=@@saved_V;

	&Xtail_avx(\&body_20_39);
	&Xtail_avx(\&body_20_39);
	&Xtail_avx(\&body_20_39);

	&vzeroall();

	&mov	(@@T[1],&DWP(192,"esp"));	# update context
	&add	($A,&DWP(0,@@T[1]));
	&mov	("esp",&DWP(192+12,"esp"));	# restore %esp
	&add	(@@T[0],&DWP(4,@@T[1]));		# $b
	&add	($C,&DWP(8,@@T[1]));
	&mov	(&DWP(0,@@T[1]),$A);
	&add	($D,&DWP(12,@@T[1]));
	&mov	(&DWP(4,@@T[1]),@@T[0]);
	&add	($E,&DWP(16,@@T[1]));
	&mov	(&DWP(8,@@T[1]),$C);
	&mov	(&DWP(12,@@T[1]),$D);
	&mov	(&DWP(16,@@T[1]),$E);
&function_end("_sha1_block_data_order_avx");
}
&set_label("K_XX_XX",64);
&data_word(0x5a827999,0x5a827999,0x5a827999,0x5a827999);	# K_00_19
&data_word(0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1);	# K_20_39
&data_word(0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc);	# K_40_59
&data_word(0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6);	# K_60_79
&data_word(0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f);	# pbswap mask
}
@


