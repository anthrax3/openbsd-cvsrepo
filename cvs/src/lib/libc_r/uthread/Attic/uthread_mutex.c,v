head	1.16;
access;
symbols
	OPENBSD_3_2:1.14.0.4
	OPENBSD_3_2_BASE:1.14
	OPENBSD_3_1:1.14.0.2
	OPENBSD_3_1_BASE:1.14
	OPENBSD_3_0:1.11.0.8
	OPENBSD_3_0_BASE:1.11
	OPENBSD_2_9:1.11.0.6
	OPENBSD_2_9_BASE:1.11
	OPENBSD_2_8:1.11.0.4
	OPENBSD_2_8_BASE:1.11
	OPENBSD_2_7:1.11.0.2
	OPENBSD_2_7_BASE:1.11
	OPENBSD_2_6:1.8.0.2
	OPENBSD_2_6_BASE:1.8
	OPENBSD_2_5:1.6.0.2
	OPENBSD_2_5_BASE:1.6
	OPENBSD_2_4:1.1.0.2
	OPENBSD_2_4_BASE:1.1;
locks; strict;
comment	@ * @;


1.16
date	2003.01.20.18.14.07;	author marc;	state dead;
branches;
next	1.15;

1.15
date	2002.10.30.19.11.56;	author marc;	state Exp;
branches;
next	1.14;

1.14
date	2002.01.23.23.11.57;	author fgsch;	state Exp;
branches;
next	1.13;

1.13
date	2001.11.12.21.13.34;	author fgsch;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.12.02.24.30;	author marc;	state Exp;
branches;
next	1.11;

1.11
date	2000.01.06.07.19.35;	author d;	state Exp;
branches;
next	1.10;

1.10
date	2000.01.06.07.18.46;	author d;	state Exp;
branches;
next	1.9;

1.9
date	99.11.25.07.01.38;	author d;	state Exp;
branches;
next	1.8;

1.8
date	99.06.09.07.06.54;	author d;	state Exp;
branches;
next	1.7;

1.7
date	99.05.26.00.18.25;	author d;	state Exp;
branches;
next	1.6;

1.6
date	99.01.06.05.29.25;	author d;	state Exp;
branches;
next	1.5;

1.5
date	98.12.23.22.45.31;	author d;	state Exp;
branches;
next	1.4;

1.4
date	98.12.18.14.34.31;	author d;	state Exp;
branches;
next	1.3;

1.3
date	98.12.10.00.40.19;	author d;	state Exp;
branches;
next	1.2;

1.2
date	98.11.20.12.13.32;	author d;	state Exp;
branches;
next	1.1;

1.1
date	98.08.27.09.01.13;	author d;	state Exp;
branches;
next	;


desc
@@


1.16
log
@
bye-bye libc_r sources.
the sources have been moved (with history) to /usr/src/lib/libpthread
@
text
@/*	$OpenBSD: uthread_mutex.c,v 1.15 2002/10/30 19:11:56 marc Exp $	*/
/*
 * Copyright (c) 1995 John Birrell <jb@@cimlogic.com.au>.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by John Birrell.
 * 4. Neither the name of the author nor the names of any co-contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY JOHN BIRRELL AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: uthread_mutex.c,v 1.16 1999/08/28 00:03:40 peter Exp $
 */
#include <stdlib.h>
#include <errno.h>
#include <string.h>
#include <sys/param.h>
#include <sys/queue.h>
#ifdef _THREAD_SAFE
#include <pthread.h>
#include "pthread_private.h"

#if defined(_PTHREADS_INVARIANTS)
#define _MUTEX_INIT_LINK(m) 		do {		\
	(m)->m_qe.tqe_prev = NULL;			\
	(m)->m_qe.tqe_next = NULL;			\
} while (0)
#define _MUTEX_ASSERT_IS_OWNED(m)	do {		\
	if ((m)->m_qe.tqe_prev == NULL)			\
		PANIC("mutex is not on list");		\
} while (0)
#define _MUTEX_ASSERT_NOT_OWNED(m)	do {		\
	if (((m)->m_qe.tqe_prev != NULL) ||		\
	    ((m)->m_qe.tqe_next != NULL))		\
		PANIC("mutex is on list");		\
} while (0)
#else
#define _MUTEX_INIT_LINK(m)
#define _MUTEX_ASSERT_IS_OWNED(m)
#define _MUTEX_ASSERT_NOT_OWNED(m)
#endif

/*
 * Prototypes
 */
static inline int	mutex_self_trylock(pthread_mutex_t);
static inline int	mutex_self_lock(pthread_mutex_t);
static inline int	mutex_unlock_common(pthread_mutex_t *, int);
static void		mutex_priority_adjust(pthread_mutex_t);
static void		mutex_rescan_owned (pthread_t, pthread_mutex_t);
static inline pthread_t	mutex_queue_deq(pthread_mutex_t);
static inline void	mutex_queue_remove(pthread_mutex_t, pthread_t);
static inline void	mutex_queue_enq(pthread_mutex_t, pthread_t);


static spinlock_t static_init_lock = _SPINLOCK_INITIALIZER;

static struct pthread_mutex_attr	static_mutex_attr =
    PTHREAD_MUTEXATTR_STATIC_INITIALIZER;
static pthread_mutexattr_t		static_mattr = &static_mutex_attr;

/* Reinitialize a mutex to defaults. */
int
_mutex_reinit(pthread_mutex_t * mutex)
{
	int	ret = 0;

	if (mutex == NULL)
		ret = EINVAL;
	else if (*mutex == NULL)
		ret = pthread_mutex_init(mutex, NULL);
	else {
		/*
		 * Initialize the mutex structure:
		 */
		(*mutex)->m_type = PTHREAD_MUTEX_DEFAULT;
		(*mutex)->m_protocol = PTHREAD_PRIO_NONE;
		TAILQ_INIT(&(*mutex)->m_queue);
		(*mutex)->m_owner = NULL;
		(*mutex)->m_data.m_count = 0;
		(*mutex)->m_flags &= MUTEX_FLAGS_PRIVATE;
		(*mutex)->m_flags |= MUTEX_FLAGS_INITED;
		(*mutex)->m_refcount = 0;
		(*mutex)->m_prio = 0;
		(*mutex)->m_saved_prio = 0;
		_MUTEX_INIT_LINK(*mutex);
		_SPINLOCK_INIT(&(*mutex)->lock);
	}
	return (ret);
}

int
pthread_mutex_init(pthread_mutex_t * mutex,
		   const pthread_mutexattr_t * mutex_attr)
{
	enum pthread_mutextype	type;
	int		protocol;
	int		ceiling;
	int		flags;
	pthread_mutex_t	pmutex;
	int		ret = 0;

	if (mutex == NULL)
		ret = EINVAL;

	/* Check if default mutex attributes: */
	else if (mutex_attr == NULL || *mutex_attr == NULL) {
		/* Default to a (error checking) POSIX mutex: */
		type = PTHREAD_MUTEX_ERRORCHECK;
		protocol = PTHREAD_PRIO_NONE;
		ceiling = PTHREAD_MAX_PRIORITY;
		flags = 0;
	}

	/* Check mutex type: */
	else if (((*mutex_attr)->m_type < PTHREAD_MUTEX_ERRORCHECK) ||
	    ((*mutex_attr)->m_type >= MUTEX_TYPE_MAX))
		/* Return an invalid argument error: */
		ret = EINVAL;

	/* Check mutex protocol: */
	else if (((*mutex_attr)->m_protocol < PTHREAD_PRIO_NONE) ||
	    ((*mutex_attr)->m_protocol > PTHREAD_PRIO_PROTECT))
		/* Return an invalid argument error: */
		ret = EINVAL;

	else {
		/* Use the requested mutex type and protocol: */
		type = (*mutex_attr)->m_type;
		protocol = (*mutex_attr)->m_protocol;
		ceiling = (*mutex_attr)->m_ceiling;
		flags = (*mutex_attr)->m_flags;
	}

	/* Check no errors so far: */
	if (ret == 0) {
		if ((pmutex = (pthread_mutex_t)
		    malloc(sizeof(struct pthread_mutex))) == NULL)
			ret = ENOMEM;
		else {
			/* Set the mutex flags: */
			pmutex->m_flags = flags;

			/* Process according to mutex type: */
			switch (type) {
			/* case PTHREAD_MUTEX_DEFAULT: */
			case PTHREAD_MUTEX_ERRORCHECK:
			case PTHREAD_MUTEX_NORMAL:
				/* Nothing to do here. */
				break;

			/* Single UNIX Spec 2 recursive mutex: */
			case PTHREAD_MUTEX_RECURSIVE:
				/* Reset the mutex count: */
				pmutex->m_data.m_count = 0;
				break;

			/* Trap invalid mutex types: */
			default:
				/* Return an invalid argument error: */
				ret = EINVAL;
				break;
			}
			if (ret == 0) {
				/* Initialise the rest of the mutex: */
				TAILQ_INIT(&pmutex->m_queue);
				pmutex->m_flags |= MUTEX_FLAGS_INITED;
				pmutex->m_owner = NULL;
				pmutex->m_type = type;
				pmutex->m_protocol = protocol;
				pmutex->m_refcount = 0;
				if (protocol == PTHREAD_PRIO_PROTECT)
					pmutex->m_prio = ceiling;
				else
					pmutex->m_prio = 0;
				pmutex->m_saved_prio = 0;
				_MUTEX_INIT_LINK(pmutex);
				_SPINLOCK_INIT(&pmutex->lock);
				*mutex = pmutex;
			} else {
				free((void *)pmutex);
				*mutex = NULL;
			}
		}
	}
	/* Return the completion status: */
	return(ret);
}

int
pthread_mutex_destroy(pthread_mutex_t * mutex)
{
	int	ret = 0;

	if (mutex == NULL || *mutex == NULL)
		ret = EINVAL;
	else {
		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);

		/*
		 * Check to see if this mutex is in use:
		 */
		if (((*mutex)->m_owner != NULL) ||
		    (TAILQ_FIRST(&(*mutex)->m_queue) != NULL) ||
		    ((*mutex)->m_refcount != 0)) {
			ret = EBUSY;

			/* Unlock the mutex structure: */
			_SPINUNLOCK(&(*mutex)->lock);
		}
		else {
			/*
			 * Free the memory allocated for the mutex
			 * structure:
			 */
			_MUTEX_ASSERT_NOT_OWNED(*mutex);
			free((void *)*mutex);

			/*
			 * Leave the caller's pointer NULL now that
			 * the mutex has been destroyed:
			 */
			*mutex = NULL;
		}
	}

	/* Return the completion status: */
	return (ret);
}

static int
init_static(pthread_mutex_t *mutex)
{
	int	ret;

	_SPINLOCK(&static_init_lock);

	if (*mutex == NULL)
		ret = pthread_mutex_init(mutex, NULL);
	else
		ret = 0;

	_SPINUNLOCK(&static_init_lock);

	return(ret);
}

static int
init_static_private(pthread_mutex_t *mutex)
{
	int	ret;

	_SPINLOCK(&static_init_lock);

	if (*mutex == NULL)
		ret = pthread_mutex_init(mutex, &static_mattr);
	else
		ret = 0;

	_SPINUNLOCK(&static_init_lock);

	return(ret);
}

static int
mutex_trylock_common(pthread_mutex_t *mutex)
{
	struct pthread	*curthread = _get_curthread();
	int	ret = 0;

	PTHREAD_ASSERT((mutex != NULL) && (*mutex != NULL),
	    "Uninitialized mutex in mutex_trylock_common");

	/*
	 * Defer signals to protect the scheduling queues from
	 * access by the signal handler:
	 */
	_thread_kern_sig_defer();

	/* Lock the mutex structure: */
	_SPINLOCK(&(*mutex)->lock);

	/*
	 * If the mutex was statically allocated, properly
	 * initialize the tail queue.
	 */
	if (((*mutex)->m_flags & MUTEX_FLAGS_INITED) == 0) {
		TAILQ_INIT(&(*mutex)->m_queue);
		_MUTEX_INIT_LINK(*mutex);
		(*mutex)->m_flags |= MUTEX_FLAGS_INITED;
	}

	/* Process according to mutex type: */
	switch ((*mutex)->m_protocol) {
	/* Default POSIX mutex: */
	case PTHREAD_PRIO_NONE:	
		/* Check if this mutex is not locked: */
		if ((*mutex)->m_owner == NULL) {
			/* Lock the mutex for the running thread: */
			(*mutex)->m_owner = curthread;

			/* Add to the list of owned mutexes: */
			_MUTEX_ASSERT_NOT_OWNED(*mutex);
			TAILQ_INSERT_TAIL(&curthread->mutexq,
			    (*mutex), m_qe);
		} else if ((*mutex)->m_owner == curthread)
			ret = mutex_self_trylock(*mutex);
		else
			/* Return a busy error: */
			ret = EBUSY;
		break;

	/* POSIX priority inheritence mutex: */
	case PTHREAD_PRIO_INHERIT:
		/* Check if this mutex is not locked: */
		if ((*mutex)->m_owner == NULL) {
			/* Lock the mutex for the running thread: */
			(*mutex)->m_owner = curthread;

			/* Track number of priority mutexes owned: */
			curthread->priority_mutex_count++;

			/*
			 * The mutex takes on the attributes of the
			 * running thread when there are no waiters.
			 */
			(*mutex)->m_prio = curthread->active_priority;
			(*mutex)->m_saved_prio =
			    curthread->inherited_priority;

			/* Add to the list of owned mutexes: */
			_MUTEX_ASSERT_NOT_OWNED(*mutex);
			TAILQ_INSERT_TAIL(&curthread->mutexq,
			    (*mutex), m_qe);
		} else if ((*mutex)->m_owner == curthread)
			ret = mutex_self_trylock(*mutex);
		else
			/* Return a busy error: */
			ret = EBUSY;
		break;

	/* POSIX priority protection mutex: */
	case PTHREAD_PRIO_PROTECT:
		/* Check for a priority ceiling violation: */
		if (curthread->active_priority > (*mutex)->m_prio)
			ret = EINVAL;

		/* Check if this mutex is not locked: */
		else if ((*mutex)->m_owner == NULL) {
			/* Lock the mutex for the running thread: */
			(*mutex)->m_owner = curthread;

			/* Track number of priority mutexes owned: */
			curthread->priority_mutex_count++;

			/*
			 * The running thread inherits the ceiling
			 * priority of the mutex and executes at that
			 * priority.
			 */
			curthread->active_priority = (*mutex)->m_prio;
			(*mutex)->m_saved_prio =
			    curthread->inherited_priority;
			curthread->inherited_priority =
			    (*mutex)->m_prio;

			/* Add to the list of owned mutexes: */
			_MUTEX_ASSERT_NOT_OWNED(*mutex);
			TAILQ_INSERT_TAIL(&curthread->mutexq,
			    (*mutex), m_qe);
		} else if ((*mutex)->m_owner == curthread)
			ret = mutex_self_trylock(*mutex);
		else
			/* Return a busy error: */
			ret = EBUSY;
		break;

	/* Trap invalid mutex types: */
	default:
		/* Return an invalid argument error: */
		ret = EINVAL;
		break;
	}

	/* Unlock the mutex structure: */
	_SPINUNLOCK(&(*mutex)->lock);

	/*
	 * Undefer and handle pending signals, yielding if
	 * necessary:
	 */
	_thread_kern_sig_undefer();

	/* Return the completion status: */
	return (ret);
}

int
pthread_mutex_trylock(pthread_mutex_t *mutex)
{
	int	ret = 0;

	if (mutex == NULL)
		ret = EINVAL;

	/*
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization:
	 */
	else if ((*mutex != NULL) || (ret = init_static(mutex)) == 0)
		ret = mutex_trylock_common(mutex);

	return (ret);
}

int
_pthread_mutex_trylock(pthread_mutex_t *mutex)
{
	int	ret = 0;

	if (mutex == NULL)
		ret = EINVAL;

	/*
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization marking the mutex private (delete safe):
	 */
	else if ((*mutex != NULL) || (ret = init_static_private(mutex)) == 0)
		ret = mutex_trylock_common(mutex);

	return (ret);
}

static int
mutex_lock_common(pthread_mutex_t * mutex)
{
	struct pthread	*curthread = _get_curthread();
	int	ret = 0;

	PTHREAD_ASSERT((mutex != NULL) && (*mutex != NULL),
	    "Uninitialized mutex in mutex_lock_common");

	/* Reset the interrupted flag: */
	curthread->interrupted = 0;

	/*
	 * Enter a loop waiting to become the mutex owner.  We need a
	 * loop in case the waiting thread is interrupted by a signal
	 * to execute a signal handler.  It is not (currently) possible
	 * to remain in the waiting queue while running a handler.
	 * Instead, the thread is interrupted and backed out of the
	 * waiting queue prior to executing the signal handler.
	 */
	do {
		/*
		 * Defer signals to protect the scheduling queues from
		 * access by the signal handler:
		 */
		_thread_kern_sig_defer();

		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);

		/*
		 * If the mutex was statically allocated, properly
		 * initialize the tail queue.
		 */
		if (((*mutex)->m_flags & MUTEX_FLAGS_INITED) == 0) {
			TAILQ_INIT(&(*mutex)->m_queue);
			(*mutex)->m_flags |= MUTEX_FLAGS_INITED;
			_MUTEX_INIT_LINK(*mutex);
		}

		/* Process according to mutex type: */
		switch ((*mutex)->m_protocol) {
		/* Default POSIX mutex: */
		case PTHREAD_PRIO_NONE:
			if ((*mutex)->m_owner == NULL) {
				/* Lock the mutex for this thread: */
				(*mutex)->m_owner = curthread;

				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&curthread->mutexq,
				    (*mutex), m_qe);

			} else if ((*mutex)->m_owner == curthread)
				ret = mutex_self_lock(*mutex);
			else {
				/*
				 * Join the queue of threads waiting to lock
				 * the mutex: 
				 */
				mutex_queue_enq(*mutex, curthread);

				/*
				 * Keep a pointer to the mutex this thread
				 * is waiting on:
				 */
				curthread->data.mutex = *mutex;

				/*
				 * Unlock the mutex structure and schedule the
				 * next thread:
				 */
				_thread_kern_sched_state_unlock(PS_MUTEX_WAIT,
				    &(*mutex)->lock, __FILE__, __LINE__);

				/* Lock the mutex structure again: */
				_SPINLOCK(&(*mutex)->lock);
			}
			break;

		/* POSIX priority inheritence mutex: */
		case PTHREAD_PRIO_INHERIT:
			/* Check if this mutex is not locked: */
			if ((*mutex)->m_owner == NULL) {
				/* Lock the mutex for this thread: */
				(*mutex)->m_owner = curthread;

				/* Track number of priority mutexes owned: */
				curthread->priority_mutex_count++;

				/*
				 * The mutex takes on attributes of the
				 * running thread when there are no waiters.
				 */
				(*mutex)->m_prio = curthread->active_priority;
				(*mutex)->m_saved_prio =
				    curthread->inherited_priority;
				curthread->inherited_priority =
				    (*mutex)->m_prio;

				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&curthread->mutexq,
				    (*mutex), m_qe);

			} else if ((*mutex)->m_owner == curthread)
				ret = mutex_self_lock(*mutex);
			else {
				/*
				 * Join the queue of threads waiting to lock
				 * the mutex: 
				 */
				mutex_queue_enq(*mutex, curthread);

				/*
				 * Keep a pointer to the mutex this thread
				 * is waiting on:
				 */
				curthread->data.mutex = *mutex;

				if (curthread->active_priority >
				    (*mutex)->m_prio)
					/* Adjust priorities: */
					mutex_priority_adjust(*mutex);

				/*
				 * Unlock the mutex structure and schedule the
				 * next thread:
				 */
				_thread_kern_sched_state_unlock(PS_MUTEX_WAIT,
				    &(*mutex)->lock, __FILE__, __LINE__);

				/* Lock the mutex structure again: */
				_SPINLOCK(&(*mutex)->lock);
			}
			break;

		/* POSIX priority protection mutex: */
		case PTHREAD_PRIO_PROTECT:
			/* Check for a priority ceiling violation: */
			if (curthread->active_priority > (*mutex)->m_prio)
				ret = EINVAL;

			/* Check if this mutex is not locked: */
			else if ((*mutex)->m_owner == NULL) {
				/*
				 * Lock the mutex for the running
				 * thread:
				 */
				(*mutex)->m_owner = curthread;

				/* Track number of priority mutexes owned: */
				curthread->priority_mutex_count++;

				/*
				 * The running thread inherits the ceiling
				 * priority of the mutex and executes at that
				 * priority:
				 */
				curthread->active_priority = (*mutex)->m_prio;
				(*mutex)->m_saved_prio =
				    curthread->inherited_priority;
				curthread->inherited_priority =
				    (*mutex)->m_prio;

				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&curthread->mutexq,
				    (*mutex), m_qe);
			} else if ((*mutex)->m_owner == curthread)
				ret = mutex_self_lock(*mutex);
			else {
				/*
				 * Join the queue of threads waiting to lock
				 * the mutex: 
				 */
				mutex_queue_enq(*mutex, curthread);

				/*
				 * Keep a pointer to the mutex this thread
				 * is waiting on:
				 */
				curthread->data.mutex = *mutex;

				/* Clear any previous error: */
				curthread->error = 0;

				/*
				 * Unlock the mutex structure and schedule the
				 * next thread:
				 */
				_thread_kern_sched_state_unlock(PS_MUTEX_WAIT,
				    &(*mutex)->lock, __FILE__, __LINE__);

				/* Lock the mutex structure again: */
				_SPINLOCK(&(*mutex)->lock);

				/*
				 * The threads priority may have changed while
				 * waiting for the mutex causing a ceiling
				 * violation.
				 */
				ret = curthread->error;
				curthread->error = 0;
			}
			break;

		/* Trap invalid mutex types: */
		default:
			/* Return an invalid argument error: */
			ret = EINVAL;
			break;
		}

		/*
		 * Check to see if this thread was interrupted and
		 * is still in the mutex queue of waiting threads:
		 */
		if (curthread->interrupted != 0)
			mutex_queue_remove(*mutex, curthread);

		/* Unlock the mutex structure: */
		_SPINUNLOCK(&(*mutex)->lock);

		/*
		 * Undefer and handle pending signals, yielding if
		 * necessary:
		 */
		_thread_kern_sig_undefer();
	} while (((*mutex)->m_owner != curthread) && (ret == 0) &&
	    (curthread->interrupted == 0));

	if (curthread->interrupted != 0 &&
	    curthread->continuation != NULL)
		curthread->continuation((void *) curthread);

	/* Return the completion status: */
	return (ret);
}

int
pthread_mutex_lock(pthread_mutex_t *mutex)
{
	int	ret = 0;

	if (_thread_initial == NULL)
		_thread_init();

	if (mutex == NULL)
		ret = EINVAL;

	/*
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization:
	 */
	else if ((*mutex != NULL) || ((ret = init_static(mutex)) == 0))
		ret = mutex_lock_common(mutex);

	return (ret);
}

int
pthread_mutex_unlock(pthread_mutex_t * mutex)
{
	return (mutex_unlock_common(mutex, /* add reference */ 0));
}

int
_mutex_cv_unlock(pthread_mutex_t * mutex)
{
	return (mutex_unlock_common(mutex, /* add reference */ 1));
}

int
_mutex_cv_lock(pthread_mutex_t * mutex)
{
	int	ret;
	if ((ret = pthread_mutex_lock(mutex)) == 0)
		(*mutex)->m_refcount--;
	return (ret);
}

static inline int
mutex_self_trylock(pthread_mutex_t mutex)
{
	int	ret = 0;

	switch (mutex->m_type) {

	/* case PTHREAD_MUTEX_DEFAULT: */
	case PTHREAD_MUTEX_ERRORCHECK:
	case PTHREAD_MUTEX_NORMAL:
		/*
		 * POSIX specifies that mutexes should return EDEADLK if a
		 * recursive lock is detected.
		 */
		ret = EBUSY; 
		break;

	case PTHREAD_MUTEX_RECURSIVE:
		/* Increment the lock count: */
		mutex->m_data.m_count++;
		break;

	default:
		/* Trap invalid mutex types; */
		ret = EINVAL;
	}

	return(ret);
}

static inline int
mutex_self_lock(pthread_mutex_t mutex)
{
	int ret = 0;

	switch (mutex->m_type) {
	/* case PTHREAD_MUTEX_DEFAULT: */
	case PTHREAD_MUTEX_ERRORCHECK:
		/*
		 * POSIX specifies that mutexes should return EDEADLK if a
		 * recursive lock is detected.
		 */
		ret = EDEADLK; 
		break;

	case PTHREAD_MUTEX_NORMAL:
		/*
		 * What SS2 define as a 'normal' mutex.  Intentionally
		 * deadlock on attempts to get a lock you already own.
		 */
		_thread_kern_sched_state_unlock(PS_DEADLOCK,
		    &mutex->lock, __FILE__, __LINE__);
		break;

	case PTHREAD_MUTEX_RECURSIVE:
		/* Increment the lock count: */
		mutex->m_data.m_count++;
		break;

	default:
		/* Trap invalid mutex types; */
		ret = EINVAL;
	}

	return(ret);
}

static inline int
mutex_unlock_common(pthread_mutex_t * mutex, int add_reference)
{
	struct pthread	*curthread = _get_curthread();
	int	ret = 0;

	if (mutex == NULL || *mutex == NULL) {
		ret = EINVAL;
	} else {
		/*
		 * Defer signals to protect the scheduling queues from
		 * access by the signal handler:
		 */
		_thread_kern_sig_defer();

		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);

		/* Process according to mutex type: */
		switch ((*mutex)->m_protocol) {
		/* Default POSIX mutex: */
		case PTHREAD_PRIO_NONE:
			/*
			 * Check if the running thread is not the owner of the
			 * mutex:
			 */
			if ((*mutex)->m_owner != curthread) {
				/*
				 * Return an invalid argument error for no
				 * owner and a permission error otherwise:
				 */
				ret = (*mutex)->m_owner == NULL ? EINVAL : EPERM;
			}
			else if (((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE) &&
			    ((*mutex)->m_data.m_count > 0)) {
				/* Decrement the count: */
				(*mutex)->m_data.m_count--;
			} else {
				/*
				 * Clear the count in case this is recursive
				 * mutex.
				 */
				(*mutex)->m_data.m_count = 0;

				/* Remove the mutex from the threads queue. */
				_MUTEX_ASSERT_IS_OWNED(*mutex);
				TAILQ_REMOVE(&(*mutex)->m_owner->mutexq,
				    (*mutex), m_qe);
				_MUTEX_INIT_LINK(*mutex);

				/*
				 * Get the next thread from the queue of
				 * threads waiting on the mutex: 
				 */
				if (((*mutex)->m_owner =
			  	    mutex_queue_deq(*mutex)) != NULL) {
					/*
					 * Unless the new owner of the mutex is
					 * currently suspended, allow the owner
					 * to run.  If the thread is suspended,
					 * make a note that the thread isn't in
					 * a wait queue any more.
					 */
					if (((*mutex)->m_owner->state !=
					    PS_SUSPENDED)) {
						PTHREAD_NEW_STATE((*mutex)->m_owner,
						    PS_RUNNING);
					} else {
						(*mutex)->m_owner->suspended =
						    SUSP_NOWAIT;
					}

					/*
					 * Add the mutex to the threads list of
					 * owned mutexes:
					 */
					TAILQ_INSERT_TAIL(&(*mutex)->m_owner->mutexq,
					    (*mutex), m_qe);

					/*
					 * The owner is no longer waiting for
					 * this mutex:
					 */
					(*mutex)->m_owner->data.mutex = NULL;
				}
			}
			break;

		/* POSIX priority inheritence mutex: */
		case PTHREAD_PRIO_INHERIT:
			/*
			 * Check if the running thread is not the owner of the
			 * mutex:
			 */
			if ((*mutex)->m_owner != curthread) {
				/*
				 * Return an invalid argument error for no
				 * owner and a permission error otherwise:
				 */
				ret = (*mutex)->m_owner == NULL ? EINVAL : EPERM;
			}
			else if (((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE) &&
			    ((*mutex)->m_data.m_count > 0)) {
				/* Decrement the count: */
				(*mutex)->m_data.m_count--;
			} else {
				/*
				 * Clear the count in case this is recursive
				 * mutex.
				 */
				(*mutex)->m_data.m_count = 0;

				/*
				 * Restore the threads inherited priority and
				 * recompute the active priority (being careful
				 * not to override changes in the threads base
				 * priority subsequent to locking the mutex).
				 */
				curthread->inherited_priority =
					(*mutex)->m_saved_prio;
				curthread->active_priority =
				    MAX(curthread->inherited_priority,
				    curthread->base_priority);

				/*
				 * This thread now owns one less priority mutex.
				 */
				curthread->priority_mutex_count--;

				/* Remove the mutex from the threads queue. */
				_MUTEX_ASSERT_IS_OWNED(*mutex);
				TAILQ_REMOVE(&(*mutex)->m_owner->mutexq,
				    (*mutex), m_qe);
				_MUTEX_INIT_LINK(*mutex);

				/*
				 * Get the next thread from the queue of threads
				 * waiting on the mutex: 
				 */
				if (((*mutex)->m_owner = 
				    mutex_queue_deq(*mutex)) == NULL)
					/* This mutex has no priority. */
					(*mutex)->m_prio = 0;
				else {
					/*
					 * Track number of priority mutexes owned:
					 */
					(*mutex)->m_owner->priority_mutex_count++;

					/*
					 * Add the mutex to the threads list
					 * of owned mutexes:
					 */
					TAILQ_INSERT_TAIL(&(*mutex)->m_owner->mutexq,
					    (*mutex), m_qe);

					/*
					 * The owner is no longer waiting for
					 * this mutex:
					 */
					(*mutex)->m_owner->data.mutex = NULL;

					/*
					 * Set the priority of the mutex.  Since
					 * our waiting threads are in descending
					 * priority order, the priority of the
					 * mutex becomes the active priority of
					 * the thread we just dequeued.
					 */
					(*mutex)->m_prio =
					    (*mutex)->m_owner->active_priority;

					/*
					 * Save the owning threads inherited
					 * priority:
					 */
					(*mutex)->m_saved_prio =
						(*mutex)->m_owner->inherited_priority;

					/*
					 * The owning threads inherited priority
					 * now becomes his active priority (the
					 * priority of the mutex).
					 */
					(*mutex)->m_owner->inherited_priority =
						(*mutex)->m_prio;

					/*
					 * Unless the new owner of the mutex is
					 * currently suspended, allow the owner
					 * to run.  If the thread is suspended,
					 * make a note that the thread isn't in
					 * a wait queue any more.
					 */
					if (((*mutex)->m_owner->state !=
					    PS_SUSPENDED)) {
						PTHREAD_NEW_STATE((*mutex)->m_owner,
						    PS_RUNNING);
					} else {
						(*mutex)->m_owner->suspended =
						    SUSP_NOWAIT;
					}
				}
			}
			break;

		/* POSIX priority ceiling mutex: */
		case PTHREAD_PRIO_PROTECT:
			/*
			 * Check if the running thread is not the owner of the
			 * mutex:
			 */
			if ((*mutex)->m_owner != curthread) {
				/*
				 * Return an invalid argument error for no
				 * owner and a permission error otherwise:
				 */
				ret = (*mutex)->m_owner == NULL ? EINVAL : EPERM;
			}
			else if (((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE) &&
			    ((*mutex)->m_data.m_count > 0)) {
				/* Decrement the count: */
				(*mutex)->m_data.m_count--;
			} else {
				/*
				 * Clear the count in case this is recursive
				 * mutex.
				 */
				(*mutex)->m_data.m_count = 0;

				/*
				 * Restore the threads inherited priority and
				 * recompute the active priority (being careful
				 * not to override changes in the threads base
				 * priority subsequent to locking the mutex).
				 */
				curthread->inherited_priority =
					(*mutex)->m_saved_prio;
				curthread->active_priority =
				    MAX(curthread->inherited_priority,
				    curthread->base_priority);

				/*
				 * This thread now owns one less priority mutex.
				 */
				curthread->priority_mutex_count--;

				/* Remove the mutex from the threads queue. */
				_MUTEX_ASSERT_IS_OWNED(*mutex);
				TAILQ_REMOVE(&(*mutex)->m_owner->mutexq,
				    (*mutex), m_qe);
				_MUTEX_INIT_LINK(*mutex);

				/*
				 * Enter a loop to find a waiting thread whose
				 * active priority will not cause a ceiling
				 * violation:
				 */
				while ((((*mutex)->m_owner =
				    mutex_queue_deq(*mutex)) != NULL) &&
				    ((*mutex)->m_owner->active_priority >
				     (*mutex)->m_prio)) {
					/*
					 * Either the mutex ceiling priority
					 * been lowered and/or this threads
					 * priority has been raised subsequent
					 * to this thread being queued on the
					 * waiting list.
					 */
					(*mutex)->m_owner->error = EINVAL;
					PTHREAD_NEW_STATE((*mutex)->m_owner,
					    PS_RUNNING);
					/*
					 * The thread is no longer waiting for
					 * this mutex:
					 */
					(*mutex)->m_owner->data.mutex = NULL;
				}

				/* Check for a new owner: */
				if ((*mutex)->m_owner != NULL) {
					/*
					 * Track number of priority mutexes owned:
					 */
					(*mutex)->m_owner->priority_mutex_count++;

					/*
					 * Add the mutex to the threads list
					 * of owned mutexes:
					 */
					TAILQ_INSERT_TAIL(&(*mutex)->m_owner->mutexq,
					    (*mutex), m_qe);

					/*
					 * The owner is no longer waiting for
					 * this mutex:
					 */
					(*mutex)->m_owner->data.mutex = NULL;

					/*
					 * Save the owning threads inherited
					 * priority:
					 */
					(*mutex)->m_saved_prio =
						(*mutex)->m_owner->inherited_priority;

					/*
					 * The owning thread inherits the
					 * ceiling priority of the mutex and
					 * executes at that priority:
					 */
					(*mutex)->m_owner->inherited_priority =
					    (*mutex)->m_prio;
					(*mutex)->m_owner->active_priority =
					    (*mutex)->m_prio;

					/*
					 * Unless the new owner of the mutex is
					 * currently suspended, allow the owner
					 * to run.  If the thread is suspended,
					 * make a note that the thread isn't in
					 * a wait queue any more.
					 */
					if (((*mutex)->m_owner->state !=
					    PS_SUSPENDED)) {
						PTHREAD_NEW_STATE((*mutex)->m_owner,
						    PS_RUNNING);
					} else {
						(*mutex)->m_owner->suspended =
						    SUSP_NOWAIT;
					}
				}
			}
			break;

		/* Trap invalid mutex types: */
		default:
			/* Return an invalid argument error: */
			ret = EINVAL;
			break;
		}

		if ((ret == 0) && (add_reference != 0)) {
			/* Increment the reference count: */
			(*mutex)->m_refcount++;
		}

		/* Unlock the mutex structure: */
		_SPINUNLOCK(&(*mutex)->lock);

		/*
		 * Undefer and handle pending signals, yielding if
		 * necessary:
		 */
		_thread_kern_sig_undefer();
	}

	/* Return the completion status: */
	return (ret);
}


/*
 * This function is called when a change in base priority occurs for
 * a thread that is holding or waiting for a priority protection or
 * inheritence mutex.  A change in a threads base priority can effect
 * changes to active priorities of other threads and to the ordering
 * of mutex locking by waiting threads.
 *
 * This must be called while thread scheduling is deferred.
 */
void
_mutex_notify_priochange(pthread_t pthread)
{
	/* Adjust the priorites of any owned priority mutexes: */
	if (pthread->priority_mutex_count > 0) {
		/*
		 * Rescan the mutexes owned by this thread and correct
		 * their priorities to account for this threads change
		 * in priority.  This has the side effect of changing
		 * the threads active priority.
		 */
		mutex_rescan_owned(pthread, /* rescan all owned */ NULL);
	}

	/*
	 * If this thread is waiting on a priority inheritence mutex,
	 * check for priority adjustments.  A change in priority can
	 * also effect a ceiling violation(*) for a thread waiting on
	 * a priority protection mutex; we don't perform the check here
	 * as it is done in pthread_mutex_unlock.
	 *
	 * (*) It should be noted that a priority change to a thread
	 *     _after_ taking and owning a priority ceiling mutex
	 *     does not affect ownership of that mutex; the ceiling
	 *     priority is only checked before mutex ownership occurs.
	 */
	if (pthread->state == PS_MUTEX_WAIT) {
		/* Lock the mutex structure: */
		_SPINLOCK(&pthread->data.mutex->lock);

		/*
		 * Check to make sure this thread is still in the same state
		 * (the spinlock above can yield the CPU to another thread):
		 */
		if (pthread->state == PS_MUTEX_WAIT) {
			/*
			 * Remove and reinsert this thread into the list of
			 * waiting threads to preserve decreasing priority
			 * order.
			 */
			mutex_queue_remove(pthread->data.mutex, pthread);
			mutex_queue_enq(pthread->data.mutex, pthread);

			if (pthread->data.mutex->m_protocol ==
			     PTHREAD_PRIO_INHERIT) {
				/* Adjust priorities: */
				mutex_priority_adjust(pthread->data.mutex);
			}
		}

		/* Unlock the mutex structure: */
		_SPINUNLOCK(&pthread->data.mutex->lock);
	}
}

/*
 * Called when a new thread is added to the mutex waiting queue or
 * when a threads priority changes that is already in the mutex
 * waiting queue.
 */
static void
mutex_priority_adjust(pthread_mutex_t mutex)
{
	pthread_t	pthread_next, pthread = mutex->m_owner;
	int		temp_prio;
	pthread_mutex_t	m = mutex;

	/*
	 * Calculate the mutex priority as the maximum of the highest
	 * active priority of any waiting threads and the owning threads
	 * active priority(*).
	 *
	 * (*) Because the owning threads current active priority may
	 *     reflect priority inherited from this mutex (and the mutex
	 *     priority may have changed) we must recalculate the active
	 *     priority based on the threads saved inherited priority
	 *     and its base priority.
	 */
	pthread_next = TAILQ_FIRST(&m->m_queue);  /* should never be NULL */
	temp_prio = MAX(pthread_next->active_priority,
	    MAX(m->m_saved_prio, pthread->base_priority));

	/* See if this mutex really needs adjusting: */
	if (temp_prio == m->m_prio)
		/* No need to propagate the priority: */
		return;

	/* Set new priority of the mutex: */
	m->m_prio = temp_prio;

	while (m != NULL) {
		/*
		 * Save the threads priority before rescanning the
		 * owned mutexes:
		 */
		temp_prio = pthread->active_priority;

		/*
		 * Fix the priorities for all the mutexes this thread has
		 * locked since taking this mutex.  This also has a
		 * potential side-effect of changing the threads priority.
		 */
		mutex_rescan_owned(pthread, m);

		/*
		 * If the thread is currently waiting on a mutex, check
		 * to see if the threads new priority has affected the
		 * priority of the mutex.
		 */
		if ((temp_prio != pthread->active_priority) &&
		    (pthread->state == PS_MUTEX_WAIT) &&
		    (pthread->data.mutex->m_protocol == PTHREAD_PRIO_INHERIT)) {
			/* Grab the mutex this thread is waiting on: */
			m = pthread->data.mutex;

			/*
			 * The priority for this thread has changed.  Remove
			 * and reinsert this thread into the list of waiting
			 * threads to preserve decreasing priority order.
			 */
			mutex_queue_remove(m, pthread);
			mutex_queue_enq(m, pthread);

			/* Grab the waiting thread with highest priority: */
			pthread_next = TAILQ_FIRST(&m->m_queue);

			/*
			 * Calculate the mutex priority as the maximum of the
			 * highest active priority of any waiting threads and
			 * the owning threads active priority.
			 */
			temp_prio = MAX(pthread_next->active_priority,
			    MAX(m->m_saved_prio, m->m_owner->base_priority));

			if (temp_prio != m->m_prio) {
				/*
				 * The priority needs to be propagated to the
				 * mutex this thread is waiting on and up to
				 * the owner of that mutex.
				 */
				m->m_prio = temp_prio;
				pthread = m->m_owner;
			}
			else
				/* We're done: */
				m = NULL;

		}
		else
			/* We're done: */
			m = NULL;
	}
}

static void
mutex_rescan_owned(pthread_t pthread, pthread_mutex_t mutex)
{
	int		active_prio, inherited_prio;
	pthread_mutex_t	m;
	pthread_t	pthread_next;

	/*
	 * Start walking the mutexes the thread has taken since
	 * taking this mutex.
	 */
	if (mutex == NULL) {
		/*
		 * A null mutex means start at the beginning of the owned
		 * mutex list.
		 */
		m = TAILQ_FIRST(&pthread->mutexq);

		/* There is no inherited priority yet. */
		inherited_prio = 0;
	}
	else {
		/*
		 * The caller wants to start after a specific mutex.  It
		 * is assumed that this mutex is a priority inheritence
		 * mutex and that its priority has been correctly
		 * calculated.
		 */
		m = TAILQ_NEXT(mutex, m_qe);

		/* Start inheriting priority from the specified mutex. */
		inherited_prio = mutex->m_prio;
	}
	active_prio = MAX(inherited_prio, pthread->base_priority);

	while (m != NULL) {
		/*
		 * We only want to deal with priority inheritence
		 * mutexes.  This might be optimized by only placing
		 * priority inheritence mutexes into the owned mutex
		 * list, but it may prove to be useful having all
		 * owned mutexes in this list.  Consider a thread
		 * exiting while holding mutexes...
		 */
		if (m->m_protocol == PTHREAD_PRIO_INHERIT) {
			/*
			 * Fix the owners saved (inherited) priority to
			 * reflect the priority of the previous mutex.
			 */
			m->m_saved_prio = inherited_prio;

			if ((pthread_next = TAILQ_FIRST(&m->m_queue)) != NULL)
				/* Recalculate the priority of the mutex: */
				m->m_prio = MAX(active_prio,
				     pthread_next->active_priority);
			else
				m->m_prio = active_prio;

			/* Recalculate new inherited and active priorities: */
			inherited_prio = m->m_prio;
			active_prio = MAX(m->m_prio, pthread->base_priority);
		}

		/* Advance to the next mutex owned by this thread: */
		m = TAILQ_NEXT(m, m_qe);
	}

	/*
	 * Fix the threads inherited priority and recalculate its
	 * active priority.
	 */
	pthread->inherited_priority = inherited_prio;
	active_prio = MAX(inherited_prio, pthread->base_priority);

	if (active_prio != pthread->active_priority) {
		/*
		 * If this thread is in the priority queue, it must be
		 * removed and reinserted for its new priority.
	 	 */
		if (pthread->flags & PTHREAD_FLAGS_IN_PRIOQ) {
			/*
			 * Remove the thread from the priority queue
			 * before changing its priority:
			 */
			PTHREAD_PRIOQ_REMOVE(pthread);

			/*
			 * POSIX states that if the priority is being
			 * lowered, the thread must be inserted at the
			 * head of the queue for its priority if it owns
			 * any priority protection or inheritence mutexes.
			 */
			if ((active_prio < pthread->active_priority) &&
			    (pthread->priority_mutex_count > 0)) {
				/* Set the new active priority. */
				pthread->active_priority = active_prio;

				PTHREAD_PRIOQ_INSERT_HEAD(pthread);
			}
			else {
				/* Set the new active priority. */
				pthread->active_priority = active_prio;

				PTHREAD_PRIOQ_INSERT_TAIL(pthread);
			}
		}
		else {
			/* Set the new active priority. */
			pthread->active_priority = active_prio;
		}
	}
}

void
_mutex_unlock_private(pthread_t pthread)
{
	struct pthread_mutex volatile	*m, *m_next;

	for (m = TAILQ_FIRST(&pthread->mutexq); m != NULL; m = m_next) {
		m_next = TAILQ_NEXT(m, m_qe);
		if ((m->m_flags & MUTEX_FLAGS_PRIVATE) != 0)
			pthread_mutex_unlock(&m);
	}
}

void
_mutex_lock_backout(pthread_t pthread)
{
	struct pthread_mutex volatile	*mutex;

	/*
	 * Defer signals to protect the scheduling queues from
	 * access by the signal handler:
	 */
	_thread_kern_sig_defer();
	if ((pthread->flags & PTHREAD_FLAGS_IN_MUTEXQ) != 0) {
		mutex = pthread->data.mutex;

		/* Lock the mutex structure: */
		_SPINLOCK(&mutex->lock);

		mutex_queue_remove(mutex, pthread);

		/* This thread is no longer waiting for the mutex: */
		pthread->data.mutex = NULL;

		/* Unlock the mutex structure: */
		_SPINUNLOCK(&mutex->lock);

	}
	/*
	 * Undefer and handle pending signals, yielding if
	 * necessary:
	 */
	_thread_kern_sig_undefer();
}

/*
 * Dequeue a waiting thread from the head of a mutex queue in descending
 * priority order.
 */
static inline pthread_t
mutex_queue_deq(pthread_mutex_t mutex)
{
	pthread_t pthread;

	while ((pthread = TAILQ_FIRST(&mutex->m_queue)) != NULL) {
		TAILQ_REMOVE(&mutex->m_queue, pthread, sqe);
		pthread->flags &= ~PTHREAD_FLAGS_IN_MUTEXQ;

		/*
		 * Only exit the loop if the thread hasn't been
		 * cancelled.
		 */
		if (pthread->interrupted == 0)
			break;
	}

	return(pthread);
}

/*
 * Remove a waiting thread from a mutex queue in descending priority order.
 */
static inline void
mutex_queue_remove(pthread_mutex_t mutex, pthread_t pthread)
{
	if ((pthread->flags & PTHREAD_FLAGS_IN_MUTEXQ) != 0) {
		TAILQ_REMOVE(&mutex->m_queue, pthread, sqe);
		pthread->flags &= ~PTHREAD_FLAGS_IN_MUTEXQ;
	}
}

/*
 * Enqueue a waiting thread to a queue in descending priority order.
 */
static inline void
mutex_queue_enq(pthread_mutex_t mutex, pthread_t pthread)
{
	pthread_t tid = TAILQ_LAST(&mutex->m_queue, mutex_head);

	PTHREAD_ASSERT_NOT_IN_SYNCQ(pthread);
	/*
	 * For the common case of all threads having equal priority,
	 * we perform a quick check against the priority of the thread
	 * at the tail of the queue.
	 */
	if ((tid == NULL) || (pthread->active_priority <= tid->active_priority))
		TAILQ_INSERT_TAIL(&mutex->m_queue, pthread, sqe);
	else {
		tid = TAILQ_FIRST(&mutex->m_queue);
		while (pthread->active_priority <= tid->active_priority)
			tid = TAILQ_NEXT(tid, sqe);
		TAILQ_INSERT_BEFORE(tid, pthread, sqe);
	}
	pthread->flags |= PTHREAD_FLAGS_IN_MUTEXQ;
}

#endif
@


1.15
log
@signal handling changes.   This corrects several signal
handling errors in the threads library.   Most of the libc_r regression
tests are now ok.   thread specific kill semantics are still not correct.
No negative comments after posting to tech@@ a week or so ago.
siginfo test fails on sparc64 due to sparc64 oddity.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.14 2002/01/23 23:11:57 fgsch Exp $	*/
@


1.14
log
@From FreeBSD:
o Use _get_curthread() instead of _thread_run.
o Correctly deal with cancellations.
o Add libc internal versions of pthread_mutex_lock() and
  pthread_mutex_trylock(), unused by now.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.13 2001/11/12 21:13:34 fgsch Exp $	*/
a711 21
		ret = mutex_lock_common(mutex);

	return (ret);
}

int
_pthread_mutex_lock(pthread_mutex_t *mutex)
{
	int	ret = 0;

	if (_thread_initial == NULL)
		_thread_init();

	if (mutex == NULL)
		ret = EINVAL;

	/*
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization marking it private (delete safe):
	 */
	else if ((*mutex != NULL) || ((ret = init_static_private(mutex)) == 0))
@


1.13
log
@a better fix for recursive mutex.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.12 2001/11/12 02:24:30 marc Exp $	*/
d79 4
d87 1
a87 1
	int ret = 0;
d102 2
a103 1
		(*mutex)->m_flags = MUTEX_FLAGS_INITED;
d120 1
d122 1
a122 1
	int             ret = 0;
d133 1
d144 1
a144 1
		 ((*mutex_attr)->m_protocol > PTHREAD_PRIO_PROTECT))
d153 1
d162 2
a163 2
			/* Reset the mutex flags: */
			pmutex->m_flags = 0;
d214 1
a214 1
	int ret = 0;
d254 1
a254 1
init_static (pthread_mutex_t *mutex)
d256 1
a256 1
	int ret;
d270 19
a288 2
int
pthread_mutex_trylock(pthread_mutex_t * mutex)
d290 2
a291 1
	int             ret = 0;
d293 2
a294 2
	if (mutex == NULL)
		ret = EINVAL;
d297 2
a298 2
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization:
d300 1
a300 6
	else if (*mutex != NULL || (ret = init_static(mutex)) == 0) {
		/*
		 * Defer signals to protect the scheduling queues from
		 * access by the signal handler:
		 */
		_thread_kern_sig_defer();
d302 2
a303 2
		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);
d305 9
a313 9
		/*
		 * If the mutex was statically allocated, properly
		 * initialize the tail queue.
		 */
		if (((*mutex)->m_flags & MUTEX_FLAGS_INITED) == 0) {
			TAILQ_INIT(&(*mutex)->m_queue);
			_MUTEX_INIT_LINK(*mutex);
			(*mutex)->m_flags |= MUTEX_FLAGS_INITED;
		}
d315 8
a322 8
		/* Process according to mutex type: */
		switch ((*mutex)->m_protocol) {
		/* Default POSIX mutex: */
		case PTHREAD_PRIO_NONE:	
			/* Check if this mutex is not locked: */
			if ((*mutex)->m_owner == NULL) {
				/* Lock the mutex for the running thread: */
				(*mutex)->m_owner = _thread_run;
d324 10
a333 4
				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
				    (*mutex), m_qe);
d335 6
a340 6
			} else if ((*mutex)->m_owner == _thread_run)
				ret = mutex_self_trylock(*mutex);
			else
				/* Return a busy error: */
				ret = EBUSY;
			break;
d342 2
a343 6
		/* POSIX priority inheritence mutex: */
		case PTHREAD_PRIO_INHERIT:
			/* Check if this mutex is not locked: */
			if ((*mutex)->m_owner == NULL) {
				/* Lock the mutex for the running thread: */
				(*mutex)->m_owner = _thread_run;
d345 7
a351 2
				/* Track number of priority mutexes owned: */
				_thread_run->priority_mutex_count++;
d353 10
a362 7
				/*
				 * The mutex takes on the attributes of the
				 * running thread when there are no waiters.
				 */
				(*mutex)->m_prio = _thread_run->active_priority;
				(*mutex)->m_saved_prio =
				    _thread_run->inherited_priority;
d364 5
a368 10
				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
				    (*mutex), m_qe);
			} else if ((*mutex)->m_owner == _thread_run)
				ret = mutex_self_trylock(*mutex);
			else
				/* Return a busy error: */
				ret = EBUSY;
			break;
d370 4
a373 5
		/* POSIX priority protection mutex: */
		case PTHREAD_PRIO_PROTECT:
			/* Check for a priority ceiling violation: */
			if (_thread_run->active_priority > (*mutex)->m_prio)
				ret = EINVAL;
d375 2
a376 4
			/* Check if this mutex is not locked: */
			else if ((*mutex)->m_owner == NULL) {
				/* Lock the mutex for the running thread: */
				(*mutex)->m_owner = _thread_run;
d378 10
a387 2
				/* Track number of priority mutexes owned: */
				_thread_run->priority_mutex_count++;
d389 10
a398 10
				/*
				 * The running thread inherits the ceiling
				 * priority of the mutex and executes at that
				 * priority.
				 */
				_thread_run->active_priority = (*mutex)->m_prio;
				(*mutex)->m_saved_prio =
				    _thread_run->inherited_priority;
				_thread_run->inherited_priority =
				    (*mutex)->m_prio;
d400 6
a405 17
				/* Add to the list of owned mutexes: */
				_MUTEX_ASSERT_NOT_OWNED(*mutex);
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
				    (*mutex), m_qe);
			} else if ((*mutex)->m_owner == _thread_run)
				ret = mutex_self_trylock(*mutex);
			else
				/* Return a busy error: */
				ret = EBUSY;
			break;

		/* Trap invalid mutex types: */
		default:
			/* Return an invalid argument error: */
			ret = EINVAL;
			break;
		}
d407 2
a408 2
		/* Unlock the mutex structure: */
		_SPINUNLOCK(&(*mutex)->lock);
d410 5
a414 6
		/*
		 * Undefer and handle pending signals, yielding if
		 * necessary:
		 */
		_thread_kern_sig_undefer();
	}
d421 1
a421 1
pthread_mutex_lock(pthread_mutex_t * mutex)
d423 1
a423 1
	int             ret = 0;
d432 45
a476 1
	else if (*mutex != NULL || (ret = init_static(mutex)) == 0) {
d502 1
a502 1
				(*mutex)->m_owner = _thread_run;
d506 1
a506 1
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
d509 1
a509 1
			} else if ((*mutex)->m_owner == _thread_run)
d516 1
a516 1
				mutex_queue_enq(*mutex, _thread_run);
d522 1
a522 1
				_thread_run->data.mutex = *mutex;
d541 1
a541 1
				(*mutex)->m_owner = _thread_run;
d544 1
a544 1
				_thread_run->priority_mutex_count++;
d550 1
a550 1
				(*mutex)->m_prio = _thread_run->active_priority;
d552 2
a553 2
				    _thread_run->inherited_priority;
				_thread_run->inherited_priority =
d558 1
a558 1
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
d561 1
a561 1
			} else if ((*mutex)->m_owner == _thread_run)
d568 1
a568 1
				mutex_queue_enq(*mutex, _thread_run);
d574 1
a574 1
				_thread_run->data.mutex = *mutex;
d576 1
a576 1
				if (_thread_run->active_priority >
d596 1
a596 1
			if (_thread_run->active_priority > (*mutex)->m_prio)
d605 1
a605 1
				(*mutex)->m_owner = _thread_run;
d608 1
a608 1
				_thread_run->priority_mutex_count++;
d615 1
a615 1
				_thread_run->active_priority = (*mutex)->m_prio;
d617 2
a618 2
				    _thread_run->inherited_priority;
				_thread_run->inherited_priority =
d623 1
a623 1
				TAILQ_INSERT_TAIL(&_thread_run->mutexq,
d625 1
a625 1
			} else if ((*mutex)->m_owner == _thread_run)
d632 1
a632 1
				mutex_queue_enq(*mutex, _thread_run);
d638 1
a638 1
				_thread_run->data.mutex = *mutex;
d641 1
a641 1
				_thread_run->error = 0;
d658 2
a659 2
				ret = _thread_run->error;
				_thread_run->error = 0;
d670 7
d685 6
a690 1
	}
d697 42
d753 1
a753 1
	int ret;
d762 1
a762 1
	int ret = 0;
d829 2
a830 1
	int ret = 0;
d852 1
a852 1
			if ((*mutex)->m_owner != _thread_run) {
d854 2
a855 2
				 * Return a permission error when the
				 * thread doesn't own the lock.
d857 1
a857 1
				ret = EPERM;
d883 5
a887 2
					 * Allow the new owner of the mutex to
					 * run:
d889 8
a896 2
					PTHREAD_NEW_STATE((*mutex)->m_owner,
					    PS_RUNNING);
d920 1
a920 1
			if ((*mutex)->m_owner != _thread_run) {
d922 2
a923 2
				 * Return a permission error when the
				 * thread doesn't own the lock.
d925 1
a925 1
				ret = EPERM;
d944 1
a944 1
				_thread_run->inherited_priority =
d946 3
a948 3
				_thread_run->active_priority =
				    MAX(_thread_run->inherited_priority,
				    _thread_run->base_priority);
d953 1
a953 1
				_thread_run->priority_mutex_count--;
d1014 5
a1018 2
					 * Allow the new owner of the mutex to
					 * run:
d1020 8
a1027 2
					PTHREAD_NEW_STATE((*mutex)->m_owner,
					    PS_RUNNING);
d1038 1
a1038 1
			if ((*mutex)->m_owner != _thread_run) {
d1040 2
a1041 2
				 * Return a permission error when the
				 * thread doesn't own the lock.
d1043 1
a1043 1
				ret = EPERM;
d1062 1
a1062 1
				_thread_run->inherited_priority =
d1064 3
a1066 3
				_thread_run->active_priority =
				    MAX(_thread_run->inherited_priority,
				    _thread_run->base_priority);
d1071 1
a1071 1
				_thread_run->priority_mutex_count--;
d1143 5
a1147 2
					 * Allow the new owner of the mutex to
					 * run:
d1149 8
a1156 2
					PTHREAD_NEW_STATE((*mutex)->m_owner,
					    PS_RUNNING);
d1352 1
a1352 1
mutex_rescan_owned (pthread_t pthread, pthread_mutex_t mutex)
d1464 44
d1517 11
a1527 2
	if ((pthread = TAILQ_FIRST(&mutex->m_queue)) != NULL)
		TAILQ_REMOVE(&mutex->m_queue, pthread, qe);
d1538 4
a1541 1
	TAILQ_REMOVE(&mutex->m_queue, pthread, qe);
d1552 1
d1559 1
a1559 1
		TAILQ_INSERT_TAIL(&mutex->m_queue, pthread, qe);
d1563 2
a1564 2
			tid = TAILQ_NEXT(tid, qe);
		TAILQ_INSERT_BEFORE(tid, pthread, qe);
d1566 1
@


1.12
log
@This fixes the recursive mutex problem with pthreads.
fgs@@ says their is a better fix... if so he can back these changes out
and apply his fix at his convenience.  In the meanwhile we'll have
mutexen that work.
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.11 2000/01/06 07:19:35 d Exp $	*/
a440 4
				/* if recursive, increment the lock count */
				if ((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE)
					(*mutex)->m_data.m_count++;

a479 4
				/* if recursive, increment the lock count */
				if ((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE)
					(*mutex)->m_data.m_count++;

a543 4
				/* if recursive, increment the lock count */
				if ((*mutex)->m_type == PTHREAD_MUTEX_RECURSIVE)
					(*mutex)->m_data.m_count++;

d742 1
a742 1
			    ((*mutex)->m_data.m_count > 1)) {
d801 1
a801 1
			    ((*mutex)->m_data.m_count > 1)) {
d910 1
a910 1
			    ((*mutex)->m_data.m_count > 1)) {
@


1.11
log
@oops
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.10 2000/01/06 07:18:46 d Exp $	*/
d137 1
a137 1
	    ((*mutex_attr)->m_protocol > PTHREAD_MUTEX_RECURSIVE))
d441 4
d484 4
d551 4
@


1.10
log
@SPINLOCK_INIT, volatile
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.9 1999/11/25 07:01:38 d Exp $	*/
d1357 1
a1357 2
		TAILQ_INSERT_TAIL(&mutex->m_queue, pthread, qe); 
		/* (pthread)->qe.tqe_prev = ((&mutex->m_queue))->tqh_last; */
@


1.9
log
@sync with FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d191 1
a191 1
				memset(&pmutex->lock, 0, sizeof(pmutex->lock));
d194 1
a194 1
				free(pmutex);
d231 1
a231 1
			free(*mutex);
d307 1
d1357 2
a1358 1
		TAILQ_INSERT_TAIL(&mutex->m_queue, pthread, qe);
@


1.8
log
@sync with freebsd
@
text
@d1 1
a1 1
/*	$OpenBSD: uthread_mutex.c,v 1.7 1999/05/26 00:18:25 d Exp $	*/
d33 1
d44 19
d79 28
d190 2
a191 1
				_SPINUNLOCK(&pmutex->lock);
d200 1
a200 1
	return (ret);
d230 1
d276 9
d290 1
a293 14
		/*
		 * Guard against being preempted by a scheduling signal.
		 * To support priority inheritence mutexes, we need to
		 * maintain lists of mutex ownerships for each thread as
		 * well as lists of waiting threads for each mutex.  In
		 * order to propagate priorities we need to atomically
		 * walk these lists and cannot rely on a single mutex
		 * lock to provide protection against modification.
		 */
		_thread_kern_sched_defer();

		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);

d304 1
d333 1
d369 1
d390 2
a391 2
		 * Renable preemption and yield if a scheduling signal
		 * arrived while in the critical region:
d393 1
a393 1
		_thread_kern_sched_undefer();
d414 9
d429 1
a431 14
		/*
		 * Guard against being preempted by a scheduling signal.
		 * To support priority inheritence mutexes, we need to
		 * maintain lists of mutex ownerships for each thread as
		 * well as lists of waiting threads for each mutex.  In
		 * order to propagate priorities we need to atomically
		 * walk these lists and cannot rely on a single mutex
		 * lock to provide protection against modification.
		 */
		_thread_kern_sched_defer();

		/* Lock the mutex structure: */
		_SPINLOCK(&(*mutex)->lock);

d441 1
a468 6

				/*
				 * This thread is no longer waiting for
				 * the mutex:
				 */
				_thread_run->data.mutex = NULL;
d493 1
a525 6

				/*
				 * This thread is no longer waiting for
				 * the mutex:
				 */
				_thread_run->data.mutex = NULL;
d558 1
a595 6

				/*
				 * This thread is no longer waiting for
				 * the mutex:
				 */
				_thread_run->data.mutex = NULL;
d610 2
a611 2
		 * Renable preemption and yield if a scheduling signal
		 * arrived while in the critical region:
d613 1
a613 1
		_thread_kern_sched_undefer();
d717 2
a718 7
		 * Guard against being preempted by a scheduling signal.
		 * To support priority inheritence mutexes, we need to
		 * maintain lists of mutex ownerships for each thread as
		 * well as lists of waiting threads for each mutex.  In
		 * order to propagate priorities we need to atomically
		 * walk these lists and cannot rely on a single mutex
		 * lock to provide protection against modification.
d720 1
a720 1
		_thread_kern_sched_defer();
d735 2
a736 2
				 * Return a permission error when the thread
				 * doesn't own the lock:
d752 1
d755 1
d769 13
d794 2
a795 2
				 * Return a permission error when the thread
				 * doesn't own the lock:
d828 1
d831 1
d903 2
a904 2
				 * Return a permission error when the thread
				 * doesn't own the lock:
d937 1
d939 2
a940 1
					    (*mutex), m_qe);
d961 5
d1031 2
a1032 2
		 * Renable preemption and yield if a scheduling signal
		 * arrived while in the critical region:
d1034 1
a1034 1
		_thread_kern_sched_undefer();
d1043 5
a1047 5
 * This function is called when a change in base priority occurs
 * for a thread that is thread holding, or waiting for, a priority
 * protection or inheritence mutex.  A change in a threads base
 * priority can effect changes to active priorities of other threads
 * and to the ordering of mutex locking by waiting threads.
d1284 1
a1284 2
		if ((pthread != _thread_run) &&
		    (pthread->state == PS_RUNNING)) {
@


1.7
log
@sync with FreeBSD
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d226 9
d364 9
@


1.6
log
@ident
@
text
@d1 1
d24 1
a24 1
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
a32 2
 * $OpenBSD$
 *
d37 2
d43 14
d59 1
d64 3
a66 1
	enum pthread_mutextype type;
d70 21
a90 1
	if (mutex == NULL) {
a91 5
	} else {
		/* Check if default mutex attributes: */
		if (mutex_attr == NULL || *mutex_attr == NULL)
			/* Default to a fast mutex: */
			type = PTHREAD_MUTEX_DEFAULT;
d93 23
a115 6
		else if ((*mutex_attr)->m_type >= MUTEX_TYPE_MAX)
			/* Return an invalid argument error: */
			ret = EINVAL;
		else
			/* Use the requested mutex type: */
			type = (*mutex_attr)->m_type;
d117 5
a121 8
		/* Check no errors so far: */
		if (ret == 0) {
			if ((pmutex = (pthread_mutex_t)
			    malloc(sizeof(struct pthread_mutex))) == NULL)
				ret = ENOMEM;
			else {
				/* Reset the mutex flags: */
				pmutex->m_flags = 0;
d123 24
a146 33
				/* Process according to mutex type: */
				switch (type) {
				/* Fast mutex: */
				case PTHREAD_MUTEX_DEFAULT:
				case PTHREAD_MUTEX_NORMAL:
				case PTHREAD_MUTEX_ERRORCHECK:
					/* Nothing to do here. */
					break;

				/* Counting mutex: */
				case PTHREAD_MUTEX_RECURSIVE:
					/* Reset the mutex count: */
					pmutex->m_data.m_count = 0;
					break;

				/* Trap invalid mutex types: */
				default:
					/* Return an invalid argument error: */
					ret = EINVAL;
					break;
				}
				if (ret == 0) {
					/* Initialise the rest of the mutex: */
					_thread_queue_init(&pmutex->m_queue);
					pmutex->m_flags |= MUTEX_FLAGS_INITED;
					pmutex->m_owner = NULL;
					pmutex->m_type = type;
					_SPINUNLOCK(&pmutex->lock);
					*mutex = pmutex;
				} else {
					free(pmutex);
					*mutex = NULL;
				}
d166 1
a166 2
		 * Free the memory allocated for the mutex
		 * structure:
d168 14
a181 1
		free(*mutex);
d183 6
a188 5
		/*
		 * Leave the caller's pointer NULL now that
		 * the mutex has been destroyed:
		 */
		*mutex = NULL;
d225 11
d240 3
a242 5
		switch ((*mutex)->m_type) {
		/* Fast mutex: */
		case PTHREAD_MUTEX_NORMAL:
		case PTHREAD_MUTEX_DEFAULT:
		case PTHREAD_MUTEX_ERRORCHECK:
d247 35
a281 1
			} else {
a283 1
			}
d286 8
a293 16
		/* Counting mutex: */
		case PTHREAD_MUTEX_RECURSIVE:
			/* Check if this mutex is locked: */
			if ((*mutex)->m_owner != NULL) {
				/*
				 * Check if the mutex is locked by the running
				 * thread: 
				 */
				if ((*mutex)->m_owner == _thread_run) {
					/* Increment the lock count: */
					(*mutex)->m_data.m_count++;
				} else {
					/* Return a busy error: */
					ret = EBUSY;
				}
			} else {
d296 23
a318 1
			}
d330 6
d355 11
d370 41
a410 17
		switch ((*mutex)->m_type) {
		/* What SS2 define as a 'normal' mutex.  This has to deadlock
		   on attempts to get a lock you already own. */
		case PTHREAD_MUTEX_NORMAL:
			if ((*mutex)->m_owner == _thread_run) {
				/* Intentionally deadlock: */
				_thread_run->data.mutex = mutex;
				for (;;)
					_thread_kern_sched_state(PS_MUTEX_WAIT, __FILE__, __LINE__);
			}
			goto COMMON_LOCK;
			
		 /* Return error (not OK) on attempting to re-lock */
		case PTHREAD_MUTEX_ERRORCHECK:
			if ((*mutex)->m_owner == _thread_run) {
				ret = EDEADLK;
				break;
d412 52
a463 25
			
		/* Fast mutexes do not check for any error conditions: */
		case PTHREAD_MUTEX_DEFAULT:
		COMMON_LOCK:
			/*
			 * Enter a loop to wait for the mutex to be locked by the
			 * current thread: 
			 */
			while ((*mutex)->m_owner != _thread_run) {
				/* Check if the mutex is not locked: */
				if ((*mutex)->m_owner == NULL) {
					/* Lock the mutex for this thread: */
					(*mutex)->m_owner = _thread_run;
				} else {
					/*
					 * Join the queue of threads waiting to lock
					 * the mutex: 
					 */
					_thread_queue_enq(&(*mutex)->m_queue, _thread_run);
					_thread_run->data.mutex = mutex;

					/* Wait for the mutex: */
					_thread_kern_sched_state_unlock(
					    PS_MUTEX_WAIT, &(*mutex)->lock,
					    __FILE__, __LINE__);
d465 8
a472 3
					/* Lock the mutex again: */
					_SPINLOCK(&(*mutex)->lock);
				}
d476 66
a541 26
		/* Counting mutex: */
		case PTHREAD_MUTEX_RECURSIVE:
			/*
			 * Enter a loop to wait for the mutex to be locked by the
			 * current thread: 
			 */
			while ((*mutex)->m_owner != _thread_run) {
				/* Check if the mutex is not locked: */
				if ((*mutex)->m_owner == NULL) {
					/* Lock the mutex for this thread: */
					(*mutex)->m_owner = _thread_run;

					/* Reset the lock count for this mutex: */
					(*mutex)->m_data.m_count = 0;
				} else {
					/*
					 * Join the queue of threads waiting to lock
					 * the mutex: 
					 */
					_thread_queue_enq(&(*mutex)->m_queue, _thread_run);
					_thread_run->data.mutex = mutex;

					/* Wait for the mutex: */
					_thread_kern_sched_state_unlock(
					    PS_MUTEX_WAIT, &(*mutex)->lock,
					    __FILE__, __LINE__);
d543 5
a547 3
					/* Lock the mutex again: */
					_SPINLOCK(&(*mutex)->lock);
				}
a548 3

			/* Increment the lock count for this mutex: */
			(*mutex)->m_data.m_count++;
d560 6
d575 89
a663 1
	int             ret = 0;
d668 11
d683 7
a689 7
		switch ((*mutex)->m_type) {
		/* Default & normal mutexes do not really need to check for
		   any error conditions: */
		case PTHREAD_MUTEX_NORMAL:
		case PTHREAD_MUTEX_DEFAULT:
		case PTHREAD_MUTEX_ERRORCHECK:
			/* Check if the running thread is not the owner of the mutex: */
d691 4
a694 1
				/* This thread doesn't have permission: */
d697 33
d731 2
a732 2
			 * Get the next thread from the queue of threads waiting on
			 * the mutex: 
d734 98
a831 3
			else if (((*mutex)->m_owner = _thread_queue_deq(&(*mutex)->m_queue)) != NULL) {
				/* Allow the new owner of the mutex to run: */
				PTHREAD_NEW_STATE((*mutex)->m_owner,PS_RUNNING);
d835 6
a840 3
		/* Counting mutex: */
		case PTHREAD_MUTEX_RECURSIVE:
			/* Check if the running thread is not the owner of the mutex: */
d842 5
a846 2
				/* Return an invalid argument error: */
				ret = EINVAL;
d848 2
a849 2
			/* Check if there are still counts: */
			else if ((*mutex)->m_data.m_count > 1) {
d853 4
d858 13
d872 12
a883 2
				 * Get the next thread from the queue of threads waiting on
				 * the mutex: 
d885 59
a943 3
				if (((*mutex)->m_owner = _thread_queue_deq(&(*mutex)->m_queue)) != NULL) {
					/* Allow the new owner of the mutex to run: */
					PTHREAD_NEW_STATE((*mutex)->m_owner,PS_RUNNING);
d955 5
d962 6
d973 326
@


1.5
log
@typo
@
text
@d32 2
d103 1
a103 2
					memset(&pmutex->lock, 0,
					    sizeof(pmutex->lock));
d252 1
d283 1
d316 1
@


1.4
log
@comply with posix when double-unlocking a mutex
@
text
@d250 1
a250 1
				/* Intetionally deadlock */
@


1.3
log
@sync with FreeBSD
@
text
@d363 2
a364 2
				/* Return an invalid argument error: */
				ret = (*mutex)->m_owner ? EPERM : EINVAL;
@


1.2
log
@sync with freebsd
@
text
@d56 1
a56 1
			type = MUTEX_TYPE_FAST;
d77 3
a79 1
				case MUTEX_TYPE_FAST:
d84 1
a84 1
				case MUTEX_TYPE_COUNTING_FAST:
d179 3
a181 1
		case MUTEX_TYPE_FAST:
d193 1
a193 1
		case MUTEX_TYPE_COUNTING_FAST:
d246 17
d264 2
a265 1
		case MUTEX_TYPE_FAST:
d294 1
a294 1
		case MUTEX_TYPE_COUNTING_FAST:
d356 5
a360 2
		/* Fast mutexes do not check for any error conditions: */
		case MUTEX_TYPE_FAST:
d364 1
a364 1
				ret = EINVAL;
d377 1
a377 1
		case MUTEX_TYPE_COUNTING_FAST:
d384 1
a384 1
			else if ((*mutex)->m_data.m_count) {
d387 10
a396 8
			}
			/*
			 * Get the next thread from the queue of threads waiting on
			 * the mutex: 
			 */
			else if (((*mutex)->m_owner = _thread_queue_deq(&(*mutex)->m_queue)) != NULL) {
				/* Allow the new owner of the mutex to run: */
				PTHREAD_NEW_STATE((*mutex)->m_owner,PS_RUNNING);
@


1.1
log
@experimental threaded libc - kernel only
@
text
@d260 4
a263 5
					/* Unlock the mutex structure: */
					_SPINUNLOCK(&(*mutex)->lock);

					/* Block signals: */
					_thread_kern_sched_state(PS_MUTEX_WAIT, __FILE__, __LINE__);
d292 4
a295 5
					/* Unlock the mutex structure: */
					_SPINUNLOCK(&(*mutex)->lock);

					/* Block signals: */
					_thread_kern_sched_state(PS_MUTEX_WAIT, __FILE__, __LINE__);
@

