head	1.8;
access;
symbols
	OPENBSD_3_2:1.6.0.8
	OPENBSD_3_2_BASE:1.6
	OPENBSD_3_1:1.6.0.6
	OPENBSD_3_1_BASE:1.6
	OPENBSD_3_0:1.6.0.4
	OPENBSD_3_0_BASE:1.6
	OPENBSD_2_9:1.6.0.2
	OPENBSD_2_9_BASE:1.6
	OPENBSD_2_8:1.5.0.2
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.4.0.6
	OPENBSD_2_7_BASE:1.4
	OPENBSD_2_6:1.4.0.4
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.4.0.2
	OPENBSD_2_5_BASE:1.4;
locks; strict;
comment	@ * @;


1.8
date	2003.01.20.18.14.05;	author marc;	state dead;
branches;
next	1.7;

1.7
date	2002.10.11.19.08.41;	author marc;	state Exp;
branches;
next	1.6;

1.6
date	2001.03.13.00.05.51;	author d;	state Exp;
branches;
next	1.5;

1.5
date	2000.10.03.02.58.58;	author d;	state Exp;
branches;
next	1.4;

1.4
date	99.03.10.09.45.40;	author d;	state Exp;
branches;
next	1.3;

1.3
date	98.12.21.07.58.45;	author d;	state Exp;
branches;
next	1.2;

1.2
date	98.12.18.05.59.17;	author d;	state Exp;
branches;
next	1.1;

1.1
date	98.11.20.11.15.36;	author d;	state Exp;
branches;
next	;


desc
@@


1.8
log
@
bye-bye libc_r sources.
the sources have been moved (with history) to /usr/src/lib/libpthread
@
text
@/*	$OpenBSD: _atomic_lock.c,v 1.7 2002/10/11 19:08:41 marc Exp $	*/
/* David Leonard, <d@@csee.uq.edu.au>. Public domain. */

/*
 * Atomic lock for i386
 */

#include "spinlock.h"

int
_atomic_lock(volatile _spinlock_lock_t *lock)
{
	_spinlock_lock_t old;

	/*
	 * Use the eXCHanGe instruction to swap the lock value with
	 * a local variable containing the locked state.
	 */
	old = _SPINLOCK_LOCKED;
	__asm__("xchg %0,%1"
		: "=r" (old), "=m" (*lock)
		: "0"  (old), "1"  (*lock));

	return (old != _SPINLOCK_UNLOCKED);
}
@


1.7
log
@remove unused/unreferenced code.
ok fgs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.6 2001/03/13 00:05:51 d Exp $	*/
@


1.6
log
@typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.5 2000/10/03 02:58:58 d Exp $	*/
a24 8
}

int
_atomic_is_locked(volatile _spinlock_lock_t *lock)
{

	/* Return true if the lock is locked (i.e., is not unlocked). */
	return (*lock != _SPINLOCK_UNLOCKED);
@


1.5
log
@public domain
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.4 1999/03/10 09:45:40 d Exp $	*/
d17 1
a17 1
	 * a local variable containg the locked state.
@


1.4
log
@tidy
@
text
@d1 3
a3 1
/*	$OpenBSD: _atomic_lock.c,v 1.3 1998/12/21 07:58:45 d Exp $	*/
@


1.3
log
@md spinlock
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.2 1998/12/18 05:59:17 d Exp $	*/
d18 4
a21 7
	__asm__("xchg %0, %1"
		: "=r" (old), "=m" (*lock) : "0"(old), "1" (*lock)  );
	/*
	 * So now LOCKED is in *lock and 'old' contains what
	 * used to be in *lock. We return 0 if the lock was acquired,
	 * (ie its old value was UNLOCKED) or 1 otherwise.
	 */
@


1.2
log
@add md _atomic_is_locked; clean
@
text
@d1 1
a1 1
/*	$OpenBSD: _atomic_lock.c,v 1.1 1998/11/20 11:15:36 d Exp $	*/
d8 2
a9 2
register_t
_atomic_lock(volatile register_t *lock)
d11 1
a11 1
	register_t old;
d15 1
a15 1
	 * a local variable containg '1' (the locked state).
d17 1
a17 1
	old = 1;
d21 3
a23 3
	 * So now there is a 1 in *lock and 'old' contains what
	 * used to be in the lock. We return 0 if the lock was acquired,
	 * (ie its old value was 0) or 1 otherwise.
d25 1
a25 1
	return old;
d29 1
a29 1
_atomic_is_locked(volatile register_t *lock)
d32 2
a33 1
	return *lock;
@


1.1
log
@Move atomic_lock code from asm to C with inline asm;
Add m68k, mips and sparc. (needs more careful checking)
Add 'slow_atomic_lock' for crippled archs.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d3 1
a3 1
 * Atomic lock aquire for i386.
d26 7
@

