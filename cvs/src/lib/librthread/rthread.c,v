head	1.96;
access;
symbols
	OPENBSD_6_2:1.96.0.4
	OPENBSD_6_2_BASE:1.96
	OPENBSD_6_1:1.94.0.4
	OPENBSD_6_1_BASE:1.94
	OPENBSD_6_0:1.91.0.2
	OPENBSD_6_0_BASE:1.91
	OPENBSD_5_9:1.87.0.2
	OPENBSD_5_9_BASE:1.87
	OPENBSD_5_8:1.83.0.4
	OPENBSD_5_8_BASE:1.83
	OPENBSD_5_7:1.79.0.2
	OPENBSD_5_7_BASE:1.79
	OPENBSD_5_6:1.78.0.4
	OPENBSD_5_6_BASE:1.78
	OPENBSD_5_5:1.76.0.4
	OPENBSD_5_5_BASE:1.76
	OPENBSD_5_4:1.73.0.2
	OPENBSD_5_4_BASE:1.73
	OPENBSD_5_3:1.68.0.2
	OPENBSD_5_3_BASE:1.68
	OPENBSD_5_2:1.62.0.2
	OPENBSD_5_2_BASE:1.62
	OPENBSD_5_1_BASE:1.50
	OPENBSD_5_1:1.50.0.2
	OPENBSD_5_0:1.42.0.8
	OPENBSD_5_0_BASE:1.42
	OPENBSD_4_9:1.42.0.6
	OPENBSD_4_9_BASE:1.42
	OPENBSD_4_8:1.42.0.4
	OPENBSD_4_8_BASE:1.42
	OPENBSD_4_7:1.42.0.2
	OPENBSD_4_7_BASE:1.42
	OPENBSD_4_6:1.40.0.6
	OPENBSD_4_6_BASE:1.40
	OPENBSD_4_5:1.40.0.2
	OPENBSD_4_5_BASE:1.40
	OPENBSD_4_4:1.35.0.2
	OPENBSD_4_4_BASE:1.35
	OPENBSD_4_3:1.34.0.4
	OPENBSD_4_3_BASE:1.34
	OPENBSD_4_2:1.34.0.2
	OPENBSD_4_2_BASE:1.34
	OPENBSD_4_1:1.33.0.6
	OPENBSD_4_1_BASE:1.33
	OPENBSD_4_0:1.33.0.4
	OPENBSD_4_0_BASE:1.33
	OPENBSD_3_9:1.33.0.2
	OPENBSD_3_9_BASE:1.33;
locks; strict;
comment	@ * @;


1.96
date	2017.09.05.02.40.54;	author guenther;	state Exp;
branches;
next	1.95;
commitid	5DW3WOQF0YGGx8lJ;

1.95
date	2017.07.27.16.35.08;	author tedu;	state Exp;
branches;
next	1.94;
commitid	GI4jTSM9rhCPSmX0;

1.94
date	2016.09.04.10.13.35;	author akfaew;	state Exp;
branches;
next	1.93;
commitid	tPNEomz2X1xlRc3u;

1.93
date	2016.09.03.16.44.20;	author akfaew;	state Exp;
branches;
next	1.92;
commitid	A8DISbEBB3jwsSL3;

1.92
date	2016.09.01.10.41.02;	author otto;	state Exp;
branches;
next	1.91;
commitid	HatwrthzJhfQpDr7;

1.91
date	2016.05.07.19.05.22;	author guenther;	state Exp;
branches;
next	1.90;
commitid	d9R7VGw9CHTkwXE1;

1.90
date	2016.04.02.19.56.53;	author guenther;	state Exp;
branches;
next	1.89;
commitid	8mfZyQLsoIGIAaFG;

1.89
date	2016.04.02.19.00.51;	author guenther;	state Exp;
branches;
next	1.88;
commitid	HcJGR3mYEiomHNg2;

1.88
date	2016.03.20.02.30.28;	author guenther;	state Exp;
branches;
next	1.87;
commitid	NBSvZnB5vLEhTQkX;

1.87
date	2015.11.10.04.30.59;	author guenther;	state Exp;
branches;
next	1.86;
commitid	GTAoVjpQRAdNHXVz;

1.86
date	2015.11.01.03.52.17;	author guenther;	state Exp;
branches;
next	1.85;
commitid	foeEYDHCuczbDswZ;

1.85
date	2015.10.23.04.39.24;	author guenther;	state Exp;
branches;
next	1.84;
commitid	OrVNAJkzchWBNbO0;

1.84
date	2015.10.18.08.02.58;	author guenther;	state Exp;
branches;
next	1.83;
commitid	lDpsAqCZ03zHaXJb;

1.83
date	2015.05.19.20.50.06;	author guenther;	state Exp;
branches;
next	1.82;
commitid	9XB6auZflgiDfN67;

1.82
date	2015.05.10.18.33.15;	author guenther;	state Exp;
branches;
next	1.81;
commitid	DJrEhfkYnXxo41SD;

1.81
date	2015.04.29.06.01.37;	author guenther;	state Exp;
branches;
next	1.80;
commitid	KgxdDBMASll0o49H;

1.80
date	2015.04.07.01.27.07;	author guenther;	state Exp;
branches;
next	1.79;
commitid	oQvh7XA3Kql35r0J;

1.79
date	2014.11.16.05.26.20;	author guenther;	state Exp;
branches;
next	1.78;
commitid	4z0VgJZVjLhJAVF6;

1.78
date	2014.07.01.03.32.18;	author guenther;	state Exp;
branches;
next	1.77;
commitid	p1Dkx943Gz6X4IpJ;

1.77
date	2014.03.16.18.38.30;	author guenther;	state Exp;
branches;
next	1.76;

1.76
date	2013.12.12.08.12.08;	author guenther;	state Exp;
branches;
next	1.75;

1.75
date	2013.11.29.16.27.40;	author guenther;	state Exp;
branches;
next	1.74;

1.74
date	2013.10.23.05.59.46;	author guenther;	state Exp;
branches;
next	1.73;

1.73
date	2013.07.30.15.31.01;	author guenther;	state Exp;
branches;
next	1.72;

1.72
date	2013.07.05.21.10.50;	author miod;	state Exp;
branches;
next	1.71;

1.71
date	2013.06.01.23.06.26;	author tedu;	state Exp;
branches;
next	1.70;

1.70
date	2013.06.01.20.47.40;	author tedu;	state Exp;
branches;
next	1.69;

1.69
date	2013.04.06.04.25.01;	author tedu;	state Exp;
branches;
next	1.68;

1.68
date	2013.02.15.22.01.24;	author guenther;	state Exp;
branches;
next	1.67;

1.67
date	2013.02.14.03.38.15;	author guenther;	state Exp;
branches;
next	1.66;

1.66
date	2012.08.22.23.43.32;	author matthew;	state Exp;
branches;
next	1.65;

1.65
date	2012.08.22.22.34.57;	author matthew;	state Exp;
branches;
next	1.64;

1.64
date	2012.08.15.17.07.49;	author matthew;	state Exp;
branches;
next	1.63;

1.63
date	2012.08.13.04.52.42;	author matthew;	state Exp;
branches;
next	1.62;

1.62
date	2012.06.21.00.56.59;	author guenther;	state Exp;
branches;
next	1.61;

1.61
date	2012.04.10.21.10.45;	author guenther;	state Exp;
branches;
next	1.60;

1.60
date	2012.03.22.15.26.04;	author kurt;	state Exp;
branches;
next	1.59;

1.59
date	2012.03.20.00.47.23;	author guenther;	state Exp;
branches;
next	1.58;

1.58
date	2012.03.14.21.23.37;	author guenther;	state Exp;
branches;
next	1.57;

1.57
date	2012.03.03.10.02.26;	author guenther;	state Exp;
branches;
next	1.56;

1.56
date	2012.03.02.17.49.58;	author fgsch;	state Exp;
branches;
next	1.55;

1.55
date	2012.02.24.05.37.51;	author guenther;	state Exp;
branches;
next	1.54;

1.54
date	2012.02.19.04.47.49;	author guenther;	state Exp;
branches;
next	1.53;

1.53
date	2012.02.18.22.03.21;	author guenther;	state Exp;
branches;
next	1.52;

1.52
date	2012.02.18.21.12.09;	author guenther;	state Exp;
branches;
next	1.51;

1.51
date	2012.02.16.20.55.09;	author kettenis;	state Exp;
branches;
next	1.50;

1.50
date	2012.01.17.02.34.18;	author guenther;	state Exp;
branches;
next	1.49;

1.49
date	2011.12.28.04.59.31;	author guenther;	state Exp;
branches;
next	1.48;

1.48
date	2011.12.27.17.33.21;	author guenther;	state Exp;
branches;
next	1.47;

1.47
date	2011.12.05.06.56.09;	author guenther;	state Exp;
branches;
next	1.46;

1.46
date	2011.12.05.04.02.03;	author guenther;	state Exp;
branches;
next	1.45;

1.45
date	2011.11.09.10.28.01;	author guenther;	state Exp;
branches;
next	1.44;

1.44
date	2011.11.06.11.48.58;	author guenther;	state Exp;
branches;
next	1.43;

1.43
date	2011.10.17.06.39.20;	author guenther;	state Exp;
branches;
next	1.42;

1.42
date	2009.11.27.19.45.54;	author guenther;	state Exp;
branches;
next	1.41;

1.41
date	2009.11.27.19.42.24;	author guenther;	state Exp;
branches;
next	1.40;

1.40
date	2009.02.20.01.24.05;	author tedu;	state Exp;
branches;
next	1.39;

1.39
date	2008.10.13.05.42.46;	author kevlo;	state Exp;
branches;
next	1.38;

1.38
date	2008.08.14.05.57.06;	author guenther;	state Exp;
branches;
next	1.37;

1.37
date	2008.08.14.05.20.44;	author guenther;	state Exp;
branches;
next	1.36;

1.36
date	2008.08.14.05.15.41;	author guenther;	state Exp;
branches;
next	1.35;

1.35
date	2008.06.05.21.06.11;	author kurt;	state Exp;
branches;
next	1.34;

1.34
date	2007.05.18.14.36.17;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2006.01.06.07.29.36;	author marc;	state Exp;
branches;
next	1.32;

1.32
date	2006.01.06.06.13.27;	author tedu;	state Exp;
branches;
next	1.31;

1.31
date	2006.01.05.22.17.17;	author otto;	state Exp;
branches;
next	1.30;

1.30
date	2006.01.05.04.24.30;	author tedu;	state Exp;
branches;
next	1.29;

1.29
date	2006.01.05.04.06.48;	author marc;	state Exp;
branches;
next	1.28;

1.28
date	2006.01.04.19.48.52;	author otto;	state Exp;
branches;
next	1.27;

1.27
date	2006.01.04.08.48.01;	author marc;	state Exp;
branches;
next	1.26;

1.26
date	2006.01.01.19.32.30;	author marc;	state Exp;
branches;
next	1.25;

1.25
date	2005.12.31.08.51.20;	author otto;	state Exp;
branches;
next	1.24;

1.24
date	2005.12.30.21.51.27;	author otto;	state Exp;
branches;
next	1.23;

1.23
date	2005.12.30.20.35.11;	author otto;	state Exp;
branches;
next	1.22;

1.22
date	2005.12.30.04.10.23;	author tedu;	state Exp;
branches;
next	1.21;

1.21
date	2005.12.30.04.05.55;	author tedu;	state Exp;
branches;
next	1.20;

1.20
date	2005.12.29.20.34.22;	author otto;	state Exp;
branches;
next	1.19;

1.19
date	2005.12.29.19.10.20;	author otto;	state Exp;
branches;
next	1.18;

1.18
date	2005.12.23.03.27.06;	author tedu;	state Exp;
branches;
next	1.17;

1.17
date	2005.12.22.06.49.48;	author tedu;	state Exp;
branches;
next	1.16;

1.16
date	2005.12.22.06.33.12;	author tedu;	state Exp;
branches;
next	1.15;

1.15
date	2005.12.22.00.37.25;	author marco;	state Exp;
branches;
next	1.14;

1.14
date	2005.12.21.23.44.56;	author marco;	state Exp;
branches;
next	1.13;

1.13
date	2005.12.21.00.53.28;	author tedu;	state Exp;
branches;
next	1.12;

1.12
date	2005.12.21.00.49.07;	author tedu;	state Exp;
branches;
next	1.11;

1.11
date	2005.12.19.15.41.00;	author brad;	state Exp;
branches;
next	1.10;

1.10
date	2005.12.19.06.47.40;	author tedu;	state Exp;
branches;
next	1.9;

1.9
date	2005.12.19.06.45.14;	author tedu;	state Exp;
branches;
next	1.8;

1.8
date	2005.12.18.01.35.06;	author tedu;	state Exp;
branches;
next	1.7;

1.7
date	2005.12.14.06.07.54;	author tedu;	state Exp;
branches;
next	1.6;

1.6
date	2005.12.14.05.44.49;	author tedu;	state Exp;
branches;
next	1.5;

1.5
date	2005.12.14.04.43.04;	author tedu;	state Exp;
branches;
next	1.4;

1.4
date	2005.12.14.04.01.44;	author tedu;	state Exp;
branches;
next	1.3;

1.3
date	2005.12.13.05.56.55;	author tedu;	state Exp;
branches;
next	1.2;

1.2
date	2005.12.03.18.17.55;	author tedu;	state Exp;
branches;
next	1.1;

1.1
date	2005.12.03.18.16.19;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.96
log
@Move mutex, condvar, and thread-specific data routes, pthread_once, and
pthread_exit from libpthread to libc, along with low-level bits to
support them.  Major bump to both libc and libpthread.

Requested by libressl team.  Ports testing by naddy@@
ok kettenis@@
@
text
@/*	$OpenBSD: rthread.c,v 1.95 2017/07/27 16:35:08 tedu Exp $ */
/*
 * Copyright (c) 2004,2005 Ted Unangst <tedu@@openbsd.org>
 * All Rights Reserved.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * The heart of rthreads.  Basic functions like creating and joining
 * threads.
 */

#include <sys/types.h>
#ifndef NO_PIC
#include <sys/exec_elf.h>
#pragma weak _DYNAMIC
#endif

#include <stdlib.h>
#include <unistd.h>
#include <signal.h>
#include <stdio.h>
#include <string.h>
#include <errno.h>
#include <dlfcn.h>
#include <tib.h>

#include <pthread.h>

#include "cancel.h"		/* in libc/include */
#include "rthread.h"
#include "rthread_cb.h"

/*
 * Call nonstandard functions via names in the reserved namespace:
 *	dlctl() -> _dlctl()
 *	getthrid -> _thread_sys_getthrid
 */
typeof(dlctl) dlctl asm("_dlctl") __attribute__((weak));
REDIRECT_SYSCALL(getthrid);

/* weak stub to be overriden by ld.so */
int	dlctl(void *handle, int cmd, void *data) { return 0; }

/*
 * libc's signal wrappers hide SIGTHR; we need to call the real syscall
 * stubs _thread_sys_* directly.
 */
REDIRECT_SYSCALL(sigaction);
REDIRECT_SYSCALL(sigprocmask);
REDIRECT_SYSCALL(thrkill);

static int concurrency_level;	/* not used */

int _threads_ready;
int _post_threaded;
size_t _thread_pagesize;
struct listhead _thread_list = LIST_HEAD_INITIALIZER(_thread_list);
_atomic_lock_t _thread_lock = _SPINLOCK_UNLOCKED;
static struct pthread_queue _thread_gc_list
    = TAILQ_HEAD_INITIALIZER(_thread_gc_list);
static _atomic_lock_t _thread_gc_lock = _SPINLOCK_UNLOCKED;
static struct pthread _initial_thread;

struct pthread_attr _rthread_attr_default = {
	.stack_addr			= NULL,
	.stack_size			= RTHREAD_STACK_SIZE_DEF,
/*	.guard_size		set in _rthread_init */
	.detach_state			= PTHREAD_CREATE_JOINABLE,
	.contention_scope		= PTHREAD_SCOPE_SYSTEM,
	.sched_policy			= SCHED_OTHER,
	.sched_param = { .sched_priority = 0 },
	.sched_inherit			= PTHREAD_INHERIT_SCHED,
};

/*
 * internal support functions
 */

static void
_rthread_start(void *v)
{
	pthread_t thread = v;
	void *retval;

	retval = thread->fn(thread->arg);
	pthread_exit(retval);
}

static void
sigthr_handler(__unused int sig)
{
	struct tib *tib = TIB_GET();
	pthread_t self = tib->tib_thread;

	/*
	 * Do nothing unless
	 * 1) pthread_cancel() has been called on this thread,
	 * 2) cancelation is enabled for it, and
	 * 3) we're not already in cancelation processing
	 */
	if (!tib->tib_canceled || tib->tib_cantcancel)
		return;

	/*
	 * If delaying cancels inside complex ops (pthread_cond_wait,
	 * pthread_join, etc), just mark that this has happened to
	 * prevent a race with going to sleep
	 */
	if (tib->tib_cancel_point & CANCEL_POINT_DELAYED) {
		self->delayed_cancel = 1;
		return;
	}

	/*
	 * otherwise, if in a cancel point or async cancels are
	 * enabled, then exit
	 */
	if (tib->tib_cancel_point ||
	    (tib->tib_thread_flags & TIB_THREAD_ASYNC_CANCEL))
		pthread_exit(PTHREAD_CANCELED);
}


/*
 * A few basic callbacks for libc.  The first couple are only used
 * on archs where there isn't a fast TCB_GET()
 */
#ifndef TCB_HAVE_MD_GET
static int *
multi_threaded_errnoptr(void)
{
        return (&TIB_GET()->tib_errno);
}

static void *
multi_threaded_tcb(void)
{
	return (TCB_GET());
}
#endif /* TCB_HAVE_MD_GET */

static void
_rthread_free(pthread_t thread)
{
	_spinlock(&_thread_gc_lock);
	TAILQ_INSERT_TAIL(&_thread_gc_list, thread, waiting);
	_spinunlock(&_thread_gc_lock);
}

static void
_thread_release(pthread_t thread)
{
	_spinlock(&_thread_lock);
	LIST_REMOVE(thread, threads);
	_spinunlock(&_thread_lock);

	_spinlock(&thread->flags_lock);
	if (thread->flags & THREAD_DETACHED) {
		_spinunlock(&thread->flags_lock);
		_rthread_free(thread);
	} else {
		thread->flags |= THREAD_DONE;
		_spinunlock(&thread->flags_lock);
		_sem_post(&thread->donesem);
	}
}

static void
_thread_key_zero(int key)
{
	pthread_t thread;
	struct rthread_storage *rs;

	LIST_FOREACH(thread, &_thread_list, threads) {
		for (rs = thread->local_storage; rs; rs = rs->next) {
			if (rs->keyid == key)
				rs->data = NULL;
		}
	}
}

void
_rthread_init(void)
{
	pthread_t thread = pthread_self();
	struct sigaction sa;

	if (_threads_ready)
		return;

	LIST_INSERT_HEAD(&_thread_list, thread, threads);

	_thread_pagesize = (size_t)sysconf(_SC_PAGESIZE);
	_rthread_attr_default.guard_size = _thread_pagesize;
	thread->attr = _rthread_attr_default;

	/* get libc to start using our callbacks */
	{
		struct thread_callbacks cb = { 0 };

#ifndef TCB_HAVE_MD_GET
		cb.tc_errnoptr		= multi_threaded_errnoptr;
		cb.tc_tcb		= multi_threaded_tcb;
#endif
		cb.tc_fork		= _thread_fork;
		cb.tc_vfork		= _thread_vfork;
		cb.tc_thread_release	= _thread_release;
		cb.tc_thread_key_zero	= _thread_key_zero;
		_thread_set_callbacks(&cb, sizeof(cb));
	}

#ifndef NO_PIC
	if (_DYNAMIC) {
		dlctl(NULL, DL_SETTHREADLCK, _rthread_dl_lock);
	}
#endif

	/*
	 * Set the handler on the signal used for cancelation and
	 * suspension, and make sure it's unblocked
	 */
	memset(&sa, 0, sizeof(sa));
	sigemptyset(&sa.sa_mask);
	sa.sa_handler = sigthr_handler;
	sigaction(SIGTHR, &sa, NULL);
	sigaddset(&sa.sa_mask, SIGTHR);
	sigprocmask(SIG_UNBLOCK, &sa.sa_mask, NULL);

	_threads_ready = 1;

	_malloc_init(1);

	_rthread_debug(1, "rthread init\n");
}

static void
_rthread_reaper(void)
{
	pthread_t thread;

restart:
	_spinlock(&_thread_gc_lock);
	TAILQ_FOREACH(thread, &_thread_gc_list, waiting) {
		if (thread->tib->tib_tid != 0)
			continue;
		TAILQ_REMOVE(&_thread_gc_list, thread, waiting);
		_spinunlock(&_thread_gc_lock);
		if (thread != &_initial_thread) {
			_rthread_debug(3, "rthread reaping %p stack %p\n",
			    (void *)thread, (void *)thread->stack);
			_rthread_free_stack(thread->stack);
			_dl_free_tib(thread->tib, sizeof(*thread));
		} else {
			/* initial thread isn't part of TIB allocation */
			_rthread_debug(3, "rthread reaping %p (initial)\n",
			    (void *)thread);
			_dl_free_tib(thread->tib, 0);
		}
		goto restart;
	}
	_spinunlock(&_thread_gc_lock);
}

/*
 * real pthread functions
 */

int
pthread_join(pthread_t thread, void **retval)
{
	int e;
	struct tib *tib = TIB_GET();
	pthread_t self;
	PREP_CANCEL_POINT(tib);

	if (_post_threaded) {
#define GREATSCOTT "great scott! serious repercussions on future events!\n"
		write(2, GREATSCOTT, sizeof(GREATSCOTT) - 1);
		abort();
	}
	if (!_threads_ready)
		_rthread_init();
	self = tib->tib_thread;

	e = 0;
	ENTER_DELAYED_CANCEL_POINT(tib, self);
	if (thread == NULL)
		e = EINVAL;
	else if (thread == self)
		e = EDEADLK;
	else if (thread->flags & THREAD_DETACHED)
		e = EINVAL;
	else if ((e = _sem_wait(&thread->donesem, 0, NULL,
	    &self->delayed_cancel)) == 0) {
		if (retval)
			*retval = thread->retval;

		/*
		 * We should be the last having a ref to this thread,
		 * but someone stupid or evil might haved detached it;
		 * in that case the thread will clean up itself
		 */
		if ((thread->flags & THREAD_DETACHED) == 0)
			_rthread_free(thread);
	}

	LEAVE_CANCEL_POINT_INNER(tib, e);
	_rthread_reaper();
	return (e);
}

int
pthread_detach(pthread_t thread)
{
	int rc = 0;

	_spinlock(&thread->flags_lock);
	if (thread->flags & THREAD_DETACHED) {
		rc = EINVAL;
		_spinunlock(&thread->flags_lock);
	} else if (thread->flags & THREAD_DONE) {
		_spinunlock(&thread->flags_lock);
		_rthread_free(thread);
	} else {
		thread->flags |= THREAD_DETACHED;
		_spinunlock(&thread->flags_lock);
	}
	_rthread_reaper();
	return (rc);
}

int
pthread_create(pthread_t *threadp, const pthread_attr_t *attr,
    void *(*start_routine)(void *), void *arg)
{
	extern int __isthreaded;
	struct tib *tib;
	pthread_t thread;
	struct __tfork param;
	int rc;

	if (!_threads_ready)
		_rthread_init();

	_rthread_reaper();

	tib = _dl_allocate_tib(sizeof(*thread));
	if (tib == NULL)
		return (ENOMEM);
	thread = tib->tib_thread;
	memset(thread, 0, sizeof(*thread));
	thread->tib = tib;
	thread->donesem.lock = _SPINLOCK_UNLOCKED;
	thread->flags_lock = _SPINLOCK_UNLOCKED;
	thread->fn = start_routine;
	thread->arg = arg;
	tib->tib_tid = -1;

	thread->attr = attr != NULL ? *(*attr) : _rthread_attr_default;
	if (thread->attr.sched_inherit == PTHREAD_INHERIT_SCHED) {
		pthread_t self = pthread_self();

		thread->attr.sched_policy = self->attr.sched_policy;
		thread->attr.sched_param = self->attr.sched_param;
	}
	if (thread->attr.detach_state == PTHREAD_CREATE_DETACHED)
		thread->flags |= THREAD_DETACHED;

	thread->stack = _rthread_alloc_stack(thread);
	if (!thread->stack) {
		rc = errno;
		goto fail1;
	}

	param.tf_tcb = TIB_TO_TCB(tib);
	param.tf_tid = &tib->tib_tid;
	param.tf_stack = thread->stack->sp;

	_spinlock(&_thread_lock);
	LIST_INSERT_HEAD(&_thread_list, thread, threads);
	_spinunlock(&_thread_lock);

	/* we're going to be multi-threaded real soon now */
	__isthreaded = 1;
	rc = __tfork_thread(&param, sizeof(param), _rthread_start, thread);
	if (rc != -1) {
		/* success */
		*threadp = thread;
		return (0);
	}
		
	rc = errno;

	_spinlock(&_thread_lock);
	LIST_REMOVE(thread, threads);
	_spinunlock(&_thread_lock);
	_rthread_free_stack(thread->stack);
fail1:
	_dl_free_tib(tib, sizeof(*thread));

	return (rc);
}

int
pthread_kill(pthread_t thread, int sig)
{
	struct tib *tib = thread->tib;

	if (sig == SIGTHR)
		return (EINVAL);
	if (thrkill(tib->tib_tid, sig, TIB_TO_TCB(tib)))
		return (errno);
	return (0);
}

int
pthread_cancel(pthread_t thread)
{
	struct tib *tib = thread->tib;
	pid_t tid = tib->tib_tid;

	if (tib->tib_canceled == 0 && tid != 0 &&
	    (tib->tib_cantcancel & CANCEL_DYING) == 0) {
		tib->tib_canceled = 1;

		if ((tib->tib_cantcancel & CANCEL_DISABLED) == 0) {
			thrkill(tid, SIGTHR, TIB_TO_TCB(tib));
			return (0);
		}
	}
	return (0);
}

void
pthread_testcancel(void)
{
	struct tib *tib = TIB_GET();

	if (tib->tib_canceled && (tib->tib_cantcancel & CANCEL_DISABLED) == 0)
		pthread_exit(PTHREAD_CANCELED);
}

int
pthread_setcancelstate(int state, int *oldstatep)
{
	struct tib *tib = TIB_GET();
	int oldstate;

	oldstate = tib->tib_cantcancel & CANCEL_DISABLED ?
	    PTHREAD_CANCEL_DISABLE : PTHREAD_CANCEL_ENABLE;
	if (state == PTHREAD_CANCEL_ENABLE) {
		tib->tib_cantcancel &= ~CANCEL_DISABLED;
	} else if (state == PTHREAD_CANCEL_DISABLE) {
		tib->tib_cantcancel |= CANCEL_DISABLED;
	} else {
		return (EINVAL);
	}
	if (oldstatep)
		*oldstatep = oldstate;

	return (0);
}
DEF_STD(pthread_setcancelstate);

int
pthread_setcanceltype(int type, int *oldtypep)
{
	struct tib *tib = TIB_GET();
	int oldtype;

	oldtype = tib->tib_thread_flags & TIB_THREAD_ASYNC_CANCEL ?
	    PTHREAD_CANCEL_ASYNCHRONOUS : PTHREAD_CANCEL_DEFERRED;
	if (type == PTHREAD_CANCEL_DEFERRED) {
		tib->tib_thread_flags &=~ TIB_THREAD_ASYNC_CANCEL;
	} else if (type == PTHREAD_CANCEL_ASYNCHRONOUS) {
		tib->tib_thread_flags |= TIB_THREAD_ASYNC_CANCEL;
	} else {
		return (EINVAL);
	}
	if (oldtypep)
		*oldtypep = oldtype;

	return (0);
}

void
pthread_cleanup_push(void (*fn)(void *), void *arg)
{
	struct rthread_cleanup_fn *clfn;
	pthread_t self = pthread_self();

	clfn = calloc(1, sizeof(*clfn));
	if (!clfn)
		return;
	clfn->fn = fn;
	clfn->arg = arg;
	clfn->next = self->cleanup_fns;
	self->cleanup_fns = clfn;
}

void
pthread_cleanup_pop(int execute)
{
	struct rthread_cleanup_fn *clfn;
	pthread_t self = pthread_self();

	clfn = self->cleanup_fns;
	if (clfn) {
		self->cleanup_fns = clfn->next;
		if (execute)
			clfn->fn(clfn->arg);
		free(clfn);
	}
}

int
pthread_getconcurrency(void)
{
	return (concurrency_level);
}

int
pthread_setconcurrency(int new_level)
{
	if (new_level < 0)
		return (EINVAL);
	concurrency_level = new_level;
	return (0);
}

/*
 * compat debug stuff
 */
void
_thread_dump_info(void)
{
	pthread_t thread;

	_spinlock(&_thread_lock);
	LIST_FOREACH(thread, &_thread_list, threads)
		printf("thread %d flags 0x%x name %s\n", thread->tib->tib_tid,
		    thread->tib->tib_thread_flags, thread->name);
	_spinunlock(&_thread_lock);
}

#ifndef NO_PIC
/*
 * _rthread_dl_lock() provides the locking for dlopen(), dlclose(), and
 * the function called via atexit() to invoke all destructors.  The latter
 * two call shared-object destructors, which may need to call dlclose(),
 * so this lock needs to permit recursive locking.
 * The specific code here was extracted from _rthread_mutex_lock() and
 * pthread_mutex_unlock() and simplified to use the static variables.
 */
void
_rthread_dl_lock(int what)
{
	static _atomic_lock_t lock = _SPINLOCK_UNLOCKED;
	static pthread_t owner = NULL;
	static struct pthread_queue lockers = TAILQ_HEAD_INITIALIZER(lockers);
	static int count = 0;

	if (what == 0) {
		pthread_t self = pthread_self();

		/* lock, possibly recursive */
		_spinlock(&lock);
		if (owner == NULL) {
			owner = self;
		} else if (owner != self) {
			TAILQ_INSERT_TAIL(&lockers, self, waiting);
			while (owner != self) {
				__thrsleep(self, 0, NULL, &lock, NULL);
				_spinlock(&lock);
			}
		}
		count++;
		_spinunlock(&lock);
	} else if (what == 1) {
		/* unlock, possibly recursive */
		if (--count == 0) {
			pthread_t next;

			_spinlock(&lock);
			owner = next = TAILQ_FIRST(&lockers);
			if (next != NULL)
				TAILQ_REMOVE(&lockers, next, waiting);
			_spinunlock(&lock);
			if (next != NULL)
				__thrwakeup(next, 1);
		}
	} else {
		/* reinit: used in child after fork to clear the queue */
		lock = _SPINLOCK_UNLOCKED;
		if (--count == 0)
			owner = NULL;
		TAILQ_INIT(&lockers);
	}
}
#endif
@


1.95
log
@bad things can (and will) happen if a threaded program calls fork() and
then strays off the path to exec(). one common manifestation of this
problem occurs in pthread_join(), so we can add a little check there.
first person to hit this in real life gets to change the error message.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.94 2016/09/04 10:13:35 akfaew Exp $ */
a40 1
#include "thread_private.h"
a88 18
void
_spinlock(volatile _atomic_lock_t *lock)
{
	while (_atomic_lock(lock))
		sched_yield();
}

int
_spinlocktry(volatile _atomic_lock_t *lock)
{
	return 0 == _atomic_lock(lock);
}

void
_spinunlock(volatile _atomic_lock_t *lock)
{
	*lock = _ATOMIC_LOCK_UNLOCKED;
}
d153 28
a180 2
void
_thread_canceled(void)
d182 9
a190 1
	pthread_exit(PTHREAD_CANCELED);
d196 1
a196 2
	pthread_t thread = &_initial_thread;
	struct tib *tib;
d199 2
a200 3
	tib = TIB_GET();
	tib->tib_thread = thread;
	thread->tib = tib;
a201 4
	thread->donesem.lock = _SPINLOCK_UNLOCKED;
	tib->tib_thread_flags = TIB_THREAD_INITIAL_STACK;
	thread->flags_lock = _SPINLOCK_UNLOCKED;
	strlcpy(thread->name, "Main process", sizeof(thread->name));
a202 1
	_rthread_debug_init();
a215 18
		cb.tc_canceled		= _thread_canceled;
		cb.tc_flockfile		= _thread_flockfile;
		cb.tc_ftrylockfile	= _thread_ftrylockfile;
		cb.tc_funlockfile	= _thread_funlockfile;
		cb.tc_malloc_lock	= _thread_malloc_lock;
		cb.tc_malloc_unlock	= _thread_malloc_unlock;
		cb.tc_atexit_lock	= _thread_atexit_lock;
		cb.tc_atexit_unlock	= _thread_atexit_unlock;
		cb.tc_atfork_lock	= _thread_atfork_lock;
		cb.tc_atfork_unlock	= _thread_atfork_unlock;
		cb.tc_arc4_lock		= _thread_arc4_lock;
		cb.tc_arc4_unlock	= _thread_arc4_unlock;
		cb.tc_mutex_lock	= _thread_mutex_lock;
		cb.tc_mutex_unlock	= _thread_mutex_unlock;
		cb.tc_mutex_destroy	= _thread_mutex_destroy;
		cb.tc_tag_lock		= _thread_tag_lock;
		cb.tc_tag_unlock	= _thread_tag_unlock;
		cb.tc_tag_storage	= _thread_tag_storage;
d218 2
a247 21
_rthread_free(pthread_t thread)
{
	_spinlock(&_thread_gc_lock);
	TAILQ_INSERT_TAIL(&_thread_gc_list, thread, waiting);
	_spinunlock(&_thread_gc_lock);
}

/*
 * real pthread functions
 */
pthread_t
pthread_self(void)
{
	if (!_threads_ready)
		_rthread_init();

	return (TIB_GET()->tib_thread);
}
DEF_STD(pthread_self);

static void
d275 3
a277 49
void
pthread_exit(void *retval)
{
	struct rthread_cleanup_fn *clfn;
	struct tib *tib = TIB_GET();
	pthread_t thread;

	if (!_threads_ready)
		_rthread_init();
	thread = tib->tib_thread;

	if (tib->tib_cantcancel & CANCEL_DYING) {
		/*
		 * Called pthread_exit() from destructor or cancelation
		 * handler: blow up.  XXX write something to stderr?
		 */
		abort();
		//_exit(42);
	}

	tib->tib_cantcancel |= CANCEL_DYING;

	thread->retval = retval;

	for (clfn = thread->cleanup_fns; clfn; ) {
		struct rthread_cleanup_fn *oclfn = clfn;
		clfn = clfn->next;
		oclfn->fn(oclfn->arg);
		free(oclfn);
	}
	_rthread_tls_destructors(thread);
	_spinlock(&_thread_lock);
	LIST_REMOVE(thread, threads);
	_spinunlock(&_thread_lock);

	_spinlock(&thread->flags_lock);
	if (thread->flags & THREAD_DETACHED) {
		_spinunlock(&thread->flags_lock);
		_rthread_free(thread);
	} else {
		thread->flags |= THREAD_DONE;
		_spinunlock(&thread->flags_lock);
		_sem_post(&thread->donesem);
	}

	__threxit(&tib->tib_tid);
	for(;;);
}
DEF_STD(pthread_exit);
a424 6
}

int
pthread_equal(pthread_t t1, pthread_t t2)
{
	return (t1 == t2);
@


1.94
log
@Get rid of ticket support, replace "struct _spinlock" with "_atomic_lock_t".

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.93 2016/09/03 16:44:20 akfaew Exp $ */
d67 1
d362 5
@


1.93
log
@Remove _USING_TICKETS, it's defined as 0. No functional change.

ok tedu@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.92 2016/09/01 10:41:02 otto Exp $ */
a65 2
struct _spinlock _SPINLOCK_UNLOCKED_ASSIGN = _SPINLOCK_UNLOCKED;

d69 1
a69 1
struct _spinlock _thread_lock = _SPINLOCK_UNLOCKED;
d72 1
a72 1
static struct _spinlock _thread_gc_lock = _SPINLOCK_UNLOCKED;
d90 1
a90 1
_spinlock(volatile struct _spinlock *lock)
d92 1
a92 1
	while (_atomic_lock(&lock->ticket))
d97 1
a97 1
_spinlocktry(volatile struct _spinlock *lock)
d99 1
a99 1
	return 0 == _atomic_lock(&lock->ticket);
d103 1
a103 1
_spinunlock(volatile struct _spinlock *lock)
d105 1
a105 1
	lock->ticket = _ATOMIC_LOCK_UNLOCKED;
d188 1
a188 1
	thread->donesem.lock = _SPINLOCK_UNLOCKED_ASSIGN;
d190 1
a190 1
	thread->flags_lock = _SPINLOCK_UNLOCKED_ASSIGN;
d433 2
a434 2
	thread->donesem.lock = _SPINLOCK_UNLOCKED_ASSIGN;
	thread->flags_lock = _SPINLOCK_UNLOCKED_ASSIGN;
d644 1
a644 1
	static struct _spinlock lock = _SPINLOCK_UNLOCKED;
d659 1
a659 1
				__thrsleep(self, 0, NULL, &lock.ticket, NULL);
d680 1
a680 1
		lock = _SPINLOCK_UNLOCKED_ASSIGN;
@


1.92
log
@Less lock contention by using more pools for mult-threaded programs.
tested by many (thanks!) ok tedu, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.91 2016/05/07 19:05:22 guenther Exp $ */
d661 1
a661 2
				__thrsleep(self, 0 | _USING_TICKETS, NULL,
				    &lock.ticket, NULL);
@


1.91
log
@Use a Thread Information Block in both single and multi-threaded programs.
This stores errno, the cancelation flags, and related bits for each thread
and is allocated by ld.so or libc.a.  This is an ABI break from 5.9-stable!

Make libpthread dlopen'able by moving the cancelation wrappers into libc
and doing locking and fork/errno handling via callbacks that libpthread
registers when it first initializes.  'errno' *must* be declared via
<errno.h> now!

Clean up libpthread's symbol exports like libc.

On powerpc, offset the TIB/TCB/TLS data from the register per the ELF spec.

Testing by various, particularly sthen@@ and patrick@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.90 2016/04/02 19:56:53 guenther Exp $ */
d41 1
d250 2
@


1.90
log
@Wrap <pthread.h> and <pthread_np.h> to eliminate PLT entries for internal
references.  Use _thread_pagesize for the semaphore mmap size instead of
calling getpagesize() each time.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.89 2016/04/02 19:00:51 guenther Exp $ */
a23 5
#include <sys/uio.h>
#include <sys/wait.h>
#include <sys/socket.h>
#include <sys/mman.h>
#include <sys/msg.h>
d36 1
a36 2
#include <fcntl.h>
#include <poll.h>
d40 1
a40 1
#include "thread_private.h"	/* in libc/include */
d42 1
a42 1
#include "tcb.h"
d46 1
a46 1
 *	NOT YET dlctl() -> _dlctl()
d49 1
d52 3
a74 1
static struct thread_control_block _initial_thread_tcb;
a108 32
/*
 * This sets up the thread base for the initial thread so that it
 * references the errno location provided by libc.  For other threads
 * this is handled by __tfork_thread()
 */
void _rthread_initlib(void) __attribute__((constructor));
void
_rthread_initlib(void)
{
	static int tcb_set;
	struct thread_control_block *tcb;

	if (__predict_false(tcb_set == 0)) {
		tcb_set = 1;

		tcb = __get_tcb();
		if (tcb == NULL) {
			tcb = &_initial_thread_tcb;
			TCB_SET(tcb);
		}

		/* use libc's errno for the main thread */
		TCB_INIT(tcb, &_initial_thread, ___errno());
	}
}

int *
__errno(void)
{
	return (TCB_ERRNOPTR());
}

d122 2
a123 1
	pthread_t self = pthread_self();
d131 1
a131 2
	if ((self->flags & (THREAD_CANCELED|THREAD_CANCEL_ENABLE|THREAD_DYING))
	    != (THREAD_CANCELED|THREAD_CANCEL_ENABLE))
d139 1
a139 1
	if (self->flags & THREAD_CANCEL_DELAY) {
d148 2
a149 1
	if (self->cancel_point || (self->flags & THREAD_CANCEL_DEFERRED) == 0)
d153 26
a178 1
int
d182 1
d185 4
a188 1
	thread->tid = getthrid();
d190 1
a190 2
	thread->flags |= THREAD_CANCEL_ENABLE | THREAD_CANCEL_DEFERRED |
	    THREAD_ORIGINAL | THREAD_INITIAL_STACK;
d200 30
a229 5
	_rthread_initlib();

	_threads_ready = 1;

	_rthread_debug(1, "rthread init\n");
d248 3
a250 1
	return (0);
d256 3
a258 26
	/* _initial_thread is static, so don't free it */
	if (thread != &_initial_thread) {
		/*
		 * thread->tid is written to by __threxit in the thread
		 * itself, so it's not safe to touch it here
		 */
		_spinlock(&_thread_gc_lock);
		TAILQ_INSERT_TAIL(&_thread_gc_list, thread, waiting);
		_spinunlock(&_thread_gc_lock);
	}
}

void
_rthread_setflag(pthread_t thread, int flag)
{
	_spinlock(&thread->flags_lock);
	thread->flags |= flag;
	_spinunlock(&thread->flags_lock);
}

void
_rthread_clearflag(pthread_t thread, int flag)
{
	_spinlock(&thread->flags_lock);
	thread->flags &= ~flag;
	_spinunlock(&thread->flags_lock);
d268 1
a268 2
		if (_rthread_init())
			return (NULL);
d270 1
a270 1
	return (TCB_THREAD());
d282 1
a282 1
		if (thread->tid != 0)
d286 11
a296 6
		_rthread_debug(3, "rthread reaping %p stack %p\n",
		    (void *)thread, (void *)thread->stack);
		_rthread_free_stack(thread->stack);
		_rtld_free_tls(thread->tcb,
		    sizeof(struct thread_control_block), sizeof(void *));
		free(thread);
d306 6
a311 1
	pthread_t thread = pthread_self();
d313 1
a313 1
	if (thread->flags & THREAD_DYING) {
d318 2
a319 1
		_exit(42);
d322 1
a322 1
	_rthread_setflag(thread, THREAD_DYING);
d347 1
a347 1
	__threxit(&thread->tid);
d356 7
a362 1
	pthread_t self = pthread_self();
d365 1
a365 1
	_enter_delayed_cancel(self);
d386 1
a386 1
	_leave_delayed_cancel(self, e);
d416 1
a416 1
	struct thread_control_block *tcb;
d419 1
a419 1
	int rc = 0;
d422 1
a422 2
		if ((rc = _rthread_init()))
		    return (rc);
d426 6
a431 3
	thread = calloc(1, sizeof(*thread));
	if (!thread)
		return (errno);
d436 1
a436 1
	thread->tid = -1;
a446 1
	thread->flags |= THREAD_CANCEL_ENABLE|THREAD_CANCEL_DEFERRED;
d454 2
a455 10
	tcb = _rtld_allocate_tls(NULL, sizeof(*tcb), sizeof(void *));
	if (tcb == NULL) {
		rc = errno;
		goto fail2;
	}
	TCB_INIT(tcb, thread, &thread->myerrno);
	thread->tcb = tcb;

	param.tf_tcb = tcb;
	param.tf_tid = &thread->tid;
a475 2
	_rtld_free_tls(tcb, sizeof(*tcb), sizeof(void *));
fail2:
d478 1
a478 1
	_rthread_free(thread);
d486 2
d490 1
a490 1
	if (thrkill(thread->tid, sig, thread->tcb))
d504 6
a509 1
	pid_t tid;
d511 2
a512 9
	_spinlock(&thread->flags_lock);
	tid = thread->tid;
	if ((thread->flags & (THREAD_DYING | THREAD_CANCELED)) == 0 &&
	    tid != 0) {
		thread->flags |= THREAD_CANCELED;

		if (thread->flags & THREAD_CANCEL_ENABLE) {
			_spinunlock(&thread->flags_lock);
			thrkill(tid, SIGTHR, thread->tcb);
a515 1
	_spinunlock(&thread->flags_lock);
d522 3
a524 2
	if ((pthread_self()->flags & (THREAD_CANCELED|THREAD_CANCEL_ENABLE)) ==
	    (THREAD_CANCELED|THREAD_CANCEL_ENABLE))
a525 1

d531 1
a531 1
	pthread_t self = pthread_self();
d534 2
a535 2
	oldstate = self->flags & THREAD_CANCEL_ENABLE ?
	    PTHREAD_CANCEL_ENABLE : PTHREAD_CANCEL_DISABLE;
d537 1
a537 1
		_rthread_setflag(self, THREAD_CANCEL_ENABLE);
d539 1
a539 1
		_rthread_clearflag(self, THREAD_CANCEL_ENABLE);
d553 1
a553 1
	pthread_t self = pthread_self();
d556 2
a557 2
	oldtype = self->flags & THREAD_CANCEL_DEFERRED ?
	    PTHREAD_CANCEL_DEFERRED : PTHREAD_CANCEL_ASYNCHRONOUS;
d559 1
a559 1
		_rthread_setflag(self, THREAD_CANCEL_DEFERRED);
d561 1
a561 1
		_rthread_clearflag(self, THREAD_CANCEL_DEFERRED);
d626 2
a627 2
		printf("thread %d flags %d name %s\n",
		    thread->tid, thread->flags, thread->name);
a686 64


/*
 * XXX: Bogus type signature, but we only need to be able to emit a
 * reference to it below.
 */
extern void __cerror(void);

/*
 * All weak references used within libc that are redefined in libpthread
 * MUST be in this table.   This is necessary to force the proper version to
 * be used when linking -static.
 */
static void *__libc_overrides[] __used = {
	&__cerror,
	&__errno,
	&_thread_arc4_lock,
	&_thread_arc4_unlock,
	&_thread_atexit_lock,
	&_thread_atexit_unlock,
	&_thread_atfork_lock,
	&_thread_atfork_unlock,
	&_thread_malloc_lock,
	&_thread_malloc_unlock,
	&_thread_mutex_destroy,
	&_thread_mutex_lock,
	&_thread_mutex_unlock,
	&_thread_tag_lock,
	&_thread_tag_storage,
	&_thread_tag_unlock,
	&accept,
	&close,
	&closefrom,
	&connect,
	&fcntl,
	&flockfile,
	&fork,
	&fsync,
	&ftrylockfile,
	&funlockfile,
	&msgrcv,
	&msgsnd,
	&msync,
	&nanosleep,
	&open,
	&openat,
	&poll,
	&pread,
	&preadv,
	&pwrite,
	&pwritev,
	&read,
	&readv,
	&recvfrom,
	&recvmsg,
	&select,
	&sendmsg,
	&sendto,
	&sigsuspend,
	&vfork,
	&wait4,
	&write,
	&writev,
};
@


1.89
log
@Eliminate the need to explicitly invoke syscalls via their _thread_sys_*
aliases by using a macro REDIRECT_SYSCALL() to map the symbols.  Apply
that to getthrid(), sysctl(), and issetugid() as well.

ok mpi@@ beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.88 2016/03/20 02:30:28 guenther Exp $ */
d275 1
d342 1
d544 1
@


1.88
log
@Prepare for future ld.so/libc bump: update <tib.h> with the definitions
that will be needed and make libpthread work when ld.so/libc.a provide an
initial TIB.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.87 2015/11/10 04:30:59 guenther Exp $ */
d50 15
d225 1
a225 1
	_thread_sys_sigaction(SIGTHR, &sa, NULL);
d227 1
a227 1
	_thread_sys_sigprocmask(SIG_UNBLOCK, &sa.sa_mask, NULL);
d481 1
a481 1
	if (_thread_sys_thrkill(thread->tid, sig, thread->tcb))
d505 1
a505 1
			_thread_sys_thrkill(tid, SIGTHR, thread->tcb);
@


1.87
log
@Split the intra-thread functionality from kill(2) into its own syscall
thrkill(2), rolling the kill(2) syscall number with the ABI change to
avoid breaking binaries during during the transition.  thrkill(2) includes
a 'tcb' argument that eliminates the need for locking in pthread_kill()
and simplifies pthread_cancel().  Switch __stack_smash_handler() to use
thrkill(2) and explicitly unblock SIGABRT.

Minor bump to both libc and libpthread: make sure you install a new kernel!

ok semarie@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.86 2015/11/01 03:52:17 guenther Exp $ */
d109 1
a109 1
	if (__predict_false(tcb_set == 0) && __get_tcb() == NULL) {
d112 6
a118 1
		tcb = &_initial_thread_tcb;
a119 1
		TCB_SET(tcb);
a120 14
}

/*
 * This is invoked by ___start() in crt0.  Eventually, when ld.so handles
 * TCB setup for dynamic executables, this will only be called to handle
 * the TCB setup for static executables and may migrate to libc.  The
 * envp argument is so that it can (someday) use that to find the Auxinfo
 * array and thus the ELF phdr and the PT_TLS info.
 */
void __init_tcb(char **_envp);
void
__init_tcb(__unused char **envp)
{
	_rthread_initlib();
@


1.86
log
@delete old lint ARGSUSED comments
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.85 2015/10/23 04:39:24 guenther Exp $ */
d286 1
a286 1
		_rtld_free_tls(thread->arg,
a322 5
#ifdef TCB_GET
	thread->arg = TCB_GET();
#else
	thread->arg = __get_tcb();
#endif
d438 1
d474 5
a478 21
	pid_t tid;
	int ret;

	/* killing myself?  do it without locking */
	if (thread == TCB_THREAD())
		return (kill(thread->tid, sig) == 0 ? 0 : errno);

	/* block the other thread from exiting */
	_spinlock(&thread->flags_lock);
	if (thread->flags & THREAD_DYING)
		ret = (thread->flags & THREAD_DETACHED) ? ESRCH : 0;
	else {
		tid = thread->tid;
		if (tid == 0) {
			/* should be impossible without DYING being set */
			ret = ESRCH;
		} else
			ret = kill(tid, sig) == 0 ? 0 : errno;
	}
	_spinunlock(&thread->flags_lock);
	return (ret);
d499 3
a501 9

			/* canceling myself?  release the lock first */
			if (thread == TCB_THREAD()) {
				_spinunlock(&thread->flags_lock);
				kill(tid, SIGTHR);
				return (0);
			}

			kill(tid, SIGTHR);
@


1.85
log
@Merge the sigaction() and sigprocmask() overloads/wrappers from libpthread
into libc, and move pthread_sigmask() as well (just a trivial wrapper).
This provides consistent handling of SIGTHR between single- and multi-threaded
programs and is a step in the merge of all the libpthread overloads, providing
some ASM and Makefile bits that the other wrappers will need.

ok deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.84 2015/10/18 08:02:58 guenther Exp $ */
a148 1
/* ARGSUSED0 */
@


1.84
log
@ld.so no longer needs or uses a bind lock, so stop setting it.  This
eliminates a chunk of complexity from the libpthread init and the fork
wrapper, as it was the bind lock that needed prebinding before use.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.83 2015/05/19 20:50:06 guenther Exp $ */
d223 1
a223 1
	sigprocmask(SIG_UNBLOCK, &sa.sa_mask, NULL);
a759 2
	&sigaction,
	&sigprocmask,
@


1.83
log
@Instead of testing for __ELF__ and/or vax, leave out the bits for interfacing
with ld.so locking whenever building NOPIC

pointless use of __ELF__ noted by brad@@
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.82 2015/05/10 18:33:15 guenther Exp $ */
a209 10
		/*
		 * To avoid recursion problems in ld.so, we need to trigger the
		 * functions once to fully bind them before registering them
		 * for use.
		 */
		_rthread_dl_lock(0);
		_rthread_dl_lock(1);
		_rthread_bind_lock(0);
		_rthread_bind_lock(1);
		sched_yield();
a210 1
		dlctl(NULL, DL_SETBINDLCK, _rthread_bind_lock);
a699 11
}

void
_rthread_bind_lock(int what)
{
	static struct _spinlock lock = _SPINLOCK_UNLOCKED;

	if (what == 0)
		_spinlock(&lock);
	else
		_spinunlock(&lock);
@


1.82
log
@In the child after fork, the dl lock has to be forced as its inner spinlock
may have been grabbed by another thread in the parent before the fork

problem report from dcoppa@@, ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.81 2015/04/29 06:01:37 guenther Exp $ */
d29 1
a29 1
#if defined(__ELF__)
d208 1
a208 1
#if defined(__ELF__) && !defined(__vax__)
d657 1
a657 1
#if defined(__ELF__)
a724 5
#ifdef __ELF__
#define CERROR_SYMBOL __cerror
#else
#define CERROR_SYMBOL _cerror
#endif
d730 1
a730 1
extern void CERROR_SYMBOL(void);
d738 1
a738 1
	&CERROR_SYMBOL,
@


1.81
log
@Delete the duplicated sched_{policy,param} members from the internal struct
pthread and instead use the values from the embedded struct pthread_attr.
For bonus points, pay attention to the sched_inherit attribute and possibly
set the values from the parent thread.

Problem noted by natano of bitrig.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.80 2015/04/07 01:27:07 guenther Exp $ */
d674 1
a674 2
	if (what == 0)
	{
d691 1
a691 3
	}
	else
	{
d704 6
@


1.80
log
@Make pthread_atfork() track the DSO that called it like atexit() does,
unregistering callbacks if the DSO is unloaded.  Move the callback
handling from libpthread to libc, though libpthread still overrides the
inner call to handle locking and thread-library reinitialization.
Major version bump for both libc and libpthread.

verification that this fixes various ports ajacoutot@@
asm assistance miod@@; ok millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.79 2014/11/16 05:26:20 guenther Exp $ */
d200 1
d433 6
@


1.79
log
@Don't restart syscalls on SIGTHR.  When a cancellation happens, we need to
unroll to the cancellation check

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.78 2014/07/01 03:32:18 guenther Exp $ */
d739 2
@


1.78
log
@Use a flag on the pthread_t to indicate that the thread's stack was
allocated by the kernel's execve bits.

ok matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.77 2014/03/16 18:38:30 guenther Exp $ */
a229 1
	sa.sa_flags = SA_RESTART;
@


1.77
log
@lint is dead (long live the lint!), so stop using it as a cpp conditional
(namespace pollution!) or talking about its opinion on code.

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.76 2013/12/12 08:12:08 guenther Exp $ */
d61 2
a62 2
struct pthread _initial_thread;
struct thread_control_block _initial_thread_tcb;
d192 1
a192 1
	    THREAD_ORIGINAL;
@


1.76
log
@Fix static linking of libpthread: have crt0 invoke __init_tcb() if it's
defined and we don't think ld.so has already done the TCB setup.

ok and much discussion miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.75 2013/11/29 16:27:40 guenther Exp $ */
a64 1
#ifndef lint
a72 3
#else
	0
#endif
@


1.75
log
@Don't try to reuse _initial_thread in the fork() wrapper, as the
thread's existing handle must continue to be valid and it didn't
fully 'change' the thread handle anyway.  For pthread_main_np(),
use a new flag, THREAD_ORIGINAL, to indicate that the flagged thread
is the original thread for *this* process.

Fixes some ConsoleKit failures according to aja@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.74 2013/10/23 05:59:46 guenther Exp $ */
d107 2
a108 1
void _rthread_initlib(void)
d110 12
a121 1
	struct thread_control_block *tcb = &_initial_thread_tcb;
d123 12
a134 3
	/* use libc's errno for the main thread */
	TCB_INIT(tcb, &_initial_thread, ___errno());
	TCB_SET(tcb);
d204 2
@


1.74
log
@In pthread_kill()/pthread_cancel(), hold the target thread's flags
lock across the kill, so that it can't exit (and its tid be reused!)
before the kill()

discussed with tedu@@ marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.73 2013/07/30 15:31:01 guenther Exp $ */
d174 2
a175 1
	thread->flags |= THREAD_CANCEL_ENABLE|THREAD_CANCEL_DEFERRED;
d223 1
a223 1
	/* initial_thread.tid must remain valid */
@


1.73
log
@Stop overwriting the dying thread's struct thread as its tid member may
be concurrently updated by __threxit()

prompted by a report from Alf Schlichting (a.schlichting (at) lemarit.com)
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.72 2013/07/05 21:10:50 miod Exp $ */
d464 21
a484 1
	return (kill(thread->tid, sig) == 0 ? 0 : errno);
d496 1
d498 19
a516 3
	_rthread_setflag(thread, THREAD_CANCELED);
	if (thread->flags & THREAD_CANCEL_ENABLE)
		kill(thread->tid, SIGTHR);
@


1.72
log
@VAX ELF userland bits. Consists mostly of register prefix additions.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.71 2013/06/01 23:06:26 tedu Exp $ */
a221 1
	/* catch wrongdoers for the moment */
d224 4
a227 9
		struct stack *stack = thread->stack;
		pid_t tid = thread->tid;
		void *arg = thread->arg;

		/* catch wrongdoers for the moment */
		memset(thread, 0xd0, sizeof(*thread));
		thread->stack = stack;
		thread->tid = tid;
		thread->arg = arg;
@


1.71
log
@something's not quite right yet. ticket locks result in more CPU usage
and spinning in kernel. partially back out, but in a way that makes going
forward again easy.
seen by ajacoutot
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.70 2013/06/01 20:47:40 tedu Exp $ */
d187 1
a187 1
#if defined(__ELF__)
@


1.70
log
@cleanup and consolidate the spinlock_lock (what a name!) code.
it's now atomic_lock to better reflect its usage, and librthread now
features a new spinlock that's really a ticket lock.
thrlseep can handle both types of lock via a flag in the clock arg.
(temp back compat hack)
remove some old stuff that's accumulated along the way and no longer used.
some feedback from dlg, who is concerned with all things ticket lock.
(you need to boot a new kernel before installing librthread)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.69 2013/04/06 04:25:01 tedu Exp $ */
d85 1
a85 3
	uint32_t me;

	while (_atomic_lock(&lock->atomiclock))
a86 11
	me = lock->waiter++;
	lock->atomiclock = _ATOMIC_LOCK_UNLOCKED;
	while (me != lock->ready) {
		if (me < lock->ready) {
			_rthread_debug(0, "SPINLOCK FAIL: %d %d\n",
			    me, lock->ready);
			_exit(1);
		}
		if (me > lock->ready + 1)
			sched_yield();
	}
d92 1
a92 11
	int gotit = 0;

	while (_atomic_lock(&lock->atomiclock))
		sched_yield();
	if (lock->waiter == lock->ready) {
		lock->waiter++;
		gotit = 1;
	}
	lock->atomiclock = _ATOMIC_LOCK_UNLOCKED;

	return gotit;
d98 1
a98 1
	lock->ready++;
d628 2
a629 1
				__thrsleep(self, 0 | 0x8, NULL, &lock.ready, NULL);
@


1.69
log
@fix race when exiting a detached thread. observed by and ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.68 2013/02/15 22:01:24 guenther Exp $ */
d52 2
d57 1
a57 1
_spinlock_lock_t _thread_lock = _SPINLOCK_UNLOCKED;
d60 1
a60 1
static _spinlock_lock_t _thread_gc_lock = _SPINLOCK_UNLOCKED;
d83 21
a103 1
_spinlock(_spinlock_lock_t *lock)
d105 1
d107 1
a107 1
	while (_atomic_lock(lock))
d109 7
d119 1
a119 1
_spinunlock(_spinlock_lock_t *lock)
d121 1
a121 2

	*lock = _SPINLOCK_UNLOCKED;
d196 1
a196 1
	thread->donesem.lock = _SPINLOCK_UNLOCKED;
d198 1
a198 1
	thread->flags_lock = _SPINLOCK_UNLOCKED;
d435 2
a436 2
	thread->donesem.lock = _SPINLOCK_UNLOCKED;
	thread->flags_lock = _SPINLOCK_UNLOCKED;
d635 1
a635 1
	static _spinlock_lock_t lock = _SPINLOCK_UNLOCKED;
d651 1
a651 1
				__thrsleep(self, 0, NULL, &lock, NULL);
d678 1
a678 1
	static _spinlock_lock_t lock = _SPINLOCK_UNLOCKED;
@


1.68
log
@Revert previous diff: sparc and sparc64 don't set the TCB to NULL in exec, yet,
and vax doesn't support symbols that are both weak and undefined (yet?).

sparc issue diagnosed by kettenis@@, vax problem found by todd@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.67 2013/02/14 03:38:15 guenther Exp $ */
d320 3
a322 1
	if (thread->flags & THREAD_DETACHED)
d324 3
a326 2
	else {
		_rthread_setflag(thread, THREAD_DONE);
@


1.67
log
@Make libpthread compatible with an ld.so that does TCB allocation:
if the initial thread already has a TCB then don't override it, and
if the _dl_allocate_tls() and _dl_free_tls() symbols exist then use
them instead of malloc/free

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.66 2012/08/22 23:43:32 matthew Exp $ */
a49 3
#pragma weak _dl_allocate_tls
#pragma weak _dl_free_tls

d103 1
a103 11
	struct thread_control_block *tcb = __get_tcb();
 
	/*
	 * A newer ld.so may provide a TCB allocation for us.
	 * If not, use a static allocation
	 */
	if (tcb == NULL) {
		tcb = &_initial_thread_tcb;
		tcb->tcb_dtv = 0;
		TCB_SET(tcb);
	}
d107 1
a262 24
static inline struct thread_control_block *
allocate_tcb(void)
{
	struct thread_control_block *tcb;

	if (&_dl_allocate_tls != 0)
		tcb = _dl_allocate_tls(NULL);
	else {
		tcb = malloc(sizeof(*tcb));
		tcb->tcb_dtv = 0;
	}
	return (tcb);
}

static inline void
free_tcb(struct thread_control_block *tcb)
{
	if (&_dl_free_tls != 0)
		_dl_free_tls(tcb);
	else
		free(tcb);
}


d278 2
a279 1
		free_tcb(thread->arg);
d420 1
a420 1
	tcb = allocate_tcb();
d449 1
a449 1
	free_tcb(tcb);
@


1.66
log
@We want to check that the dynamic linker is available at run-time, so
we should actually check for _DYNAMIC at run-time rather than checking
for __PIC__ at compile time, since the two are actually independent.

Problem and solution identified by guenther; minor tweaks by me.
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.65 2012/08/22 22:34:57 matthew Exp $ */
d50 3
d106 11
a116 1
	struct thread_control_block *tcb = &_initial_thread_tcb;
a119 1
	TCB_SET(tcb);
d275 24
d314 1
a314 2
		_rtld_free_tls(thread->arg,
		    sizeof(struct thread_control_block), sizeof(void *));
d455 1
a455 1
	tcb = _rtld_allocate_tls(NULL, sizeof(*tcb), sizeof(void *));
d484 1
a484 1
	_rtld_free_tls(tcb, sizeof(*tcb), sizeof(void *));
@


1.65
log
@Test for __PIC__ instead of PIC in preparation for eliminating -DPIC
from bsd.lib.mk for C source files.

ok deraadt, pascal
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.64 2012/08/15 17:07:49 matthew Exp $ */
d29 4
d181 15
a195 13
#if defined(__ELF__) && defined(__PIC__)
	/*
	 * To avoid recursion problems in ld.so, we need to trigger the
	 * functions once to fully bind them before registering them
	 * for use.
	 */
	_rthread_dl_lock(0);
	_rthread_dl_lock(1);
	_rthread_bind_lock(0);
	_rthread_bind_lock(1);
	sched_yield();
	dlctl(NULL, DL_SETTHREADLCK, _rthread_dl_lock);
	dlctl(NULL, DL_SETBINDLCK, _rthread_bind_lock);
d591 1
a591 1
#if defined(__ELF__) && defined(__PIC__)
@


1.64
log
@Oops, on a.out architectures __cerror() is called _cerror().  Fix
accordingly so vax can build again.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.63 2012/08/13 04:52:42 matthew Exp $ */
d177 1
a177 1
#if defined(__ELF__) && defined(PIC)
d585 1
a585 1
#if defined(__ELF__) && defined(PIC)
@


1.63
log
@Add explicit references from rthread.o to all of the weak symbol
overrides provided by libpthread.a.  This ensures that statically
linked threaded programs use (e.g.) __cerror() from libpthread.a
instead of libc.a.  (Same idea previously used by libuthread.)

Thanks to fgsch@@ for pointing out libuthread's solution to the static
linking problem.

ok guenther@@, tedu@@;
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.62 2012/06/21 00:56:59 guenther Exp $ */
d649 6
d659 1
a659 1
extern void __cerror(void);
d667 1
a667 1
	&__cerror,
@


1.62
log
@__tfork() needs to set the stack address of the new thread in the kernel,
so that it can't get a signal while still running on the parent thread's
stack.  Also, pass in sizeof(struct __tfork) to provide forward compat
when more members are added.  This is an ABI change, so switch syscall
numbers and bump lib majors this time.

ok deraadt@@ matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.61 2012/04/10 21:10:45 guenther Exp $ */
d24 5
d37 2
d648 63
@


1.61
log
@pthread_setcanceltype() shouldn't be a cancelation point

ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.60 2012/03/22 15:26:04 kurt Exp $ */
a50 2
int __tfork_thread(const struct __tfork *, void *, void (*)(void *), void *);

d87 1
a87 1
 * this is handled by the block in _rthread_start().
d416 1
a416 1
	param.tf_flags = 0;
d424 1
a424 1
	rc = __tfork_thread(&param, thread->stack->sp, _rthread_start, thread);
@


1.60
log
@Remove pthread_suspend* and related functions. This is part of the
rthreads major library bump from last night. okay kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.59 2012/03/20 00:47:23 guenther Exp $ */
a508 1
		pthread_testcancel();
@


1.59
log
@Permit recursive locking in _rthread_dl_lock(), as an so's destructor
may need to call dlclose().

problem observed by Antti Harri (iku at openbsd.fi), ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.58 2012/03/14 21:23:37 guenther Exp $ */
a62 1
	.create_suspended		= 0,
@


1.58
log
@Force sched_yield() to be resolved before calling dlctl(DL_SET*LCK) with
a function that can call sched_yield(), to avoid recursion

ok kurt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.57 2012/03/03 10:02:26 guenther Exp $ */
d583 8
d595 3
d600 4
d605 12
d618 14
a631 1
		_spinunlock(&lock);
@


1.57
log
@Add sem_timewait() and fix sem_wait()'s handling of signals, so
that it resumes waiting unless the thread was canceled.  As part
of this, change the internal _sem_wait() function to return zero
on success and an errno value on failure instead of 1 on success
and zero on failure.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.56 2012/03/02 17:49:58 fgsch Exp $ */
d183 1
@


1.56
log
@for readability, put the label on it's own line.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.55 2012/02/24 05:37:51 guenther Exp $ */
d323 1
a323 1
	int e, r;
d326 1
a326 1
	e = r = 0;
d334 2
a335 1
	else if ((r = _sem_wait(&thread->donesem, 0, &self->delayed_cancel))) {
d348 1
a348 1
	_leave_delayed_cancel(self, !r);
@


1.55
log
@sched_yield() is the standard name while pthread_yield() is the
non-standard alias, so prefer the former
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.54 2012/02/19 04:47:49 guenther Exp $ */
d257 2
a258 1
restart:_spinlock(&_thread_gc_lock);
@


1.54
log
@Use a form of designated initializer that works with gcc2
lint doesn't understand designated initializers, so hide them from it
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.53 2012/02/18 22:03:21 guenther Exp $ */
d77 1
a77 1
		pthread_yield();
@


1.53
log
@Fix previous commit: _rthread_init() was static.
Improve consistency of error naming
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.52 2012/02/18 21:12:09 guenther Exp $ */
d54 1
d61 1
a61 1
	.sched_param.sched_priority	= 0,
d64 3
d118 1
@


1.52
log
@Fix the handling of the stackaddr, stacksize, and guardsize attributes:
don't try to merge values, round the sizes separately, and don't try to
unmap application-supplied stacks.
Copy from uthread the caching of default-sized stacks.
Have pthread_attr_init() and pthread_create() get the default attributes
from staticly allocated pthread_attr_t.
Cache the pagesize in _rthread_init() and provide a macro for rounding to it

based on suggestions from kettenis@@ and tedu@@, ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.51 2012/02/16 20:55:09 kettenis Exp $ */
d147 1
a147 1
static int
@


1.51
log
@Set __isthreaded in pthread_create() instead of _rthread_init() such that it
properly represents whether a process is multi-threaded or not.  This fixes
a bug where if a forked child would create a new thread we would not set
__isthreaded.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.50 2012/01/17 02:34:18 guenther Exp $ */
d42 1
d53 12
d161 3
d391 1
a391 7
	if (attr)
		thread->attr = *(*attr);
	else {
		thread->attr.stack_size = RTHREAD_STACK_SIZE_DEF;
		thread->attr.guard_size = sysconf(_SC_PAGESIZE);
		thread->attr.stack_size -= thread->attr.guard_size;
	}
@


1.50
log
@Reimplement mutexes, condvars, and rwlocks to eliminate bugs,
particularly the "consume the signal you just sent" hang, and putting
the wait queues in userspace.

Do cancellation handling in pthread_cond_*wait(), pthread_join(),
and sem_wait().

Add __ prefix to thr{sleep,wakeup,exit,sigdivert}() syscalls; add
'abort" argument to thrsleep to close cancellation race; make
thr{sleep,wakeup} return errno values via *retval to avoid touching
userspace errno.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.49 2011/12/28 04:59:31 guenther Exp $ */
a137 1
	extern int __isthreaded;
a148 1
	__isthreaded = 1;
d354 1
d407 2
@


1.49
log
@pthread_self() may be much cheaper and never more expensive than getthrid()
so prefer it for identifying the current thread
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.48 2011/12/27 17:33:21 guenther Exp $ */
d106 25
a130 3
	if ((self->flags & (THREAD_CANCELED | THREAD_CANCEL_COND)) ==
	    THREAD_CANCELED && (self->cancel_point ||
	    (self->flags & THREAD_CANCEL_DEFERRED) == 0))
d148 1
a148 1
	_rthread_debug(1, "rthread init\n");
d152 2
d204 1
a204 1
static void
d212 1
a212 1
static void
d296 1
a296 1
	threxit(&thread->tid);
d303 1
a303 1
	int e;
d306 2
d314 1
a314 2
	else {
		_sem_wait(&thread->donesem, 0);
d317 6
a322 4
		e = 0;
		/* We should be the last having a ref to this thread, but
		 * someone stupid or evil might haved detached it;
		 * in that case the thread will cleanup itself */
d327 1
a469 1
		pthread_testcancel();
@


1.48
log
@On failure, pthread_kill() should return the errno, not -1
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.47 2011/12/05 06:56:09 guenther Exp $ */
d280 1
d284 1
a284 1
	else if (thread->tid == getthrid())
@


1.47
log
@Mark sigthr()'s sig argument as __unused
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.46 2011/12/05 04:02:03 guenther Exp $ */
d403 1
a403 1
	return (kill(thread->tid, sig));
@


1.46
log
@Implement cancelation for the basic syscall cancelation points,
using previously allocated SIGTHR to interrupt in-process syscalls
and fixing the spelling of "cancelled" along the way.
Modeled on FreeBSD's libthr
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.45 2011/11/09 10:28:01 guenther Exp $ */
d102 1
a102 1
sigthr_handler(int sig)
@


1.45
log
@Oh yeah, with TLS-lite we can get the thread handle without walking the
thread list

reminded by dhill@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.44 2011/11/06 11:48:58 guenther Exp $ */
d101 11
d117 1
d144 12
d237 10
d416 3
a418 1
	_rthread_setflag(thread, THREAD_CANCELLED);
d425 2
a426 2
	if ((pthread_self()->flags & (THREAD_CANCELLED|THREAD_CANCEL_ENABLE)) ==
	    (THREAD_CANCELLED|THREAD_CANCEL_ENABLE))
@


1.44
log
@Move <machine/spinlock.h> into rthread.h; strip out unnecessary #includes
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.43 2011/10/17 06:39:20 guenther Exp $ */
a90 15

static pthread_t
_rthread_findself(void)
{
	pthread_t me;
	pid_t tid = getthrid();

	LIST_FOREACH(me, &_thread_list, threads) 
		if (me->tid == tid)
			break;

	return (me);
}


a177 2
	pthread_t thread;

d182 1
a182 5
	_spinlock(&_thread_lock);
	thread = _rthread_findself();
	_spinunlock(&_thread_lock);

	return (thread);
@


1.43
log
@Use __tfork, __get_tcb, and __set_tcb to have a real TCB and per-thread
errno.  The ASM bits for _cerror are sketchy or missing for some archs
but that can be corrected in-tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.42 2009/11/27 19:45:54 guenther Exp $ */
d23 1
a23 4
#include <sys/param.h>
#include <sys/event.h>
#include <sys/mman.h>
#include <sys/wait.h>
a24 3
#include <machine/spinlock.h>

#include <fcntl.h>
a29 1
#include <err.h>
@


1.42
log
@Convert thrsleep() to an absolute timeout with clockid to eliminate a
race condition and prep for later support of pthread_condattr_setclock()

"get it in" deraadt@@, tedu@@, cheers by others
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.41 2009/11/27 19:42:24 guenther Exp $ */
d44 1
d55 1
d57 1
a57 1
int rfork_thread(int, void *, void (*)(void *), void *);
d77 22
a118 3
	/* ensure parent returns from rfork, sets up tid */
	_spinlock(&_thread_lock);
	_spinunlock(&_thread_lock);
d165 1
d171 1
d227 2
a238 2
	pid_t tid;
	struct stack *stack;
d254 5
a258 2
	stack = thread->stack;
	tid = thread->tid;
d321 1
d323 1
a323 1
	pid_t tid;
d339 2
d350 1
a350 2

	_spinlock(&_thread_lock);
a356 1
	LIST_INSERT_HEAD(&_thread_list, thread, threads);
d358 2
a359 3
	tid = rfork_thread(RFPROC | RFTHREAD | RFMEM | RFNOWAIT,
	    thread->stack->sp, _rthread_start, thread);
	if (tid == -1) {
d363 1
a363 4
	/* new thread will appear _rthread_start */
	thread->tid = tid;
	thread->flags |= THREAD_CANCEL_ENABLE|THREAD_CANCEL_DEFERRED;
	*threadp = thread;
d365 3
a367 6
	/*
	 * Since _rthread_start() aquires the thread lock and due to the way
	 * signal delivery is implemented, this is not a race.
	 */
	if (thread->attr.create_suspended)
		kill(thread->tid, SIGSTOP);
d369 2
d373 8
a380 1
	return (0);
d382 4
a387 1
	LIST_REMOVE(thread, threads);
a388 1
	_spinunlock(&_thread_lock);
@


1.41
log
@Change threxit() to take a pointer to a pid_t to zero out from the
kernel so that librthread can detect when a thread is completely
done with its stack without need a kqueue.  The dying thread moves
itself to a GC list, other threads scan the GC list on pthread_create()
and pthread_join() and free the stack and handle once the thread's
thread id is zeroed.

"get it in" deraadt@@, tedu@@, cheers by others
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.40 2009/02/20 01:24:05 tedu Exp $ */
d256 1
a256 1
		_sem_wait(&thread->donesem, 0, 0);
@


1.40
log
@Fix a race in the reaper discovered by Tobias Ulmer.  kevents are identified by pid,
so in the event that two threads get the same pid in a row, as the second is dying it
will update (not add) the kevent for the previous thread with its own stack, which it
will then unmap soon after, which is bad.  Doing the reaping first guarantees that there
are no kevents with the same pid as the exiting thread when it registers itself.
ok guenther kurt
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.39 2008/10/13 05:42:46 kevlo Exp $ */
d50 3
a101 10
int
_rthread_open_kqueue(void)
{
	_rthread_kq = kqueue();
	if (_rthread_kq == -1)
		return 1;
	fcntl(_rthread_kq, F_SETFD, FD_CLOEXEC);
	return 0;
}

a113 2
	if (_rthread_open_kqueue())
		return (errno);
d140 1
d142 4
a145 1
		/* initial_thread.tid must remain valid */
d147 5
a151 1
		free(thread);
d190 20
d219 1
a219 1
	
d240 1
a240 6
	/* reap before adding self, we don't want to disappear too soon */
	_rthread_reaper();
	if (tid != _initial_thread.tid)
		_rthread_add_to_reaper(tid, stack);

	threxit(0);
d302 2
@


1.39
log
@use calloc() instead of malloc() and memset()

"look good" tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.38 2008/08/14 05:57:06 guenther Exp $ */
d221 2
a225 1
	_rthread_reaper();
@


1.38
log
@Match libpthread's behavior and make pthread_join(NULL, whatever) fail
instead of crashing

ok kurt@@
first observed by Jung <moorang at gmail.com>
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.37 2008/08/14 05:20:44 guenther Exp $ */
d288 1
a288 1
	thread = malloc(sizeof(*thread));
a290 1
	memset(thread, 0, sizeof(*thread));
d425 1
a425 1
	clfn = malloc(sizeof(*clfn));
a427 1
	memset(clfn, 0, sizeof(*clfn));
@


1.37
log
@Fix 5771/library: in pthread_exit(), delay the call to _sem_post() that
unblocks the pthread_join() for this thread until the exiting thread is
completely done with its thread structure, as the joining thread will
free it once unblocked.  Also, don't bother to call _sem_post() if the
thread is detached.

ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.36 2008/08/14 05:15:41 guenther Exp $ */
d234 3
a236 1
	if (thread->tid == getthrid())
@


1.36
log
@If the initial thread calls pthread_exit(), don't overwrite its thread
structure, as the 'tid' member there is used by other parts of
librthread to determine whether the current thread is the initial thread
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.35 2008/06/05 21:06:11 kurt Exp $ */
a211 2
	_sem_post(&thread->donesem);

d216 1
a216 1
	else
d218 2
@


1.35
log
@- Add fork/vfork wrapper functions to reset state in the child process.
- Make an effort to protect important libc and ld.so critical areas during
the fork(2)/vfork(2) sys call.
- Add pthread_atfork(3) implementation based on Daniel Eischen's code.

Original diff by Philip Guenther <guenther at gmail.com> with some
additions and refinements by me. Positive test report from brad@@ with
many kde apps. fork(2) and pthread_atfork(3) pthread regresses pass.
okay tedu@@, kettenis@@, marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.34 2007/05/18 14:36:17 art Exp $ */
d149 3
a151 2
	memset(thread, 0xd0, sizeof(*thread));
	if (thread != &_initial_thread)
d153 1
@


1.34
log
@Register the locking hooks with ld.so.
drahn@@ ok (long time ago)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.33 2006/01/06 07:29:36 marc Exp $ */
d30 1
a53 5
#if defined(__ELF__) && defined(PIC)
static void rthread_dl_lock(int what);
static void rthread_bind_lock(int what);
#endif

d99 10
d121 1
a121 2
	_rthread_kq = kqueue();
	if (_rthread_kq == -1)
d134 6
a139 6
	rthread_dl_lock(0);
	rthread_dl_lock(1);
	rthread_bind_lock(0);
	rthread_bind_lock(1);
	dlctl(NULL, DL_SETTHREADLCK, rthread_dl_lock);
	dlctl(NULL, DL_SETBINDLCK, rthread_bind_lock);
d478 2
a479 2
static void
rthread_dl_lock(int what)
d489 2
a490 2
static void
rthread_bind_lock(int what)
@


1.33
log
@
Initialize thread debug in _rthread_init.   The debug verbosity
can be set in the environment using RTHREAD_DEBUG.
ok, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.32 2006/01/06 06:13:27 tedu Exp $ */
d37 1
d53 5
d123 14
d471 24
@


1.32
log
@guess it's time to remove the init printf; sturm found a port that
didn't like it.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.31 2006/01/05 22:17:17 otto Exp $ */
d112 2
@


1.31
log
@In pthread_join(), check if we create a deadlock trying to join
with ourself and only free thread after a succesful join. ok marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.30 2006/01/05 04:24:30 tedu Exp $ */
a101 2

	printf("rthread init\n");
@


1.30
log
@move malloc lock to libc interface file
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.29 2006/01/05 04:06:48 marc Exp $ */
d207 3
a209 1
	if (thread->flags & THREAD_DETACHED)
d216 5
a221 5
	/* We should be the last having a ref to this thread, but
	 * someone stupid or evil might haved detached it;
	 * in that case the thread will cleanup itself */
	if ((thread->flags & THREAD_DETACHED) == 0)
		_rthread_free(thread);
@


1.29
log
@
add -Wstrict-prototypes -Wmissing-prototypes -Wsign-compare
Minor tweaks to compile with the above, primarily in fixing
the conflicts between semaphore.h and rthread.h
"i like the additional warnings" tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.28 2006/01/04 19:48:52 otto Exp $ */
a447 22
}

/*
 * the malloc lock
 */
static _spinlock_lock_t malloc_lock = _SPINLOCK_UNLOCKED;

void
_thread_malloc_lock(void)
{
	_spinlock(&malloc_lock);
}

void
_thread_malloc_unlock(void)
{
	_spinunlock(&malloc_lock);
}

void
_thread_malloc_init(void)
{
@


1.28
log
@Cleanup struct pthread and stack after thread exits. This version does
not use a separate reaper thread. ok tedu@@ marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.27 2006/01/04 08:48:01 marc Exp $ */
d40 1
d456 1
a456 1
_thread_malloc_lock()
d462 1
a462 1
_thread_malloc_unlock()
d468 1
a468 1
_thread_malloc_init()
@


1.27
log
@
allow threads to be created in a detached state
do not allow a join to a detached thread
"it looks good" otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.26 2006/01/01 19:32:30 marc Exp $ */
d24 1
d107 1
d110 3
d119 25
d167 2
a171 1
	thread->flags |= THREAD_DONE;
a172 1
	_sem_post(&thread->donesem);
d183 5
a187 1
#if 0
d189 8
a196 2
		free(thread);
#endif
d214 5
d220 1
d227 10
a236 6
	_spinlock(&_thread_lock);
#if 0
	if (thread->flags & THREAD_DONE)
		free(thread);
	else
#endif
d238 4
a241 2
	_spinunlock(&_thread_lock);
	return (0);
d261 1
d310 1
a310 1
	free(thread);
d331 1
a331 1
	thread->flags |= THREAD_CANCELLED;
d353 1
a353 1
		self->flags |= THREAD_CANCEL_ENABLE;
d356 1
a356 1
		self->flags &= ~THREAD_CANCEL_ENABLE;
d375 1
a375 1
		self->flags |= THREAD_CANCEL_DEFERRED;
d378 1
a378 1
		self->flags &= ~THREAD_CANCEL_DEFERRED;
@


1.26
log
@
thread stack handling changes.  Add guard zones and allow stack
size (and guard zone size) to be set using pthread_attr.   Guard
zones are specified in bytes, but implemented in terms of a
page size.

OK Otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.25 2005/12/31 08:51:20 otto Exp $ */
d164 1
d166 8
a173 3
	_sem_wait(&thread->donesem, 0, 0);
	if (retval)
		*retval = thread->retval;
d175 1
a175 1
	return (0);
d218 2
@


1.25
log
@Implement suspend/resume and creation of initially suspended threads.
With this, java seems to be operational.  Also make threads_ready
non-static, which is needed for an upcoming diff.  ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.24 2005/12/30 21:51:27 otto Exp $ */
a113 34
static struct stack *
_rthread_alloc_stack(size_t len, void *base)
{
	struct stack *stack;

	stack = malloc(sizeof(*stack));
	if (!stack)
		return (NULL);
	if (base) {
		stack->base = base;
	} else {
		stack->base = mmap(NULL, len, PROT_READ | PROT_WRITE,
		    MAP_ANON, -1, 0);
		if (stack->base == MAP_FAILED) {
			free(stack);
			return (NULL);
		}
	}
#ifdef MACHINE_STACK_GROWS_UP
	stack->sp = (void *)(((size_t)stack->base + 64) & ~63);
#else
	stack->sp = (void *)(((size_t)stack->base + len - 16) & ~15);
#endif
	stack->len = len;
	return (stack);
}

static void
_rthread_free_stack(struct stack *stack)
{
	munmap(stack->base, stack->len);
	free(stack);
}

d207 5
d215 1
a215 1
	thread->stack = _rthread_alloc_stack(64 * 1024, NULL);
@


1.24
log
@Remove exitinng thread from the list of threads. ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.23 2005/12/30 20:35:11 otto Exp $ */
a40 1
static int threads_ready;
d43 2
a44 1
struct listhead _thread_list = LIST_HEAD_INITIALIZER(thread_list);
d108 1
a108 1
	threads_ready = 1;
d156 1
a156 1
	if (!threads_ready)
d228 1
a228 1
	if (!threads_ready)
d239 2
d261 8
@


1.23
log
@use queue.h macros for thread house keeping; make some vars non-static
and fix a bug in thread creation error path. ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.22 2005/12/30 04:10:23 tedu Exp $ */
d184 3
@


1.22
log
@__aligned__ isn't really necessary on the struct, an aligned member
will make the right thing happen.  verified by kettenis and drahn
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.21 2005/12/30 04:05:55 tedu Exp $ */
a41 2
static pthread_t thread_list;
static _spinlock_lock_t thread_lock = _SPINLOCK_UNLOCKED;
d44 2
d74 1
a74 1
	for (me = thread_list; me; me = me->next)
d89 2
a90 2
	_spinlock(&thread_lock);
	_spinunlock(&thread_lock);
d107 1
a107 1
	thread_list = thread;
d160 1
a160 1
	_spinlock(&thread_lock);
d162 1
a162 1
	_spinunlock(&thread_lock);
d206 1
a206 1
	_spinlock(&thread_lock);
d213 1
a213 1
	_spinunlock(&thread_lock);
d237 1
a237 1
	_spinlock(&thread_lock);
d244 1
a244 2
	thread->next = thread_list;
	thread_list = thread;
d256 1
a256 1
	_spinunlock(&thread_lock);
d262 1
d264 1
a264 2
	thread_list = thread->next;
	_spinunlock(&thread_lock);
d397 2
a398 2
	_spinlock(&thread_lock);
	for (thread = thread_list; thread; thread = thread->next)
d401 1
a401 1
	_spinunlock(&thread_lock);
@


1.21
log
@prototype all the thread syscalls in rthread.h for now.
update for new thrwakeup that takes a count argument
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.20 2005/12/29 20:34:22 otto Exp $ */
d46 1
a46 1
struct pthread _initial_thread __attribute__((__aligned__(16)));
@


1.20
log
@Put the existing _np functions into separate file; introduce
pthread_stackseg_np(), from existing pthread lib.
discussed with tedu@@ ok marc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.19 2005/12/29 19:10:20 otto Exp $ */
a47 2
int getthrid(void);
void threxit(int);
@


1.19
log
@implement pthread_main_np(); ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.18 2005/12/23 03:27:06 tedu Exp $ */
d46 1
a46 1
static struct pthread initial_thread __attribute__((__aligned__(16)));
d100 1
a100 1
	pthread_t thread = &initial_thread;
a391 22

/*
 * _np functions
 */
void
pthread_set_name_np(pthread_t thread, char *name)
{
	strlcpy(thread->name, name, sizeof(thread->name));
}

int
pthread_main_np(void)
{
	pthread_t me = pthread_self();

	if (me == NULL)
		return (-1);
	else
		return (me == &initial_thread ? 1 : 0);
}


a405 1

@


1.18
log
@for reasons that do not make any sense whatsoever, _rthread_alloc_stack
must be called with the thread_lock held, or we crash in rfork_thread
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.17 2005/12/22 06:49:48 tedu Exp $ */
d401 12
@


1.17
log
@more consistently use _rthread prefix for all not meant to be exported
interfaces that aren't static, and a few that are but which will change
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.16 2005/12/22 06:33:12 tedu Exp $ */
a234 6
	thread->stack = _rthread_alloc_stack(64 * 1024, NULL);
	if (!thread->stack) {
		rc = errno;
		goto fail1;
	}

d241 5
d260 1
d264 2
a267 2
	_rthread_free_stack(thread->stack);
fail1:
@


1.16
log
@change init code to handle failure.  (actually, make it impossible to fail,
but still have the callers check for good measure).
prompted by miod "err() in a library???"
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.15 2005/12/22 00:37:25 marco Exp $ */
d71 1
a71 1
thread_findself(void)
d85 1
a85 1
thread_start(void *v)
d98 1
a98 1
thread_init(void)
d100 1
a100 1
	pthread_t thread = &inital_thread;
d117 1
a117 1
alloc_stack(size_t len, void *base)
d144 1
a144 1
free_stack(struct stack *stack)
d159 1
a159 1
		if (thread_init())
d163 1
a163 1
	thread = thread_findself();
d185 1
a185 1
	rthread_tls_destructors(thread);
d228 1
a228 1
		if ((rc = thread_init()))
d235 1
a235 1
	thread->stack = alloc_stack(64 * 1024, NULL);
d251 1
a251 1
	    thread->stack->sp, thread_start, thread);
d256 1
a256 1
	/* new thread will appear thread_start */
d266 1
a266 1
	free_stack(thread->stack);
@


1.15
log
@Bad commit, breaks hppa64.  Prompted by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.14 2005/12/21 23:44:56 marco Exp $ */
d46 3
a48 1
int getthrid();
d97 1
a97 1
static void
d100 1
a100 1
	pthread_t thread;
a104 6
	__isthreaded = 1;

	thread = malloc(sizeof(*thread));
	if (!thread) /* should never happen, but have to do something */
		err(1, "rthread_init");
	memset(thread, 0, sizeof(*thread));
d108 1
a108 1
	snprintf(thread->name, sizeof(thread->name), "Main process");
d111 3
d159 2
a160 1
		thread_init();
d228 2
a229 1
		thread_init();
@


1.14
log
@Align hppa stack to 64 bytes.

ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.13 2005/12/21 00:53:28 tedu Exp $ */
d136 1
a136 2
	stack->sp = (void *)(((size_t)stack->base + MACHINE_STACK_ALIGN) &
	    ~(MACHINE_STACK_ALIGN - 1));
@


1.13
log
@make alloc_stack take an optional base, preparation for stack attributes
deal with machines that have upside down stacks
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.12 2005/12/21 00:49:07 tedu Exp $ */
d136 2
a137 1
	stack->sp = (void *)(((size_t)stack->base + 16) & ~15);
@


1.12
log
@check a few remaining mallocs for failure, along with mmap and rfork
started by miod
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.11 2005/12/19 15:41:00 brad Exp $ */
d118 1
a118 1
alloc_stack(size_t len)
d125 9
a133 4
	stack->base = mmap(NULL, len, PROT_READ | PROT_WRITE, MAP_ANON, -1, 0);
	if (stack->base == MAP_FAILED) {
		free(stack);
		return (NULL);
d135 3
d139 1
d234 1
a234 1
	thread->stack = alloc_stack(64 * 1024);
@


1.11
log
@fix rev 1.9

pthread_concurrency -> pthread_setconcurrency
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.10 2005/12/19 06:47:40 tedu Exp $ */
d34 1
d106 2
d123 2
d126 4
d131 1
d135 7
d216 1
d222 2
d225 6
d236 1
a239 2
	thread->stack = alloc_stack(64 * 1024);

d242 4
d251 8
d260 1
a260 1
	return (0);
@


1.10
log
@update copyright to 2005
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.9 2005/12/19 06:45:14 tedu Exp $ */
d337 1
a337 1
pthread_concurrency(int new_level)
@


1.9
log
@add pthread_get/set_concurrency (useless for now)
add pthread_get/set_stack[addr] (info not used yet)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.8 2005/12/18 01:35:06 tedu Exp $ */
d3 1
a3 1
 * Copyright (c) 2004 Ted Unangst <tedu@@openbsd.org>
@


1.8
log
@initialize all spinlocks to _SPINLOCK_UNLOCKED
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.7 2005/12/14 06:07:54 tedu Exp $ */
d43 1
d329 16
@


1.7
log
@add pthread_cleanup_push and pthread_cleanup_pop
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.6 2005/12/14 05:44:49 tedu Exp $ */
d42 1
a42 1
static _spinlock_lock_t thread_lock;
d106 1
d204 1
d357 1
a357 1
static _spinlock_lock_t malloc_lock;
@


1.6
log
@add pthread_kill (just kill(2) really)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.5 2005/12/14 04:43:04 tedu Exp $ */
d144 1
d151 6
d294 31
@


1.5
log
@add bits for pthread_cancel.  we don't really have cancellation points yet
but some of the functions are here now.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.4 2005/12/14 04:01:44 tedu Exp $ */
d214 6
@


1.4
log
@change keys to use table instead of list, makes a sane destructor implementation
possible
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.3 2005/12/13 05:56:55 tedu Exp $ */
d106 1
d209 1
d220 61
@


1.3
log
@correct implementation of pthread_cond_signal.  it doesn't raise the sem
value if there are no waiters.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.2 2005/12/03 18:17:55 tedu Exp $ */
d99 2
a121 1

d147 1
d149 1
@


1.2
log
@syscall is actually sched_yield now, as millert suggested
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread.c,v 1.1 2005/12/03 18:16:19 tedu Exp $ */
d146 1
a146 1
	_sem_wakeup(&thread->donesem);
a159 1
	printf("joined %d %p\n", thread->tid, thread->retval);
a204 1
	printf("new thread %d\n", tid);
@


1.1
log
@add userland thread library.  incomplete, but functional
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a43 1
int yield();
@

