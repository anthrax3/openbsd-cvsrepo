head	1.47;
access;
symbols
	OPENBSD_6_1:1.44.0.4
	OPENBSD_6_1_BASE:1.44
	OPENBSD_6_0:1.42.0.2
	OPENBSD_6_0_BASE:1.42
	OPENBSD_5_9:1.39.0.10
	OPENBSD_5_9_BASE:1.39
	OPENBSD_5_8:1.39.0.12
	OPENBSD_5_8_BASE:1.39
	OPENBSD_5_7:1.39.0.4
	OPENBSD_5_7_BASE:1.39
	OPENBSD_5_6:1.39.0.8
	OPENBSD_5_6_BASE:1.39
	OPENBSD_5_5:1.39.0.6
	OPENBSD_5_5_BASE:1.39
	OPENBSD_5_4:1.39.0.2
	OPENBSD_5_4_BASE:1.39
	OPENBSD_5_3:1.36.0.4
	OPENBSD_5_3_BASE:1.36
	OPENBSD_5_2:1.36.0.2
	OPENBSD_5_2_BASE:1.36
	OPENBSD_5_1_BASE:1.30
	OPENBSD_5_1:1.30.0.2
	OPENBSD_5_0:1.23.0.6
	OPENBSD_5_0_BASE:1.23
	OPENBSD_4_9:1.23.0.4
	OPENBSD_4_9_BASE:1.23
	OPENBSD_4_8:1.23.0.2
	OPENBSD_4_8_BASE:1.23
	OPENBSD_4_7:1.22.0.2
	OPENBSD_4_7_BASE:1.22
	OPENBSD_4_6:1.20.0.6
	OPENBSD_4_6_BASE:1.20
	OPENBSD_4_5:1.20.0.2
	OPENBSD_4_5_BASE:1.20
	OPENBSD_4_4:1.19.0.4
	OPENBSD_4_4_BASE:1.19
	OPENBSD_4_3:1.19.0.2
	OPENBSD_4_3_BASE:1.19
	OPENBSD_4_2:1.18.0.2
	OPENBSD_4_2_BASE:1.18
	OPENBSD_4_1:1.16.0.6
	OPENBSD_4_1_BASE:1.16
	OPENBSD_4_0:1.16.0.4
	OPENBSD_4_0_BASE:1.16
	OPENBSD_3_9:1.16.0.2
	OPENBSD_3_9_BASE:1.16;
locks; strict;
comment	@ * @;


1.47
date	2017.09.05.02.40.54;	author guenther;	state dead;
branches;
next	1.46;
commitid	5DW3WOQF0YGGx8lJ;

1.46
date	2017.07.29.16.42.10;	author deraadt;	state Exp;
branches;
next	1.45;
commitid	ICfLVHfCLvzGEvCp;

1.45
date	2017.07.29.08.36.23;	author pirofti;	state Exp;
branches;
next	1.44;
commitid	LxkQgPLmJBjgc3Mx;

1.44
date	2016.09.04.10.13.35;	author akfaew;	state Exp;
branches;
next	1.43;
commitid	tPNEomz2X1xlRc3u;

1.43
date	2016.09.03.16.44.20;	author akfaew;	state Exp;
branches;
next	1.42;
commitid	A8DISbEBB3jwsSL3;

1.42
date	2016.05.07.19.05.22;	author guenther;	state Exp;
branches;
next	1.41;
commitid	d9R7VGw9CHTkwXE1;

1.41
date	2016.04.15.21.04.04;	author guenther;	state Exp;
branches;
next	1.40;
commitid	jNn7bowRxmTDPNM6;

1.40
date	2016.04.02.19.56.53;	author guenther;	state Exp;
branches;
next	1.39;
commitid	8mfZyQLsoIGIAaFG;

1.39
date	2013.06.01.23.06.26;	author tedu;	state Exp;
branches;
next	1.38;

1.38
date	2013.06.01.20.47.40;	author tedu;	state Exp;
branches;
next	1.37;

1.37
date	2013.06.01.19.47.28;	author tedu;	state Exp;
branches;
next	1.36;

1.36
date	2012.04.14.12.07.49;	author kurt;	state Exp;
branches;
next	1.35;

1.35
date	2012.04.13.13.50.37;	author kurt;	state Exp;
branches;
next	1.34;

1.34
date	2012.04.13.12.39.28;	author kurt;	state Exp;
branches;
next	1.33;

1.33
date	2012.02.28.02.41.56;	author guenther;	state Exp;
branches;
next	1.32;

1.32
date	2012.02.23.07.58.25;	author guenther;	state Exp;
branches;
next	1.31;

1.31
date	2012.02.23.04.43.06;	author guenther;	state Exp;
branches;
next	1.30;

1.30
date	2012.01.25.06.55.08;	author guenther;	state Exp;
branches;
next	1.29;

1.29
date	2012.01.17.02.34.18;	author guenther;	state Exp;
branches;
next	1.28;

1.28
date	2012.01.04.17.43.34;	author mpi;	state Exp;
branches;
next	1.27;

1.27
date	2011.12.21.23.59.03;	author guenther;	state Exp;
branches;
next	1.26;

1.26
date	2011.12.21.00.49.47;	author guenther;	state Exp;
branches;
next	1.25;

1.25
date	2011.11.06.11.48.59;	author guenther;	state Exp;
branches;
next	1.24;

1.24
date	2011.09.22.04.54.38;	author guenther;	state Exp;
branches;
next	1.23;

1.23
date	2010.04.12.03.34.31;	author guenther;	state Exp;
branches;
next	1.22;

1.22
date	2009.11.27.19.45.54;	author guenther;	state Exp;
branches;
next	1.21;

1.21
date	2009.11.19.03.31.36;	author guenther;	state Exp;
branches;
next	1.20;

1.20
date	2008.10.13.05.42.46;	author kevlo;	state Exp;
branches;
next	1.19;

1.19
date	2008.02.22.09.18.28;	author tedu;	state Exp;
branches;
next	1.18;

1.18
date	2007.06.05.18.11.49;	author kurt;	state Exp;
branches;
next	1.17;

1.17
date	2007.05.25.22.38.39;	author kurt;	state Exp;
branches;
next	1.16;

1.16
date	2006.01.05.04.06.48;	author marc;	state Exp;
branches;
next	1.15;

1.15
date	2005.12.31.20.07.41;	author brad;	state Exp;
branches;
next	1.14;

1.14
date	2005.12.30.04.05.55;	author tedu;	state Exp;
branches;
next	1.13;

1.13
date	2005.12.29.11.35.54;	author otto;	state Exp;
branches;
next	1.12;

1.12
date	2005.12.22.06.49.48;	author tedu;	state Exp;
branches;
next	1.11;

1.11
date	2005.12.19.06.47.40;	author tedu;	state Exp;
branches;
next	1.10;

1.10
date	2005.12.18.01.35.06;	author tedu;	state Exp;
branches;
next	1.9;

1.9
date	2005.12.14.07.02.47;	author tedu;	state Exp;
branches;
next	1.8;

1.8
date	2005.12.14.04.14.19;	author tedu;	state Exp;
branches;
next	1.7;

1.7
date	2005.12.13.17.22.46;	author tedu;	state Exp;
branches;
next	1.6;

1.6
date	2005.12.13.07.04.34;	author tedu;	state Exp;
branches;
next	1.5;

1.5
date	2005.12.13.06.04.53;	author tedu;	state Exp;
branches;
next	1.4;

1.4
date	2005.12.13.05.56.55;	author tedu;	state Exp;
branches;
next	1.3;

1.3
date	2005.12.07.03.18.39;	author tedu;	state Exp;
branches;
next	1.2;

1.2
date	2005.12.06.06.19.31;	author tedu;	state Exp;
branches;
next	1.1;

1.1
date	2005.12.03.18.16.19;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.47
log
@Move mutex, condvar, and thread-specific data routes, pthread_once, and
pthread_exit from libpthread to libc, along with low-level bits to
support them.  Major bump to both libc and libpthread.

Requested by libressl team.  Ports testing by naddy@@
ok kettenis@@
@
text
@/*	$OpenBSD: rthread_sync.c,v 1.46 2017/07/29 16:42:10 deraadt Exp $ */
/*
 * Copyright (c) 2004,2005 Ted Unangst <tedu@@openbsd.org>
 * Copyright (c) 2012 Philip Guenther <guenther@@openbsd.org>
 * All Rights Reserved.
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
/*
 * Mutexes and conditions - synchronization functions.
 */

#include <assert.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <errno.h>

#include <pthread.h>

#include "rthread.h"
#include "cancel.h"		/* in libc/include */

static _atomic_lock_t static_init_lock = _SPINLOCK_UNLOCKED;

/*
 * mutexen
 */
int
pthread_mutex_init(pthread_mutex_t *mutexp, const pthread_mutexattr_t *attr)
{
	struct pthread_mutex *mutex;

	mutex = calloc(1, sizeof(*mutex));
	if (!mutex)
		return (errno);
	mutex->lock = _SPINLOCK_UNLOCKED;
	TAILQ_INIT(&mutex->lockers);
	if (attr == NULL) {
		mutex->type = PTHREAD_MUTEX_DEFAULT;
		mutex->prioceiling = -1;
	} else {
		mutex->type = (*attr)->ma_type;
		mutex->prioceiling = (*attr)->ma_protocol ==
		    PTHREAD_PRIO_PROTECT ? (*attr)->ma_prioceiling : -1;
	}
	*mutexp = mutex;

	return (0);
}
DEF_STD(pthread_mutex_init);

int
pthread_mutex_destroy(pthread_mutex_t *mutexp)
{
	struct pthread_mutex *mutex;

	assert(mutexp);
	mutex = (struct pthread_mutex *)*mutexp;
	if (mutex) {
		if (mutex->count || mutex->owner != NULL ||
		    !TAILQ_EMPTY(&mutex->lockers)) {
#define MSG "pthread_mutex_destroy on mutex with waiters!\n"
			write(2, MSG, sizeof(MSG) - 1);
#undef MSG
			return (EBUSY);
		}
		free(mutex);
		*mutexp = NULL;
	}
	return (0);
}
DEF_STD(pthread_mutex_destroy);

static int
_rthread_mutex_lock(pthread_mutex_t *mutexp, int trywait,
    const struct timespec *abstime)
{
	struct pthread_mutex *mutex;
	pthread_t self = pthread_self();
	int ret = 0;

	/*
	 * If the mutex is statically initialized, perform the dynamic
	 * initialization. Note: _thread_mutex_lock() in libc requires
	 * _rthread_mutex_lock() to perform the mutex init when *mutexp
	 * is NULL.
	 */
	if (*mutexp == NULL) {
		_spinlock(&static_init_lock);
		if (*mutexp == NULL)
			ret = pthread_mutex_init(mutexp, NULL);
		_spinunlock(&static_init_lock);
		if (ret != 0)
			return (EINVAL);
	}
	mutex = (struct pthread_mutex *)*mutexp;

	_rthread_debug(5, "%p: mutex_lock %p\n", (void *)self, (void *)mutex);
	_spinlock(&mutex->lock);
	if (mutex->owner == NULL && TAILQ_EMPTY(&mutex->lockers)) {
		assert(mutex->count == 0);
		mutex->owner = self;
	} else if (mutex->owner == self) {
		assert(mutex->count > 0);

		/* already owner?  handle recursive behavior */
		if (mutex->type != PTHREAD_MUTEX_RECURSIVE)
		{
			if (trywait ||
			    mutex->type == PTHREAD_MUTEX_ERRORCHECK) {
				_spinunlock(&mutex->lock);
				return (trywait ? EBUSY : EDEADLK);
			}

			/* self-deadlock is disallowed by strict */
			if (mutex->type == PTHREAD_MUTEX_STRICT_NP &&
			    abstime == NULL)
				abort();

			/* self-deadlock, possibly until timeout */
			while (__thrsleep(self, CLOCK_REALTIME, abstime,
			    &mutex->lock, NULL) != EWOULDBLOCK)
				_spinlock(&mutex->lock);
			return (ETIMEDOUT);
		}
		if (mutex->count == INT_MAX) {
			_spinunlock(&mutex->lock);
			return (EAGAIN);
		}
	} else if (trywait) {
		/* try failed */
		_spinunlock(&mutex->lock);
		return (EBUSY);
	} else {
		/* add to the wait queue and block until at the head */
		TAILQ_INSERT_TAIL(&mutex->lockers, self, waiting);
		while (mutex->owner != self) {
			ret = __thrsleep(self, CLOCK_REALTIME, abstime,
			    &mutex->lock, NULL);
			_spinlock(&mutex->lock);
			assert(mutex->owner != NULL);
			if (ret == EWOULDBLOCK) {
				if (mutex->owner == self)
					break;
				TAILQ_REMOVE(&mutex->lockers, self, waiting);
				_spinunlock(&mutex->lock);
				return (ETIMEDOUT);
			}
		}
	}

	mutex->count++;
	_spinunlock(&mutex->lock);

	return (0);
}

int
pthread_mutex_lock(pthread_mutex_t *p)
{
	return (_rthread_mutex_lock(p, 0, NULL));
}
DEF_STD(pthread_mutex_lock);

int
pthread_mutex_trylock(pthread_mutex_t *p)
{
	return (_rthread_mutex_lock(p, 1, NULL));
}

int
pthread_mutex_timedlock(pthread_mutex_t *p, const struct timespec *abstime)
{
	return (_rthread_mutex_lock(p, 0, abstime));
}

int
pthread_mutex_unlock(pthread_mutex_t *mutexp)
{
	pthread_t self = pthread_self();
	struct pthread_mutex *mutex = (struct pthread_mutex *)*mutexp;

	_rthread_debug(5, "%p: mutex_unlock %p\n", (void *)self,
	    (void *)mutex);

	if (mutex == NULL)
#if PTHREAD_MUTEX_DEFAULT == PTHREAD_MUTEX_ERRORCHECK
		return (EPERM);
#elif PTHREAD_MUTEX_DEFAULT == PTHREAD_MUTEX_NORMAL
		return(0);
#else
		abort();
#endif

	if (mutex->owner != self) {
		if (mutex->type == PTHREAD_MUTEX_ERRORCHECK ||
		    mutex->type == PTHREAD_MUTEX_RECURSIVE)
			return (EPERM);
		else {
			/*
			 * For mutex type NORMAL our undefined behavior for
			 * unlocking an unlocked mutex is to succeed without
			 * error.  All other undefined behaviors are to
			 * abort() immediately.
			 */
			if (mutex->owner == NULL &&
			    mutex->type == PTHREAD_MUTEX_NORMAL)
				return (0);
			else
				abort();
		}
	}

	if (--mutex->count == 0) {
		pthread_t next;

		_spinlock(&mutex->lock);
		mutex->owner = next = TAILQ_FIRST(&mutex->lockers);
		if (next != NULL)
			TAILQ_REMOVE(&mutex->lockers, next, waiting);
		_spinunlock(&mutex->lock);
		if (next != NULL)
			__thrwakeup(next, 1);
	}

	return (0);
}
DEF_STD(pthread_mutex_unlock);

/*
 * condition variables
 */
int
pthread_cond_init(pthread_cond_t *condp, const pthread_condattr_t *attr)
{
	pthread_cond_t cond;

	cond = calloc(1, sizeof(*cond));
	if (!cond)
		return (errno);
	cond->lock = _SPINLOCK_UNLOCKED;
	TAILQ_INIT(&cond->waiters);
	if (attr == NULL)
		cond->clock = CLOCK_REALTIME;
	else
		cond->clock = (*attr)->ca_clock;
	*condp = cond;

	return (0);
}
DEF_STD(pthread_cond_init);

int
pthread_cond_destroy(pthread_cond_t *condp)
{
	pthread_cond_t cond;

	assert(condp);
	cond = *condp;
	if (cond) {
		if (!TAILQ_EMPTY(&cond->waiters)) {
#define MSG "pthread_cond_destroy on condvar with waiters!\n"
			write(2, MSG, sizeof(MSG) - 1);
#undef MSG
			return (EBUSY);
		}
		free(cond);
	}
	*condp = NULL;

	return (0);
}
DEF_STD(pthread_cond_destroy);

int
pthread_cond_timedwait(pthread_cond_t *condp, pthread_mutex_t *mutexp,
    const struct timespec *abstime)
{
	pthread_cond_t cond;
	struct pthread_mutex *mutex = (struct pthread_mutex *)*mutexp;
	struct tib *tib = TIB_GET();
	pthread_t self = tib->tib_thread;
	pthread_t next;
	int mutex_count;
	int canceled = 0;
	int rv = 0;
	int error;
	PREP_CANCEL_POINT(tib);

	if (!*condp)
		if ((error = pthread_cond_init(condp, NULL)))
			return (error);
	cond = *condp;
	_rthread_debug(5, "%p: cond_timed %p,%p\n", (void *)self,
	    (void *)cond, (void *)mutex);

	if (mutex == NULL)
#if PTHREAD_MUTEX_DEFAULT == PTHREAD_MUTEX_ERRORCHECK
		return (EPERM);
#else
		abort();
#endif

	if (mutex->owner != self) {
		if (mutex->type == PTHREAD_MUTEX_ERRORCHECK)
			return (EPERM);
		else
			abort();
	}

	if (abstime == NULL || abstime->tv_sec < 0 || abstime->tv_nsec < 0 ||
	    abstime->tv_nsec >= 1000000000)
		return (EINVAL);

	ENTER_DELAYED_CANCEL_POINT(tib, self);

	_spinlock(&cond->lock);

	/* mark the condvar as being associated with this mutex */
	if (cond->mutex == NULL) {
		cond->mutex = mutex;
		assert(TAILQ_EMPTY(&cond->waiters));
	} else if (cond->mutex != mutex) {
		assert(cond->mutex == mutex);
		_spinunlock(&cond->lock);
		LEAVE_CANCEL_POINT_INNER(tib, 1);
		return (EINVAL);
	} else
		assert(! TAILQ_EMPTY(&cond->waiters));

	/* snag the count in case this is a recursive mutex */
	mutex_count = mutex->count;

	/* transfer from the mutex queue to the condvar queue */
	_spinlock(&mutex->lock);
	self->blocking_cond = cond;
	TAILQ_INSERT_TAIL(&cond->waiters, self, waiting);
	_spinunlock(&cond->lock);

	/* wake the next guy blocked on the mutex */
	mutex->count = 0;
	mutex->owner = next = TAILQ_FIRST(&mutex->lockers);
	if (next != NULL) {
		TAILQ_REMOVE(&mutex->lockers, next, waiting);
		__thrwakeup(next, 1);
	}

	/* wait until we're the owner of the mutex again */
	while (mutex->owner != self) {
		error = __thrsleep(self, cond->clock, abstime,
		    &mutex->lock, &self->delayed_cancel);

		/*
		 * If abstime == NULL, then we're definitely waiting
		 * on the mutex instead of the condvar, and are
		 * just waiting for mutex ownership, regardless of
		 * why we woke up.
		 */
		if (abstime == NULL) {
			_spinlock(&mutex->lock);
			continue;
		}

		/*
		 * If we took a normal signal (not from
		 * cancellation) then we should just go back to
		 * sleep without changing state (timeouts, etc).
		 */
		if (error == EINTR && (tib->tib_canceled == 0 ||
		    (tib->tib_cantcancel & CANCEL_DISABLED))) {
			_spinlock(&mutex->lock);
			continue;
		}

		/*
		 * The remaining reasons for waking up (normal
		 * wakeup, timeout, and cancellation) all mean that
		 * we won't be staying in the condvar queue and
		 * we'll no longer time out or be cancelable.
		 */
		abstime = NULL;
		LEAVE_CANCEL_POINT_INNER(tib, 0);

		/*
		 * If we're no longer in the condvar's queue then
		 * we're just waiting for mutex ownership.  Need
		 * cond->lock here to prevent race with cond_signal().
		 */
		_spinlock(&cond->lock);
		if (self->blocking_cond == NULL) {
			_spinunlock(&cond->lock);
			_spinlock(&mutex->lock);
			continue;
		}
		assert(self->blocking_cond == cond);

		/* if timeout or canceled, make note of that */
		if (error == EWOULDBLOCK)
			rv = ETIMEDOUT;
		else if (error == EINTR)
			canceled = 1;

		/* transfer between the queues */
		TAILQ_REMOVE(&cond->waiters, self, waiting);
		assert(mutex == cond->mutex);
		if (TAILQ_EMPTY(&cond->waiters))
			cond->mutex = NULL;
		self->blocking_cond = NULL;
		_spinunlock(&cond->lock);
		_spinlock(&mutex->lock);

		/* mutex unlocked right now? */
		if (mutex->owner == NULL &&
		    TAILQ_EMPTY(&mutex->lockers)) {
			assert(mutex->count == 0);
			mutex->owner = self;
			break;
		}
		TAILQ_INSERT_TAIL(&mutex->lockers, self, waiting);
	}

	/* restore the mutex's count */
	mutex->count = mutex_count;
	_spinunlock(&mutex->lock);

	LEAVE_CANCEL_POINT_INNER(tib, canceled);

	return (rv);
}

int
pthread_cond_wait(pthread_cond_t *condp, pthread_mutex_t *mutexp)
{
	pthread_cond_t cond;
	struct pthread_mutex *mutex = (struct pthread_mutex *)*mutexp;
	struct tib *tib = TIB_GET();
	pthread_t self = tib->tib_thread;
	pthread_t next;
	int mutex_count;
	int canceled = 0;
	int error;
	PREP_CANCEL_POINT(tib);

	if (!*condp)
		if ((error = pthread_cond_init(condp, NULL)))
			return (error);
	cond = *condp;
	_rthread_debug(5, "%p: cond_wait %p,%p\n", (void *)self,
	    (void *)cond, (void *)mutex);

	if (mutex == NULL)
#if PTHREAD_MUTEX_DEFAULT == PTHREAD_MUTEX_ERRORCHECK
		return (EPERM);
#else
		abort();
#endif

	if (mutex->owner != self) {
		if (mutex->type == PTHREAD_MUTEX_ERRORCHECK)
			return (EPERM);
		else
			abort();
	}

	ENTER_DELAYED_CANCEL_POINT(tib, self);

	_spinlock(&cond->lock);

	/* mark the condvar as being associated with this mutex */
	if (cond->mutex == NULL) {
		cond->mutex = mutex;
		assert(TAILQ_EMPTY(&cond->waiters));
	} else if (cond->mutex != mutex) {
		assert(cond->mutex == mutex);
		_spinunlock(&cond->lock);
		LEAVE_CANCEL_POINT_INNER(tib, 1);
		return (EINVAL);
	} else
		assert(! TAILQ_EMPTY(&cond->waiters));

	/* snag the count in case this is a recursive mutex */
	mutex_count = mutex->count;

	/* transfer from the mutex queue to the condvar queue */
	_spinlock(&mutex->lock);
	self->blocking_cond = cond;
	TAILQ_INSERT_TAIL(&cond->waiters, self, waiting);
	_spinunlock(&cond->lock);

	/* wake the next guy blocked on the mutex */
	mutex->count = 0;
	mutex->owner = next = TAILQ_FIRST(&mutex->lockers);
	if (next != NULL) {
		TAILQ_REMOVE(&mutex->lockers, next, waiting);
		__thrwakeup(next, 1);
	}

	/* wait until we're the owner of the mutex again */
	while (mutex->owner != self) {
		error = __thrsleep(self, 0, NULL, &mutex->lock,
		    &self->delayed_cancel);

		/*
		 * If we took a normal signal (not from
		 * cancellation) then we should just go back to
		 * sleep without changing state (timeouts, etc).
		 */
		if (error == EINTR && (tib->tib_canceled == 0 ||
		    (tib->tib_cantcancel & CANCEL_DISABLED))) {
			_spinlock(&mutex->lock);
			continue;
		}

		/*
		 * The remaining reasons for waking up (normal
		 * wakeup and cancellation) all mean that we won't
		 * be staying in the condvar queue and we'll no
		 * longer be cancelable.
		 */
		LEAVE_CANCEL_POINT_INNER(tib, 0);

		/*
		 * If we're no longer in the condvar's queue then
		 * we're just waiting for mutex ownership.  Need
		 * cond->lock here to prevent race with cond_signal().
		 */
		_spinlock(&cond->lock);
		if (self->blocking_cond == NULL) {
			_spinunlock(&cond->lock);
			_spinlock(&mutex->lock);
			continue;
		}
		assert(self->blocking_cond == cond);

		/* if canceled, make note of that */
		if (error == EINTR)
			canceled = 1;

		/* transfer between the queues */
		TAILQ_REMOVE(&cond->waiters, self, waiting);
		assert(mutex == cond->mutex);
		if (TAILQ_EMPTY(&cond->waiters))
			cond->mutex = NULL;
		self->blocking_cond = NULL;
		_spinunlock(&cond->lock);
		_spinlock(&mutex->lock);

		/* mutex unlocked right now? */
		if (mutex->owner == NULL &&
		    TAILQ_EMPTY(&mutex->lockers)) {
			assert(mutex->count == 0);
			mutex->owner = self;
			break;
		}
		TAILQ_INSERT_TAIL(&mutex->lockers, self, waiting);
	}

	/* restore the mutex's count */
	mutex->count = mutex_count;
	_spinunlock(&mutex->lock);

	LEAVE_CANCEL_POINT_INNER(tib, canceled);

	return (0);
}
DEF_STD(pthread_cond_wait);


int
pthread_cond_signal(pthread_cond_t *condp)
{
	pthread_cond_t cond;
	struct pthread_mutex *mutex;
	pthread_t thread;
	int wakeup;

	/* uninitialized?  Then there's obviously no one waiting! */
	if (!*condp)
		return 0;

	cond = *condp;
	_rthread_debug(5, "%p: cond_signal %p,%p\n", (void *)pthread_self(),
	    (void *)cond, (void *)cond->mutex);
	_spinlock(&cond->lock);
	thread = TAILQ_FIRST(&cond->waiters);
	if (thread == NULL) {
		assert(cond->mutex == NULL);
		_spinunlock(&cond->lock);
		return (0);
	}

	assert(thread->blocking_cond == cond);
	TAILQ_REMOVE(&cond->waiters, thread, waiting);
	thread->blocking_cond = NULL;

	mutex = cond->mutex;
	assert(mutex != NULL);
	if (TAILQ_EMPTY(&cond->waiters))
		cond->mutex = NULL;

	/* link locks to prevent race with timedwait */
	_spinlock(&mutex->lock);
	_spinunlock(&cond->lock);

	wakeup = mutex->owner == NULL && TAILQ_EMPTY(&mutex->lockers);
	if (wakeup)
		mutex->owner = thread;
	else
		TAILQ_INSERT_TAIL(&mutex->lockers, thread, waiting);
	_spinunlock(&mutex->lock);
	if (wakeup)
		__thrwakeup(thread, 1);

	return (0);
}
DEF_STD(pthread_cond_signal);

int
pthread_cond_broadcast(pthread_cond_t *condp)
{
	pthread_cond_t cond;
	struct pthread_mutex *mutex;
	pthread_t thread;
	pthread_t p;
	int wakeup;

	/* uninitialized?  Then there's obviously no one waiting! */
	if (!*condp)
		return 0;

	cond = *condp;
	_rthread_debug(5, "%p: cond_broadcast %p,%p\n", (void *)pthread_self(),
	    (void *)cond, (void *)cond->mutex);
	_spinlock(&cond->lock);
	thread = TAILQ_FIRST(&cond->waiters);
	if (thread == NULL) {
		assert(cond->mutex == NULL);
		_spinunlock(&cond->lock);
		return (0);
	}

	mutex = cond->mutex;
	assert(mutex != NULL);

	/* walk the list, clearing the "blocked on condvar" pointer */
	p = thread;
	do
		p->blocking_cond = NULL;
	while ((p = TAILQ_NEXT(p, waiting)) != NULL);

	/*
	 * We want to transfer all the threads from the condvar's list
	 * to the mutex's list.  The TAILQ_* macros don't let us do that
	 * efficiently, so this is direct list surgery.  Pay attention!
	 */

	/* 1) attach the first thread to the end of the mutex's list */
	_spinlock(&mutex->lock);
	wakeup = mutex->owner == NULL && TAILQ_EMPTY(&mutex->lockers);
	thread->waiting.tqe_prev = mutex->lockers.tqh_last;
	*(mutex->lockers.tqh_last) = thread;

	/* 2) fix up the end pointer for the mutex's list */
	mutex->lockers.tqh_last = cond->waiters.tqh_last;

	if (wakeup) {
		TAILQ_REMOVE(&mutex->lockers, thread, waiting);
		mutex->owner = thread;
		_spinunlock(&mutex->lock);
		__thrwakeup(thread, 1);
	} else
		_spinunlock(&mutex->lock);

	/* 3) reset the condvar's list and mutex pointer */
	TAILQ_INIT(&cond->waiters);
	assert(cond->mutex != NULL);
	cond->mutex = NULL;
	_spinunlock(&cond->lock);

	return (0);
}
DEF_STD(pthread_cond_broadcast);
@


1.46
log
@not all the world is an i386.  Back out breakage.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.44 2016/09/04 10:13:35 akfaew Exp $ */
@


1.45
log
@Use memory barriers to prevent pointer use before initialization.

This work was sparked by the topic posted on hn by wuch. I am still not
sure that this fixes the defect he claims to have observed because I was
not able to create a proper regress test for it to manifest.

To that end, a proof of concept is more than welcomed!
Thank you for the report!

Discussed with and OK kettenis@@, tedu@@.
@
text
@a56 2

	membar_producer();
@


1.44
log
@Get rid of ticket support, replace "struct _spinlock" with "_atomic_lock_t".

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.43 2016/09/03 16:44:20 akfaew Exp $ */
d57 2
@


1.43
log
@Remove _USING_TICKETS, it's defined as 0. No functional change.

ok tedu@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.42 2016/05/07 19:05:22 guenther Exp $ */
a22 1

d34 1
a34 1
static struct _spinlock static_init_lock = _SPINLOCK_UNLOCKED;
d47 1
a47 1
	mutex->lock = _SPINLOCK_UNLOCKED_ASSIGN;
d133 1
a133 1
			    &mutex->lock.ticket, NULL) != EWOULDBLOCK)
d150 1
a150 1
			    &mutex->lock.ticket, NULL);
d252 1
a252 1
	cond->lock = _SPINLOCK_UNLOCKED_ASSIGN;
d362 1
a362 1
		    &mutex->lock.ticket, &self->delayed_cancel);
d511 1
a511 1
		error = __thrsleep(self, 0, NULL, &mutex->lock.ticket,
@


1.42
log
@Use a Thread Information Block in both single and multi-threaded programs.
This stores errno, the cancelation flags, and related bits for each thread
and is allocated by ld.so or libc.a.  This is an ABI break from 5.9-stable!

Make libpthread dlopen'able by moving the cancelation wrappers into libc
and doing locking and fork/errno handling via callbacks that libpthread
registers when it first initializes.  'errno' *must* be declared via
<errno.h> now!

Clean up libpthread's symbol exports like libc.

On powerpc, offset the TIB/TCB/TLS data from the register per the ELF spec.

Testing by various, particularly sthen@@ and patrick@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.41 2016/04/15 21:04:04 guenther Exp $ */
d133 1
a133 2
			while (__thrsleep(self, CLOCK_REALTIME |
			    _USING_TICKETS, abstime,
d150 2
a151 2
			ret = __thrsleep(self, CLOCK_REALTIME | _USING_TICKETS,
			    abstime, &mutex->lock.ticket, NULL);
d362 1
a362 1
		error = __thrsleep(self, cond->clock | _USING_TICKETS, abstime,
d512 2
a513 2
		error = __thrsleep(self, 0 | _USING_TICKETS, NULL,
		    &mutex->lock.ticket, &self->delayed_cancel);
@


1.41
log
@PROTO_NORMAL(pthread_cond_signal) requires DEF_STD(pthread_cond_signal)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.40 2016/04/02 19:56:53 guenther Exp $ */
d33 1
d294 2
a295 1
	pthread_t self = pthread_self();
d301 1
d328 1
a328 1
	_enter_delayed_cancel(self);
d339 1
a339 1
		_leave_delayed_cancel(self, 1);
d382 2
a383 1
		if (error == EINTR && !IS_CANCELED(self)) {
d395 1
a395 1
		_leave_delayed_cancel(self, 0);
d439 1
a439 1
	_leave_delayed_cancel(self, canceled);
d449 2
a450 1
	pthread_t self = pthread_self();
d455 1
d478 1
a478 1
	_enter_delayed_cancel(self);
d489 1
a489 1
		_leave_delayed_cancel(self, 1);
d521 2
a522 1
		if (error == EINTR && !IS_CANCELED(self)) {
d533 1
a533 1
		_leave_delayed_cancel(self, 0);
d575 1
a575 1
	_leave_delayed_cancel(self, canceled);
@


1.40
log
@Wrap <pthread.h> and <pthread_np.h> to eliminate PLT entries for internal
references.  Use _thread_pagesize for the semaphore mmap size instead of
calling getpagesize() each time.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.39 2013/06/01 23:06:26 tedu Exp $ */
d622 1
@


1.39
log
@something's not quite right yet. ticket locks result in more CPU usage
and spinning in kernel. partially back out, but in a way that makes going
forward again easy.
seen by ajacoutot
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.38 2013/06/01 20:47:40 tedu Exp $ */
d61 1
d83 1
d175 1
d240 1
d263 1
d285 1
d572 1
d687 1
@


1.38
log
@cleanup and consolidate the spinlock_lock (what a name!) code.
it's now atomic_lock to better reflect its usage, and librthread now
features a new spinlock that's really a ticket lock.
thrlseep can handle both types of lock via a flag in the clock arg.
(temp back compat hack)
remove some old stuff that's accumulated along the way and no longer used.
some feedback from dlg, who is concerned with all things ticket lock.
(you need to boot a new kernel before installing librthread)
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.37 2013/06/01 19:47:28 tedu Exp $ */
d130 3
a132 2
			while (__thrsleep(self, CLOCK_REALTIME | 0x8, abstime,
			    &mutex->lock.ready, NULL) != EWOULDBLOCK)
d148 2
a149 2
			ret = __thrsleep(self, CLOCK_REALTIME | 0x8, abstime,
			    &mutex->lock.ready, NULL);
d354 2
a355 2
		error = __thrsleep(self, cond->clock | 0x8, abstime,
		    &mutex->lock.ready, &self->delayed_cancel);
d501 2
a502 2
		error = __thrsleep(self, 0 | 0x8, NULL, &mutex->lock.ready,
		    &self->delayed_cancel);
@


1.37
log
@fix wrong fn name in debug
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.36 2012/04/14 12:07:49 kurt Exp $ */
d34 1
a34 1
static _spinlock_lock_t static_init_lock = _SPINLOCK_UNLOCKED;
d47 1
a47 1
	mutex->lock = _SPINLOCK_UNLOCKED;
d130 2
a131 2
			while (__thrsleep(self, CLOCK_REALTIME, abstime,
			    &mutex->lock, NULL) != EWOULDBLOCK)
d147 2
a148 2
			ret = __thrsleep(self, CLOCK_REALTIME, abstime,
			    &mutex->lock, NULL);
d248 1
a248 1
	cond->lock = _SPINLOCK_UNLOCKED;
d353 2
a354 2
		error = __thrsleep(self, cond->clock, abstime, &mutex->lock,
		    &self->delayed_cancel);
d500 1
a500 1
		error = __thrsleep(self, 0, NULL, &mutex->lock,
@


1.36
log
@Add new mutex type, PTHREAD_MUTEX_STRICT_NP which checks for application
errors similar to PTHREAD_MUTEX_ERRORCHECK, however upon error it aborts.
The rational is that many applications don't check the return values on
pthread functions and will miss the errors that ERRORCHECK returns.
PTHREAD_MUTEX_STRICT_NP will be our default mutex type for awhile
okay guenther@@ dcoppa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.35 2012/04/13 13:50:37 kurt Exp $ */
d448 1
a448 1
	_rthread_debug(5, "%p: cond_timed %p,%p\n", (void *)self,
@


1.35
log
@Use PTHREAD_MUTEX_DEFAULT in static init and mutexattr_init. If the
default mutex type changes to NORMAL, when there is an uninitialized
mutex provided to unlock, allow it to succeed similar to an unlocked
mutex. For other cases abort instead of segfault. okay guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.34 2012/04/13 12:39:28 kurt Exp $ */
d124 5
a129 1
			assert(mutex->type == PTHREAD_MUTEX_NORMAL);
d214 2
a215 1
			if (mutex->owner == NULL)
@


1.34
log
@Per POSIX, PTHREAD_MUTEX_NORMAL type mutexes have undefined behavior for
certain conditions. In the case of unlocking an unlocked mutex we will
allow that to succeed, all other undefined behaviors will result in an
immediate abort(). okay guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.33 2012/02/28 02:41:56 guenther Exp $ */
d50 1
a50 1
		mutex->type = PTHREAD_MUTEX_ERRORCHECK;
d190 1
a191 1
	if (mutex == NULL)
d193 4
d295 1
a296 1
	if (mutex == NULL)
d298 2
d446 1
a447 1
	if (mutex == NULL)
d449 2
@


1.33
log
@Our default mutex type is PTHREAD_MUTEX_ERRORCHECK, for which trying
to unlock an uninitialized mutex is required to return EPERM, so add
the necessary checks.

For recursive mutexes, return an error from pthread_mutex_lock()
if the count would overflow.

problem observed in glib testing by aja@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.32 2012/02/23 07:58:25 guenther Exp $ */
d195 17
a211 2
	if (mutex->owner != self)
		return (EPERM);
d296 7
a302 2
	if (mutex->owner != self)
		return (EPERM);
d445 6
a450 2
	if (mutex->owner != self)
		return (EPERM);
@


1.32
log
@PTHREAD_MUTEX_NORMAL mutexes are supposed to self-deadlock, not abort.
That deadlock can be exited via a timeout for pthread_mutex_timedlock().
Unwind all the state when pthread_mutex_timedlock() times out
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.31 2012/02/23 04:43:06 guenther Exp $ */
d131 4
d190 5
d276 5
d419 5
@


1.31
log
@Add pthread_condattr_{get,set}clock(), requested by aja@@
Add pthread_mutex_timedlock(), requested by dcoppa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.30 2012/01/25 06:55:08 guenther Exp $ */
d123 7
a129 1
			abort();
d139 1
a139 1
			__thrsleep(self, CLOCK_REALTIME, abstime,
d143 7
@


1.30
log
@@@($*&  Merging patches resulted in a line being duplicated instead of
moved.  Unlocking a spinlock twice is bad, mmkay?
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.29 2012/01/17 02:34:18 guenther Exp $ */
d84 2
a85 1
_rthread_mutex_lock(pthread_mutex_t *mutexp, int trywait)
d133 2
a134 1
			__thrsleep(self, 0, NULL, &mutex->lock, NULL);
d149 1
a149 1
	return (_rthread_mutex_lock(p, 0));
d155 7
a161 1
	return (_rthread_mutex_lock(p, 1));
a193 1
/* ARGSUSED1 */
d195 1
a195 2
pthread_cond_init(pthread_cond_t *condp,
    const pthread_condattr_t *attrp __unused)
d204 4
a207 1

d295 1
a295 1
		error = __thrsleep(self, CLOCK_REALTIME, abstime, &mutex->lock,
a609 26

/*
 * condition variable attributes
 */
int
pthread_condattr_init(pthread_condattr_t *attrp)
{
	pthread_condattr_t attr;

	attr = calloc(1, sizeof(*attr));
	if (!attr)
		return (errno);
	*attrp = attr;

	return (0);
}

int
pthread_condattr_destroy(pthread_condattr_t *attrp)
{
	free(*attrp);
	*attrp = NULL;

	return (0);
}

@


1.29
log
@Reimplement mutexes, condvars, and rwlocks to eliminate bugs,
particularly the "consume the signal you just sent" hang, and putting
the wait queues in userspace.

Do cancellation handling in pthread_cond_*wait(), pthread_join(),
and sem_wait().

Add __ prefix to thr{sleep,wakeup,exit,sigdivert}() syscalls; add
'abort" argument to thrsleep to close cancellation race; make
thr{sleep,wakeup} return errno values via *retval to avoid touching
userspace errno.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.28 2012/01/04 17:43:34 mpi Exp $ */
a583 1
	_spinunlock(&mutex->lock);
@


1.28
log
@Split out the semaphore functions.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.27 2011/12/21 23:59:03 guenther Exp $ */
d4 1
d24 1
d26 1
d42 1
a42 1
	pthread_mutex_t mutex;
d47 2
a48 2
	mutex->sem.lock = _SPINLOCK_UNLOCKED;
	mutex->sem.value = 1;	/* unlocked */
d51 1
a51 1
		mutex->prioceiling = PTHREAD_PRIO_NONE;
d65 1
d67 5
a71 1
	if ((*mutexp) && (*mutexp)->count) {
d73 1
a73 1
		write(2, MSG, sizeof(MSG) - 1);
d75 4
a78 1
		return (EBUSY);
a79 2
	free((void *)*mutexp);
	*mutexp = NULL;
d86 2
a87 2
	pthread_mutex_t mutex;
	pthread_t thread = pthread_self();
d104 31
a134 5
	mutex = *mutexp;
	if (mutex->owner == thread) {
		if (mutex->type == PTHREAD_MUTEX_RECURSIVE) {
			mutex->count++;
			return (0);
a135 2
		if (mutex->type == PTHREAD_MUTEX_ERRORCHECK)
			return (trywait ? EBUSY : EDEADLK);
d137 3
a139 4
	if (!_sem_wait((void *)&mutex->sem, trywait))
		return (EBUSY);
	mutex->owner = thread;
	mutex->count = 1;
d159 5
a163 2
	pthread_t thread = pthread_self();
	pthread_mutex_t mutex = *mutexp;
d165 1
a165 1
	if (mutex->owner != thread)
d169 9
a177 2
		mutex->owner = NULL;
		_sem_post((void *)&mutex->sem);
d186 1
d188 2
a189 1
pthread_cond_init(pthread_cond_t *condp, const pthread_condattr_t *attrp)
d196 2
a197 1
	cond->sem.lock = _SPINLOCK_UNLOCKED;
d207 1
d209 11
a219 1
	free(*condp);
d229 7
a236 1
	int rv;
d241 68
d310 46
a355 4
	_spinlock(&(*condp)->sem.lock);
	pthread_mutex_unlock(mutexp);
	rv = _sem_waitl(&(*condp)->sem, 0, CLOCK_REALTIME, abstime);
	error = pthread_mutex_lock(mutexp);
d357 7
a363 1
	return (error ? error : rv ? 0 : ETIMEDOUT);
d369 117
a485 1
	return (pthread_cond_timedwait(condp, mutexp, NULL));
d488 1
d492 4
a495 1
	int error;
d497 1
d499 12
a510 2
		if ((error = pthread_cond_init(condp, NULL)))
			return (error);
d512 21
a532 1
	_sem_wakeup(&(*condp)->sem);
d540 7
d548 27
a574 1
		pthread_cond_init(condp, NULL);
d576 23
a598 1
	_sem_wakeall(&(*condp)->sem);
@


1.27
log
@Split out the pthread_rwlock* and pthread_once() functions from rthread_sync.c
to new files rthread_rwlock.c, rthread_rwlockattr.c, and rthread_once.c
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.26 2011/12/21 00:49:47 guenther Exp $ */
d19 1
a19 1
 * Mutexes, conditions, and semaphores - synchronization functions.
a31 176

/*
 * Internal implementation of semaphores
 */
int
_sem_wait(sem_t sem, int tryonly)
{

	_spinlock(&sem->lock);
	return (_sem_waitl(sem, tryonly, 0, NULL));
}

int
_sem_waitl(sem_t sem, int tryonly, clockid_t clock_id,
    const struct timespec *abstime)
{
	int do_sleep;

again:
	if (sem->value == 0) {
		if (tryonly) {
			_spinunlock(&sem->lock);
			return (0);
		}
		sem->waitcount++;
		do_sleep = 1;
	} else {
		sem->value--;
		do_sleep = 0;
	}

	if (do_sleep) {
		if (thrsleep(sem, clock_id, abstime, &sem->lock) == -1 &&
		    errno == EWOULDBLOCK)
			return (0);
		_spinlock(&sem->lock);
		sem->waitcount--;
		goto again;
	}
	_spinunlock(&sem->lock);
	return (1);
}

/* always increment count */
int
_sem_post(sem_t sem)
{
	int rv = 0;

	_spinlock(&sem->lock);
	sem->value++;
	if (sem->waitcount) {
		thrwakeup(sem, 1);
		rv = 1;
	}
	_spinunlock(&sem->lock);
	return (rv);
}

/* only increment count if a waiter */
int
_sem_wakeup(sem_t sem)
{
	int rv = 0;

	_spinlock(&sem->lock);
	if (sem->waitcount) {
		sem->value++;
		thrwakeup(sem, 1);
		rv = 1;
	}
	_spinunlock(&sem->lock);
	return (rv);
}


int
_sem_wakeall(sem_t sem)
{
	int rv;

	_spinlock(&sem->lock);
	rv = sem->waitcount;
	sem->value += rv;
	thrwakeup(sem, 0);
	_spinunlock(&sem->lock);

	return (rv);
}

/*
 * exported semaphores
 */
int
sem_init(sem_t *semp, int pshared, unsigned int value)
{
	sem_t sem;

	if (pshared) {
		errno = EPERM;
		return (-1);
	}

	sem = calloc(1, sizeof(*sem));
	if (!sem)
		return (-1);
	sem->value = value;
	*semp = sem;

	return (0);
}

int
sem_destroy(sem_t *semp)
{
	if (!*semp)
		return (EINVAL);
	if ((*semp)->waitcount) {
#define MSG "sem_destroy on semaphore with waiters!\n"
		write(2, MSG, sizeof(MSG) - 1);
#undef MSG
		return (EBUSY);
	}
	free(*semp);
	*semp = NULL;

	return (0);
}

int
sem_getvalue(sem_t *semp, int *sval)
{
	sem_t sem = *semp;

	_spinlock(&sem->lock);
	*sval = sem->value;
	_spinunlock(&sem->lock);

	return (0);
}

int
sem_post(sem_t *semp)
{
	sem_t sem = *semp;

	_sem_post(sem);

	return (0);
}

int
sem_wait(sem_t *semp)
{
	sem_t sem = *semp;

	_sem_wait(sem, 0);

	return (0);
}

int
sem_trywait(sem_t *semp)
{
	sem_t sem = *semp;
	int rv;

	rv = _sem_wait(sem, 1);

	if (!rv) {
		errno = EAGAIN;
		return (-1);
	}

	return (0);
}
@


1.26
log
@Split out the pthread_mutexattr_* functions from rthread_sync.c to
new file rthread_mutexattr.c.  Add basic implementations of
pthread_mutexattr_{set,get}{protocol,prioceiling}

Requested by aja
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.25 2011/11/06 11:48:59 guenther Exp $ */
d19 1
a19 1
 * Mutexes, conditions, rwlocks, and semaphores - synchronization functions.
a419 273
/*
 * rwlocks
 */
int
pthread_rwlock_init(pthread_rwlock_t *lockp, const pthread_rwlockattr_t *attrp)
{
	pthread_rwlock_t lock;

	lock = calloc(1, sizeof(*lock));
	if (!lock)
		return (errno);
	lock->lock = _SPINLOCK_UNLOCKED;
	lock->sem.lock = _SPINLOCK_UNLOCKED;
	*lockp = lock;

	return (0);
}

int
pthread_rwlock_destroy(pthread_rwlock_t *lockp)
{
	if ((*lockp) && ((*lockp)->readers || (*lockp)->writer)) {
#define MSG "pthread_rwlock_destroy on rwlock with waiters!\n"
		write(2, MSG, sizeof(MSG) - 1);
#undef MSG
		return (EBUSY);
	}
	free(*lockp);
	*lockp = NULL;

	return (0);
}

static int
_rthread_rwlock_ensure_init(pthread_rwlock_t *lockp)
{
	int ret = 0;

	/*
	 * If the rwlock is statically initialized, perform the dynamic
	 * initialization.
	 */
	if (*lockp == NULL)
	{
		_spinlock(&static_init_lock);
		if (*lockp == NULL)
			ret = pthread_rwlock_init(lockp, NULL);
		_spinunlock(&static_init_lock);
	}
	return (ret);
}


int
pthread_rwlock_rdlock(pthread_rwlock_t *lockp)
{
	pthread_rwlock_t lock;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;
again:
	_spinlock(&lock->lock);
	if (lock->writer) {
		_spinlock(&lock->sem.lock);
		_spinunlock(&lock->lock);
		_sem_waitl(&lock->sem, 0, 0, NULL);
		goto again;
	}
	lock->readers++;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_timedrdlock(pthread_rwlock_t *lockp,
    const struct timespec *abstime)
{
	pthread_rwlock_t lock;
	int do_wait = 1;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;
	_spinlock(&lock->lock);
	while (lock->writer && do_wait) {
		_spinlock(&lock->sem.lock);
		_spinunlock(&lock->lock);
		do_wait = _sem_waitl(&lock->sem, 0, CLOCK_REALTIME, abstime);
		_spinlock(&lock->lock);
	}
	if (lock->writer) {
		/* do_wait must be 0, so timed out */
		_spinunlock(&lock->lock);
		return (ETIMEDOUT);
	}
	lock->readers++;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_tryrdlock(pthread_rwlock_t *lockp)
{
	pthread_rwlock_t lock;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;

	_spinlock(&lock->lock);
	if (lock->writer) {
		_spinunlock(&lock->lock);
		return (EBUSY);
	}
	lock->readers++;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_wrlock(pthread_rwlock_t *lockp)
{
	pthread_rwlock_t lock;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;

	_spinlock(&lock->lock);
	lock->writer++;
	while (lock->readers) {
		_spinlock(&lock->sem.lock);
		_spinunlock(&lock->lock);
		_sem_waitl(&lock->sem, 0, 0, NULL);
		_spinlock(&lock->lock);
	}
	lock->readers = -pthread_self()->tid;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_timedwrlock(pthread_rwlock_t *lockp,
    const struct timespec *abstime)
{
	pthread_rwlock_t lock;
	int do_wait = 1;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;

	_spinlock(&lock->lock);
	lock->writer++;
	while (lock->readers && do_wait) {
		_spinlock(&lock->sem.lock);
		_spinunlock(&lock->lock);
		do_wait = _sem_waitl(&lock->sem, 0, CLOCK_REALTIME, abstime);
		_spinlock(&lock->lock);
	}
	if (lock->readers) {
		/* do_wait must be 0, so timed out */
		lock->writer--;
		_spinunlock(&lock->lock);
		return (ETIMEDOUT);
	}
	lock->readers = -pthread_self()->tid;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_trywrlock(pthread_rwlock_t *lockp)
{
	pthread_rwlock_t lock;
	int error;

	if ((error = _rthread_rwlock_ensure_init(lockp)))
		return (error);

	lock = *lockp;

	_spinlock(&lock->lock);
	if (lock->readers || lock->writer) {
		_spinunlock(&lock->lock);
		return (EBUSY);
	}
	lock->writer = 1;
	lock->readers = -pthread_self()->tid;
	_spinunlock(&lock->lock);

	return (0);
}

int
pthread_rwlock_unlock(pthread_rwlock_t *lockp)
{
	pthread_rwlock_t lock;

	lock = *lockp;

	_spinlock(&lock->lock);
	if (lock->readers == -pthread_self()->tid) {
		lock->readers = 0;
		lock->writer--;
	} else if (lock->readers > 0) {
		lock->readers--;
	} else {
		_spinunlock(&lock->lock);
		return (EPERM);
	}
	_spinunlock(&lock->lock);
	_sem_wakeall(&lock->sem);

	return (0);
}

/*
 * rwlock attributes
 */
int
pthread_rwlockattr_init(pthread_rwlockattr_t *attrp)
{
	pthread_rwlockattr_t attr;

	attr = calloc(1, sizeof(*attr));
	if (!attr)
		return (errno);
	*attrp = attr;

	return (0);
}

int
pthread_rwlockattr_destroy(pthread_rwlockattr_t *attrp)
{
	free(*attrp);
	*attrp = NULL;

	return (0);
}

/*
 * pthread_once
 */
int
pthread_once(pthread_once_t *once_control, void (*init_routine)(void))
{
	pthread_mutex_lock(&once_control->mutex);
	if (once_control->state == PTHREAD_NEEDS_INIT) {
		init_routine();
		once_control->state = PTHREAD_DONE_INIT;
	}
	pthread_mutex_unlock(&once_control->mutex);

	return (0);
}
@


1.25
log
@Move <machine/spinlock.h> into rthread.h; strip out unnecessary #includes
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.24 2011/09/22 04:54:38 guenther Exp $ */
d222 8
a229 1
	mutex->type = attr ? (*attr)->type : PTHREAD_MUTEX_ERRORCHECK;
a312 42

	return (0);
}

/*
 * mutexen attributes
 */
int
pthread_mutexattr_init(pthread_mutexattr_t *attrp)
{
	pthread_mutexattr_t attr;

	attr = calloc(1, sizeof(*attr));
	if (!attr)
		return (errno);
	attr->type = PTHREAD_MUTEX_ERRORCHECK;
	*attrp = attr;

	return (0);
}

int
pthread_mutexattr_destroy(pthread_mutexattr_t *attrp)
{
	free(*attrp);
	*attrp = NULL;

	return (0);
}

int
pthread_mutexattr_settype(pthread_mutexattr_t *attrp, int type)
{
	(*attrp)->type = type;

	return (0);
}

int
pthread_mutexattr_gettype(pthread_mutexattr_t *attrp, int *type)
{
	*type = (*attrp)->type;
@


1.24
log
@Return the correct errno (EBUSY) when pthread_mutex_trylock() is
called on an ERRORCHECK mutex that is locked by this thread.  Problem
observed by uwe@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.23 2010/04/12 03:34:31 guenther Exp $ */
a22 6
#include <sys/param.h>
#include <sys/mman.h>
#include <sys/wait.h>

#include <machine/spinlock.h>

a24 3
#include <signal.h>
#include <stdio.h>
#include <string.h>
@


1.23
log
@Add pthread_rwlock_timed{rd,wr}lock().
Avoid missed wakeups in pthread_rwlock_{rd,wr}lock() by linking the spinlocks.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.22 2009/11/27 19:45:54 guenther Exp $ */
d280 1
a280 1
			return (EDEADLK);
@


1.22
log
@Convert thrsleep() to an absolute timeout with clockid to eliminate a
race condition and prep for later support of pthread_condattr_setclock()

"get it in" deraadt@@, tedu@@, cheers by others
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.21 2009/11/19 03:31:36 guenther Exp $ */
d530 1
d532 1
a532 1
		_sem_wait(&lock->sem, 0);
d542 30
d607 1
d609 1
a609 1
		_sem_wait(&lock->sem, 0);
d611 33
@


1.21
log
@pthread_rwlock_wrlock() should increment the count of writers just
once and not on every wakeup.

ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.20 2008/10/13 05:42:46 kevlo Exp $ */
d46 1
a46 1
_sem_wait(sem_t sem, int tryonly, int timo)
d50 1
a50 1
	return (_sem_waitl(sem, tryonly, timo));
d54 2
a55 1
_sem_waitl(sem_t sem, int tryonly, int timo)
d73 1
a73 1
		if (thrsleep(sem, timo, &sem->lock) == -1 &&
d197 1
a197 1
	_sem_wait(sem, 0, 0);
d208 1
a208 1
	rv = _sem_wait(sem, 1, 0);
d282 1
a282 1
	if (!_sem_wait((void *)&mutex->sem, trywait, 0))
a394 3
	int timo = 0;
	struct timeval timenow;
	struct timespec tmspec;
a399 12
	if (abstime && gettimeofday(&timenow, NULL) == 0) {
		TIMEVAL_TO_TIMESPEC(&timenow, &tmspec);
		if (timespeccmp(abstime, &tmspec, <)) {
			pthread_mutex_unlock(mutexp);
			error = pthread_mutex_lock(mutexp);
			return (error ? error : ETIMEDOUT);
		}
		timespecsub(abstime, &tmspec, &tmspec);
		timo = tmspec.tv_sec * 1000 + tmspec.tv_nsec / 1000000;
	}


d402 1
a402 1
	rv = _sem_waitl(&(*condp)->sem, 0, timo);
d531 1
a531 1
		_sem_wait(&lock->sem, 0, 0);
d577 1
a577 1
		_sem_wait(&lock->sem, 0, 0);
@


1.20
log
@use calloc() instead of malloc() and memset()

"look good" tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.19 2008/02/22 09:18:28 tedu Exp $ */
a586 1
again:
d589 1
a589 1
	if (lock->readers) {
d592 1
a592 1
		goto again;
@


1.19
log
@fix rwlocks to work with the "initialized" form, from Philip Guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.18 2007/06/05 18:11:49 kurt Exp $ */
d143 1
a143 1
	sem = malloc(sizeof(*sem));
a145 1
	memset(sem, 0, sizeof(*sem));
d225 1
a225 1
	mutex = malloc(sizeof(*mutex));
a227 1
	memset((void *)mutex, 0, sizeof(*mutex));
d326 1
a326 1
	attr = malloc(sizeof(*attr));
a328 1
	memset(attr, 0, sizeof(*attr));
d368 1
a368 1
	cond = malloc(sizeof(*cond));
a370 1
	memset(cond, 0, sizeof(*cond));
d461 1
a461 1
	attr = malloc(sizeof(*attr));
a463 1
	memset(attr, 0, sizeof(*attr));
d486 1
a486 1
	lock = malloc(sizeof(*lock));
a488 1
	memset(lock, 0, sizeof(*lock));
d655 1
a655 1
	attr = malloc(sizeof(*attr));
a657 1
	memset(attr, 0, sizeof(*attr));
@


1.18
log
@_FD_LOCK/UNLOCK() is libpthread specific and not needed for librthread, so
isolate its usage to libpthread only and replace with generic non-static
mutex support in the one place it is needed:
 - remove _FD_LOCK/UNLOCK from lseek and ftruncate in libc and make the
   functions weak so that libpthread can override with its own new
   versions that do the locking.
 - remove _thread_fd_lock/unlock() weak functions from libc and adjust
   libpthread for the change.
 - add generic _thread_mutex_lock/unlock/destroy() weak functions in libc
   to support non-static mutexes in libc and add libpthread and librthread
   implementations for them. libc can utilize non-static mutexes via the
   new _MUTEX_LOCK/UNLOCK/DESTROY() macros. Actually these new macros can
   support both static and non-static mutexes but currently only using
   them for non-static.
 - make opendir/closedir/readdir/readdir_r/seekdir/telldir() thread-safe
   for both thread libraries by using a non-static mutex in the struct
   _dirdesc (typedef DIR), utilizing it in the *dir functions and remove
   remaining and incorrect _FD_LOCK/UNLOCK() use in libc.
 - add comments to both thread libraries to indicate libc depends on the
   current implementation of static mutex initialization. suggested by
   marc@@
 - major bump libc and libpthread due to function removal, structure
   change and weak symbol conversions.
okay marc@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.17 2007/05/25 22:38:39 kurt Exp $ */
d160 1
a160 1
		write(2, MSG, sizeof(MSG));
d244 1
a244 1
		write(2, MSG, sizeof(MSG));
d505 6
d517 20
d541 4
d564 4
d586 4
d611 4
@


1.17
log
@protect against races while initializing static mutexes. okay marc@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.16 2006/01/05 04:06:48 marc Exp $ */
d260 6
@


1.16
log
@
add -Wstrict-prototypes -Wmissing-prototypes -Wsign-compare
Minor tweaks to compile with the above, primarily in fixing
the conflicts between semaphore.h and rthread.h
"i like the additional warnings" tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.15 2005/12/31 20:07:41 brad Exp $ */
d40 1
d256 1
a256 1
	pthread_mutex_t mutex = *mutexp;
d258 1
d260 7
a266 3
	if (!mutex) {
		pthread_mutex_init(mutexp, NULL);
		mutex = *mutexp;
d268 1
@


1.15
log
@add pthread_mutexattr_gettype.

ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.14 2005/12/30 04:05:55 tedu Exp $ */
d252 1
a252 1
int
@


1.14
log
@prototype all the thread syscalls in rthread.h for now.
update for new thrwakeup that takes a count argument
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.13 2005/12/29 11:35:54 otto Exp $ */
d338 8
@


1.13
log
@the story in pthread_rwlockattr_destory() shoudl be destroyed
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.12 2005/12/22 06:49:48 tedu Exp $ */
a40 3
int thrsleep(void *, int, void *);
int thrwakeup(void *);

d91 1
a91 1
		thrwakeup(sem);
d107 1
a107 1
		thrwakeup(sem);
d123 1
a123 1
	thrwakeup(sem);
@


1.12
log
@more consistently use _rthread prefix for all not meant to be exported
interfaces that aren't static, and a few that are but which will change
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.11 2005/12/19 06:47:40 tedu Exp $ */
d611 1
a611 1
pthread_rwlockattr_destory(pthread_rwlockattr_t *attrp)
@


1.11
log
@update copyright to 2005
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.10 2005/12/18 01:35:06 tedu Exp $ */
d256 1
a256 1
rthread_mutex_lock(pthread_mutex_t *mutexp, int trywait)
d284 1
a284 1
	return (rthread_mutex_lock(p, 0));
d290 1
a290 1
	return (rthread_mutex_lock(p, 1));
@


1.10
log
@initialize all spinlocks to _SPINLOCK_UNLOCKED
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.9 2005/12/14 07:02:47 tedu Exp $ */
d3 1
a3 1
 * Copyright (c) 2004 Ted Unangst <tedu@@openbsd.org>
@


1.9
log
@all is made clear: pthread_cond_timedwait takes absolute timeouts
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.8 2005/12/14 04:14:19 tedu Exp $ */
d232 1
d357 1
d477 2
@


1.8
log
@check for waiters when destroying a mutex or semaphore
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.7 2005/12/13 17:22:46 tedu Exp $ */
d378 3
d386 12
d400 1
a400 2
	rv = _sem_waitl(&(*condp)->sem, 0, (abstime ?
	    abstime->tv_sec * 1000 + abstime->tv_nsec / 1000000 : 0));
@


1.7
log
@make the mutex unlock and sleep in pthread_cond_wait properly atomic
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.6 2005/12/13 07:04:34 tedu Exp $ */
d158 8
a165 1
	/* should check for waiters */
d243 6
a248 1
	/* check for waiters */
@


1.6
log
@several silly bugs in pthread_cond_timedwait.
1.  the time was off by a factor of 10
2.  wouldn't return error code if timeout was reached
3.  the big one.  thrsleep syscall doesn't return EWOULDBLOCK.  it
returns -1 and puts the error in errno.  doh.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.5 2005/12/13 06:04:53 tedu Exp $ */
d50 8
a59 1
	_spinlock(&sem->lock);
d371 1
d373 1
a373 1
	rv = _sem_wait(&(*condp)->sem, 0, (abstime ?
@


1.5
log
@update thrsleep and thrwakeup - first arg changed from long to void *
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.4 2005/12/13 05:56:55 tedu Exp $ */
d67 2
a68 1
		if (thrsleep(sem, timo, &sem->lock) == EWOULDBLOCK)
d358 1
d365 2
a366 2
	_sem_wait(&(*condp)->sem, 0, (abstime ?
	    abstime->tv_sec * 100 + abstime->tv_nsec / 10000000 : 0));
d369 1
a369 1
	return (error);
@


1.4
log
@correct implementation of pthread_cond_signal.  it doesn't raise the sem
value if there are no waiters.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.3 2005/12/07 03:18:39 tedu Exp $ */
d41 2
a42 2
int thrsleep(long, int, void *);
int thrwakeup(long);
d67 1
a67 1
		if (thrsleep((long)sem, timo, &sem->lock) == EWOULDBLOCK)
d86 1
a86 1
		thrwakeup((long)sem);
d102 1
a102 1
		thrwakeup((long)sem);
d118 1
a118 1
	thrwakeup((long)sem);
@


1.3
log
@add the posix semaphore functions.  this lets vlc work.
ok brad
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.2 2005/12/06 06:19:31 tedu Exp $ */
d40 1
d77 17
a99 1
	sem->value++;
d101 1
d109 1
d174 1
a174 1
	_sem_wakeup(sem);
d283 1
a283 1
		_sem_wakeup((void *)&mutex->sem);
@


1.2
log
@add pthread_once.  unfortunately, the public pthread.h header
defines the pthread_once_t internals, so we're stuck with them.
@
text
@d1 1
a1 1
/*	$OpenBSD: rthread_sync.c,v 1.1 2005/12/03 18:16:19 tedu Exp $ */
d49 1
a49 1
	int sleep;
d59 1
a59 1
		sleep = 1;
d62 1
a62 1
		sleep = 0;
d65 1
a65 1
	if (sleep) {
d106 81
d560 3
@


1.1
log
@add userland thread library.  incomplete, but functional
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d475 13
@

