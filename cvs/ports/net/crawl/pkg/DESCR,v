head	1.3;
access;
symbols
	OPENBSD_6_2:1.3.0.56
	OPENBSD_6_2_BASE:1.3
	OPENBSD_6_1:1.3.0.54
	OPENBSD_6_1_BASE:1.3
	OPENBSD_6_0:1.3.0.52
	OPENBSD_6_0_BASE:1.3
	OPENBSD_5_9:1.3.0.48
	OPENBSD_5_9_BASE:1.3
	OPENBSD_5_8:1.3.0.50
	OPENBSD_5_8_BASE:1.3
	OPENBSD_5_7:1.3.0.46
	OPENBSD_5_7_BASE:1.3
	OPENBSD_5_6:1.3.0.44
	OPENBSD_5_6_BASE:1.3
	OPENBSD_5_5:1.3.0.42
	OPENBSD_5_5_BASE:1.3
	OPENBSD_5_4:1.3.0.40
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.38
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.3.0.36
	OPENBSD_5_2_BASE:1.3
	OPENBSD_5_1_BASE:1.3
	OPENBSD_5_1:1.3.0.34
	OPENBSD_5_0:1.3.0.32
	OPENBSD_5_0_BASE:1.3
	OPENBSD_4_9:1.3.0.30
	OPENBSD_4_9_BASE:1.3
	OPENBSD_4_8:1.3.0.28
	OPENBSD_4_8_BASE:1.3
	OPENBSD_4_7:1.3.0.26
	OPENBSD_4_7_BASE:1.3
	OPENBSD_4_6:1.3.0.24
	OPENBSD_4_6_BASE:1.3
	OPENBSD_4_5:1.3.0.22
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.3.0.20
	OPENBSD_4_4_BASE:1.3
	OPENBSD_4_3:1.3.0.18
	OPENBSD_4_3_BASE:1.3
	OPENBSD_4_2:1.3.0.16
	OPENBSD_4_2_BASE:1.3
	OPENBSD_4_1:1.3.0.14
	OPENBSD_4_1_BASE:1.3
	OPENBSD_4_0:1.3.0.12
	OPENBSD_4_0_BASE:1.3
	OPENBSD_3_9:1.3.0.10
	OPENBSD_3_9_BASE:1.3
	OPENBSD_3_8:1.3.0.8
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.6
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.3.0.4
	OPENBSD_3_6_BASE:1.3
	OPENBSD_3_5:1.3.0.2
	OPENBSD_3_5_BASE:1.3
	OPENBSD_3_4:1.2.0.4
	OPENBSD_3_4_BASE:1.2
	OPENBSD_3_3:1.2.0.2
	OPENBSD_3_3_BASE:1.2
	OPENBSD_3_2:1.1.1.1.0.6
	OPENBSD_3_2_BASE:1.1.1.1
	OPENBSD_3_1:1.1.1.1.0.4
	OPENBSD_3_1_BASE:1.1.1.1
	OPENBSD_3_0:1.1.1.1.0.2
	OPENBSD_3_0_BASE:1.1.1.1
	obecian_2001-Sep-09:1.1.1.1
	obecian:1.1.1;
locks; strict;
comment	@# @;


1.3
date	2003.09.25.06.25.13;	author jolan;	state Exp;
branches;
next	1.2;

1.2
date	2003.03.02.05.37.25;	author david;	state Exp;
branches;
next	1.1;

1.1
date	2001.09.09.21.57.12;	author obecian;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2001.09.09.21.57.12;	author obecian;	state Exp;
branches;
next	;


desc
@@


1.3
log
@drop maintainership on some stuff i don't use anymore, lop off
WWW: ${HOMEPAGE} while touching 'em
@
text
@The crawl utility starts a depth-first traversal of the web at the
specified URLs.  It stores all JPEG images that match the configured
constraints. Crawl is fairly fast and allows for graceful termination. 
After terminating crawl, it is possible to restart it at exactly the
same spot where it was terminated. Crawl keeps a persistent database
that allows multiple crawls without revisiting sites.

The main reason for writing crawl was the lack of simple open source
web crawlers. Crawl is only a few thousand lines of code and fairly
easy to debug and customize. 

Features

+ Saves encountered JPEG images 
+ Image selection based on regular expressions and size constraints 
+ Resume previous crawl after graceful termination 
+ Persistent database of visited URLs 
+ Very small and efficient code 
+ Supports robots.txt 
@


1.2
log
@fix more spelling errors/typos
ok pvalchev@@
@
text
@a19 2

WWW: ${HOMEPAGE}
@


1.1
log
@Initial revision
@
text
@d15 1
a15 1
+ Image selection based on regular expressions and size contraints 
@


1.1.1.1
log
@crawl-0.1b import - provos@@ ok

The crawl utility starts a depth-first traversal of the web at the
specified URLs.  It stores all JPEG images that match the configured
constraints. Crawl is fairly fast and allows for graceful termination. 
After terminating crawl, it is possible to restart it at exactly the
same spot where it was terminated. Crawl keeps a persistent database
that allows multiple crawls without revisiting sites.

The main reason for writing crawl was the lack of simple open source
web crawlers. Crawl is only a few thousand lines of code and fairly
easy to debug and customize. 

Features

+ Saves encountered JPEG images 
+ Image selection based on regular expressions and size contraints 
+ Resume previous crawl after graceful termination 
+ Persistent database of visited URLs 
+ Very small and efficient code 
+ Supports robots.txt 

@
text
@@
